Title: AI Avatars / Digital People | Tech HQ
URL: https://techhq.dc.lilly.com/docs/innovate/emerging_tech/ai-avatars
Description: - Last Update: 2025-08-05

AI Avatars / Digital People | Tech HQ
Skip to main content
Tech HQ
Innovate
Plan
Solution
Access
Learn
Contribute
Updates
LillyFlow
üí° Tech@Lilly Innovation Pipeline
üõû Our Innovation Stages
üèéÔ∏è Fast Start
üì° Emerging Tech
AI Avatars / Digital People
AI Speech
AI Video Generation
üì° Emerging Tech
AI Avatars / Digital People
On this page
AI Avatars / Digital People
Emerging Tech Report
Last Update: 2025-08-05
Tech Innovation Pipeline
, Enterprise Business Architecture, Tech@Lilly Enterprise
Author: Doug Gorr
HeyGen Examples
Executive Summary
German
Spanish
Hindi
Japanese
HeyGen Examples
Executive Summary
1 of 6
Executive Summary
‚Äã
AI Avatars
‚Äì the highly realistic, AI-generated digital humans ‚Äì are emerging as a transformative technology for corporate communications, training, marketing and more. Instead of filming live actors, organizations can now use AI to generate videos or interactive agents that look and talk like real people. These avatars can speak in dozens of languages, clone voices and likenesses, and even emulate human mannerisms and emotions. Major enterprises are already embracing this trend: for example, over 70% of Fortune 100 companies are using AI avatar platforms like Synthesia to create training and internal communications videos at scale. In pharma, Merck KGaA uses AI avatars for multilingual product training and updates, cutting the cost and time of producing content across regions.
Several categories of AI avatar solutions have matured: AI video generators (e.g. Synthesia, HeyGen, D-ID) enable anyone to turn a script into a polished video of a human presenter; meanwhile, interactive ‚Äúdigital people‚Äù (e.g. Soul Machines, UneeQ) allow real-time conversations with a lifelike virtual agent. These technologies rely on advanced AI models ‚Äì combining deep learning for speech synthesis, computer vision, and natural language processing ‚Äì to produce believable performances. Unlike traditional avatars (static cartoons or pre-recorded actors), AI avatars are dynamic and data-driven, capable of automatically lip-syncing to new dialogue, switching languages on the fly, and even responding to user input.
For Lilly, AI avatars present exciting opportunities in marketing, training, HR, and patient engagement. In marketing, personalized AI-driven videos can deliver consistent product messages to healthcare providers or consumers worldwide. In learning & development, training modules can be rapidly produced in multiple languages with a friendly virtual instructor, improving engagement and retention. HR and corporate communications teams can use avatars modeled from executives to broadcast updates in a relatable, cost-effective way. Early adopters report significant efficiency gains ‚Äì ServiceNow cut video production time by 50% for global learning programs using AI avatars, and companies like Mondelƒìz produced tens of thousands of videos in a year via AI avatars.
Key vendors in this space include Synthesia and HeyGen (leading AI video creation platforms), Soul Machines and UneeQ (interactive 3D digital humans), D-ID (AI avatar and dubbing technology), and tech giants like Microsoft and NVIDIA providing avatar generation services and tools. Their offerings already support features like cloning a person‚Äôs voice and face with consent, adding emotional expressions, 2D or full-body 3D avatars, and integration with chatbots or VR/AR environments. These capabilities will continue to expand. The technology has progressed remarkably over the past 3 years (fueled by advances in deepfake algorithms and generative AI), and recent developments point to even more realistic and controllable avatars on the horizon ‚Äì with more life-like gestures, interactivity, and integration into everyday workflows.
To capitalize on AI avatars, innovators should consider a phased adoption strategy (from Alpha experimentation to General Release) outlined by the company‚Äôs Tech Innovation Pipeline. Early on, focus on internal, low-risk use cases like training videos or a virtual assistant for HR FAQs. As the tech and internal comfort grows, scale up to customer-facing applications (with appropriate oversight and compliance checks, given pharma‚Äôs regulatory environment). Throughout, maintain a focus on ethical guidelines ‚Äì ensure avatars are used with consent, clearly identified as AI where appropriate, and content is medically accurate and approved. With right-sized governance, AI avatars can unlock cost-effective content at scale, enhance engagement (especially for video-preferring younger generations), and offer personalized support to both employees and customers. This report provides a deep dive into the technologies, top vendors, capabilities, use cases, and recommendations to help a global pharma company navigate the AI avatar landscape.
[Notice: While the research and companies included in this report are primarily focused on AI Avatars (a talking digital person visible on a screen), it is important to note that many of the same use cases, opportunities, risks, and outlooks apply to voice-only interactions.]
Introduction
‚Äã
AI Avatars are computer-generated characters that look and sounds like real people. They leverage AI-driven animation and voice synthesis to speak and gesture in realistic ways, often indistinguishable from a video of a human. Companies adopt AI avatars to streamline video content creation and create interactive virtual agents (aka Digital People, Digital Humans) without the cost and time of using human actors, studios, and multilingual voiceover teams. The goal is to move beyond text-based communication to more engaging, human-like experiences, which deepens user engagement knowledge retention.
Evolution (Last ~10 Years)
The concept of digital avatars isn‚Äôt new ‚Äì video game characters and virtual assistants have existed for decades ‚Äì but the last 2‚Äì5 years saw a revolution in realism thanks to AI. Early academic breakthroughs in deep learning-based face animation (often dubbed ‚Äúdeepfakes‚Äù) demonstrated that an AI model could make one person appear to speak words they never uttered. By the late 2010s, startups began harnessing this for business: Synthesia (founded in 2017) pioneered turning written text into videos of people speaking, and Soul Machines (founded in 2016) applied neural networks to create emotionally responsive 3D avatars. Initially, these technologies were experimental, but around 2020‚Äì2021, they hit an inflection point of quality and usability. The COVID-19 pandemic also accelerated demand for digital communication tools. By 2022, notable deployments like the World Health Organization‚Äôs ‚ÄúFlorence‚Äù digital health worker (built by Soul Machines) showed avatars being used to disseminate health guidance interactively. In 2023‚Äì2024, generative AI and large language models (like GPT-4) further boosted avatars‚Äô intelligence (for example, giving them conversational abilities) and big tech players entered the arena (Microsoft‚Äôs Azure AI introduced an avatar service, and NVIDIA launched its Avatar Cloud Engine). Today in 2025, AI avatars are increasingly mainstream in enterprise settings, with rapidly improving fidelity and capabilities. Major updates in the past year alone include full-body avatar animations, real-time interactive video experiences, and easier creation of custom avatars from a few photos or brief recordings.
How AI Avatars Work
‚Äã
At a high level, an AI avatar system combines several AI components:
Text/Dialogue Generation:
Many avatars start with a script or real-time generated text (which could come from a human-written script or an AI chatbot brain). This text is the content the avatar will speak.
Text-to-Speech (TTS):
The text is converted to spoken audio using advanced TTS models. Modern neural TTS can produce very natural, human-like speech, and can be trained to mimic specific voices.
Visual Rendering (Lip Sync & Animation):
The core unique step: the avatar‚Äôs face (and possibly body) must be animated to speak the audio convincingly. Different platforms use different techniques:
Some use 2D video-based models: e.g. Synthesia, HeyGen and D-ID leverage deep neural networks trained on real photos or video footage. These models take the generated speech audio (plus a base visage of the avatar) and predict corresponding lip movements and facial expressions frame-by-frame. Essentially, they ‚Äúpuppet‚Äù a human face to match the new audio. The underlying tech often extends from GANs or similar deep learning models that have learned how human lips move for each phoneme.
Others use 3D animated characters: e.g. Soul Machines, Convai, and UneeQ create a full 3D digital character (with a detailed face rig and body) and drive it with AI. Here, the system might use an animation engine (Unreal Engine, Unity, or proprietary) and AI to control the avatar‚Äôs facial muscles, eye gaze, and gestures in real time based on the speech and the context/emotion of the dialogue. These avatars can allow dynamic camera angles or be placed in AR/VR scenes because they exist as 3D models.
Some hybrid approaches use single-image animation: e.g. D-ID can take a single photo of a person and animate it talking, by using a pretrained model that infers the 2D facial movements from audio. This is a fast way to create a talking head from one picture (though with less realism than multi-angle video training).
Intelligence & Interactivity (optional):
The more exciting and advanced capabilities include emotional awareness and emotionally adaptive responses. These interactive avatar systems (aka Digital People, Digital Humans) take video and audio inputs, assess a user‚Äôs emotional context and respond like a human would. To make this possible, other AI capabilities, like sentiment analysis, need to observe, assess, and animate on the fly in order to drive the avatar‚Äôs emotional responses. For example, an avatar acting as a virtual concierge would include speech recognition to hear the user‚Äôs question, decipher tone and tenor, and adjust animation in real-time. This loop happens continuously so the avatar can converse fluidly. Leading frameworks like NVIDIA‚Äôs ACE provide pipeline components such as speech-to-text, NLP, and Audio2Face animation to support such real-time interactions.
Despite the complexity under the hood, many modern platforms abstract this away, offering a simple studio interface or API. Users might only need to select an avatar style, input a script (or integrate their chatbot), and the system handles the rest ‚Äì from generating a voice to producing a video or live avatar stream.
AI Training Techniques
‚Äã
Creating lifelike avatars requires training AI on large datasets of human speech and video. For instance, Synthesia has reportedly trained large video/audio foundation models on recordings of thousands of people to improve avatar realism. These models learn the nuances of facial movement, pronunciation, and emotional expression. D-ID‚Äôs newest avatar models can learn an individual‚Äôs specific mannerisms from just a short video: their ‚ÄúExpress Avatar‚Äù needs ~1 minute of footage to learn your head movements, while the premium model uses a few minutes of video to capture your face and even hand gestures for full-body shots. This is a significant reduction in training data compared to earlier years, thanks to more efficient AI architectures. Many systems also use pretrained models as starting points ‚Äì e.g. starting from a general face-animation model and fine-tuning it to a specific person. Voice models are similarly trained on vast speech datasets and then fine-tuned to mimic a target voice via transfer learning on a smaller voice sample. The result is that with surprisingly little input data, an AI avatar platform can clone a person‚Äôs likeness (with permission). Even TIME Magazine has noted that D-ID‚Äôs avatars can replicate a person‚Äôs voice, expressions and subtle mannerisms from only a brief video input ‚Äì a testament to the efficiency of modern training techniques.
Traditional Avatars vs AI Avatars
‚Äã
It‚Äôs useful to distinguish these new AI avatars from the ‚Äútraditional‚Äù avatars or digital characters of the past:
Traditional avatars (in corporate use) might include cartoon profile images, 3D game-like characters, or video footage of actors that is manually edited. They require human animators or video shoots to create or update content. For example, a classic e-learning module might have a pre-recorded video of an instructor or a manually animated cartoon character ‚Äì if the script changes, you must reshoot or re-animate.
AI avatars, by contrast, are generative. They can be instantly updated or completely created by feeding new text (for speech) or new data (to train a custom persona). There‚Äôs no need for a studio re-shoot to change what the avatar says or the language it speaks ‚Äì the AI system generates the new performance on the fly. Traditional avatars also tended to be less realistic (purposefully cartoonish to avoid the uncanny valley), whereas today‚Äôs AI avatars strive for photorealism or believable human-like presence.
Another key differentiator is interactivity: Traditional virtual agents (like earlier chatbots with a face) often had fixed, scripted interactions. Modern AI avatars can leverage AI models to respond flexibly to users, giving a far more natural conversational experience. For instance, Soul Machines‚Äô digital people are not pre-scripted ‚Äì they react to conversational context, even recognizing user emotions via camera. They ‚Äúsee, hear, react, and emote like a real human,‚Äù creating a natural experience. Such fluid behavior was not possible with older rule-based avatars.
Finally, scale and personalization: AI avatars enable automatically generating thousands of variant videos (say, one per customer with personalized greeting) ‚Äì something infeasible with manually made content. This opens new use cases like personalized marketing messages delivered by a lifelike spokesperson just for you. As cost and technologies continue to improve, we are fast approaching a world where everyone will have their own interactive avatars.
In summary, AI avatars represent the convergence of video production, animation, and AI. They allow digital content to reach people in a humanized form, but with the speed and scalability of the digital world.
Capabilities and Features of Modern AI Avatars
‚Äã
Today‚Äôs state-of-the-art AI avatar platforms boast a rich array of features. Below we outline key capabilities, along with their significance:
Lifelike Visuals (2D and 3D):
AI avatars can appear as talking heads in a 2D video frame or as full-body 3D characters. 2D avatars look like real humans filmed with a camera, typically from the chest up. 3D avatars have a virtual body and environment, allowing more camera angles and movement. Recently, platforms have begun expanding from static torso-up shots to full-body movement. In 2024 full-body avatars emerged, capable of a range of motions and gestures, enabling them to use hand movements and body language in videos. This adds realism and engagement, as body language is a huge part of human communication.
Voice Cloning and Multilingual Speech:
AI avatars can be given almost any voice. Many platforms provide a library of synthetic voices in numerous languages. Increasingly, they also offer custom voice cloning, where you can create a unique voice font (e.g. to use your own voice for your avatar). Synthesia and HeyGen both advertise voice cloning features, and Microsoft‚Äôs Azure AI specifically allows training a custom neural voice. This means an avatar could speak with the tone of a specific executive or brand ambassador (with consent). The multilingual abilities are also striking ‚Äì most leading avatars support dozens of languages. Synthesia supports 140+ languages for text-to-speech, and D-ID‚Äôs video translate tool can convert videos into over 120 languages with accurate cloned voices and lip-sync. For a global company, this feature is invaluable: one master video can be automatically localized for regional audiences, maintaining the same visual but speaking the local language. This vastly reduces the need for subtitles or separate video shoots per language.
Realistic Facial Expressions and Emotions:
Earlier AI avatars sometimes appeared emotionless or ‚Äúwooden.‚Äù Now, there‚Äôs a big emphasis on making them expressive and emotionally engaging. Providers use two approaches: (1) have the avatar detect sentiment in the script or conversation and adjust expression/tone accordingly, and (2) allow users to specify an emotion or speaking style. Synthesia‚Äôs new Expressive model (EXPRESS-1) enables avatars to understand context and sentiment in text and change their tone and facial expression appropriately. HeyGen likewise launched ‚ÄúAvatar 3.0‚Äù with emotion-aware animation, where the avatar‚Äôs movements reflect the emotional cues in the script (for example, smiling, frowning, or gesturing more dramatically for emphasis). This makes the avatars more engaging and human-like, rather than speaking in a flat monotone. It‚Äôs particularly useful in training or marketing content to convey enthusiasm, empathy, or seriousness as needed.
Personalized Avatar Creation (Custom Avatars):
While there are many stock avatars available (Synthesia alone offers 230+ diverse avatar personas), organizations often want their own people represented or a custom character aligning with their brand. Modern platforms enable this in a few ways:
Self-Service Avatar Creation:
Some solutions now let a user create a custom avatar by simply providing a few images or a short video of a person. Synthesia‚Äôs experimental ‚ÄúSelfie Avatars‚Äù feature allows creating a custom avatar from just a few images of yourself. D-ID‚Äôs ‚ÄúPhoto Avatar‚Äù similarly can animate an uploaded photo. HeyGen has taken it further, offering a pipeline to create ‚ÄúCommunity Avatars‚Äù and even generate avatar faces from text prompts (AI-generated faces). This means you can get a unique virtual presenter without hiring an actor ‚Äì either based on a real person‚Äôs likeness or an AI-designed face.
Custom Avatar with Your Own Video:
For higher fidelity, enterprise-oriented platforms allow training the avatar on a professionally recorded video of a person speaking. Microsoft‚Äôs Azure Avatar service, for instance, supports custom avatars by uploading your own video recordings, which it uses to train a neural avatar that closely resembles that person (especially if combined with a custom voice). D-ID‚Äôs Premium+ avatars require a few minutes of footage of the person but in return can even emulate their torso and hand movements for more dynamic shots.
The result of these innovations is that a company can clone an executive, instructor, or actor as an avatar (with their permission). That avatar can then deliver any message you need, saving that person from having to be on camera each time. It‚Äôs already used in practice ‚Äì many Synthesia customers have their own CEO or spokesperson avatar. This raises interesting questions (addressed later) about disclosure and authenticity, but technologically it‚Äôs a game-changer for scalable content creation.
Interactive and Conversational Abilities:
One of the most exciting capabilities turns avatars from one-way, pre-recorded video presenters into two-way, interactive agents. Interactive avatars can listen to users (via text or voice input) and respond in real time with speech and appropriate facial reactions. Soul Machines specializes in this ‚Äì their avatars are essentially AI assistants with a face, capable of free-form conversation. They even possess memory and computer vision: a Soul Machines avatar can recognize objects or people it sees through a camera and remember past interactions to personalize the dialogue. This enables use cases like virtual coaches or advisors. For example, Soul Machines reports that their AI wellness coach can watch a user practice a task (like a job interview answer) and give real-time feedback, noticing if the user appears nervous or if their attire is appropriate. On the more business side, HeyGen introduced Interactive Avatar characters that can serve as a ‚ÄúZoom assistant‚Äù or product expert ‚Äì effectively the avatar is tied to a chatbot/LLM that answers questions, while the avatar provides the face and voice for the AI‚Äôs answers. D-ID also has a chat API where you can hook an avatar up to systems like ChatGPT to create a conversational agent; their avatars were recognized by TIME as an innovative way to make chatbot interactions feel more human. This capability opens doors for virtual customer service reps, virtual patient guides, or interactive training simulations (more on use cases later).
Physics and Environment Integration:
In more advanced or 3D scenarios, avatars can be placed in virtual or real environments and respond to those contexts. For instance, in VR/AR, an avatar might need to match lighting and physics of the world so it appears believable next to real objects. Some platforms allow adjusting lighting on the avatar or use graphics engines to ensure the avatar casts shadows, etc., if composited into real scenes. While this is a niche feature mostly for AR/VR or high-end video production, it‚Äôs worth noting. NVIDIA‚Äôs toolkit, for example, provides ways to embed avatars in game engines so they have spatial awareness ‚Äì e.g. an NPC (non-player character) avatar can navigate around and pick up virtual objects. In enterprise settings, this could translate to AR training where a digital human guides a technician through a task, appearing anchored in the physical space.
Camera Movement and Shot Control:
Initially, most AI avatar videos were simple ‚Äúlocked-off‚Äù shots (avatar centered, direct-to-camera). Now, tools are adding more cinematographic control. Users can choose camera zoom levels, angles, or even have the avatar change position between cuts. HeyGen‚Äôs platform, for example, offers an ‚ÄúAI Studio‚Äù editor with the ability to add motion ‚Äì so you could start with a close-up and then switch to a wider shot with the avatar in a different background, all within one generated video. They tout features like the ability to change backgrounds, outfits, stances, and camera angles with the same avatar footage. This is achieved by their ‚ÄúUnlimited Looks‚Äù technology ‚Äì essentially re-synthesizing the avatar with different attire or environment without needing new video shoots. Such control allows more dynamic, multi-scene videos (akin to a normal video edit) entirely through AI synthesis.
Storyboarding and Workflow Tools:
To make avatar video creation accessible, many platforms include template libraries and storyboard tools. Users can script a sequence of scenes ‚Äì e.g. first scene: avatar A delivers intro, second scene: switch to a screen-share or slides, third scene: avatar B concludes. Tools like Synthesia and HeyGen provide pre-built templates for common video types (e.g. corporate training, marketing pitch) and allow inserting other stock media (images, charts, screen recordings) alongside the avatar‚Äôs footage. This workflow integration is crucial for enterprise use, letting a non-video-specialist create complete, polished videos. For example, Synthesia‚Äôs platform now includes an AI screen recorder (record your screen with voice, and it will generate an avatar presenting it) and one-click translation/localization of an entire video. D-ID recently introduced a Scenes feature to build video sequences with multiple avatars, backgrounds and dynamic text overlays for storytelling. These workflow features mean AI avatars are not a standalone novelty, but part of a broader video production solution that fits into corporate content pipelines (often with collaboration, review, and versioning support).
Content Moderation and Safety Features:
Given the potential misuse of hyper-realistic avatars (e.g. deepfake concerns), responsible vendors have built-in controls and moderation guardrails. Content moderation includes preventing the generation of disallowed content (hate speech, sexual or violent content, etc.) and ensuring avatars of real people are only made with consent. Synthesia, for instance, has stringent policies ‚Äì it does not allow public figures to be cloned and politically oriented content is restricted to enterprise accounts with disclosure. They even participated in a public red-team test of their moderation, showing resilience against generating deepfake abuse. Many platforms tag or watermark the AI videos in metadata as synthetic. Another aspect is ethical use of actors‚Äô likeness ‚Äì Synthesia has a talent program to compensate the real actors who have provided their faces for stock avatars, including a revenue share and early insight into how their avatar is used. These measures, while not ‚Äúfeatures‚Äù in the traditional sense, are important capabilities of a mature enterprise-ready avatar platform: to ensure safe, consensual and transparent use of the technology.
Cost Efficiency and Scale:
Though not a feature per se, it‚Äôs a key selling point ‚Äì AI avatars dramatically reduce costs of video production. Once a custom avatar is made, producing a new video is essentially generating data (no studios, cameras, travel, or on-camera talent needed). The cost is typically a software subscription or per-video fee. Many companies report saving significantly. For instance, what used to require a film crew and weeks of work can now be done by a single employee in a day with an AI platform. The marginal cost of additional videos is low, enabling large-scale video content strategies (such as personalized videos for thousands of recipients). Additionally, some vendors highlight energy efficiency ‚Äì Synthesia published that AI video generation can have a smaller carbon footprint than traditional video production, aligning with sustainability goals.
Integration and API Access:
Finally, modern avatar services offer APIs and integrations so that they can plug into other systems. For example, Synthesia and D-ID provide API endpoints where another software can programmatically generate videos. This could allow, say, a learning management system to automatically generate an avatar video for each new policy update, or a CRM to send a personalized avatar video to a customer when they sign up. For example, D-ID‚Äôs enterprise suite integrates with tools like Canva and PowerPoint for easy content import, and can connect with CRMs/marketing automation to insert avatar videos into campaigns. Microsoft‚Äôs Azure avatar service integrates naturally with Azure‚Äôs bot framework and other cognitive services (one can imagine an Azure bot with an avatar front-end in Teams). These integrations indicate the technology is maturing into an ecosystem component rather than a standalone novelty.
In summary, AI avatars today can clone voices and faces, express emotion, deliver content in 100+ languages, interact with users, and be flexibly customized ‚Äì all while offering enterprise-grade workflow integration and moderation. This wide feature set underpins their broad applicability across business functions.
Top Vendors and Key Players
‚Äã
The AI avatar landscape features a mix of specialized startups and large technology companies. Below we provide a balanced overview of major players, their offerings, and recent developments, with a focus on those relevant to Lilly use. In addition to these overviews and list of features, many additional factors must be considered prior to beginning a project with any AI Avatar vendor.
Contact your Enterprise Business Architect for the latest in vendor positioning and availability for Lilly use.
Synthesia
Widely regarded as a market leader in AI avatar videos, Synthesia is a U.K.-based company that provides a platform to create videos with photorealistic avatars from text. It offers a web studio with many stock avatars (diverse ethnicities, ages, professional looks) and the ability to create custom avatars. Synthesia has a strong enterprise focus ‚Äì boasting over 60,000 customers by 2023 and serving 70% of Fortune 100 firms. Use cases center on corporate training, how-to videos, and internal communications. What sets Synthesia apart is its creator platform with notable polish and reliability: it produces high-quality lip-sync, supports 130+ languages, and provides features like one-click video translation and an AI script assistant. Recent innovations from Synthesia include:
Synthesia 2.0 Platform: Launched in mid-2024, bringing full-body avatars with gestures and an interactive video player. The interactive video feature will let viewers click on elements in the video (e.g. a calendar pop-up to book a meeting or a quiz question). This blurs the line between passive video and e-learning app, all within an avatar-led video.
Expressive Avatars: A new ‚ÄúExpress‚Äù model that gives avatars context awareness to modulate tone. An avatar can sound upbeat or concerned depending on the content of the script, without the user manually specifying it ‚Äì making videos feel less robotic.
Personal Avatars and Selfie Avatars: Synthesia opened up easier custom avatar creation. Instead of requiring a studio filming session, users can record themselves with a normal webcam (with a natural background) to create a personal avatar. This lowers the barrier for organizations to let many employees create their own avatars for internal use. The avatars‚Äô lip sync and voice can then be auto-generated, including translation of the user‚Äôs own voice into other languages.
Scale and Governance: Synthesia has focused on responsible AI ‚Äì implementing rigorous content moderation (e.g., no public figure avatars, no political disinformation usage) and even achieving an ISO 42001 certification for AI trust and security (a first in the industry). They also partner with major tech (Adobe invested in them, and they use NVIDIA GPUs on Google Cloud for training). With a $180M Series D funding in 2023, Synthesia is well-capitalized to drive innovation.
For Lilly, Synthesia‚Äôs extensive enterprise adoption and emphasis on compliance are reassuring ‚Äì it‚Äôs a stable choice for large-scale deployment of avatar tech for training or comms.
HeyGen
HeyGen (formerly Movio) is another prominent AI video platform, often seen as a competitor to Synthesia with a blend of similar and unique features. It provides a diverse set of 500+ avatars (including not just photoreal actors but also some stylized ones) and supports 70+ languages. HeyGen‚Äôs recent Fall 2024 release introduced cutting-edge features:
Generative Avatars from Text: Users can create avatar ‚Äúlooks‚Äù from prompts ‚Äì essentially generating a new digital actor via AI. For example, one could type ‚Äúan older doctor with kind eyes‚Äù and get a unique avatar face to use. This is still experimental but showcases the ability to tailor avatar appearance to niche audiences or brand image without hiring an actor.
Emotion-Aware and Motion Improvements: HeyGen‚Äôs ‚ÄúAvatar 3.0‚Äù avatars respond to emotion cues in the script, adding expressive depth and natural motions to deliver the message convincingly. They also rolled out 50+ new filmed actors with 400+ outfit and background combinations to cover more scenarios (e.g., an avatar at a hospital vs. in an office). The ‚ÄúUnlimited Looks‚Äù feature allows a custom avatar to be shown in different clothing and settings by uploading additional reference footage, which is valuable for continuity (your avatar can change attire like a real person would on different days).
Interactive AI Agents: Notably, HeyGen has launched an Interactive Avatar feature ‚Äì these avatars can literally hold conversations, acting as support agents or even sitting in on video calls on your behalf. They listen to user queries and respond, presumably by integrating with an AI like GPT-4 for the brain. This positions HeyGen not just for video creation but also for real-time customer service or employee assistant use cases.
Video Translation and other tools: Like competitors, HeyGen offers video dubbing (translate an existing video into other languages with the same speaker‚Äôs avatar/voice) and a Chrome extension that turns a voice recording into an avatar video automatically.
HeyGen has been aggressive in feature development (shipping updates ‚Äúat lightning speed‚Äù), making it an attractive, often slightly more consumer-friendly option. It also tends to market flexible pricing and a free tier, which helped it gain popularity among individual creators and small businesses.
For a Lilly, HeyGen could be useful for quick-turnaround projects or creative marketing experiments, especially if the goal is to prototype capabilities like interactive rep avatars or generate a variety of spokesperson styles via prompt.
D-ID
Israeli startup, D-ID started with ‚Äútalking photo‚Äù technology and has grown into a platform for both asynchronous video creation and real-time conversational avatars. D-ID is known for its ease of turning a single image into a video of a person speaking, which gained fame through products like MyHeritage‚Äôs ‚ÄúDeep Nostalgia‚Äù (animating old photos). Key features of D-ID:
Creative Reality Studio: D-ID‚Äôs web studio allows users to create videos by uploading an image (or choosing a stock avatar) and entering text. It has a library of voices and supports numerous languages. A big draw is the ability to animate any face ‚Äì even historical figures or personal photos (with appropriate rights) ‚Äì into a speaking avatar.
High-Quality Avatar Modes: In late 2024, D-ID launched new Express and Premium+ avatar tiers. Express avatars train quickly (‚âà1 minute of video input needed) and capture basic head movement, suited for fast setup. Premium+ avatars require a bit more training footage (a few minutes) but produce much more lifelike results, including upper body and hand gestures. Premium+ avatars are also capable of real-time interactions, which D-ID suggests for use cases like live webinars or live language translation of a speaker. This essentially means a person‚Äôs digital double could speak live in many languages or engage an audience in Q&A.
Enterprise Marketing Suite: D-ID is positioning strongly for marketing use cases. They report that personalized video campaigns created with their avatars significantly boost engagement (30% higher click-through, 35% higher conversion in their early data). To support this, they introduced an array of tools ‚Äì interactive embedded video (avatar asks the viewer questions or responds to choices), integration with Canva/PPT for content design, and API hooks into CRM and marketing automation. This indicates D-ID‚Äôs focus on letting brands incorporate avatars into customer outreach (from outbound personalized ads to inbound customer service via ‚ÄúAI agents‚Äù).
Partnerships and Recognition: D-ID partnered with Microsoft in 2025 to bring ‚Äúagentic AI avatars‚Äù into Microsoft‚Äôs ecosystem. One application is expected integration with Microsoft Teams or Azure, meaning you might have D-ID powered avatars for live meetings or virtual assistants in enterprise workflows. Additionally, D-ID‚Äôs tech was named one of TIME‚Äôs best inventions of 2024, underscoring its cutting-edge nature. TIME highlighted how D-ID‚Äôs conversational avatars (with LLM brains) create a more engaging, face-to-face interface for AI.
Use in Healthcare/Pharma: D-ID has already seen use in healthcare training. A noteworthy example is the Southern Illinois University (SIU) School of Medicine, which used D-ID avatars as virtual patients to simulate clinical scenarios for students. These ‚ÄúAI patients‚Äù converse with students, allowing them to practice history-taking and communication in a realistic but controlled environment. This suggests a direct application to pharma in medical education and sales training (e.g., reps practicing detailing a drug with a lifelike doctor avatar as the audience, or trainees interviewing a virtual patient about side effects).
Overall, D-ID‚Äôs strength is its versatility (from simple photo animation to enterprise-grade interactive avatars) and its focus on making avatar tech accessible (they even have a mobile app). It might appeal to marketing teams for quick content and to innovation teams for building interactive AI agent prototypes. Ensuring proper licensing and consent for any real faces used would be a consideration, which D-ID addresses through its ethics policy and partnerships (for example, they partner with stock image libraries and have usage guidelines).
Soul Machines
In contrast to the above ‚Äúvideo generator‚Äù companies, Soul Machines is all about autonomous animated digital people. A New Zealand-based company co-founded by Oscar-winning animator Mark Sagar, Soul Machines builds fully 3D avatars that are driven by AI in real time, aiming for unprecedented realism and empathy in interactions. Key highlights:
Digital DNA and ‚ÄúBiological‚Äù AI: Soul Machines created a platform that combines neural networks with models of human biology (they call it ‚ÄúExperiential AI‚Äù). This means their avatars don‚Äôt just lip-sync to text; they have a sort of digital nervous system that simulates things like micro-expressions, eye movements, and emotional responses. They often refer to their avatars having a ‚ÄúDigital Brain‚Äù that can process inputs (vision, audio) and maintain an evolving emotional state.
Interactivity and Sensing: A hallmark is that Soul Machines avatars can see through the user‚Äôs webcam, hear through the microphone, and respond with appropriate facial reactions. For example, if the user smiles or frowns, the avatar may adapt its demeanor. They can also detect objects (as of their 2024 update) ‚Äì the avatar could recognize a pill bottle the user holds up, for instance, and incorporate that into the conversation. Memory features allow them to recall user details across sessions.
Use Cases and Industries: Soul Machines targets high-value interactions like coaching, customer experience, healthcare, etc. The WHO‚Äôs Florence 2.0 digital health worker is a flagship example ‚Äì Florence (an avatar of a female health advisor) can talk about COVID-19, mental health, nutrition, and more, and is being developed to speak multiple languages to serve global populations. Florence is envisioned to help fill gaps in healthcare education where human resources are scarce. In the corporate realm, Soul Machines has created digital twins of celebrities (e.g., a digital Will.i.am as a customer engagement avatar) and works with companies like HSBC, Nestle, and Daimler for customer service pilots.
For Lilly, one could imagine Soul Machines avatars as digital health coaches (guiding patients through medication management with empathy), or as virtual sales reps that can train or even interact with doctors in a human-like way.
Recent Advancements: As cited earlier, Soul Machines recently added object recognition and long-term memory to their avatars, making interactions more context-aware. They also launched a self-service ‚ÄúSoul Studio‚Äù where clients can design and deploy their own digital people relatively quickly ‚Äì lowering the barrier from a fully custom build (which used to involve Soul Machines‚Äô in-house team modeling an avatar) to a more configurable SaaS approach. Additionally, partnerships like the one with ServiceNow integrate their avatars into existing enterprise workflow systems (imagine an avatar that can not only talk to you but also create a support ticket or pull up your account info via backend integration).
Soul Machines‚Äô avatars are perhaps the most technologically sophisticated in simulating human interaction. However, they are also resource-intensive (typically requiring GPU cloud rendering) and currently less common than video-based avatars for things like simple training videos. They shine in live, interactive scenarios that benefit from emotional engagement.
Recent exits by all founding members and new executive team has shifted Soul Machine‚Äôs product strategy, focusing attention on subscription-based life coach avatars for the consumer market.
For Lilly, using Soul Machines might be part of a high-touch digital initiative ‚Äì for example, a virtual patient companion for a chronic disease program that checks in on patients, or an internal coaching avatar for employee wellness. The ROI would need to justify the complexity (as these are not as turnkey as, say, Synthesia videos), but the differentiation in user experience is significant. A patient might respond more to a compassionate digital human that listens and remembers, than to a static chatbot or a series of videos.
UneeQ
Another player in interactive digital humans, UneeQ (originating from New Zealand as well) offers a platform to create ‚Äúdigital companions‚Äù for customer service, sales, and training. UneeQ‚Äôs avatars are 3D animated characters similar to Soul Machines‚Äô, though UneeQ emphasizes a modular approach, integrating best-of-breed AI services for voice, conversation, etc. They have an authoring platform where clients can choose or design an avatar‚Äôs look, plug in an NLP brain (like IBM Watson or Microsoft Bot Framework), and deploy the avatar on websites, kiosks, or AR/VR devices.
UneeQ was behind some well-known demos, like the ‚ÄúDigital Einstein‚Äù avatar that one could chat with to learn facts or just experience conversing with a historical figure. In healthcare, UneeQ created ‚ÄúSophie,‚Äù a COVID-19 advice avatar in early 2020 that provided information to the public about the pandemic. They have case studies such as a digital human for a diabetes charity to answer questions, and a virtual shopping assistant for a retail brand.
UneeQ highlights its ability to deploy avatars across channels ‚Äì from web to even holographic displays for events. They also tout integrations: for instance, a digital human concierge that ties into hotel databases, or a training coach that connects to LMS content. Their Synapse (or Synanim) engine handles the coordination of speech, gestures, expressions, and remembering conversation context.
For enterprise usage, UneeQ has packaged solutions like a ‚ÄúSales Trainer‚Äù digital human for employee training role-plays, and customer-facing digital greeters. They might not have the massive customer count of Synthesia, but UneeQ is a strong option for bespoke interactive avatar projects. In a pharma setting, UneeQ could be leveraged to create, say, a virtual medical representative to handle common doctor questions on demand, or a digital human resources assistant to help employees navigate benefits (in a more engaging way than reading FAQs). The choice between Soul Machines and UneeQ often comes down to the style and complexity of interaction desired, business relationships, and potentially cost ‚Äì UneeQ markets a more scalable cloud platform which might be more accessible for mid-tier use.
Microsoft (Azure AI Services)
Microsoft entered the avatar arena by leveraging its strengths in speech and cloud integration. Azure‚Äôs Text-to-Speech Avatar service became publicly available in late 2023. This service allows Azure customers to generate photorealistic talking head videos from text, using either Microsoft‚Äôs pre-made avatars or a custom one:
Prebuilt avatars are provided by Microsoft out-of-box, each with a certain look, and they can speak any text in supported languages using Azure‚Äôs neural voices.
Custom avatars enable an organization to upload video of a person (and optionally create a custom voice) to train an avatar that looks and sounds like that person. Microsoft emphasizes that this is a Limited Access feature ‚Äì you must apply and describe your use case, to ensure it‚Äôs used ethically. They have guardrails in place as part of their Responsible AI commitment (e.g., presumably ensuring the person has consented if you clone them).
Microsoft‚Äôs entry is significant because of integration: these avatars can be tied into Azure‚Äôs AI ecosystem. For example, Lilly could use Azure OpenAI (GPT-4) to generate dynamic dialogue, Azure Speech to handle input/output, and the Avatar to present it. A cited scenario is a ‚Äúvirtual HR assistant‚Äù where an avatar answers employees‚Äô HR questions, or a ‚ÄúCEO‚Äôs digital twin presenting at a conference‚Äù in multiple languages. Being able to swap a live appearance with a generated one that still feels personal is an attractive use case (particularly in a world of virtual meetings).
Microsoft has also hinted at using avatars in Teams (their enterprise meeting app). Although currently Teams offers cartoon-like 3D avatars, the partnership with D-ID suggests they might bring more realistic avatars for certain meeting or training scenarios. Also, Azure‚Äôs avatar is available via API, meaning developers can embed it in custom apps (e.g., a telehealth portal could show a doctor avatar explaining how to take a medication).
For Lilly, already invested in Azure platform, the Microsoft AI Avatar service could be appealing. It would allow leveraging existing Azure security, scalability, and perhaps even pricing benefits under enterprise agreements. It‚Äôs relatively new and part of a recent startup acquisition, so it lacks many features (e.g., the studio interface is likely more utilitarian than any other). But Microsoft‚Äôs investment in both real-time bots and content generation with avatars is a strong sign this tech is considered important in the future of communication. One could imagine a near future where every corporate employee has a little AI avatar helper (instead of just a chat icon), powered by these services.
NVIDIA
Nvidia does not directly offer an end-user avatar app but provides the tools to build avatars, especially for gaming, simulation, and high-end interactive use. Their Omniverse Avatar Cloud Engine (ACE) is a toolkit of AI models and microservices for developers to create interactive avatars. This includes:
Audio2Face ‚Äì which generates facial animation from audio input.
Riva ‚Äì for speech-to-text and text-to-speech, supporting multiple languages.
NeMo / LLM services ‚Äì for conversational AI (NVIDIA offers customizable large language models that can run locally or in cloud).
By combining these, a developer can give an avatar the ability to listen, think (via an LLM), and speak with realistic lip-sync. NVIDIA demonstrated this at CES 2024 showing AI-powered NPCs in video games that have unscripted dialogues and react to player behavior. Game studios like Ubisoft and companies like Convai and Inworld (which create AI NPCs) are using ACE. Even enterprise avatar companies such as UneeQ are also listed as partners leveraging NVIDIA‚Äôs tech.
Why this matters to Lilly
: If we want to build something very custom ‚Äì say a proprietary virtual trainer with a unique look integrated deeply into a simulation or our own creator platform ‚Äì we may want to use NVIDIA‚Äôs stack under the hood. NVIDIA also pushes the envelope in innovation: for example, their research in eye contact simulation, voice modulation, and instant animation could filter into commercial products soon. They also ensure these capabilities run efficiently on GPUs, which is crucial for real-time avatars. Additionally, NVIDIA‚Äôs tech can be used on-device (for privacy) or via cloud.
As a basic building block for AI Avatar animation, NVIDIA could likely act as a major catalyst that continues to drive the entire AI Avatar market forward.
For Lilly, a ready-made solution from the above vendors will be sufficient. But it‚Äôs worth noting that NVIDIA‚Äôs ACE is enabling a new wave of AI Avatar startups offering immersive experiences (especially in AR/VR). If, for instance, Lilly innovators envision an advanced VR training lab where trainees talk to virtual patients, the developers building that could use ACE components to achieve the realism and interactivity required.
Convai
Convai is a company that focuses on creating AI-driven avatars, also known as digital humans, that can simulate human-like behavior, speech, and expressions in real-time. These avatars leverage advances in AI to move beyond pre-defined animations, enabling more natural and personalized interactions. Convai offers a Unity SDK to add conversational AI characters into websites and AR applications, making it relevant for training simulations. This technology allows for the creation of virtual patients or doctors in a simulation, like the SIU case but embedded in a 3D training environment.
Convai's avatars are dynamic and can hold conversations, answer questions, and even show context-appropriate emotions. They can be used in various applications, including marketing, learning and development, HR, training, and corporate communications. The key differences between traditional and AI avatars include automation of behavior, intelligence and interactivity, personalization, multilingual and scalable content, and real-time responsiveness.
Interestingly for Lilly
, Convai offers large customers the opportunity to self-host, which often reduces/eliminates many confidentiality and security risks typically found with third-party integrations.
Others and Emerging Players
A myriad startups are coming up in specific domains: e.g., some focus on avatars for education (a virtual tutor that looks like a friendly teacher), some on virtual influencers for social media, and so on. The ecosystem is vibrant, but the ones outlined above are the key players likely to be relevant to a large global company‚Äôs needs today.
Meta (Facebook) is researching ‚ÄúCodec Avatars‚Äù (ultra-realistic telepresence avatars) and deploying simpler avatars in Horizon Workrooms (though those are cartoonish). Apple is using a face-scanning approach for its Vision Pro ‚ÄúPersona‚Äù avatar (not AI-generated, but potentially Apple could incorporate AI for expressions or automated behaviors in future). Alibaba and other Chinese firms have their own avatar initiatives (for virtual influencers on e-commerce, etc.), but those are region-specific.
In summary, the field has both platform players (Synthesia, HeyGen, D-ID ‚Äì competing to be the go-to video avatar tool) and interactive avatar specialists (Soul Machines, UneeQ ‚Äì offering rich real-time digital humans). Additionally, big tech (Microsoft, Nvidia) provides core infrastructure that may either underpin those platforms or be used directly for custom solutions. When evaluating vendors, innovators should consider factors like: quality of lip-sync and visuals, language/voice support, ease of use, data privacy (where is the avatar generation happening ‚Äì on vendor cloud or Lilly-hosted), integration capabilities, and of course cost and licensing.
Secondary ‚ÄúWrapper‚Äù Vendors: Pattern & Pitfalls
‚Äã
Every time a major technology platform reaches the Peak of Inflated Expectations, a long tail of fast-moving service shops and startups springs up to ‚Äúwrap‚Äù the underlying APIs with a lighter UX or vertical pitch. Gartner flagged this behavior in the early cloud era, noting that cloud resellers would soon ‚Äútip over the peak and experience disillusionment‚Äù once enterprises realized they could contract AWS or Azure directly.
This same dynamic is repeating with AI Avatars, indicated by the following points:
High failure‚Äêto-scale rate. A Forbes analysis of 230 gen-AI pilots found ‚âà 90 % never progress beyond proof-of-concept; IDC places the attrition rate at 88 % in production settings
Commodity risk. TechCrunch notes that many ‚ÄúGPT-for-X‚Äù firms are thin wrappers ‚Äúcreating little more than UI glue around someone else‚Äôs model,‚Äù leaving them exposed the instant the platform owner launches a native feature
Platform consolidation. Analyst reporting shows large model providers ‚Äúaggressively launching native features that steam-roll wrapper startups‚Äù
Due-Diligence Checklist for Assessing Avatar Vendors
Disclosure of underlying stack: Demand the contract list which foundation model, TTS engine, and rendering pipeline generates the avatar.
Direct-license availability: Verify you cannot obtain the same capability straight from Synthesia, Hey Gen, D-ID, Microsoft Azure, or NVIDIA at enterprise terms.
Scalability proof: Request production metrics (QPS, concurrent sessions, regional fail-over) tied to their own infrastructure, not screenshots of the core platform‚Äôs SLA dashboard.
Security boundary review: Map OAuth scopes, data residency, and retention policies; extra hops often double the blast radius of a breach.
Exit & portability clause: Insert a 30-day transition provision and insist that all model artifacts, custom avatar assets, and scripts be exportable in a standard format.
Strategic Guidance for Business Stakeholders
Prioritize core-platform partnerships for production work (key players listed in this report).
Leverage boutique ‚Äúwrapper‚Äù vendors only for examples and time-boxed engagements when they provide domain expertise your teams lack, then seek to fill the team gaps. Ensure agreements include a pre-agreed sunset or hand-off plan.
Retain the vertical value layer internally. Internal functions (Medical Affairs, L&D, HR, etc.) should own scriptwriting, compliance, and standards; third parties should supply technology plumbing, not the industry narrative.
Maintain a consolidated vendor scorecard to monitor financial health, release cadence, and security posture; thin-wrapper firms often plateau once feature overlap grows with the underlying platform.
Overall, minimize exploration into intermediaries. In the generative AI ‚Äúgold-rush‚Äù, longevity and compliance will belong to vendors with control over the core AI models and infrastructure. Think of these pop-up offerings as riders in someone else‚Äôs car - useful for conversations and insights, hazardous for mission-critical scale.
Applications in Pharma (and Other Industries)
‚Äã
AI avatars have cross-industry applicability wherever communication, instruction, or personal interaction is involved. Below we explore use cases and examples, focusing on pharmaceuticals and healthcare, but also drawing from other industries to illustrate possibilities and spur thought.
Marketing and Sales Communication
Product Marketing Videos: Pharma companies can use AI avatars to create explainer videos for new drug launches, mechanism-of-action animations with an avatar narrator, or patient education videos about a disease. Traditionally, creating multilingual promotional videos or KOL (key opinion leader) testimonial videos is expensive. With avatars, one could, for example, generate a video of a ‚Äúvirtual doctor‚Äù explaining the benefits of a treatment in 10 different languages, all without a single physical shoot. Merck KGaA‚Äôs team already replaced many live-recorded product update videos with Synthesia avatars to rapidly localize content for internal sales teams. Similarly, marketing departments can A/B test different avatar personas (perhaps one avatar resonates better with cardiologists vs another with oncologists) by simply switching the digital actor ‚Äì something impractical with real actors.
Personalized Outreach: Imagine sending a personalized video message to HCPs where an avatar addresses the doctor by name and provides tailored info (e.g., ‚ÄúDr. Smith, here‚Äôs data relevant to your practice from our new clinical trial‚Äù). Companies like Vidyard or BombBomb have shown that personalized video in sales outreach increases engagement; AI avatars can deliver this personalization at scale without individual reps recording hundreds of videos.
Virtual Sales Reps (Interactive): In some cases, an avatar could act as a stand-in for a human sales rep, especially for ‚Äúno-see‚Äù accounts or during odd hours. For instance, a doctor could click a button to ‚ÄúChat with an expert now‚Äù and a friendly avatar appears (perhaps a digital twin of their medical science liaison), ready to answer questions via an AI backend honed on a particular drug product. This avatar could show slides or data on prompt and talk through them. While it won‚Äôt replace important face-to-face rep visits, it can provide on-demand support and product information. GSK reportedly piloted a virtual sales rep avatar for some product inquiries (hypothetical example to illustrate, as many companies are considering hybrid models). This is analogous to how some banks use digital avatar tellers in kiosks ‚Äì the pharma version would need to be closely governed (ensuring accurate, approved responses only).
Digital Brand Ambassadors: Outside of HCPs, avatars could appear in consumer-facing campaigns. For example, Lilly could create a virtual influencer avatar (perhaps a character representing a patient advocate) to raise awareness on social media about a condition or wellness tips, answering questions in comments using an AI brain. This has been tried in other industries ‚Äì e.g., cosmetic brands have virtual influencer avatars ‚Äì and though experimental, it can humanize the brand in a controlled way (since the company scripts or approves the avatar‚Äôs messaging).
Learning & Development (L&D) and Training
Employee Training Videos: Compliance training, safety protocols, new SOP rollouts ‚Äì all staples in pharma ‚Äì require frequent training refreshers across global teams. AI avatars excel here by producing engaging training videos quickly and in many languages. Instead of dense documents or slides, an avatar can walk employees through a new code-of-conduct policy with visual aids. Internal communications were among the first widespread uses of Synthesia at companies like SAP. The University College London even studied AI-generated video learning and found benefits for adult learners, indicating that well-designed avatar videos can be as effective as live video in learning contexts.
Sales Training and Role-Play: Pharmaceutical sales reps undergo extensive training for product knowledge and handling doctor interactions. Avatars can play the role of a ‚Äúvirtual doctor‚Äù for reps to practice on. For instance, UneeQ offers a sales trainer avatar that can simulate customer personalities and questions. A rep could speak (or type) to the avatar, and the AI doctor might interject with tough questions or objections in a realistic manner, helping the rep practice responses in a safe environment. Given the consistency of AI, every rep can be measured on the same scenarios, and the avatar can provide feedback or even scoring (especially if hooked to speech analytics that assess confidence or accuracy). This kind of training augmentation can complement human role-play sessions, and it‚Äôs available 24/7 for practice.
Clinical Training & Medical Education: AI avatars can be used in e-learning modules for clinicians. More interestingly, as seen with the SIU School of Medicine‚Äôs AI patient simulations, avatars can take on the role of patients with certain conditions for med students or HCPs to practice diagnostic interviewing. For example, an avatar could present as a patient with migraine ‚Äì describing symptoms, possibly even showing pain via facial expression ‚Äì and the learner has to ask the right questions. The avatar (driven by an LLM with a preset case profile) can answer in a natural, non-scripted way, making the scenario realistic. This can help HCPs build communication skills and empathy. Lilly could support such training as part of educational grants or internal training for medical staff.
Onboarding and HR Training: Welcoming new employees to Lilly can include lots of orientation videos. An avatar could serve as an onboarding buddy, introducing the company‚Äôs values, giving office virtual tours, etc., in a personalized way. And when policies update (like IT security protocols), an avatar can quickly be deployed in an explainer video that might be more watched than a long memo. Some companies have even used AI avatars to answer common HR questions for employees, freeing up HR staff from repetitive queries. Microsoft‚Äôs scenario of a ‚Äúvirtual HR assistant‚Äù avatar is relevant here. This can be particularly useful in a large company where employees might have questions about benefits, travel policy, etc., and prefer a quick, spoken answer over digging through an intranet site.
Human Resources and Corporate Communications
Leadership Messages: It‚Äôs a challenge for top executives to communicate frequently with a global workforce in their native languages. AI avatars allow creation of leadership video messages with minimal time from the executive. For instance, our CEO could type out or dictate a message about the company strategy, and have their own avatar deliver it in multiple languages to all employees. This ensures consistency of message and a personal touch (seeing Dave Ricks ‚Äúspeak‚Äù) without scheduling video shoots across time zones. Lilly must balance authenticity (some employees may prefer the real Dave Ricks despite language barriers) with efficiency, but this tool can dramatically increase the reach of leadership comms. We have already examples of townhalls that leverage a mix of real person/avatar presentations. Notably, Microsoft‚Äôs avatar service explicitly lists ‚ÄúCEO digital twin for a conference‚Äù as a use case.
Recruitment: In recruitment marketing, avatars can be the face of the company ‚Äì e.g., a friendly avatar on the careers page that tells prospective candidates about company culture and answers FAQs (‚ÄúWhat is it like to work in R&D at Lilly?‚Äù). This can be interactive or simply a video. It provides a novel, engaging way to consume info compared to text. Additionally, for initial candidate screening, some firms have experimented with avatar interviewers that pose questions and record answers (to avoid human interviewer bias in facial reactions). Though the ethics of AI in interviews are debated, using a consistent avatar interviewer could standardize the process for all candidates. At minimum, avatars might guide candidates through an application process or pre-interview prep (like a virtual recruiter giving tips).
Internal Communications: Avatars might also serve roles in employee resource groups or internal campaigns. For instance, a wellness campaign could feature an avatar ‚Äúcoach‚Äù that shares daily health tips or mindfulness exercises via short videos to employees. Because avatars can reflect diversity (you can choose avatars of different genders, ethnic backgrounds, and even create ones that represent persons with disabilities), they can be selected to resonate with certain audiences or to be inclusive in company messaging. Some companies have used AI avatars to provide sign-language interpretation or simplified explanations alongside main content, improving accessibility of communications.
Customer Service and Patient Engagement
Virtual Patient Assistants: AI avatars can humanize these digital support tools. For example, instead of a text FAQ, a patient could interact with a virtual nurse avatar available on a patient portal or via a mobile app. This service could explain how to inject a medication, remind patients about refill schedules, and answer common questions about managing side effects ‚Äì all in a comforting, visual manner. Because avatars can be available 24/7 and handle unlimited simultaneous sessions, they complement human-based call centers by handling routine inquiries. Importantly, an avatar can also escalate to a human or encourage contacting a doctor when needed. Alzheimer‚Äôs Foundation of America introduced an avatar named ‚ÄúAllison‚Äù to assist caregivers with information ‚Äì a sign that healthcare organizations see promise in avatar companions for support.
Clinical Trial Education: Recruiting and retaining patients in clinical trials is challenging. Avatars could help explain trial procedures to patients in a consistent way, improving understanding and trust. A trial might use an avatar to walk a participant through the informed consent document, ensuring they grasp key points (the avatar can even quiz them interactively to confirm understanding). During the trial, avatars could be part of digital follow-ups (‚ÄúHow are you feeling after week 2? Here‚Äôs what to expect next‚Ä¶‚Äù). Since trials often happen across different countries, using an avatar that speaks the patient‚Äôs language natively is a big plus.
Telehealth and Virtual Care: Healthcare providers might integrate avatars as front-end greeters or initial assessors in telehealth platforms. For minor or routine issues, an avatar could gather symptoms from a patient in conversational form (‚ÄúWhat brings you in today?‚Äù) and either provide basic guidance or prepare a summary for the human doctor. Some telehealth apps already use chatbots for triage; adding an avatar face could make the experience less sterile. In sensitive areas like mental health, there have been studies showing some patients feel more comfortable initially talking to a virtual ‚Äúperson‚Äù than a real one (no fear of judgment). An empathetic-looking avatar could encourage patients to open up and then later transition to human care when needed. Woebot (a mental health chatbot) and similar tools use cartoon avatars; it‚Äôs conceivable that with advances in emotional AI, more realistic avatars could play a role in digital therapeutics (though caution is needed to avoid uncanny valley effects that could backfire in emotional situations).
Other Industry Use Case (for reference)
Education: Schools and e-learning platforms are using avatars as tutors or lecture presenters (a professor can scale herself via an avatar to give basic lectures so she can focus on advanced interactive sessions in person).
Financial Services: Banks employ avatars in branches or apps for customer questions (e.g., ‚ÄúWhat‚Äôs the status of my loan?‚Äù answered by a smiling digital banker who can also visually show charts or forms).
Retail & Hospitality: Hotel chains have tested lobby avatar kiosks for concierge info, and e-commerce sites have virtual sales agents that help customers find products.
Entertainment: Virtual TV anchors, game streamers, and of course NPCs in games ‚Äì avatars are increasingly content creators or characters. There are even AI-generated pop music idols (virtual singers).
The common thread is scalability of human touch ‚Äì avatars bring a face and voice to AI, which can increase engagement across many domains.
For Lilly, the most immediate high-impact uses likely lie in the internal and HCP-facing realm (training, internal comms, HCP engagement) where regulatory risk is lower compared to direct patient interactions. However, as comfort and evidence build, patient-facing avatars could become part of patient support programs or disease awareness campaigns (with compliance and careful messaging oversight).
Opportunities and Benefits
‚Äã
Implementing AI avatars offers several compelling benefits outlined below.
Scalability and Efficiency:
As noted, avatars allow you to produce content or handle interactions at scale, without linear cost increases. A single avatar can generate personalized videos for 100 or 100,000 people with minimal extra effort. This is ideal for pharma, which often needs to reach global audiences in many languages. Instead of maintaining large teams of translators, video producers, trainers in each region, a central team can deploy avatar content globally. This lowers cost per video/interaction dramatically ‚Äì companies have seen cost reductions on the order of 10x for video production. In training, consistent avatar-based modules ensure everyone gets the same quality of instruction, avoiding variability of different trainers.
Engagement and Learning Impact:
Humans are wired to respond to faces and voices. Avatars, even if known to be AI, often garner more attention than text on a page. Early career professionals particularly prefer video content. So using avatars in internal communications can increase engagement rates ‚Äì more people watching a policy update instead of ignoring an email. In external use, video messages can break through information overload better than PDF attachments. Studies in education show video with a human presenter can improve understanding and retention, and when that presenter is relatable (e.g., speaks the viewer‚Äôs language or has a comforting demeanor), it‚Äôs even more effective. Thus, avatars can improve training outcomes (e.g., better compliance adherence because employees absorbed the training), and marketing outcomes (better message recall among doctors who saw an avatar video about a drug vs. just reading a brochure).
Personalization and Localization:
Avatars let you easily localize content ‚Äì not just language but potentially cultural style (you might pick different avatar personas for different regions if appropriate). Personalization, as discussed, can make communications feel tailored. For example, an avatar video addressing a patient by name and referencing their specific treatment plan could make them feel seen and cared for by the company, potentially improving adherence. In internal settings, onboarding could include an avatar that mentions the new hire‚Äôs name and department, creating a more personalized welcome than a generic video.
Consistency and Compliance:
This is a subtle but important benefit in regulated industries: when you use an AI system to deliver information, it will stick to the script. There‚Äôs less risk of an off-the-cuff remark or deviation that could be non-compliant. Once the medical/legal team approves the avatar‚Äôs script, guardrails, and/or behavior, you can deploy it widely confident that every viewer gets the approved message exactly. It‚Äôs easier to update content as well ‚Äì if a safety warning needs adding, you edit the script and regenerate; you don‚Äôt need to schedule new shoots with an actor. Avatars also inherently will use the exact phrasing required (e.g., including all the necessary disclaimers verbatim). Of course, one must ensure the AI doesn‚Äôt hallucinate or go off-script ‚Äì which is why most one-way avatar videos are tightly scripted, and interactive ones should be anchored to approved knowledge bases for sensitive info. But compared to human agents or trainers who might paraphrase or get questions wrong, avatars can provide consistent, controlled messaging.
Innovation and Brand Image:
Using AI avatars ‚Äì if done thoughtfully ‚Äì can position Lilly as an innovator. It signals that the company is tech-forward and exploring new ways to communicate. This can have an employer branding benefit (attracting talent excited about innovation) and even an investor/public relations angle (showcasing adoption of digital transformation). For example, if Lilly uses a virtual digital human for a digital-only product launch event, it may garner positive media coverage for creativity. However, this must be balanced with sensitivity to how the audience will perceive it (one doesn‚Äôt want to appear as though replacing human touch with ‚Äúbots‚Äù in patient care ‚Äì messaging has to emphasize augmentation, not replacement).
24/7 Availability and Speed:
Avatars, unlike people, don‚Äôt require sleep or schedules. A virtual medical info avatar on a website can answer a doctor‚Äôs question at 2 AM when no MSL is available. An internal IT support avatar could handle routine questions over the weekend. This instant response potential increases productivity (employees aren‚Äôt stuck waiting for answers) and customer satisfaction (HCPs or patients get help when they need it). Furthermore, creating a new avatar video can be done in hours, meaning communications can go out faster. For instance, when there‚Äôs an urgent update (like new drug approval news or a safety bulletin), an avatar video can be generated and disseminated company-wide or to stakeholders on the same day ‚Äì much faster than arranging a global conference call or subtitled video from an executive.
Cost Savings:
We touched on production cost savings. To quantify: making a 5-minute professional training video with live actors could cost tens of thousands of dollars (considering scriptwriting, filming crew, editing, talent fees, etc.), whereas with an avatar platform, it might just be the subscription cost or a few hundred dollars equivalent in usage. Over a year, if the company produces hundreds of videos, the savings are substantial. Additionally, travel costs can be saved if some training that used to be done via flying trainers around is replaced with avatar modules. Avatars can also reduce burden on human support staff by handling first-line queries (deflecting some volume away from call centers, which translates to cost savings or ability to redeploy staff to more complex tasks).
New Capabilities:
Avatars can do things humans can‚Äôt, in a way. They can appear simultaneously in many places, as mentioned. They can also have instant knowledge of enormous datasets (if connected to AI). For example, an avatar medical assistant could instantly recall data from thousands of clinical trials to answer a question, something a human would have to research. In interactive training, an avatar could adapt the scenario on the fly to the learner‚Äôs performance (thanks to AI), providing a level of adaptive training that one human role-play partner might not manage consistently. These new capabilities ‚Äì such as an avatar noticing that a learner is struggling and dynamically adjusting difficulty ‚Äì could improve efficacy of programs.
Challenges and Risks
‚Äã
Along with opportunities come challenges and risks. Implementing AI avatars is not without potential pitfalls. It‚Äôs crucial to be aware of these challenges and plan mitigation strategies.
Accuracy and Hallucination:
If an avatar is powered by AI (especially generative models like GPT) for live conversation, it might hallucinate incorrect answers or say something that is not desired. In pharma, an avatar giving wrong medical information or an unapproved off-label suggestion would be a serious issue. This risk is mitigated by keeping sensitive applications script-based or tightly controlling the AI‚Äôs knowledge domain. For instance, one would use a closed QA database for a medical FAQ, rather than a free-roaming internet model. Testing and validation of avatar responses is needed, similar to validating a chatbot. For one-way videos, the risk is low since content is pre-approved, but for interactive avatars, a robust content moderation and fail-safe mechanism is needed (e.g., the avatar should defer or escalate to human if unsure or asked something out of scope).
Uncanny Valley and User Acceptance:
While avatars are highly realistic now, they can sometimes fall into the ‚Äúuncanny valley‚Äù ‚Äì where they look almost human but something is off, causing discomfort. Early experiences with stiff or emotionless avatars can turn off users. Even when well done, some users might find it creepy to interact with a fake human, especially if they are not informed. Transparency is key: users should know they are interacting with an AI avatar, not a real person (e.g., an on-screen label or introduction). Over time, as people get accustomed (just as everyone got used to voice assistants), this concern may fade. But for now, change management is needed. Internally, explain why the company is using avatars (‚Äúto save time, to provide multilingual support, etc.‚Äù) to avoid rumors of ‚Äúrobots replacing us‚Äù or pushback that it‚Äôs impersonal. Externally, gauge audience reactions through pilots ‚Äì maybe HCPs love the convenience of an avatar rep, or maybe they feel it‚Äôs too impersonal compared to a human rep. Adjust strategy accordingly. The design of the avatar also matters; some companies deliberately choose a more obviously animated style to avoid uncanny valley, while others go for full realism. Both pre-recorded avatar videos and interactive Digital People can be considered regulated content, so be sure to follow all existing policies and procedures for content, especially externally-facing content.
Consent and Likeness Rights:
If you create an avatar of a real person (e.g., an employee or actor), you must have their explicit consent and probably a contract outlining usage. There have been cases of actors worrying about losing control of their digital likeness. Synthesia addressed this by compensating actors and requiring consent for any custom avatars. Lilly should ensure any patient or HCP figures used in avatar form have signed off. Using a popular public figure as an avatar without permission is ethically wrong and may be illegal (and most platforms prevent it).
Deepfake Misuse:
The same tech that powers avatars can be misused for disinformation. While our company would never intend that, being associated with ‚Äúdeepfake tech‚Äù can draw scrutiny. It‚Äôs important to position the use as ‚Äúsynthetic media for positive and authorized purposes‚Äù. Following industry ethics guidelines (there are emerging frameworks for synthetic media transparency) is wise. For example, including an unobtrusive watermark or a statement at the end like ‚ÄúThis video was generated using AI‚Äù can be considered for transparency, especially in external comms.
Data Privacy:
Interactive avatars will collect data (e.g., video of users if the avatar ‚Äúsees‚Äù them, conversation logs, etc.). All this needs to be handled under privacy laws (HIPAA if any health info, GDPR in EU, etc.). Data should be secured and not used beyond the stated purpose. If patients talk to an avatar about their condition, that‚Äôs sensitive personal health information requiring proper safeguards.
Bias and Inclusion:
AI models might carry biases (e.g., an avatar‚Äôs speech recognition might struggle with certain accents, or its responses might inadvertently be skewed). It‚Äôs important to test with diverse user groups. Also, offering a variety of avatar ethnicities and genders for user choice can be a way to be inclusive (some research in customer service bots suggests people prefer an avatar they identify with). However, one must avoid stereotypes (e.g., not always making the nurse avatar female or the executive avatar male ‚Äì break stereotypical representations).
Employee concerns:
In some cases, new technology can cause distraction and concern. While the intent of a use case may be to augment existing staff, always consider proactively communicating how roles will evolve.
Quality and Technical Limitations:
Although improving rapidly, avatars can still have occasional glitches ‚Äì maybe a pronunciation error for a new drug name, or slightly mismatched lip sync on a difficult word. Visual quality might not perfectly match a professional studio video (especially if internet bandwidth is an issue for streaming interactive avatars). In cases where top-notch polish is required (e.g., a broadcast TV commercial), current avatar tech might not fully replace human footage (though it‚Äôs getting close). Also, generating very long videos can be time-consuming or costly on some platforms. There might be limitations like avatars not being able to show highly dynamic movements (running, complex interactions with objects) ‚Äì they are mostly standing or sitting and gesturing. For most corporate needs these are fine, but we must set expectations that avatars have a ‚Äúpresentation style‚Äù which might be somewhat stiff if overused. Keeping avatar videos concise and to the point helps. On the technical side, integration into legacy systems can be a challenge ‚Äì for example, making an avatar appear within an existing mobile app might require custom dev work or using an SDK, which may require custom development support and possibly waiting on vendor capabilities (e.g., an offline mode if internet is down, etc.). Ensuring you have support and possibly fallback options (text or human chat) is prudent when deploying new tech.
Infrastructure and Cost Considerations:
While individual video generation is cheap, every technology has some runtime costs. High-quality avatars, especially interactive ones, often require GPU services. If using a cloud API extensively, those costs can add up (though likely still less than human labor equivalents). There‚Äôs also the question of whether to centralize avatar generation or allow many users to do it ‚Äì a large company might need governance to avoid a patchwork of disjointed avatar experiences. License costs for enterprise plans (Synthesia, etc.) should be factored. Also, network infrastructure: streaming avatars (like a live conversation) means video data streaming; Tech@Lilly will need to ensure bandwidth and latency are acceptable, especially for global usage. If deploying via Azure or another cloud, ensure compliance with where data is processed (some may require regional data centers, etc., particularly for health data).
Regulatory and Public Perception:
In pharma, any promotional communication is potentially subject to regulatory scrutiny (FDA, EMA, etc.). It‚Äôs uncharted territory how regulators will view AI-generated spokespeople. The content itself still must meet all regulations (fair balance, no off-label, etc.), which is straightforward if we control the script. But one might consider informing regulators or adding disclaimers that ‚ÄúThis is a fictional animated presenter‚Äù in consumer content to avoid any concern of deception. Another angle: if an avatar answers questions live, ensuring it doesn‚Äôt inadvertently create an ‚Äúadverse event report‚Äù that gets missed (e.g., if a patient tells the avatar about a side effect, does that need to be reported?) ‚Äì processes must be in place as they would for any chatbot intake. On public perception, Lilly must avoid missteps like using an avatar to represent a patient without disclosure. A recent example outside pharma: a state media outlet used Synthesia avatars to deliver political propaganda, which caused controversy. Synthesia has since tightened policies. The lesson: always be transparent and truthful in usage. Use avatars to supplement human communication, not to masquerade deceptively. If a commercial, for example, shows an avatar as a ‚Äúdoctor‚Äù testimonial, consider indicating that it‚Äôs a virtual depiction unless one explicitly wants to simulate a fictional scenario (like an actor would in a dramatization).
Many of these risks can be managed with proper governance, testing, and a human-in-the-loop approach. For instance, keep humans supervising avatar interactions in early deployments (an avatar could flag a live agent to step in for complex queries). Maintain logs of avatar outputs for auditing (especially in medical info contexts). And engage ethicists or compliance early in the project design.
Adoption Strategies
‚Äã
To successfully adopt AI avatars or Digital People technology, a phased and strategic approach is advisable. We propose aligning with the Tech@Lilly Innovation Pipeline framework ‚Äì moving from early innovation stages like Alpha and Beta, to small-scale real-world deployment, before scaling with a general release. This innovation process is designed to be both rapid and controlled.
Given the breadth of possibilities, it‚Äôs wise to prioritize use cases based on risk and business value. Throughout these stages, cross-functional collaboration is important. Involve Tech@Lilly, Quality, Legal, and subject matter experts in the function benefiting from the technology. Also, get feedback from end-users as early as possible.
Finally, stay aware of emerging trends and rapid improvement as the technology continues to mature. Examples:
Convergence with AR/VR (AR glasses could project likenesses of digital coaches in front of an employee during some hands on learning of a practice or procedure).
Improvements in AI agents (avatars becoming more autonomous in performing tasks, not just talking).
Being prepared to integrate those when ready will keep the company at the forefront of digital innovation in the industry.
Future Outlook
‚Äã
Digital People and pre-recorded Avatar video technology is advancing rapidly. Soon it will become nearly impossible to determine if the person you see on a screen is real or synthetic.
Looking ahead 2‚Äì5 years, we can expect:
Near-Photorealism and Beyond: Avatars will become indistinguishable from real humans on camera. Companies like Synthesia are already investing in next-gen models (e.g., their partnership with Shutterstock to train **‚ÄúEXPRESS-2‚Äù models for a new generation of avatars). This will improve subtle details like hair, hand movements, clothing physics, and reducing any visual artifacts. Full-body avatars will progress to walking and interacting naturally with props or environments. For corporate use, this simply means higher quality content that viewers will find credible and engaging.
Real-Time Multilingual Conversations: We will likely see avatars that can instantly translate speech in one language to another while preserving the speaker‚Äôs voice and facial expressions. Early versions exist (e.g., NVIDIA demoed real-time translation of a talking video from English to Spanish with correct lip movements). In a few years, it will likely be common for live speakers to be ‚Äúavatarized‚Äù in real time to deliver the message in all languages simultaneously on screen. This could be huge for global events or multilingual training ‚Äì a single presenter can effectively speak everyone‚Äôs language through their avatar. Microsoft‚Äôs research in speech-to-speech translation and startups like HeyGen working on live dubbing indicate this is close.
Greater Autonomy and Agency: Future avatars will not just be reactive, but proactive agents. With more integration to enterprise systems and more sophisticated AI agents, avatars could perform tasks. For example, a virtual medical assistant avatar might notice a patient‚Äôs refill is due and proactively come up (via a phone app) to remind them and offer to place an order or schedule a pharmacy delivery, completing the action within the conversation. Digital People, driven by AI agent technology, could coordinate with each other too ‚Äì one can imagine a team of digital agents handling complex workflows (one talks to the user, another fetches data in background, etc.). In games and simulations, multiple AI NPC avatars may converse among themselves, making scenarios more realistic (Convai demonstrated NPCs discussing and collaborating in a game world). In a business, maybe two avatar advisors (one representing finance and one legal) could jointly brief a user on a matter, each with expertise.
Personalized Appearance via Generative AI: We will see more user control to create custom avatar appearances with simple inputs. In the future, it may be common practice for each employee to have their own digital clone, which may attend meetings, delivery presentations, or even provide another employee career advice. Also, ‚ÄúPrompt-based‚Äù avatar generation will improve, meaning you can generate a completely unique digital spokesperson that matches your brand persona in seconds (no actors needed at all). These generative avatars will then be animated by the usual pipelines. We are already seeing companies like HeyGen offer ‚Äòcross-modal cloning‚Äô features ‚Äì e.g., creating an avatar of a person using only a sample voice recording and a photo. This could allow quick creation of avatars of real staff with minimal input (but will need careful consent workflows). It could also enable dynamic avatars that change appearance contextually (imagine an avatar that visually appears tired or wearing winter clothes if talking about winter health, etc., done automatically).
Blending Virtual and Physical (XR): Avatars will increasingly step off the 2D screen. In AR (augmented reality), you might have an avatar appear in your room via glasses or phone, guiding you through a procedure (like assembling a device or injecting a medicine). In VR, training programs will use AI characters as role-players in fully immersive environments (for example, practicing an emergency response in a virtual hospital with AI avatar patients and colleagues). Pharma companies might, in future, use AR avatar reps ‚Äì instead of a Zoom call, a doctor wearing AR glasses could see a life-size avatar of the pharma rep in their office virtually for a discussion. Companies like Meta are heavily researching ultra-realistic avatars for VR telepresence; by the time devices like Apple‚Äôs Vision Pro become common, having your ‚Äúdigital twin‚Äù join a meeting might be normalized. It will be important to ensure these avatars maintain the same compliance messaging in those mediums as well (which is another reason to have them AI-driven rather than entirely controlled by users).
Industry Regulations and Standards: We can anticipate that governments and industry bodies will issue guidelines or even regulations on AI-generated media. This might include requirements to disclose AI content in certain contexts (especially in political or public communications). Advertising standards might adapt ‚Äì e.g., requiring a disclaimer if a ‚Äúperson‚Äù endorsing a product is not real. The pharma industry might create best practices specifically for avatar usage in patient interactions to ensure trust (perhaps an adaptation of existing chatbot guidelines). Being ahead of this by self-imposing transparency will be wise. On the flip side, as it becomes mainstream, people will also become more accepting of AI avatars, just as chatbots became ubiquitous. We may also see certifications or seals (maybe an avatar platform gets certified for healthcare information delivery if it meets certain accuracy standards).
Convergence with Voice and Robotics: While this report focuses on Digital People, note the synergy with voice assistants and even physical robots. The same AI driving a Digital Person could drive a humanoid robot or a customer service animatronic kiosk.
Increased Use in Healthcare Delivery: If we look at the WHO‚Äôs Florence in 2022 as an early example, by 2030 we might commonly have ‚Äúdigital health workers‚Äù supplementing human healthcare, endorsed by major organizations. This could help address personnel shortages by handling routine interactions. Pharma companies may eventually partner in creating validated avatars that can guide patients on proper use of medicines or lifestyle changes. It‚Äôs not far-fetched that clinical guidelines or patient education materials might come with an official Digital Person that any clinic can use to educate patients in a standardized way.
In preparing for the future, Lilly should remain agile in adopting updates. It might also consider shaping the future ‚Äì e.g., participating in industry consortia on AI avatars, or even developing bespoke in-house capabilities for strategic advantage, if warranted.
Conclusion
‚Äã
Digital People and AI avatars are rapidly transitioning from novelty to a practical tool across enterprise functions. For Lilly, they offer a new medium to communicate scientific information, engage with stakeholders, and train and support the workforce more effectively. By starting with targeted use cases and scaling carefully with proper oversight, we can reap the efficiency and engagement benefits while managing risks. This technology will only get more powerful and more prevalent; as industry leaders and early adopters, we will have the advantage of experience and internal capability when avatars become a standard interface in business.
Sources
‚Äã
ainvest.com
ainvest.com/news/synthesia-ai-avatars-drive-50-efficiency-gains-fortune-100-companies-2506
businesswire.com
businesswire.com/news/home/20240927126293/en/Soul-Machines-Expands-AI-Assistant-Capabilities-With-New-Advanced-Object-Recognition-and-Memory-Features-Powered-by-Its-Patented-Experiential-AI-Technology
cutter.com/article/vr-applications-healthcare-and-medicine-digital-humans
d-id.com/news
developer.nvidia.com/blog/spotlight-uneeq-revolutionizes-customer-engagement-with-ai-powered-digital-humans
digitalhumans.com
digitalhumans.com/features/integrations-and-deployment
digitalhumans.com/features/synanim
digitalhumans.com/platform
forbes.com/sites/gilpress/2025/03/05/microsoft-transforms-communications-with-agentic-ai-avatars-from-d-id
heygen.com
heygen.com/news/latest-drops-fall-2024
intelligenthq.com
intelligenthq.com/time-recognises-d-ids-ai-avatars-as-best-innovation-of-2024
nvidianews.nvidia.com
nvidianews.nvidia.com/news/ace-avatar-cloud-engine-microservices
pharmaphorum.com
pharmaphorum.com/news/meet-florence-whos-ai-powered-digital-health-worker
soulmachines.com
springfieldherald.news
synthesia.io/blog/category/synthesia-news
synthesia.io/case-studies
techcommunity.microsoft.com
techcommunity.microsoft.com/blog/azure-ai-services-blog/azure-ai-speech-announces-public-preview-of-text-to-speech-avatar/3981448
techcrunch.com/2024/10/31/d-id-launches-new-high-quality-avatars-capable-of-real-time-conversations
venturebeat.com/ai/synthesia-announces-platform-update-with-interactive-ai-videos-full-body-avatars
Was this helpful?
Tags:
innovation
Edit this page
Previous
üì° Emerging Tech
Next
AI Speech
Executive Summary
Introduction
How AI Avatars Work
AI Training Techniques
Traditional Avatars vs AI Avatars
Capabilities and Features of Modern AI Avatars
Top Vendors and Key Players
Secondary ‚ÄúWrapper‚Äù Vendors: Pattern & Pitfalls
Applications in Pharma (and Other Industries)
Opportunities and Benefits
Challenges and Risks
Adoption Strategies
Future Outlook
Conclusion
Sources
Community
EBA Viva Engage
EBA SharePoint
Questions?
Reach us on Viva Engage!
Copyright ¬© 2026 Eli Lilly and Company