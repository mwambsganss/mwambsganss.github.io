Title: AI Speech | Tech HQ
URL: https://techhq.dc.lilly.com/docs/innovate/emerging_tech/ai_speech
Description: AI Speech

AI Speech | Tech HQ
Skip to main content
Tech HQ
Innovate
Plan
Solution
Access
Learn
Contribute
Updates
LillyFlow
ðŸ’¡ Tech@Lilly Innovation Pipeline
ðŸ›ž Our Innovation Stages
ðŸŽï¸ Fast Start
ðŸ“¡ Emerging Tech
AI Avatars / Digital People
AI Speech
AI Video Generation
ðŸ“¡ Emerging Tech
AI Speech
On this page
AI Speech
Technologies for Pharmaceutical Operations: A 12-Month Strategic Assessment for Eli Lilly and Company
Emerging Tech Report
Last Update: 2025-11-14
Tech Innovation Pipeline
, Enterprise Business Architecture, Tech@Lilly Enterprise
Author: Doug Gorr
Note
The rapidly evolving nature of AI speech technologies means that capabilities, vendors, pricing, and regulatory frameworks described may change significantly. This report represents a point-in-time assessment based on available information through November 2025.
INTERNAL USE ONLY - CONTAINS AI GENERATED CONTENT
This containts AI generated content and includes insights from over 1,000 publicly available sources including peer-reviewed publications, regulatory documents, industry reports, vendor specifications, and market analyses. The content represents AI-assisted compilation and interpretation of publicly accessible information current as of November 2025. This should not be distributed, reproduced, or shared outside the organization without explicit authorization from the author and appropriate leadership. The insights, analyses, and recommendations contained herein are intended to inform internal strategic discussions and should not be construed as official Eli Lilly positions or commitments.
Executive Summary
â€‹
AI Speech has reached 'pharmaceutical-grade' maturity.
AI speech represents far reaching opportunities, offering Eli Lilly solutions for voice-enabled documentation, clinical voice biomarkers, and conversational AI across the enterprise. The FDA issued groundbreaking guidance in January 2025 establishing risk-based frameworks for AI in drug development, while voice biomarker markets are projected to reach $5.4 billion by 2035. This creates an immediate 12-month window for strategic positioning before widespread industry adoption accelerates.
Why Now
Three converging forces make 2025 the inflection point for pharmaceutical voice AI adoption. First, transformer-based speech models (OpenAI Whisper, Microsoft VALL-E 2) achieved human parity performance in 2024 with 95%+ medical terminology accuracy. Second, regulatory clarity emerged with FDA's January 2025 credibility assessment framework and EMA's September 2024 final guidance on AI in the medicinal product lifecycle. Third, proven ROI metrics demonstrate 40-70% time savings in clinical documentation, 50% reduction in adverse event processing time, and 200-12,000% first-year returns across implementations.
The Strategic Imperative
Takeda Pharmaceuticals is concidered a leader in AI Speech amoungst competitors with a dedicated MIT voice biomarker program for neurological diseases, while Pfizer, GSK, and Sanofi deploy conversational AI for medical information services. Eli Lilly could leapfrog competitors by strategically deploying voice AI across high-value use cases: voice-enabled laboratory notebooks integrating with LIMS (16% data capture increase documented), hands-free GMP manufacturing documentation (99.86% accuracy with IoT integration), voice biomarkers for clinical trial endpoints in neuropsychiatry (80% detection accuracy for depression from 20 seconds of speech), and AI-powered medical information services (85% call deflection rates). The pharmaceutical voice AI market exhibits classic first-mover advantages with 85% of generative AI spend flowing to innovative startups over incumbents, suggesting rapid consolidation ahead.
Risks and Mitigations
Voice data constitutes Protected Health Information under HIPAA and special category data under GDPR Article 9, requiring explicit consent frameworks and privacy-by-design architectures. The FDA's August 2024 warning letter for an AI dosing algorithm that caused 10x pediatric overdose underscores the necessity for human-in-the-loop validation. Voice recognition algorithms trained on non-diverse datasets risk bias across accents and demographics, with 94% of depression voice studies having fewer than 100 participants. Mitigation strategies include federated learning architectures preserving data sovereignty (99% of centralized model quality while keeping data local), differential privacy implementations, comprehensive GxP validation protocols aligned with 21 CFR Part 11, and pilot programs with clearly defined success criteria before enterprise scaling.
Recommendation for Lilly
Leverage the Tech Innovation Pipeline framework for advanced speech technologies with structured Alpha, Beta, and Early Release phases to systematically prove value before enterprise commitment. Begin with exploratory Alpha experiments across diverse use cases: voice-enabled laboratory documentation systems, ambient clinical intelligence for field medical teams, digital voice biomarkers as exploratory endpoints, and conversational AI for patient engagement. Progress promising technologies through Beta pilots with defined success metrics and risk mitigation protocols, then advance validated solutions to Early Release with controlled user groups before considering enterprise scaling. Leverage a cross-functional review structure spanning Quality, Regulatory Affairs, Legal, Enterprise Business Architecture, and Medical Affairs to oversee the pipeline progression. Allow low-risk initiatives to progress quickly through the stages of innovation, acting as pathfinders for more strategic, higher-risk applications to follow. Allocate innovation funding with stage-gate investment decisions tied to demonstrated value and regulatory feasibility.
What is AI speech technology
â€‹
AI speech technology enables computers to understand human speech and generate human-like voices through machine learning, fundamentally transforming how pharmaceutical companies capture data, engage patients, and conduct research. At its core, the technology works bidirectionally: Speech-to-Text (STT) systems convert spoken words into written text that computers can process, while Text-to-Speech (TTS) systems transform written text into natural-sounding audio that humans can understand. Voice cloning extends these capabilities by capturing and replicating individual voice characteristics, enabling personalized patient communications and preservation of voice for medical conditions affecting speech.
The practical significance for pharmaceutical operations becomes clear in daily scenarios. A laboratory scientist wearing gloves in a biosafety cabinet can verbally dictate experimental observations directly into an electronic lab notebook without touching a keyboard, eliminating contamination risks and data transcription errors. A medical science liaison conducting a physician meeting can have the conversation automatically transcribed and structured into CRM-compliant documentation within seconds, reducing post-meeting administrative work from 45 minutes to 5 minutes. A clinical trial participant with Parkinson's disease can complete daily speech assessments on a smartphone, providing continuous disease progression monitoring without traveling to clinical sites. A quality control inspector on a manufacturing floor can verbally confirm batch parameters while maintaining hands-free GMP documentation compliance.
Modern AI speech systems fundamentally differ from older voice recognition technology
that required extensive training on individual voices and struggled with medical vocabulary. Today's transformer-based neural networks are pre-trained on millions of hours of diverse speech data, enabling them to understand medical terminology, pharmaceutical nomenclature, and scientific language out-of-the-box with 95-97% accuracy. The systems adapt to different accents, speaking styles, and acoustic environments without individual voice training. They process language contextually, understanding that "two milligrams" and "2mg" represent the same pharmaceutical dose, or that "adverse event" in regulatory context differs from casual usage of those words.
The technology stack comprises four integrated layers working seamlessly together. The acoustic layer captures raw audio and converts it to mathematical representations called spectrograms that preserve pitch, tone, and timing information. The linguistic layer applies natural language processing to understand grammar, context, and meaning within pharmaceutical domain-specific vocabularies. The integration layer connects to enterprise systems like LIMS, EHR, CRM, and quality management platforms, automatically populating structured data fields from conversational speech. The compliance layer ensures all voice interactions generate audit trails, electronic signatures, and documentation meeting FDA 21 CFR Part 11 and EU GMP requirements.
Three deployment models address different pharmaceutical use cases. Cloud-based systems (Microsoft Azure Speech, Google Cloud Speech-to-Text, Amazon Transcribe Medical) offer the highest accuracy and continuous improvement through centralized learning, ideal for non-real-time applications like transcribing investigator meetings or processing adverse event call center recordings. On-premise systems deployed within Lilly data centers provide maximum data control and sovereignty, required for highly sensitive clinical trial data or proprietary research information subject to strict access controls. Edge deployment on mobile devices or manufacturing equipment enables real-time voice interaction without network connectivity, critical for manufacturing floor operations or field-based clinical research where internet access is unreliable.
The pharmaceutical industry specifically benefits from voice biomarker capabilities that extract health information directly from voice characteristics beyond the spoken words. Subtle changes in voice quality, pitch stability, speech rate, pause patterns, and articulatory precision correlate with neurological conditions (Parkinson's disease, Alzheimer's disease, ALS), mental health status (depression, anxiety, PTSD), and respiratory function (COPD, asthma, COVID-19). A 30-second speech sample analyzed by AI algorithms can predict mild cognitive impairment progression to Alzheimer's with 80% accuracy, detect depression from voice acoustics with similar accuracy, or screen for respiratory infection before symptoms emerge. These capabilities transform voice from a data input mechanism into a continuous, non-invasive diagnostic and monitoring tool.
Understanding voice AI capabilities and limitations is essential for realistic deployment planning. The technology excels at structured tasks with clear vocabulary and context: transcribing standard operating procedures, documenting manufacturing batch records, capturing clinical trial visit notes, answering common medical information questions from predefined knowledge bases. It struggles with highly ambiguous context, novel drug names never encountered in training data, extreme background noise exceeding 85 decibels, or detecting subtle deception in patient-reported outcomes. Voice AI augments human expertise rather than replacing it, the optimal paradigm combines AI for rapid transcription and initial analysis with human review for final verification and nuanced interpretation.
How AI speech technology works
â€‹
The transformation of human speech into computer-processable information relies on sophisticated neural network architectures that have evolved dramatically over the past 18 months, with pharmaceutical applications benefiting from models specifically trained on medical terminology and clinical conversations. Understanding the technical architecture enables realistic assessment of capabilities, limitations, and integration requirements for pharmaceutical deployments.
Signal processing and acoustic feature extraction
â€‹
When someone speaks into a microphone, the analog sound wave is digitized at standard sampling rates of 16 kHz for telephony or 44-48 kHz for high-fidelity recording. The raw audio signal undergoes preprocessing to remove background noise, normalize volume levels, and enhance speech frequencies. The critical transformation converts time-domain audio waveforms into frequency-domain representations through Fourier analysis, producing spectrograms that visually represent which frequencies have energy at each time point. Modern systems use mel-scale spectrograms that emphasize frequency ranges most relevant to human speech perception, typically extracting 80-128 mel-frequency channels. These spectrograms become the input to neural networks, with each 30-second audio chunk converted to a two-dimensional image-like representation that the AI model processes.
Earlier generation systems relied on hand-engineered acoustic features like Mel-Frequency Cepstral Coefficients (MFCCs) that captured specific audio characteristics known to be important for speech. Contemporary transformer-based models learn optimal feature representations automatically from raw spectrograms through self-supervised learning on massive unlabeled audio datasets. This architectural shift explains the dramatic accuracy improvements seen in 2024-2025, with models learning subtle patterns in audio that human engineers never explicitly programmed. For pharmaceutical applications, this means the systems automatically adapt to specialized acoustic environments like cleanrooms with HEPA filter background noise or clinical examination rooms with medical equipment sounds.
Transformer architectures for speech recognition
â€‹
OpenAI Whisper
represents the current state-of-the-art for pharmaceutical speech-to-text applications, with Whisper Large-v3 (released November 2023) and Whisper Turbo (2024 optimization) offering the best combination of accuracy and speed. The architecture employs a transformer encoder-decoder design where the encoder processes the mel-spectrogram input through stacked transformer blocks with self-attention mechanisms, building progressively higher-level representations of the audio content. The decoder generates text output autoregressively, predicting one token at a time while attending to both the encoded audio features and previously generated text. Training on 5 million hours of labeled audio data across 100+ languages enables zero-shot capabilities on pharmaceutical terminology without requiring domain-specific fine-tuning, though custom vocabulary additions improve accuracy for novel drug names.
Whisper achieves 50% fewer errors than previous generation models across diverse datasets, with Word Error Rates under 5% on clean speech and 10-15% on conversational clinical interactions. The multilingual architecture uses special tokens to specify source language and task (transcription vs. translation), enabling a single model to handle Lilly's global operations across North America, Europe, and Asia-Pacific. The automatic language detection capability eliminates the need to pre-specify languages in multinational clinical trials. However, Whisper exhibits a 1.4% hallucination rate where it generates plausible but non-existent content in silent audio segments -  critical issue for pharmaceutical documentation requiring perfect fidelity.
Conformer architectures
combine convolutional neural networks with transformer attention mechanisms, leveraging CNNs to capture local audio patterns (like phoneme transitions) while transformers model long-range dependencies (like sentence-level prosody). Google's Conformer implementations and Apple's edge-optimized versions demonstrate significant efficiency improvements, with Apple achieving 5.26x real-time speed (19% Real-Time Factor) on mobile processors - fast enough for instantaneous transcription on iPads used in clinical trials. AssemblyAI's Universal-1 model built on Conformer architecture provides pharmaceutical-grade medical conversation models with enhanced accuracy on clinical terminology, doctor-patient dialogues, and adverse event descriptions.
For real-time applications like voice-enabled manufacturing documentation or live medical information call transcription, streaming architectures process audio in overlapping chunks, producing transcription with under 300 milliseconds of latency. Deepgram Nova-2 achieves sub-100ms recognition latency (faster than human reaction time), enabling natural conversational flows in AI assistants for patient engagement or medical affairs applications. The technical challenge involves look-ahead limitations: streaming models cannot use future audio context when transcribing the current word, slightly reducing accuracy compared to batch processing of complete utterances.
Neural text-to-speech synthesis
â€‹
Recent breakthroughs in TTS enable pharmaceutical companies to generate natural-sounding patient communications, multilingual training materials, and accessible content from written documentation.
Microsoft VALL-E 2
achieved the first confirmed human parity performance in June 2024, meaning listeners cannot distinguish AI-generated speech from human recordings. The architecture uses neural codec language models that treat audio as a sequence of discrete tokens similar to how language models treat text. Audio is first compressed using EnCodec into hierarchical representations at multiple bitrates, creating 8 quantization layers that capture both coarse structure (phonetics) and fine acoustic details (voice timbre, prosody). A large transformer language model then learns to generate these audio tokens conditioned on input text and a 3-second reference audio sample.
Two architectural innovations enable the human parity result. Repetition Aware Sampling refines the standard nucleus sampling algorithm used to select the next audio token, accounting for repetition patterns in the generation history to prevent infinite loops and stuttering artifacts that plagued earlier models. Grouped Code Modeling treats multiple consecutive audio tokens as atomic units, dramatically shortening the sequence length the model must predict, accelerating inference speed while maintaining fidelity. Training on 60,000 hours of English speech provides the model with sufficient coverage of speaking styles, emotions, and acoustic variations to generalize to new speakers with minimal prompting.
For pharmaceutical applications, this enables zero-shot voice cloning for personalized patient communications - recording a brief audio sample from a key opinion leader allows generating derivative content in their voice for educational materials, provided appropriate consent and ethical frameworks are in place. More immediately practical, modern TTS generates patient information in 60+ languages with regionally appropriate accents, addressing health equity requirements for diverse clinical trial populations and global commercial markets. The Mean Opinion Score benchmark measuring perceived naturalness shows current systems achieving 4.3-4.5 out of 5.0 (where 4.5+ represents "sounds like a human"), compared to 3.0-3.5 for older TTS systems with robotic qualities.
Diffusion and flow-matching models
represent an emerging alternative to autoregressive generation. These models iteratively refine random noise into high-quality speech spectrograms through a learned denoising process, similar to how diffusion models generate images in DALL-E or Stable Diffusion. DiTTo-TTS and F5-TTS (2024 models) demonstrate that Diffusion Transformer architectures outperform traditional U-Net designs, producing more natural prosody and fewer artifacts. The variable-length modeling capability handles pharmaceutical content of arbitrary duration, from short patient notifications to hour-long training narrations, without quality degradation. Non-autoregressive generation allows parallel processing of the entire utterance simultaneously rather than sequentially, reducing latency from seconds to under 100 milliseconds - crucial for real-time voice interaction in clinical call centers or patient engagement chatbots.
Speech-to-Speech vs STT/TTS Pipeline Architectures
Speech-to-Speech models dominate over pipleine architectures for natual language conversation interatction, with 2-3x lower latency
, however challenges abound for regulated applications, primarily because regulated deployments demand auditability, error traceability, and multi-stage safety verification that modular systems uniquely provide. While 2024-2025 brought commercial S2S breakthroughs with GPT RealTime Voice, GPT4o, and Gemini Live, high-risk deployments may continue to favor STT > LLM > TTS pipelines for clinical documentation, medication ordering, and diagnostic applications where regulatory compliance and patient safety outweigh conversational naturalness.
This creates a clear architectural divide:
Consumer voice AI is rapidly adopting end-to-end S2S models for their superior latency and prosody preservation, while pharmaceutical applications continue investing in pipeline systems that achieve 93-99% accuracy on medical terminology through specialized ASR models, enable FDA-mandated audit trails at each processing stage, and integrate clinical decision support to prevent medication errors - features nearly impossible to implement in monolithic S2S architectures.
The pharmaceutical sector's unique constraints - 25% of medication errors involve drug name confusion, FDA transparency requirements for AI medical devices, HIPAA compliance mandates, and the life-or-death consequences of transcription mistakes fundamentally favor architectures where errors can be isolated, verified, and corrected at multiple checkpoints rather than generated in a single black-box forward pass.
Direct speech-to-speech models deliver speed but sacrifice pharmaceutical safety requirements
â€‹
End-to-end S2S models represent a paradigm shift from cascade systems, directly mapping source speech to target speech without intermediate text representation. Google's Translatotron 3 (2024) surpassed cascade models on translation benchmarks, while Microsoft's VALL-E 2 achieved human parity on zero-shot text-to-speech with 4.2% word error rate matching ground truth performance. Meta's SeamlessM4T v2 supports 101 - 36 languages with 20% BLEU improvements over previous state-of-the-art, demonstrating the technical maturity of S2S architectures.
Latency advantages create the strongest S2S value proposition.
Kyutai's Moshi demonstrates 160ms response time compared to 510ms for optimized pipelines, approaching the 230ms human conversation baseline. OpenAI's Realtime API averages 232ms audio-to-audio response, while SpeechGPT 2.0 achieves sub-200ms latency in practical tests - 3x faster than typical pipeline implementations (500-4000ms). This speed advantage enables natural full-duplex conversations with fluid interruption handling impossible in serial pipeline processing.
Paralinguistic preservation represents S2S models' second major strength.
AudioLM's hierarchical tokenization separates semantic content from acoustic characteristics, maintaining speaker identity, prosody, and emotional tone across 60,000 hours of training data. Translatotron 2 preserves voice characteristics across language translation turns without segmentation, while SpeechGPT 2.0 offers multi-emotion, multi-style control including rap, drama, whisper, and robot modes through 750bps ultra-low bitrate codecs that jointly model semantics and acoustics.
However, pharmaceutical applications reveal critical S2S limitations that overshadow these benefits. The monolithic architecture creates a black box issue, a concern for FDA's 2024 guidance requiring "transparent, non-complex algorithms" allowing providers to "independently review the basis for recommendations." When a medication name is misheard - distinguishing "epinephrine" from "ephedrine," or "Celebrex" from "Celexa" S2S systems provide no mechanism to identify whether acoustic confusion or language model error caused the mistake, making root cause analysis and correction impossible.
Hallucinations pose significate risks.
S2S models can generate plausible-sounding but non-existent information, a well-documented phenomenon where models add phrases from training data like "subtitles by Jane Doe" or confabulate medical terms. In medication ordering, where 25% of errors involve drug name confusion and wrong medications can cause patient death, this single-point-of-failure architecture lacks the multiple verification checkpoints pharmaceutical safety demands.
Training S2S models for medical applications faces data scarcity challenges. While AudioPaLM trained on thousands of hours across 100+ languages and VALL-E used 60,000 hours of English speech, medical-specific S2S models remain largely theoretical with limited production deployments. Current S2S systems show 35-65% word error rates on medical terminology without specialized fine-tuning - unacceptable when clinical standards require sub-5% error rates for patient safety.
Pipeline architectures achieve medical-grade accuracy through specialized components and modular optimization
â€‹
The modular STT -> LLM -> TTS pipeline remains the production standard for pharmaceutical voice AI in 2024-2025, with market leaders universally adopting this architecture: Nuance DAX Copilot, Suki Assistant, Abridge, and Nabla Copilot all use pipeline approaches for clinical documentation serving major health systems like Mass General Brigham and Epic EHR integrations.
Medical-specific ASR models deliver accuracy S2S systems cannot match.
Deepgram Nova-3 Medical achieves 63.7% improvement in word error rate over competitors with 4% keyword error rate - half the errors of general systems on clinical terminology. Speechmatics Medical Model reaches 93% real-world accuracy with 50% fewer errors on medical terms versus next-best systems. AssemblyAI's Slam-1 demonstrates 66% reduction in missed medical entity rates with 72% preference in blind human evaluations, supporting up to 1,000 domain-specific terms via prompting for pharmaceutical names, procedure codes, and anatomical references.
This specialization extends beyond English. Whisper medical adaptations achieve 0.985% WER on LibriSpeech after fine-tuning, while maintaining multilingual capabilities across 98 languages. Companies like Nabla invested $5M and three years on 7,000 hours of annotated medical encounters to suppress hallucinations and improve terminology accuracy - investment levels impossible to replicate for monolithic S2S systems where component-level optimization is impossible.
Streaming optimizations reduce pipeline latency to competitive levels.
Optimized implementations using Deepgram Nova (100ms), GPT-4o-mini (200ms first token), and Cartesia Sonic TTS (90ms time-to-first-byte) achieve 465ms end-to-end latency - approaching S2S performance while maintaining modular advantages. Voice AI platforms like Vapi and Synthflow consistently deliver sub-800ms response times through parallel processing where VAD, STT, and API prefetching execute concurrently rather than serially.
Modern pipeline architectures implement sophisticated concurrent execution graphs that eliminate serial bottlenecks: sentence-level streaming sends partial LLM outputs to TTS before generation completes; predictive prefetching loads customer data on caller ID; speculative processing starts likely operations early. This transforms the mental model from serial chains to parallel graphs, with independent tasks executing simultaneously across STT, VAD, LLM reasoning, API calls, retrieval, and audio buffering stages.
Component swapping enables continuous improvement impossible in S2S systems.
When Deepgram releases Nova-3 with 30% WER reduction, pipeline architectures upgrade ASR immediately without touching LLM or TTS components. Medical facilities can deploy GPT-4 for complex reasoning while using GPT-4o-mini for simple queries, mixing models by use case. TTS can switch between ElevenLabs (naturalness), Cartesia (speed), or Deepgram (enterprise balance) based on application requirements - flexibility that monolithic S2S architectures fundamentally cannot provide.
Integration with existing pharmaceutical infrastructure represents another decisive pipeline advantage. LLMs natively support function calling for CRM lookups, real-time inventory checks, and EHR data retrieval during conversations. RAG integration with medical knowledge bases using FAISS vector search enables evidence-based responses grounded in current medical literature. Clinical decision support systems intercept text outputs to verify medication indications, check dose ranges, flag drug-drug interactions, and alert on allergy contraindications - safety mechanisms requiring structured text that S2S's direct audio-to-audio processing bypasses.
Hybrid approaches combining strengths remain largely theoretical for pharmaceutical applications
â€‹
While hybrid architectures theoretically combine S2S speed with pipeline safety, pharmaceutical implementations show minimal adoption of true hybrid systems. The regulatory burden of validating two parallel processing paths, the engineering complexity of fallback mechanisms, and the difficulty determining when to use which approach create practical barriers outweighing theoretical benefits.
Proposed hybrid patterns include risk-based routing
: using S2S for low-risk patient engagement (appointment scheduling, general health information) while defaulting to pipelines for any clinical decision-making, medication advice, or symptom triage. However, real-world deployments reveal challenges in boundary detection - conversations naturally drift from administrative topics to medical questions, requiring dynamic switching that introduces latency and complexity.
Cascaded systems using S2S for specific sub-tasks
within larger pipelines show more promise. AudioLM could handle voice cloning and speaker identity preservation while pipeline NLP performs medical entity recognition and clinical reasoning. This compartmentalized approach maintains audit trails and safety verification while leveraging S2S strengths for acoustic processing - though current pharmaceutical systems rarely implement this architecture, suggesting practical challenges exceed theoretical benefits.
Fallback mechanisms from S2S to pipeline during uncertainty add robustness but could defeat S2S's primary latency advantage. When an S2S model expresses low confidence on a drug name, falling back to pipeline verification adds 500-1000ms delay could be worse than using pipelines throughout. Some use cases may prove practical for "hybrid" approach. However, simply choosing pipeline for pharmaceutical applications and reserving S2S for low-risk voice interfaces, rather than attempting to combine both within single systems may be best for a given implementation.
The Accuracy-Latency Tradeoff
â€‹
Latency measurements show
S2S models deliver 2-3x faster response times
across consistent benchmarks. Moshi achieves 160ms, OpenAI Realtime API averages 232ms, and SpeechGPT 2.0 reaches sub-200ms all approaching human conversation's 230ms baseline. In contrast, typical production pipelines range from 500-4000ms depending on optimization level, with best-in-class implementations achieving 465-812ms through aggressive streaming and parallel processing.
However, accuracy metrics favor pipelines by substantial margins for medical applications.* Medical-specific pipeline ASR achieves 5-10% WER (90-95% accuracy) on clinical conversations compared to 35-65% WER for general S2S models on medical content. Specialized systems like Deepgram Nova-3 Medical reach 4% keyword error rate on pharmaceutical terms - critical when distinguishing "hydroxyzine" (antihistamine) from "hydralazine" (blood pressure medication) can mean life or death.
Computational requirements differ substantially between architectures.
S2S models typically require larger unified models (1-2B parameters) with higher GPU memory demands - AudioPaLM uses 600M parameter Conformers, while medical pipeline ASR runs efficiently on 4-8GB consumer GPUs. Training requirements also diverge: S2S needs 60,000+ hours of parallel speech-speech data (VALL-E, AudioLM), while pipeline components fine-tune independently on smaller domain-specific datasets - Whisper medical adaptations use 7,000 hours versus millions required for general S2S.
Voice quality metrics measured by Mean Opinion Score reveal
TTS components in pipelines match or exceed S2S output quality
. ElevenLabs achieves 81.97% pronunciation accuracy versus 77.30% for OpenAI systems, with 44.98% high naturalness scores and only 5% hallucination rates compared to 10% for competitors. VALL-E 2 reaches human parity at 4.5 MOS, while modern pipeline TTS from Cartesia Sonic achieves 40-95ms model latency - faster than many S2S systems' total response time.
Real-time factor (RTF) measurements for scalability
show pipelines handle concurrent users more efficiently through independent component scaling. A single LLM instance serves multiple STT and TTS streams, while S2S models require dedicated resources per conversation. KV cache memory in LLMs scales linearly (10 concurrent requests need ~122GB for LLaMA-2 13B), but pipelines distribute this load across specialized hardware optimized per stage - ASR on inference servers, LLM on GPU clusters, TTS on edge devices impossible in monolithic S2S architectures.
Recent S2S Developments
â€‹
2024-2025 brought major S2S commercial launches that dramatically advanced consumer voice AI.
OpenAI's GPT-4o Advanced Voice Mode
(May 2024 unveiling, September rollout) delivers native multimodal audio sensing emotional intonations with interruption handling, offering 9 voice options and custom instructions via natural language. Google's Gemini Live (August 2024) provides free-tier conversational AI with 10+ emotionally expressive voices, while Gemini 2.5 (February 2025) adds real-time audio-video understanding with style control across 24+ languages and SynthID watermarking.
Microsoft Azure AI Speech's 2024-2025 releases
illustrate enterprise focus on pipeline optimization rather than S2S: Personal Voice 2.1 offers zero-shot TTS requiring only seconds of audio for voice cloning across 100+ languages; Fast Transcription API delivers 10-minute audio transcribed in 15 seconds; Voice Live API (GA 2025) provides all-in-one STT + GenAI + TTS through a single low-latency interface. These remain modular pipeline services despite unified API presentation, maintaining component-level control enterprises demand.
Meta's SeamlessM4T v2 publication in Nature (2024)
with 20% BLEU improvements and 2-second latency represents S2S research breakthroughs, but real-world pharmaceutical implementations remain absent from literature. Searches for "S2S pharmaceutical deployment" or "end-to-end speech clinical documentation" yield no production case studies, while pipeline systems dominate published healthcare AI implementations - suggesting a research-practice gap where academic S2S advances haven't translated to pharmaceutical adoption.
Training methods and transfer learning
â€‹
Modern speech AI leverages transfer learning paradigms where large foundation models pre-trained on general domain data are adapted to pharmaceutical contexts with relatively modest amounts of domain-specific training examples.
Self-supervised learning
enables training on unlabeled audio by creating artificial prediction tasks: HuBERT (Hidden Unit BERT) masks portions of audio and trains the model to predict the missing segments, while wav2vec 2.0 learns to discriminate between true future audio frames and contrastive negative samples. These pre-training objectives force the model to learn general acoustic and linguistic representations transferable to downstream tasks.
For Lilly-specific customization, several adaptation strategies balance accuracy improvements against resource requirements and ongoing maintenance.
Custom vocabulary augmentation
adds pharmaceutical terminology (drug names, disease names, procedure codes, company-specific acronyms) to the model's recognition dictionary with appropriate phonetic representations and prior probabilities. This low-cost intervention (typically 1-2 days of ML engineering work) provides significant gains for novel or proprietary compound names not in the pre-training data.
Fine-tuning
retrains portions of the neural network on Lilly-specific audio data, such as recordings of investigator meetings, quality review meetings, or sales call transcripts. Full fine-tuning updates all model parameters and requires substantial labeled data (10,000+ hours), while parameter-efficient methods like LoRA (Low-Rank Adaptation) modify only small adapter modules inserted into the base model, achieving 80-90% of full fine-tuning benefits with 100x less training data.
The training data quality and diversity directly determines model robustness across Lilly's global operations. Models trained predominantly on North American English struggle with British, Indian, or Australian accents common in multinational clinical trials and research centers. Acoustic diversity matters equally: training datasets skewed toward quiet studio recordings generalize poorly to noisy manufacturing floors, hospital patient rooms, or outdoor clinical sites. The highest-performing pharmaceutical implementations deliberately include training data representing the actual deployment environments: recordings from manufacturing facilities with
background machinery noise, clinical consultations with typical hospital ambient sounds, field-based researchers using smartphones in suboptimal acoustic conditions. This explains why general-purpose speech models require pharmaceutical-specific adaptation despite their strong baseline performance.
Voice biomarker extraction and analysis
â€‹
Beyond transcribing words, speech AI extracts quantitative features from voice acoustics that correlate with health status, emotional state, and cognitive function - capabilities with profound implications for clinical trials and patient monitoring. Voice biomarker analysis operates on both linguistic content (what is said) and paralinguistic acoustics (how it is said). Linguistic features include semantic coherence, lexical diversity, word-finding difficulty, grammatical complexity, and pause-to-word ratios. Paralinguistic features comprise fundamental frequency (pitch), jitter (pitch variability), shimmer (amplitude variability), formant frequencies (vocal tract resonances), mel-frequency cepstral coefficients (spectral envelope), zero-crossing rate (breathiness), and speech rate.
Machine learning classifiers trained on clinically validated datasets learn to recognize patterns discriminating between healthy individuals and patients with specific conditions. For Parkinson's disease, characteristic features include reduced pitch variability (monotone speech), slower articulation rate, imprecise consonant production, and vocal tremor. Trained models analyzing 30-60 second speech samples achieve 85-95% sensitivity and specificity for Parkinson's detection, with accuracy improving when combining multiple speech tasks: sustained vowel phonation, rapid syllable repetition (pa-ta-ka), reading standardized passages, spontaneous speech describing a picture. The Framingham Heart Study analysis published in Nature (2024) demonstrated that speech content features predict conversion from mild cognitive impairment to Alzheimer's disease with 80% accuracy six years in advance, by detecting subtle declines in semantic richness and narrative coherence.
Depression and anxiety detection relies heavily on prosodic features and temporal dynamics. Depressed individuals exhibit slower speech rate, longer pauses, reduced pitch variation (flatter affect), lower speech energy, and less pitch/amplitude modulation. Sonde Health's Mental Fitness Vocal Biomarker achieved relative risk ratios of 2.00 for detecting anxiety using aggregated two-week voice data in a prospective psychiatric cohort study, improving to 8.50 relative risk for frequent users providing 5-6 samples weekly. The technology enables continuous passive monitoring of mental health status from routine phone conversations or voice journaling, providing early warning signals of decompensation for intervention before crisis points. Respiratory conditions like COPD and COVID-19 alter voice quality through mechanisms including reduced airflow (breathiness), mucus accumulation (vocal tract damping), and inflammation (hoarseness), detectable with 70-80% accuracy from 6-second "ahh" vocalizations.
Implementing voice biomarkers in pharmaceutical clinical trials requires rigorous analytical validation following FDA Biomarker Qualification Program standards. The qualification process involves demonstrating: (1) analytical validity (the biomarker reliably and accurately measures what it purports to measure), (2) clinical validity (the biomarker consistently and accurately predicts or correlates with the clinical outcome), and (3) clinical utility (the biomarker meaningfully informs clinical decision-making to improve patient outcomes). For voice biomarkers specifically, this necessitates large diverse datasets (thousands of participants across demographics, languages, and disease severities), longitudinal validation demonstrating stability of the biomarker in healthy individuals and predictable change patterns in disease progression, and prospective studies showing the biomarker improves upon existing endpoints. The Bridge2AI-Voice consortium's 10,000-participant, 50-institution initiative represents the first large-scale effort to establish the standardized protocols, reference datasets, and analytical pipelines required for regulatory qualification.
Integration with multimodal AI systems
â€‹
The most powerful pharmaceutical applications combine voice AI with complementary data modalities in unified machine learning frameworks.
Voice plus computer vision
enables ambient documentation in clinical settings: cameras track physician gaze, gestures, and activities (patient examination, screen viewing, writing) while microphones capture the clinical dialogue, with integrated AI models generating complete clinical notes including both observed physical exam findings and patient history. Johnson & Johnson's Polyphonic surgical video analysis system (partnership with NVIDIA, 2024) combines audio from the operating room with video of surgical techniques to generate highlight reels and identify best practices, reducing surgical training time by 50%.
Voice plus wearable sensors
enhances remote patient monitoring by correlating voice biomarkers with physiological signals. A clinical trial participant with Parkinson's disease provides daily 60-second speech samples via smartphone while continuously wearing an accelerometer-equipped smartwatch tracking tremor and gait. Machine learning models trained on the combined dataset achieve higher disease progression detection accuracy than either modality alone, learning that specific combinations of voice stability metrics and movement disorder features predict medication efficacy. The 5G network infrastructure and edge computing capabilities emerging in 2025 enable real-time processing of these multimodal data streams with latency under 500 milliseconds, supporting adaptive interventions.
Voice plus knowledge graphs
powers intelligent pharmaceutical information systems. A medical science liaison verbally queries "What are the latest efficacy data for compound 247 in the Phase 3 ATLAS trial?" The speech recognition system transcribes the question, a large language model interprets the intent and extracts entities (compound name, trial identifier, data type), the knowledge graph traverses relationships connecting the compound to clinical trials to efficacy endpoints to recent data updates, and the text-to-speech system verbalizes a comprehensive answer with appropriate scientific caveats. Microsoft's integration of Azure Speech Services with Azure OpenAI enables these sophisticated question-answering workflows with conversational natural language interfaces, transforming static document repositories into interactive AI assistants accessible by voice.
Market analysis: vendors, costs, and value proposition
â€‹
The AI speech technology market serving pharmaceutical operations exhibits rapid growth, vendor consolidation, and price-performance improvements creating favorable conditions for enterprise adoption. Global conversational AI in healthcare will expand from $13.68 billion in 2024 to $106.67 billion by 2033 at 25.71% compound annual growth rate, with speech recognition and voice generation representing approximately 30-35% of this total. For pharmaceutical-specific applications, voice biomarker markets project growth from $1.08 billion (2024) to $5.40 billion (2035) at 15.81% CAGR, while ambient clinical intelligence - a key use case for medical affairs and clinical operations - will reach $600 million in 2025, growing 2.4x year-over-year.
Cloud Platform Providers
â€‹
Microsoft Azure Speech Services
offers the most comprehensive pharmaceutical ecosystem through its 2022 acquisition of Nuance Healthcare for $19.7 billion, combining cloud infrastructure with proven healthcare products. Dragon Medical One serves 550,000+ physicians and operates in 77% of U.S. hospitals, providing the deepest integration with Epic, Cerner, and Meditech electronic health records. For pharmaceutical applications, Azure provides Speech-to-Text in 92+ languages at $1.00 per hour of audio processed for standard models, with custom pharmaceutical vocabulary adaptation available. Neural Text-to-Speech costs $15.00 per million characters, generating natural-sounding patient communications and training content in 215 voices across 60 languages. Enterprise contracts could include HIPAA Business Associate Agreements, GDPR-compliant data processing, and commitment tier discounts for predictable workloads (volume discounts of 20-40% for million-hour annual commitments).
The strategic value for Lilly extends beyond speech APIs to complete ambient intelligence solutions. For example, Nuance DAX Copilot integrates GPT-4 with ambient listening to generate clinical documentation from physician-patient conversations within minutes, priced at approximately $600 monthly per provider under enterprise agreements. Independent validation shows 70% reduction in clinician burnout and 50% reduction in documentation time. The Permanente Medical Group reported 15,791 hours saved annually (equivalent to 8 full-time employees) using Nuance ambient intelligence. For Lilly, this translates to medical science liaison documentation automation, clinical trial investigator meeting transcription, and medical monitor note generation with pharmaceutical-grade compliance and audit trails.
Google Cloud Speech-to-Text
emphasizes accuracy for medical conversations through its Chirp 3 foundation model trained on billions of sentences. Medical conversation models optimized for clinical dialogues cost $0.072 per minute ($4.32 per hour), representing a 4x premium over standard models but delivering superior accuracy on pharmaceutical terminology, drug names, and adverse event descriptions. Google's Healthcare Natural Language API complements speech recognition with entity extraction identifying medications, dosages, symptoms, and diagnoses from transcribed text, enabling structured data capture from unstructured clinical narratives. The platform supports 125+ languages with single-region or multi-region deployment options addressing data sovereignty requirements across Lilly's global operations. Performance benchmarks show Google's medical models achieve 15-20% lower Word Error Rates than generic speech recognition on pharmacology content.
Amazon Web Services
positions Transcribe Medical specifically for healthcare use cases at $0.0237 per minute for batch processing and $0.0395 per minute for real-time streaming, including HIPAA-eligible infrastructure with Business Associate Agreements. The service automatically recognizes medical terminology across 31 specialties including oncology, cardiology, and neurology - the therapeutic areas most relevant to Lilly's portfolio. Amazon Polly neural voices generate text-to-speech at $16 per million characters for neural quality, suitable for patient education materials and multilingual clinical trial communications. The compelling value proposition for pharmaceutical companies emerges from tight integration with AWS's broader healthcare data analytics stack: speech data flows into Amazon HealthLake for FHIR-based clinical data warehouses, with Amazon Comprehend Medical extracting structured clinical concepts for downstream analysis.
Cost modeling for enterprise speech AI adoption depends on anticipated monthly volume and use case distribution. AI speech processings ($1-2.5/hour) compares very favorably to human transcription services ($1.50-3.00 per audio minute).
Specialized pharmaceutical and healthcare vendors
â€‹
Nuance Healthcare
(Microsoft-owned) dominates ambient clinical intelligence with market-leading products purpose-built for healthcare documentation. Dragon Medical One provides continuous speech recognition for clinical note dictation at custom enterprise pricing. DAX Copilot represents the premium tier provider, offering fully automated visit note generation from ambient conversation capture without requiring specific voice commands or structured dictation. The April 2023 launch of DAX Express provides completely automated workflow with no human review in the transcription loop, though human physician review of the final note remains mandatory for medical-legal reasons. Nuance reports that 300 million patient stories are captured annually through its platform, with 77% of U.S. hospitals using Nuance technology in some capacity.
For Lilly medical affairs applications, integration with Veeva CRM could enable medical science liaisons to focus entirely on medical dialogue with healthcare providers while the ambient system automatically generates compliant interaction documentation including medical information exchanged, physician questions, and product discussions. Assuming a 50-70% documentation time reduction directly translates to increased field capacity: a 50-representative MSL team saving 10 hours per week each gains 500 hours of additional physician engagement time weekly, enabling 50-100 additional meaningful medical interactions monthly depending on meeting duration.
Suki AI
targets the mid-market sweet spot with comprehensive ambient documentation, achieving 93.2 KLAS overall performance score and serving 250+ U.S. health systems across 99 medical specialties. The platform provides end-to-end AI assistance including pre-charting (automatically pulling relevant historical information before visits), ambient note-taking during consultations, clinical reasoning support, ICD-10/CPT coding recommendations, and question-answering from medical literature. Deep EHR integrations with Epic, Cerner, Athenahealth, and MEDITECH enable bidirectional data flow, automatically pulling context from the EHR and writing structured notes back to appropriate fields. Suki reports 72% reduction in documentation time, 6 hours reduction in after-hours "pajama time" documentation, and 9x first-year ROI.
For Lilly clinical operations, this technology could apply to clinical trial site support, medical monitor documentation, and investigator meeting transcription.
Abridge
commands 30% market share in ambient clinical intelligence (second only to Nuance's 33%) with strong Epic integration and adoption by Mayo Clinic, Kaiser Permanente, and UPMC health systems. The platform uniquely captures structured patient instructions and next steps, automatically generating after-visit summaries in patient-friendly language that can be immediately shared, improving patient engagement and protocol adherence - a critical feature for clinical trial applications where participant understanding drives retention and compliance. The 90-day audio retention policy (shorter than Nuance's indefinite retention with patient consent) may limit usefulness for long-term clinical research applications requiring extended review periods or regulatory audits spanning years.
Voice biomarker companies for clinical endpoints
â€‹
Winterlight Labs
leads cognitive assessment through speech analysis, with active deployment in 12+ clinical trials for Alzheimer's disease, depression, dementia, bipolar disorder, and schizophrenia. The technology analyzes 30-40 second speech samples using natural language processing to quantify semantic coherence, lexical diversity, syntactic complexity, and acoustic features. Cambridge Cognition acquired Winterlight in January 2023 for approximately $8.6 million, integrating speech biomarkers with their established cognitive testing battery. The combined offering provides pharmaceutical sponsors a validated digital endpoint package: traditional computerized cognitive assessments plus voice biomarkers, enabling comprehensive cognitive function monitoring in Alzheimer's and dementia trials with reduced participant burden compared to lengthy in-person cognitive batteries.
For Lilly's neuroscience portfolio, voice biomarkers may offer several strategic advantages over conventional cognitive endpoints.
Continuous remote monitoring
captures daily or weekly measurements reflecting real-world cognitive fluctuations, rather than snapshot assessments at quarterly clinic visits that miss disease dynamics and treatment response variability.
Reduced participant burden
(30-60 seconds voice recording versus 45-60 minute cognitive test batteries) improves retention in elderly populations and those with significant impairment who struggle with long assessments.
Increased sensitivity
to subtle changes may detect treatment effects earlier or with smaller sample sizes, potentially reducing Phase 2 trial costs by 20-30% through better go/no-go decision-making.
Ecological validity
of speech production - a complex real-world task integrating memory, executive function, language, and motor control - arguably better represents functional capacity than abstract computerized tests.
Sonde Health
specializes in mental health and respiratory voice biomarkers, with FDA regulatory pathway engagement positioning their Mental Fitness Vocal Biomarker and Respiratory-Responsive Vocal Biomarker for clinical decision support. The MFVB demonstrated in a 104-participant prospective psychiatric cohort that aggregated two-week voice data achieved relative risk ratio of 2.00 for detecting anxiety (p=0.0068), improving to 8.50 for users providing 5-6 samples weekly. The RRVB trained on 3,000+ respiratory disease patients achieved 70% sensitivity and specificity across asthma, COPD, interstitial lung disease, and persistent cough, with validation extended to COVID-19 detection (73.2% sensitivity, 62.9% specificity for symptomatic and asymptomatic cases). Strategic partnerships with Qualcomm enable on-device processing on mobile chipsets, addressing privacy concerns by keeping voice data local rather than transmitting to cloud servers.
For Lilly's immunology brands, respiratory voice biomarkers may enable real-world evidence generation showing treatment impact on day-to-day breathing function beyond controlled spirometry at clinic visits. Depression and anxiety voice monitoring supports neuropsychiatry trials and post-market surveillance for CNS compounds where psychiatric adverse events require vigilant monitoring. Commercial deployment could differentiate patient support programs: branded smartphone apps with daily voice check-ins providing patients continuous feedback on symptom trends and automatically alerting care teams to concerning changes.
Canary Speech
emerged from former Amazon Alexa neurology and speech AI teams, offering the industry's most comprehensive multi-condition platform extracting 12+ million biomarkers per minute of speech. FDA registration (not full approval) provides credibility for healthcare system adoption. The technology addresses Alzheimer's, Parkinson's, Huntington's disease, depression, anxiety, stress assessment, and readmission risk prediction. The proactive screening capability - identifying individuals at risk before symptoms emerge aligns with pharmaceutical industry interest in earlier diagnosis enabling intervention in milder disease stages where disease-modifying therapies have best chance of success.
For Lilly, this suggests potential partnerships embedding Canary Speech screening in primary care workflows to identify patients who might benefit from neuroscience therapies, expanding the addressable patient population.
The compelling economics emerge in Phase 2/3 trials where voice biomarkers could serve as secondary or exploratory endpoints. A 200-participant Phase 2 trial adding voice biomarkers at $100 per participant represents a fractional cost of total trial cost while potentially providing crucial supportive evidence of mechanism, guiding dose selection, or enabling patient stratification. If voice biomarker data enables better Phase 3 trial design, reducing the required sample size by even 50 patients, the savings far exceed the voice biomarker investment.
High-level Value Indicators
â€‹
Quantitative ROI analysis across implemented pharmaceutical use cases reveals consistently strong business cases with payback periods of 3-18 months and multi-year returns of 200-1,200%. The most immediate value derives from productivity improvements in high-cost knowledge worker roles where voice AI eliminates low-value documentation tasks, reallocating human capital to higher-value activities.
Medical Affairs
: MSLs may spend 35% of time on administrative tasks (call notes, follow-up emails, CRM data entry, expense reports) = ~35 hours per week per MSL
Clinical Operations
: 10-minute average handling time, most requiring transcription for pharmacovigilance and regulatory documentation
LRL lab productivity
: Research scientists may be spending 2 hours a day on documentation and data entry
Data quality improvements
: Eliminating manual transcription errors reduces protocol deviations (clinical trials), batch failures (manufacturing), and data-cleaning overhead (R&D). Industry estimates suggest 2-5% of clinical trial costs relate to data issues that voice AI reduces by 30-50%.
Regulatory compliance
: Complete audit trails, elimination of paper records, improved GMP documentation quality reduce FDA 483 observations and warning letter risk. The business impact of avoided regulatory citations (production delays, consent decree costs, reputational harm) can exceed millions but proves difficult to quantify prospectively.
Employee satisfaction and retention
: Nuance reports 70-94% of physicians report reduced burnout with ambient AI. For pharmaceutical roles with high turnover costs (recruiting, training, productivity ramp), even modest retention improvements generate substantial value. Replacing knowledge workers come at a high cost (months for recruiting, onboarding, training to full productivity).
Use Case Examples
â€‹
Research and Discovery
â€‹
Voice-enabled laboratory informatics systems transform research productivity by eliminating the friction between conducting experiments and capturing data. Traditional laboratory workflows interrupt experimental procedures for researchers to remove gloves, type observations into computers, and then 're-glove' - a cycle repeated dozens of times daily that consumes 15-25% of active research time. LabTwin's AI-powered mobile voice assistant provides hands-free data capture allowing scientists to verbally dictate experimental observations, request protocol steps, log sample information, and query previous results while maintaining sterile technique and workflow continuity. Implementation at dsm-firmenich achieved 100% real-time digitization of lab data with 16% increase in data capture rate, attributed to recording observations immediately rather than deferring to end-of-day batch entry where details fade and errors accumulate.
The integration architecture connects voice AI with Laboratory Information Management Systems (LIMS) and Electronic Laboratory Notebooks (ELN) through bidirectional APIs. When a researcher verbally reports "sample concentration 247 micromolar," the voice system transcribes the utterance, applies natural language understanding to extract structured data (sample identifier from context, concentration value 247, units micromolar), validates against expected ranges for the assay, and automatically populates the corresponding LIMS field with full audit trail (timestamp, user, voice confidence score). Researchers query existing data conversationally: "What was the IC50 for compound 181 in the kinase assay?" The system retrieves relevant records and verbalizes the answer, eliminating time searching through databases or spreadsheets.
LabVantage LOTTIE (LabVantage Open Talk Interactive Experience), as an example, exemplifies purpose-built LIMS voice integration, with customizable voice commands executing complex workflows. A chromatography technician verbally triggers "begin sample prep protocol for batch 445" and the system retrieves the SOP, displays the current step, marks the protocol as in-progress, reserves necessary equipment, and logs the initiation time - actions requiring 8-10 manual clicks compressed into a 3-second voice command. The property-driven configuration language enables lab managers to define facility-specific commands without programming: "check freezer 7 temperature" could query IoT sensors and verbalize the current reading, while "reserve mass spec for Tuesday at 2pm" enters equipment scheduling systems.
Beyond convenience, voice interfaces enable novel experimental approaches. Researchers conducting extended observations (cell culture monitoring, crystallization kinetics, reaction progression) can provide continuous verbal running commentary captured as timestamped annotations correlated with automated sensor data (microscope images, spectroscopy readings, temperature profiles). Machine learning algorithms later analyze synchronized multi-modal data streams to identify subtle patterns human observers miss. In chemical synthesis, verbal description of reaction color changes, precipitate formation, or odor characteristics (safety permitting) captures qualitative information that numerical sensors cannot, providing complete experimental records valuable for troubleshooting and knowledge transfer.
The accessibility benefits significantly impact researchers with disabilities or repetitive strain injuries. Voice interaction eliminates barriers for scientists with limited hand mobility, visual impairments interfacing with screen readers, or carpal tunnel syndrome aggravated by extensive typing. This aligns with Lilly's diversity and inclusion objectives while tapping talent that might otherwise face challenges in laboratory roles.
Clinical Development
â€‹
Clinical trials represent the most transformative application domain for pharmaceutical voice AI, with implications spanning patient recruitment, continuous monitoring, digital endpoints, adverse event capture, and protocol adherence. The convergence of validated voice biomarkers, regulatory acceptance of digital health technologies, and decentralized trial models creates ideal conditions for large-scale deployment in Lilly's clinical programs.
Voice biomarkers as clinical trial endpoints
address limitations of conventional assessment scales that provide sparse infrequent measurements subject to recall bias, practice effects, and limited ecological validity. Consider a Phase 2 trial for a novel Alzheimer's therapy using the Alzheimer's Disease Assessment Scale-Cognitive (ADAS-Cog) as primary endpoint, measured at baseline and then quarterly clinic visits (months 3, 6, 9, 12). This design provides only 5 data points per participant across a year, missing disease variability between visits and potential treatment effects that emerge then plateau between measurements. Voice biomarkers captured weekly through smartphone apps provide 52 data points per participant, revealing treatment response trajectories, durability of effect, and individual variability patterns invisible in sparse quarterly assessments.
The Voice as Biomarker project (NIH-funded, Weill Cornell/University of South Florida) released 12,500 voice recordings from 306 cognitively diverse participants in December 2024, representing the first large-scale standardized voice biomarker dataset. The protocol includes 20 voice tasks: reading passages, free speech describing pictures, counting, breathing exercises, and coughing. Analysis demonstrates that specific linguistic features (semantic coherence, pause patterns, word-finding latency) predict progression from mild cognitive impairment to Alzheimer's dementia with 80% accuracy 6 years prospectively, based on Boston University's Framingham Heart Study cohort analysis published in Nature (2024). These results suggest voice biomarkers could serve as prognostic enrichment criteria (enrolling trial participants more likely to progress, increasing power to detect treatment effects) or as surrogate endpoints (early evidence of disease modification before clinical symptoms change).
For Lilly's
neuroscience
pipeline, voice biomarkers may offer immediate application in Parkinson's disease trials.
Motor function assessment via speech analysis
complements the Movement Disorder Society-Unified Parkinson's Disease Rating Scale (MDS-UPDRS) which requires in-person clinician administration. Smartphone-based speech tasks (sustained vowel "ahh" for 10 seconds, rapid syllable repetition "pa-ta-ka", reading a standardized paragraph) capture voice quality, pitch stability, articulatory precision, and speech rate - features correlating with MDS-UPDRS motor scores (correlation coefficients 0.6-0.75) and demonstrating sensitivity to dopaminergic medication effects. A 150-participant Phase 2 trial collecting voice biomarkers 3x weekly provides 150 participants x 3 samples/week x 48 weeks = 21,600 data points, versus 750 data points from quarterly MDS-UPDRS assessments. The increased measurement density improves statistical power, potentially reducing required sample size by 20-30% (saving $3-5M in trial costs) while providing richer dataset for dose selection and patient stratification.
Mental health applications parallel neurology. Sonde Health's validation study demonstrating relative risk ratio of 2.00 for anxiety detection from aggregated two-week voice data (p=0.0068) suggests utility for safety monitoring in psychiatric trials or CNS compounds where anxiety represents a potential adverse effect. Daily voice check-ins administered through patient-facing trial apps capture mental health status continuously, providing objective data complementing subjective patient-reported outcomes. The
Therabot trial
published in NEJM AI (March 2025) demonstrated that generative AI conversational agents achieved 51% average depression symptom reduction with engagement levels (6 hours over 8 weeks) comparable to traditional therapy. While Therabot serves therapeutic purposes, the underlying voice analysis capability enables passive monitoring during those interactions, creating data streams for clinical research.
Adverse event capture from natural language
represents an immediate near-term application requiring less regulatory validation than formal endpoints. Medical information contact centers receiving unsolicited AE reports currently rely on human transcription and coding, consuming 10-15 minutes per call and introducing 24-48 hour delays before pharmacovigilance database entry. ProPharma Medical Information Contact Center reports that AI-powered speech recognition with automated AE identification achieves 40% reduction in case intake processing time, accelerating safety signal detection and regulatory reporting compliance. Natural language processing algorithms trained on pharmacovigilance taxonomy automatically identify verbatim terms (patient-reported symptom descriptions), map to preferred terms (MedDRA coding), and extract metadata (onset date, severity, relationship to study drug, concomitant medications). The system flags serious adverse events (death, hospitalization, life-threatening events) for immediate escalation to medical monitors.
Implementation architecture for clinical trial voice biomarkers requires consideration of data flow, regulatory compliance, and participant experience. Example implementation pattern:
Patient-facing mobile app (iOS/Android) administering standardized voice tasks with prompts and instructions, recording high-quality audio locally
On-device initial processing using privacy-preserving algorithms extracting voice features without transmitting raw audio (addressing patient privacy concerns)
Encrypted transmission of derived features (numerical vectors, not audio) to clinical trial data management systems via HTTPS with certificate pinning
Cloud-based advanced analytics in HIPAA/GDPR-compliant infrastructure (AWS/Azure healthcare zones) performing machine learning inference
eCRF integration - writing voice biomarker scores to electronic case report forms as study data
Audit trail generation - documenting complete data provenance for regulatory inspection
The sponsor retains original audio recordings in encrypted archival storage for potential regulatory queries but primarily operates on privacy-preserving features, reducing re-identification risk and simplifying data protection compliance.
Manufacturing and Quality
â€‹
Pharmaceutical manufacturing operates under stringent Good Manufacturing Practice (GMP) regulations requiring complete documentation of every production step, environmental condition, material transfer, and quality check. The traditional paradigm demands operators interrupt hands-on work to manually record data on paper batch records or log into manufacturing execution systems (MES), creating productivity bottlenecks and introducing transcription errors. Voice-enabled manufacturing systems allow hands-free documentation while maintaining continuous production flow, improving both efficiency and data quality.
The
21 CFR Part 11 compliance
framework for electronic records and signatures applies equally to voice-captured data, requiring systems that ensure authenticity, integrity, and confidentiality. Implementation involves: (1) speaker authentication through voice biometrics or secondary credentials (badge scan, PIN), (2) secure timestamping of all voice interactions with NTP-synchronized clocks, (3) audit trail generation recording user identity, timestamp, original voice data, transcribed text, system actions, and approval chains, (4) version control for voice-enabled SOPs and work instructions, (5) electronic signature capture for critical process steps using voice passphrase plus secondary factor.
Quality control testing
exemplifies hands-free value proposition. A QC analyst performing pH measurement on 30 samples can verbally report "sample one zero three pH 6.8" for each measurement as performed, rather than recording on paper then transcribing to LIMS post-shift. The voice system integrates with laboratory equipment (pH meters, balances, spectrophotometers) via IoT protocols, cross-validating verbal reports against instrument readings. If the analyst reports "pH 6.8" but the connected pH meter indicates 6.9, the system flags the discrepancy for immediate resolution, preventing errors from entering permanent records.
Batch record documentation
transforms from paper-intensive to voice-driven workflows. An operator conducting visual inspection of fill-finish vials verbally confirms "one hundred units inspected, zero rejects, lot number A4739" and the system automatically populates corresponding batch record fields, attaches electronic signature, marks the step complete, and advances to the next protocol step. For time-critical operations like aseptic processing where operators cannot break sterile field, voice interaction maintains continuous workflow while ensuring complete documentation. Research published in PMC (AI and IoT Integration in Pharmaceutical Manufacturing, 2024) demonstrates that CNN-based visual inspection integrated with voice verification achieves 99.86% accuracy in pharmaceutical quality control, combining AI image analysis with verbal operator confirmation for optimal human-AI collaboration.
Predictive maintenance
gains from voice-enabled reporting. When operators notice unusual equipment sounds, vibrations, or performance characteristics not captured by automated sensors (subtleties like slight binding in tablet press tooling or unusual smell from HVAC systems), they verbally report observations immediately through mobile devices or floor-mounted voice terminals. Machine learning algorithms aggregate verbal reports with IoT sensor data (vibration, temperature, current draw) to predict equipment failures before breakdowns occur. The same PMC study reports 30% reduction in unplanned manufacturing downtime through this voice-augmented predictive maintenance approach, directly impacting production throughput and product supply reliability.
Deviation management
accelerates through voice-enabled workflows. When operators identify deviations (parameters outside specifications, missed process steps, material discrepancies), immediate verbal reporting initiates the deviation process: the voice system generates a deviation record with preliminary details, notifies quality and production management, and prompts the operator through initial corrective actions. This reduces median deviation reporting time from 4-6 hours (completing paper forms, obtaining management signatures, scanning to quality systems) to 15-30 minutes, enabling faster root cause analysis and corrective action while memories remain fresh.
Validation of voice-enabled manufacturing systems follows Computer System Validation (CSV) principles per GAMP 5 guidance, with voice AI systems typically classified as GAMP Category 4 (configured products) or Category 5 (custom applications). The validation lifecycle includes:
Installation Qualification (IQ)
: Verifying voice hardware (microphones, network infrastructure) and software (speech recognition engines, MES integration modules) installed per specifications with appropriate security configurations, backup systems, and disaster recovery capabilities.
Operational Qualification (OQ)
: Functional testing covering voice recognition accuracy across user population (different voices, accents, speech patterns), environmental conditions (background noise typical of production areas, cleanroom HEPA filters, equipment operation), and failure modes (network interruption, API timeout, ambiguous utterances). Testing validates voice commands execute correct MES functions, data populates proper fields, errors trigger appropriate exception handling, and alternative input methods function when voice unavailable.
Performance Qualification (PQ)
: Real-world validation during actual production runs demonstrates that voice-enabled documentation produces complete, accurate batch records meeting GMP requirements. Statistical analysis confirms voice transcription accuracy exceeds 95% with error rates below manual paper documentation. User acceptance testing with representative operators across shifts confirms usability and identifies workflow refinements.
The validation documentation package (Validation Master Plan, protocols, test scripts, test evidence, summary reports, traceability matrices) demonstrates to regulatory inspectors that voice-enabled systems consistently produce GMP-compliant documentation. Revalidation follows any significant changes: software version upgrades, MES integration modifications, or substantive SOP revisions.
Commercial and Medical Affairs
â€‹
Medical science liaisons represent pharmaceutical companies' most valuable customer-facing asset, with average fully-loaded costs of $250,000-300,000 annually per representative. The strategic imperative involves maximizing high-value medical dialogue time with healthcare providers while minimizing low-value administrative burden. Voice AI transforms MSL effectiveness through three primary mechanisms: automated interaction documentation, intelligent pre-call preparation, and real-time support during engagements.
IQVIA Field Force Agent exemplifies comprehensive voice-enabled MSL support, integrating ambient intelligence with CRM systems and medical databases. The pre-call workflow begins with voice query: "Brief me on Dr. Johnson ahead of my 2pm meeting." The AI assistant retrieves the physician's profile from IQVIA's OneKey database (25M+ HCPs, 6M+ HCOs across 118 countries), recent interactions from Veeva CRM, published research, clinical trial involvement, formulary position at their institution, and generates a voice summary highlighting key talking points and potential areas of interest. This 2-3 minute briefing replaces 15-20 minutes of manual research, producing the documented 27% reduction in call preparation time.
During the medical engagement, ambient listening captures the conversation without requiring the MSL to take notes or interrupt dialogue flow. Post-meeting, AI generates structured documentation including: medical topics discussed, product information provided, healthcare provider questions and MSL responses, scientific literature exchanged, commitments for follow-up, next interaction recommendations. The system automatically populates Veeva CRM with required fields (contact date, interaction type, products discussed, attendees), generates follow-up email drafts with referenced scientific publications, and creates reminders for promised information delivery. The MSL reviews and approves the automated documentation (human-in-the-loop compliance requirement) before finalizing, reducing post-call documentation from 45 minutes to 5-10 minutes.
ZS Field Force Effectiveness@Scale, for example, adds AI coaching capabilities analyzing MSL conversation patterns against best practices. The system identifies when MSLs effectively address objections, probe for unmet medical needs, or position scientific evidence persuasively. Aggregated across hundreds of interactions, pattern recognition algorithms surface communication techniques correlating with positive HCP engagement outcomes, enabling data-driven coaching: "MSLs who ask for HCP perspectives on emerging biomarker data before presenting clinical trial results achieve 35% higher follow-up meeting rates." This transforms anecdotal coaching into precision skill development.
For sales representatives in Lilly's commercial organization, voice technology might address unique compliance challenges. The highly regulated promotion environment requires documented evidence that sales messaging remains within FDA-approved labeling and promotional review board approvals. Voice-enabled CRM systems (integrating tools like Maggie Voice Bot deployed by 500+ pharmaceutical field representatives in India) capture sales call details through natural conversation: "Visited Dr. Patel at Memorial Hospital, discussed indication in relapsed refractory setting, left dosing guide and patient brochure." The system validates the interaction complies with approved promotional claims, automatically generates call notes, tracks sample distribution, and identifies potential off-label promotion for compliance intervention.
Medical information services examples
leverage conversational AI to scale inquiry handling while maintaining medical accuracy. Pfizer's multi-country deployment demonstrates the maturity of this approach:
Medibot (US): Answers temperature stability questions for biologics, critical for pharmacies managing complex storage requirements
Fabi (Brazil): Handles 6,000+ non-technical customer questions monthly, reducing call center volume and providing 24/7 availability
Maibo (Japan): Delivers medical information to healthcare professionals with local language fluency and cultural appropriateness
The architecture employs knowledge graphs connecting products to indications, dosing, adverse events, drug interactions, and supportive scientific evidence. When inquiries arrive via phone, web chat, or voice assistants, NLP extracts intent and entities, the knowledge graph retrieves relevant information verified by medical affairs, and responses undergo safety checks before delivery (screening for potential adverse events or product quality complaints requiring pharmacovigilance reporting). Critical inquiries exceeding the AI's confidence threshold escalate to human medical information specialists, with the AI providing draft response and relevant background information to accelerate human handling.
The integration with pharmacovigilance systems represents a critical safety capability. Conversational AI analyzing medical information contacts automatically identifies potential adverse events through pattern matching on symptom descriptions, temporal relationships to product use, and severity indicators. When detected, the system immediately generates a safety case intake form with verbatim patient/provider statements, prompts the medical information specialist for additional required information, and routes to pharmacovigilance for regulatory reporting. ProPharma reports this automation achieves 40% reduction in safety case processing time, directly supporting Lilly's patient safety obligations and regulatory compliance.
Regulatory Compliance
â€‹
The regulatory landscape for pharmaceutical AI speech technologies crystallized in 2025 with FDA's draft guidance on AI in drug submissions and EMA's final reflection paper, creating clear pathways for implementation while maintaining stringent data protection and validation requirements.
Voice data's classification as both Protected Health Information (PHI) under HIPAA and special category biometric data under GDPR Article 9 necessitates comprehensive privacy-by-design architecture with explicit consent mechanisms, data minimization, and sovereignty controls across Lilly's US, EU, and Asia-Pacific operations.
FDA Framework and Validation Requirements
â€‹
The FDA's January 2025 draft guidance "Considerations for the Use of Artificial Intelligence to Support Regulatory Decision-Making" establishes risk-based credibility assessment criteria for AI systems in drug development.
Voice AI applications fall into three FDA risk categories:
(1) Low-risk administrative uses like transcription of non-critical meetings, requiring basic validation; (2) Moderate-risk clinical documentation and adverse event detection, necessitating comprehensive Computer System Validation per 21 CFR Part 11; (3) High-risk clinical trial endpoints using voice biomarkers, demanding full analytical and clinical validation through the Biomarker Qualification Program.
The FDA explicitly requires "fit-for-purpose" validation demonstrating the AI system reliably performs its intended function within specified contexts. For voice biomarkers as clinical endpoints, this involves establishing analytical validity (consistent measurement across devices, environments, populations), clinical validity (correlation with established clinical outcomes), and clinical utility (improved patient outcomes or trial efficiency). The agency emphasizes continuous monitoring post-deployment, with documented procedures for model drift detection, retraining triggers, and version control maintaining regulatory compliance as AI systems evolve.
EMA Guidance and EU Implementation
â€‹
EMA's September 2024 final "Reflection Paper on the Use of Artificial Intelligence in the Medicinal Product Lifecycle" aligns with FDA principles while adding European-specific requirements. The guidance mandates transparency in AI decision-making processes, with pharmaceutical companies required to document model architecture, training data characteristics, performance metrics, and known limitations. For voice AI processing EU citizen data, GDPR Article 22 grants individuals the right not to be subject to solely automated decision-making, requiring human review loops for any voice-based assessments affecting trial participation or treatment decisions.
The EU Medical Device Regulation (MDR) classification of certain voice biomarker software as Class IIa medical devices (active devices for diagnosis/monitoring) triggers CE marking requirements including technical documentation, clinical evaluation, and notified body assessment. This particularly impacts standalone voice assessment apps used in decentralized clinical trials, requiring Lilly to determine whether voice tools constitute investigational medical devices subject to MDR oversight beyond pharmaceutical regulatory frameworks.
HIPAA and Data Protection Requirements
â€‹
Voice recordings containing patient health discussions or collected during clinical care constitute Protected Health Information under HIPAA, requiring Business Associate Agreements (BAAs) with all technology vendors processing voice data. The minimum necessary standard applies: voice AI systems should access only the audio segments required for their specific function, with automatic redaction of incidental PHI captured in recordings. De-identification following Safe Harbor or Expert Determination methods enables broader research use, though voice characteristics themselves may enable re-identification, limiting true anonymization potential.
HIPAA Security Rule safeguards for voice data mandate: (1) Access controls with unique user identification and automatic logoff from voice interfaces; (2) Encryption of voice data at rest (AES-256) and in transit (TLS 1.3+); (3) Audit logs capturing all voice data access, modification, and transmission; (4) Integrity controls ensuring voice recordings and transcriptions remain unaltered post-capture; (5) Transmission security for voice data crossing network boundaries, particularly relevant for cloud-based speech processing.
GDPR Article 9 and Biometric Data Governance
â€‹
Voice recordings constitute special category biometric data under GDPR Article 9 when used for unique identification or health assessment, triggering enhanced protection requirements. Processing requires explicit consent with clear disclosure of: specific purposes (research, clinical assessment, safety monitoring), data retention periods (typically 10 years for clinical trial data), third-party sharing (CROs, technology vendors, regulatory authorities), cross-border transfers (particularly US processing of EU voice data), and individual rights (access, rectification, erasure, portability).
The consent withdrawal right creates operational complexity: participants must be able to revoke consent for future voice processing while potentially maintaining already-collected data for scientific integrity. Lilly must implement granular consent management distinguishing primary trial purposes (where withdrawal may require study discontinuation) from secondary research uses (where selective withdrawal preserves trial participation). Data minimization principles favor on-device voice processing extracting only necessary features rather than transmitting full audio to centralized servers, reducing privacy risk and simplifying compliance.
Cross-Border Data Transfer Mechanisms
â€‹
Processing voice data across Lilly's global operations requires lawful transfer mechanisms addressing Schrems II decision invalidating Privacy Shield. Options include: (1) Standard Contractual Clauses with supplementary measures (encryption, pseudonymization, access controls) demonstrating US government surveillance laws cannot compromise EU voice data protection; (2) Binding Corporate Rules for intra-Lilly transfers, though these require extensive regulatory approval; (3) Explicit consent for transfers, viable for clinical trial participants but insufficient as sole basis for employee voice data; (4) Data localization with region-specific processing infrastructure, increasing cost but simplifying compliance.
Asia-Pacific Regional Variations
â€‹
China's Personal Information Protection Law (PIPL) mirrors GDPR principles with stricter localization requirements: voice data from Chinese citizens must be stored within China with government security assessments for any cross-border transfer. Japan's Act on Protection of Personal Information (APPI) requires purpose limitation and consent for voice processing but provides GDPR adequacy, simplifying EU-Japan data flows. Singapore's Personal Data Protection Act permits broader commercial use with opt-out consent for non-sensitive applications, though health-related voice analysis likely requires explicit consent comparable to Western standards.
Was this helpful?
Edit this page
Previous
AI Avatars / Digital People
Next
AI Video Generation
Executive Summary
What is AI speech technology
How AI speech technology works
Signal processing and acoustic feature extraction
Transformer architectures for speech recognition
Neural text-to-speech synthesis
Direct speech-to-speech models deliver speed but sacrifice pharmaceutical safety requirements
Pipeline architectures achieve medical-grade accuracy through specialized components and modular optimization
Hybrid approaches combining strengths remain largely theoretical for pharmaceutical applications
The Accuracy-Latency Tradeoff
Recent S2S Developments
Training methods and transfer learning
Voice biomarker extraction and analysis
Integration with multimodal AI systems
Market analysis: vendors, costs, and value proposition
Cloud Platform Providers
Specialized pharmaceutical and healthcare vendors
Voice biomarker companies for clinical endpoints
High-level Value Indicators
Use Case Examples
Research and Discovery
Clinical Development
Manufacturing and Quality
Commercial and Medical Affairs
Regulatory Compliance
FDA Framework and Validation Requirements
EMA Guidance and EU Implementation
HIPAA and Data Protection Requirements
GDPR Article 9 and Biometric Data Governance
Cross-Border Data Transfer Mechanisms
Asia-Pacific Regional Variations
Community
EBA Viva Engage
EBA SharePoint
Questions?
Reach us on Viva Engage!
Copyright Â© 2026 Eli Lilly and Company