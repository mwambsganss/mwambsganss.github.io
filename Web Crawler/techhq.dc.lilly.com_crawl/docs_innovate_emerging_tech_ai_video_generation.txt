Title: AI Video Generation | Tech HQ
URL: https://techhq.dc.lilly.com/docs/innovate/emerging_tech/ai_video_generation
Description: - Last Update: 2025-06-06

AI Video Generation | Tech HQ
Skip to main content
Tech HQ
Innovate
Plan
Solution
Access
Learn
Contribute
Updates
LillyFlow
ğŸ’¡ Tech@Lilly Innovation Pipeline
ğŸ› Our Innovation Stages
ğŸï¸ Fast Start
ğŸ“¡ Emerging Tech
AI Avatars / Digital People
AI Speech
AI Video Generation
ğŸ“¡ Emerging Tech
AI Video Generation
On this page
AI Video Generation
Emerging Tech Report
Last Update: 2025-06-06
Tech Innovation Pipeline
, Enterprise Business Architecture, Tech@Lilly Enterprise
Author: Doug Gorr
Executive Summary
â€‹
Artificial Intelligenceâ€¯video generationâ€¯is emerging as a transformative technology that enables the automatic creation
of video content from text prompts, images, or other inputs. Recent breakthroughs inâ€¯generative AI modelsâ€¯â€“ especially
diffusion and transformer-based models â€“ have dramatically improved the realism and coherence of AI-generated video
clips. Todayâ€™s leading AI video tools can produce short high-definition videos (typically a few seconds up to ~20
seconds) withâ€¯cinematic visuals, diverse styles, and even synchronized audio. Major AI providers
likeâ€¯OpenAI,â€¯Google,â€¯Adobe, and a wave of startups are rapidly advancing this field. For tech and business leaders, AI
video generation promisesâ€¯significant opportunities: dramatically lowering the cost and time required to produce
marketing videos, training content, or promotional media; enabling mass personalization of video messages; and unlocking
creative concepts that would be impractical with traditional filming. Early adopters in marketing (including pharma
commercial teams) are already experimenting with AI-generated b-roll footage, product visualizations, and even
AI-generated spokespeople. At the same time, this technology raisesâ€¯new challenges and risksâ€¯â€“ from output quality
limitations (e.g. surreal physics or inconsistent characters) to ethical concerns like deepfakes and brand safety. The
current state-of-the-art models still struggle with longer narrative coherence and some realism aspects, but progress is
rapid. Competitors in the US and China are pushing the boundaries: for example, OpenAIâ€™sâ€¯Soraâ€¯model can generate up to
20-second 1080p videos with rich detail, while Googleâ€™s newly unveiledâ€¯Veo 3â€¯model goes further byâ€¯integrating
audioâ€¯(music, sound effects, even speech) directly into generated videos. Well-funded startups like Pika Labs and
industry players like Adobe (with Firefly) are adding features such asâ€¯storyboarding tools,â€¯keyframe control, and direct
integration into creative workflows. Lilly shouldâ€¯pay close attentionâ€¯to this fast-evolving landscape â€“ the companies,
models, and capabilities discussed in this report â€“ to craft strategies for adoption. In the near term, AI video
generators can augment creative teams, accelerate content production, and enable novel marketing strategies, if
organizations also institute proper oversight (to manage legal, ethical, and quality concerns). This report provides a
deep dive into the technology, key players (and their relative strengths/weaknesses), feature comparisons, industry use
cases (with a focus on marketing, training, and pharma applications), and recommendations forâ€¯strategic adoptionâ€¯of AI
video generation.
Introduction
â€‹
The ability for AI toâ€¯generate video contentâ€¯on demand â€“ once a futuristic concept â€“ is now becoming reality. Over the
past few years, generative AI has progressed from creating still images to producing short video clips. In 2022-2023,
early text-to-video research prototypes (e.g. Metaâ€™s Make-A-Video) showed the potential but were limited to a few
seconds of low-resolution, often jittery footage. Fast-forward to 2024-2025, and we see the emergence ofâ€¯commercial AI
video generation toolsâ€¯that anyone can try. These systems take a prompt (text description and optionally reference
images or video clips) and synthesize a new video clip matching the description. Under the hood, most utilizeâ€¯diffusion
modelsâ€¯or advanced transformers that have been trained on vast amounts of video data to learn how to generate
consecutive frames coherently. Essentially, they extend the techniques that made AI image generators (like DALLÂ·E and
Stable Diffusion) so successful, adding a temporal dimension to handle motion and physics.
This technology arrives at an apt time: organizations are hungry for more video content than ever (for social media,
e-learning, personalized marketing), but traditional video production is time-consuming and costly. AI video generation
promises toâ€¯democratize video creation, making it almost as easy as writing a script. A marketer could simply â€œtellâ€ an
AI,â€¯â€œShow a medicine droplet traveling through a bloodstream and destroying viruses, in a cinematic style,â€â€¯and get a
custom animated clip within minutes â€“ something that would have taken a VFX studio weeks to produce. Likewise, a
training team could generate scenario videos with different backgrounds or actors at the click of a button. The
implications span many industries: media and entertainment (AI-assisted filmmaking and pre-visualization), advertising
(rapid content iteration and localization), education (illustrative videos), and more. In pharma marketing specifically,
we envision uses likeâ€¯mechanism-of-action animations, doctor-patient roleplay scenarios for training, or personalized
patient education videos generated for different demographics and languages.
However, itâ€™s important to approach this innovation with open eyes.â€¯Current limitationsâ€¯mean AI-generated videos are not
yet completely indistinguishable from real footage in most cases. Videos are typically short (a few seconds long) and
may exhibit artifacts or surreal glitches, especially for complex motions or human anatomy. Ensuring a consistent
character or narrative across a longer video remains challenging (some models allow chaining multiple short clips, but
true long-form consistency is still an R&D topic). There are alsoâ€¯ethical considerations: generative video opens the
door to hyper-realistic deepfakes or misleading content, which is a particular concern in regulated industries like
pharmaceuticals. The source of content leveraged, the machine learning processes used to create these capabilities,
remains an open legal concern. As with any new technology, Lilly must weigh the potential benefits and risks together
when determining a strategy for use and adoption. As we delve into the technology, differences between tools, and
strategies for adoption, we will highlight both theâ€¯opportunitiesâ€¯andâ€¯risks.
AI Video Generation Technology Overview
â€‹
At its core, AI video generation extends image generation with the dimension of time. Early approaches
involvedâ€¯Generative Adversarial Networks (GANs)â€¯andâ€¯recurrent models, but todayâ€™s state-of-the-art largely
usesâ€¯diffusion models with transformersâ€¯trained on video data. A diffusion model gradually â€œdenoisesâ€ random noise into
a coherent image; for video, this process is applied to a sequence of video frames (often in a latent compressed space
to reduce computation). For example, OpenAIâ€™sâ€¯Soraâ€¯model uses aâ€¯latent video compressorâ€¯and represents video as a series
of â€œspacetime patchesâ€ that a transformer can process. The model is conditioned on a text prompt (and optionally
starting frames or images) and learns to predict the next frames of video that match the prompt. With enough training
(using millions of videos and images), such a model can generate remarkably diverse and complex scenes.
Key architectural innovations have improved video generation quality recently. One is the use ofâ€¯unified training on
images and videosâ€¯â€“ since images are essentially 1-frame videos, including large image datasets helps the model learn
higher resolution details. Sora, for instance, is a â€œgeneralistâ€ trained on both images and videos of variable lengths,
which enables it to output higher fidelity and longer durations (up to ~1 minute internally). Another advance is the
incorporation ofâ€¯spatial-temporal attentionâ€¯mechanisms (as seen in Googleâ€™s and othersâ€™ models) that help preserve
consistency over time. Google DeepMindâ€™s latestâ€¯Veo 3â€¯model reportedly usesâ€¯3D spatiotemporal attentionâ€¯and a
specializedâ€¯3D VAEâ€¯(Variational Autoencoder) to achieve cinema-quality results. In practice, many systems break the task
into two stages: first generate a lower-resolution or lower-frame-rate video with AI, thenâ€¯upscale or interpolateâ€¯it to
HD and smooth frame rates (some platforms include built-in AI upscalers and frame interpolation to reach 1080p30 or
beyond).
Despite these advances, current models have constraints. Mostâ€¯clip lengthsâ€¯are short â€“ e.g. 4â€“5 seconds for many public
tools, sometimes extendable to ~20 seconds with higher tiers or chaining clips. Maintainingâ€¯object permanence and
character consistencyâ€¯across even those seconds is non-trivial (e.g. a personâ€™s face might subtly change between frames
if not controlled). Each frame must not only look plausible on its own but also flow logically from the previous,
respecting physics and causality. This is an area where models still stumble: early-gen systems producedâ€¯warping limbs
and melting objectsâ€¯under motion. Newer models are improving â€“ for example, Minimaxâ€™s video model was noted to handle
motion physics (inertia, shadows, momentum) far more naturally than earlier models. Still, complex sequences (e.g. a
human performing a sports move) can appear unnatural or â€œroboticâ€ if the AIâ€™s understanding of physics isnâ€™t perfect.
Resolutionâ€¯andâ€¯frame rateâ€¯are other technical factors. Producing full 1080p or 4K frames is computationally heavy, so
many models trade off length or fidelity. Runwayâ€™s Gen-2 model, for instance, initially generated only ~480p-equivalent,
low-framerate videos (appearing â€œslideshow-likeâ€) to conserve compute. Today there are models that directly output HD â€“
OpenAIâ€™s Sora Turbo can do 1080pâ€¯and Adobeâ€™s Firefly Video generates 1080p by defaultâ€¯â€“ but usually for very short clips
(~5â€“10 seconds). Some platforms provideâ€¯flexible aspect ratios and orientationsâ€¯(portrait, square, etc.), recognizing
needs for social media content.
Importantly, AI video generators are increasingly handlingâ€¯multimodal inputs: not just plain text prompts, but
alsoâ€¯image inputs (for image-to-video)â€¯andâ€¯initial video clips (video-to-video). Image-to-video means you can give a
single frame or picture and have the model animate it or use it as the scene start. Many tools (Runway, Pika, Sora,
Luma, Adobe, etc.) support this because it helps with consistency and user control â€“ e.g. you can provide a character
image to ensure the generated video revolves around that characterâ€™s appearance. Video-to-video allows taking an
existing video and transforming it (changing style or continuing it beyond original length). For example, Runway Gen-1
would take an input video and apply an AI-generatedâ€¯â€œstyle transferâ€â€¯to turn real footage into an animated look. Several
platforms now let youâ€¯extend videosâ€¯by generating new frames after the last frame (both Sora and Googleâ€™s Flow have a
feature toâ€¯â€œextendâ€â€¯orâ€¯â€œcontinueâ€â€¯a clip seamlessly). These workflow features open the door toâ€¯storyboard-driven
generation: creators can stitch multiple AI clips into a longer sequence, maintaining some continuity by carrying over
last frames as the next clipâ€™s start.
Another frontier isâ€¯audio integration. Until recently, AI video tools have only produced silent clips â€“ adding sound was
a separate task. Googleâ€™s Veo 3 made headlines by generating synchronized audio along with the video. This includes
sound effects and even rudimentary voices or music that match the scene, greatly enhancing realism and emotional impact.
While not yet common across all platforms, this indicates a trend toward full multimedia generation (Google achieved
this by drawing on its text-to-audio research and perhaps its new Gemini multimodal AI to understand the scene context).
Adobeâ€™s approach, meanwhile, integrates anâ€¯AI voice model for dubbing/translationâ€¯rather than generating sound effects
for imaginary scenesâ€“ an example of focusing on practical editing needs like translating a videoâ€™s narration into
multiple languages with matching lip sync. We anticipate more convergence ofâ€¯AI video + audio, enabling one-shot
creation of a complete video with soundtrack.
Finally,â€¯content moderation and safetyâ€¯are built into the technology stack of responsible providers. Providers like
OpenAI and Google constrain their models to avoid disallowed content (e.g. extreme violence, pornographic or hate
content), and they addâ€¯watermarks or metadataâ€¯to identify AI-generated videos. OpenAIâ€™s Sora, for example, attaches C2PA
metadata tags to all generated videos for transparency. Adobe trains Firefly on licensed, uncontroversial content to
make it â€œcommercially safeâ€ and avoid IP violations. These safeguards are critical given how easily video can mislead;
they are also important for adoption. For example, Lilly would have to ensure any AI-generated visuals do not
inadvertently create off-label claims or inappropriate imagery. Additional layers of content moderation/control/filters
will remain a Lilly-specific need.
In summary, theâ€¯key capabilitiesâ€¯of AI video generation to be aware of include multiple dimensions to be evaluated as we
approach new models and tools, including but not limited to:
quality of visual output (resolution, frame rate, fidelity)
ability to portrayâ€¯realistic physics and lighting
options forâ€¯camera movement/angles
degree ofâ€¯controlâ€¯via prompts and references (storyboards, keyframes, etc.)
support forâ€¯audioâ€¯output and moderationâ€¯features
Major Players and Platforms in AI Video Generation
â€‹
The AI video generation ecosystem in 2025 spans Big Tech companies, well-funded startups, and open-source projects.
Having surveyed these major players â€“ OpenAI, Runway, Google, Kuaishou (Kling), Pika, MiniMax, Luma, Stability, Adobe â€“
itâ€™s evident that the field is crowded with innovation.
Each brings something slightly different: OpenAI and Google push the frontier of capability, Adobe and Luma focus on
creator workflow integration, startups like Pika and MiniMax carve out specialized strengths or faster iteration, and
open projects like Stability ensure thereâ€™s room for custom solutions.
Below we break down the notable models/tools and their current state-of-the-art capabilities, as well as how they
differ:
OpenAI â€“â€¯Sora
â€‹
OpenAIâ€™sâ€¯Soraâ€¯is a cutting-edgeâ€¯text-to-videoâ€¯model introduced in early 2024 and moved out of research preview by the
end of last year. It is designed as a general-purpose video generator that can take text (and optionally image/video
inputs) and produce new videos. Sora operates as a consumer product via a web interface (ChaGPT.com and Sora.com) and
through API integration with OpenAI and the Microsoft Azure platform. The latter of which, seems to be the most likely
avenue for Lillyâ€™s adoption.
Sora was initially capable of up toâ€¯1 minuteâ€¯of video in research settings, but for end-users the service currently
allows generating clips up toâ€¯20 secondsâ€¯long at up toâ€¯1080pâ€¯resolution. Users can choose aspect ratios (widescreen,
square, vertical) and evenâ€¯bring their own assetsâ€¯â€“ e.g. provide an initial or final frame, or a short video to
beâ€¯extended or blendedâ€¯into new content. OpenAI has emphasizedâ€¯Soraâ€™sâ€¯prompt adherence and visual quality, aiming to
closely match the input description while keeping outputs photorealistic or stylistically high-quality. The model
leverages aâ€¯diffusion transformerâ€¯architecture and was trained on an extremely large dataset of image and video content,
making it a â€œgeneralistâ€ that can handle everything from cartoon-like clips to natural scenery.
One of Soraâ€™s distinguishing features is a built-inâ€¯storyboard toolâ€¯in its interface. This allows users to script out a
sequence of scenes or specify key frames, giving more control over each moment in a longer narrative. For example, a
user could specify that at second 0 a character is in Location A, and by second 5 the character moves to Location B, and
Sora will try to fill in the transition. Such tools help mitigate the â€œconsistencyâ€ problem by explicitly guiding the
model. Sora also supportsâ€¯image-to-videoâ€¯(you can upload an image to influence the first frame or style)
andâ€¯video-to-videoâ€¯(upload a short clip to have Sora modify or continue it) generation, making it flexible in creative
workflows.
Strengths:
Sora is backed by OpenAIâ€™s leading research, so it benefits from state-of-the-art training and
fine-tuning. It produces impressivelyâ€¯detailed and vivid visualsâ€¯across a range of subjects â€“ reviewers have showcased
example prompts from cinematic city scenes to fantasy creatures, rendered with convincing lighting and depth of field.
It can maintain coherence for longer durations than many competitors (20s or more), and prompt alignment is generally
strong. The integration with ChatGPT means a large user base can experiment easily, and OpenAI has put in place
robustâ€¯content moderationâ€¯andâ€¯origin taggingâ€¯(every Sora video is cryptographically tagged as AI-generated). OpenAIâ€™s
brand has also lead the GenAI field and continues to innovation across a variety of AI models and advanced capabilities.
Additionally, OpenAIâ€™s partnership with Microsoft makes it the most viable option for large enterprises, like Lilly.
Weaknesses:
Despite its prowess, Sora (like others) still has limitations. OpenAI itself notes that the model can
struggle withâ€¯realistic physics and long complex actionsâ€¯â€“ e.g. if a prompt involves a very intricate movement sequence,
Sora often introduces artifacts or odd motion. Early testers observed that human figures from Sora could have a stiff or
unnatural motion compared to real video. Also, because of the computational cost, generating 20 seconds of HD video
isâ€¯resource intensive.â€¯Theâ€¯cost structureâ€¯for heavy use is thus non-trivial; Lilly may want to negotiate custom pricing.
Another consideration is that Sora is aâ€¯closed modelâ€¯â€“ unlike some open-source alternatives, you cannot self-host or
retrain it on proprietary data, so there is reliance on OpenAIâ€™s innovation focus and reseller partner, Microsoft.
Overall, Sora represents theâ€¯high-quality, generalist approachâ€¯to AI video: it aims to handle as many scenarios as
possible with good quality and provide a safe, managed platform for users. Itâ€™s a bellwether for where mainstream AI
video tech is, demonstrating realistic scene generation and moderate length, with expectation of rapid improvement as
models scale further.
Runway â€“â€¯Gen-2â€¯(and beyond)
â€‹
Runway ML is a startup well-known among creatives for providing accessible AI tools, and they were one of the first to
offerâ€¯text-to-videoâ€¯generation to the public. Runwayâ€™sâ€¯Gen-2â€¯model (launched mid-2023) garnered attention as a
commercially available text-to-video system available via a simple web interface. It followed their earlierâ€¯Gen-1, which
was limited to video stylization (applying AI-generated styles to existing videos). Gen-2 marked the ability to generate
completely new video frames from a text prompt or an image prompt. Users could sign up on Runwayâ€™s website and generate
short clips (initially ~4 seconds long) for free or with a subscription. This early availability made Runway Gen-2 a
popular choice for designers and artists to experiment with AI video in 2023.
Gen-2 outputs a few seconds of video atâ€¯low resolutionâ€¯(around 720Ã—1280 or less) and without sound. It supportsâ€¯text
promptsâ€¯as well as anâ€¯image + text comboâ€¯â€“ for example, you can upload a reference photo and ask it to animate that
scene with some described action. The generation process is cloud-based and quick (often under a minute to get results).
Runwayâ€™s interface emphasizes ease of use: you enter a prompt, hit generate, and get a preview you can download as an
MP4. Gen-2 can produce someâ€¯artistic and creative results, but it has notable limitations. Reviewers found that Gen-2â€™s
videos were oftenâ€¯choppy in frame rateâ€¯(almost like a fast slideshow) andâ€¯grainyâ€¯in appearance. This appears to have
been a trade-off to make the service widely accessible (lowering GPU requirements by limiting quality). Gen-2 also
struggled withâ€¯complex promptsâ€¯â€“ it might latch onto one word and ignore nuance, and it failed at precise instructions
like â€œa slow zoom-inâ€ (the output didnâ€™t execute the camera move). Consistency issues were present: objects could warp,
and human figures often had surreal artifacts (blended limbs, etc.).
Runway has been continuously improving its models. By late 2023, they were experimenting with features to increase
coherence (some community members referred to a Gen-3 or new features like â€œlast frame continuityâ€ â€“ allowing one
generationâ€™s last frame to carry into the next, somewhat like what others call extend or storyboard). Itâ€™s reported
thatâ€¯Runway Gen-4â€¯is on the horizon, aiming for higher quality and longer duration, but as of early 2025 theâ€¯Gen-2â€¯model
(with incremental upgrades) is whatâ€™s publicly available. Runwayâ€™s comparative advantage is beingâ€¯creator-focused: their
platform is an all-in-one creative suite, with tools for video editing, composting, and AI effects. For instance, Runway
offersâ€¯green screen background removalâ€¯andâ€¯image generationâ€¯in the same app, so a user can mix AI video with other
editing easily. Theâ€¯priceâ€¯of Runway is subscription-based, with a free tier that gives some generation credits and paid
tiers for more usage.
Runway shows theâ€¯accessibilityâ€¯side of AI video. It may not have the absolute best fidelity, but it pioneered getting
the tech into usersâ€™ hands. This has allowed a community of artists to develop around it (for example, the first AI
music videos were often made with Runway Gen-2, accepting the rough edges as an aesthetic). Runwayâ€™s trajectory suggests
they will continue to close the quality gap. Notably, Runway collaborated in the development of Stable Diffusion (image
model) previously, indicating strong AI research chops. We anticipate Runwayâ€™s future models will improve frame
smoothness and possibly length (their roadmap mentions working towards 15+ second clips). Businesses might use Runway
currently forâ€¯quick prototypes or social media contentâ€¯where a slightly stylized or surreal look is acceptable. However,
for polished, photoreal marketing videos, other solutions (or further model maturity) would be needed.
Google â€“â€¯Veo 3â€¯andâ€¯Flow
â€‹
Google entered the AI video arena through its DeepMind team, and in May 2025 it announcedâ€¯Veo 3, a next-generation video
model, along with a tool calledâ€¯Flowâ€¯for creators. Veo 3 represents one of the most advanced text-to-video models to
date, notable for deliveringâ€¯high realism and integrated audio. In tests, Veo 3 generated videos ofâ€¯realistic human
actors with synchronized sound and music, a leap in overall cinematic quality. For example, an Ars Technica report
describes Veo 3 producing a clip of a person that was startlingly close to real footage, complete with background music
that fit the mood. This puts Veo 3 at the cutting edge, potentially ahead of OpenAIâ€™s and othersâ€™ models in certain
aspects as of mid-2025.
Veo 3 can generate HD video content with accompanying audio in response to a prompt. It excels atâ€¯text rendering within
videoâ€¯(e.g. generating legible signage or captions in the scene â€“ something many image AIs fail at); testers found Veo 3
more consistently renders written text correctly in frames compared to others. The audio component means if your prompt
implies a sound (â€œa dog barking in a parkâ€), the output video might have a barking sound. This dramatically increases
the emotional impact and usefulness of the raw generations. Veo 3â€™s visual fidelity is extremely high:â€¯natural motion,
better handling of complex scenes (e.g. multiple characters or dynamic camera movements) and fewer obvious glitches have
been reported. It uses heavy-duty AI (likely an evolution of Googleâ€™sâ€¯Imagen Videoâ€¯research combined with their Gemini
multimodal model) and thus is computationally demanding â€“ one test noted ~5 minutes or more to generate a clip with
Googleâ€™s servers.
Flow
â€‹
To make Veo 3 accessible, Google launchedâ€¯Flow, an AI filmmaking web app. Flow is essentially the user interface and
workflow around Veo (and other models like Googleâ€™s Imagen image model). Itâ€™s available to Googleâ€™s AI Cloud subscribers
(Pro and Ultra tiers) as a beta tool. Flow is built â€œwith and for creativesâ€ and introduces robust storytelling
features: you can generate individualâ€¯shotsâ€¯usingâ€¯Text to Video,â€¯Frames to Video, orâ€¯Ingredients to Video.â€¯Frames to
Videoâ€¯lets you specify aâ€¯start and end frameâ€¯(essentially keyframes) and the AI will animate the transition.â€¯Ingredients
to Videoâ€¯means you can feed the AI specific elements â€“ e.g. an image of a character or an art style â€“ and it will ensure
the output video includes those elements. This modular approach helps keep characters consistent and styles unified
across scenes. Flow also has aâ€¯Scene Builder: once you have a clip you like, you can add it to Scene Builder and then
either â€œExtendâ€ the scene (Flow will use the last second of frames to continue the action into a new clip) or â€œJump toâ€
a new scene (preserving context like the main character, but allowing a change of setting or action). This approach
leverages Googleâ€™s powerful Gemini AI behind the scenes to maintain coherence when transitioning shots. In effect, Flow
allows the creation of a multi-shot short film completely within the AI tool, something quite revolutionary. When
finished, users can upscale their clips to 1080p and download them or even get GIF previews.
Strengths:
Googleâ€™s solution is arguably the mostâ€¯comprehensiveâ€¯and advanced: highest quality visuals withâ€¯audio,
and a pro-oriented workflow that tackles many prior limitations (short length, lack of consistency, etc.). The
integrated audio is a standout â€“ in comparisons, testers found scenes from Veo 3 much more engaging due to background
sounds and music, even when Kling (a competitor) had slightly better prompt accuracy in some cases. Veo 3 also shows
strength inâ€¯cinematography: it tends to choose cinematic camera angles and color grading that enhance the promptâ€™s mood.
This may be due to training on film footage or specific tuning. Googleâ€™s cloud infrastructure means it can roll this out
at scale when ready (currently itâ€™s limited to certain users but expected to expand). Google will likely integrate these
capabilities into Google Cloud offerings (GCP), making it easier to use via API or within Googleâ€™s productivity tools in
the future.
Weaknesses:
The main drawbacks areâ€¯accessibility and cost. As of now, Flow (with Veo 3) is not open to everyone â€“
itâ€™s an exclusive service for paid subscribers, likely with limited invites. The compute intensity also means itâ€™s slow
(minutes per generation) and presumably expensive; Googleâ€™s tiered credits system in Flow indicates heavy usage could
rack up costs (Flow has â€œFastâ€ and â€œHighest Qualityâ€ modes, with Highest using Veo 3 and consuming more credits). In
testing, system congestion led to errors for Veo 3 generation, implying the service can be bottlenecked. Another
consideration is that while Veo 3 pushes realism, itâ€™s not infallible â€“ some prompts found itsâ€¯prompt adherenceâ€¯weaker
than a rival in specific aspects. For example, in a test prompt of â€œa woman runs away from a giant spider,â€ Klingâ€™s
model actually showed the woman running away from the spider (as instructed) while Veo 3â€™s output had the elements but
not the exact action. So creative control might still need multiple attempts or careful prompt tuning. Also, if using
Flowâ€™s image-to-video via â€œIngredientsâ€ mode, note that those videos apparently comeâ€¯without audioâ€¯(so the audio
generation only occurs in pure text-to-video mode, at least currently).
Googleâ€™s entry with Veo 3 is a sign thatâ€¯AI video is reaching new heights. Lilly should watch this space because what is
a limited release today could be a widely available tool in a year. If Google integrates Flow into its ecosystem it
could accelerate adoption dramatically. In terms of strategy, Googleâ€™s heavy emphasis onâ€¯storytelling workflowâ€¯(e.g. the
Scene Builder and keyframes) suggests that even as AI gets more powerful,â€¯human creative direction remains crucialâ€¯â€“ the
winners will be those who combine AI capabilities with clear narrative guidance.
Kuaishou â€“â€¯Kling
â€‹
Kling is an AI video generator developed byâ€¯Kuaishou, a major Chinese social media and short-video company (often seen
as a rival to ByteDance). Kuaishou launched Kling in 2024 and has rapidly iterated on it; by mid-2025 they
releasedâ€¯Kling 2.1, explicitly targeting the leadership in AI video generation. Kling has gained a reputation for
producingâ€¯cinema-grade short videosâ€¯and is widely used in China for creating meme videos, animated satires, and more. In
fact, Kling became popular among meme-makers to animate political satire videos of public figures (e.g. face-swapping
famous people into movie scenes). This hints at Klingâ€™s strengths in generating reasonablyâ€¯realistic human
charactersâ€¯and its permissiveness (though presumably with some safeguards).
Kling stands out for allowing relativelyâ€¯long outputs â€“ up to 2 minutesâ€¯of video in its latest version, far surpassing
most Western counterparts currently. It also supportsâ€¯high resolution (1080p)â€¯generation and smooth frame rates (25+
FPS). To handle different use cases, Kling 2.1 is offered in multiple quality tiers. According to a review, there
areâ€¯Standard,â€¯Professional, andâ€¯Masterâ€¯modes: Standard produces 720p ~5-second clips, Pro yields 1080p with good
quality, and Master (the most advanced) also at 1080p but with maximum fidelity and consistency. The Master tier uses
the latest model with those 3D spatial-temporal techniques we mentioned, and it is slower and costlier (100 credits for
5s, vs 35 credits for Pro). Notably,â€¯prompt adherenceâ€¯in Kling 2.1 is reported to be excellent â€“ it captures complex
instructions well and keeps on-script better than many models. For instance, if asked to show specific text on a robotâ€™s
body and a certain action, Kling usually gets the details right (especially in Master mode). It does have some
difficulty if the text or element is not the main focus (it might drop minor details when focusing on bigger things),
but overall itsâ€¯accuracy to the prompt is a selling point.
Kling offers a rich set of features akin to others:â€¯text-to-video, image-to-video, video extension, camera control, even
face and lip-sync capabilities. The platform lists tools likeâ€¯Motion Brushâ€¯(perhaps to guide motion in a specific
region),â€¯Start/End Frameâ€¯input (like keyframes),â€¯Virtual Try-Onâ€¯(indicating a specialization in changing outfits on
people in video), andâ€¯Lip Syncâ€¯which suggests you can input audio dialogue and have a generated character speak it. This
aligns with Kuaishouâ€™s needs for user-generated content (imagine an app where users can type a script and have an AI
avatar video speak it). Kling also provides anâ€¯API, meaning developers can integrate it into apps or pipelines. The
volume of content generated is huge â€“ by one report, Kling had already output over 10 million videos within a short time
of launch, reflecting its popularity on a large social platform.
Strengths:
Klingâ€™sâ€¯natural motion and authenticityâ€¯have been highlighted. A Decrypt comparison noted Kling 2.1
produces footage that looks genuinely cinematic, with characters moving naturally and complex action sequences playing
out without obvious AI artifacts. Emotions on faces feel more authentic than prior models in some tests. Also,
Klingâ€™sâ€¯fast pace of improvementâ€¯is notable: within one year, Kuaishou jumped from version 1.0 to 2.1, each time
tightening the gap with the cutting edge. The multi-tier offering is smart for business use â€“ one can use cheaper modes
for drafts and then upscale final output with Master mode. For an enterprise, Kling could be cost-effective especially
for image-to-video tasks; in fact, tests found Kling 2.1â€¯excels at image-to-video conversionâ€¯(turning a static image
into a moving scene) relative to peers. Itsâ€¯camera movements and VFXâ€¯are also advanced; for example, it can handle
dynamic shots (one test had it do a time-lapse city transformation with camera movement â€“ challenging, but it partially
succeeded). The presence of specialized effects (like presumably theâ€¯Elementsâ€¯orâ€¯VFX presetsâ€¯on their site) means it
might offer fun creative filters natively.
Weaknesses:
One limitation isâ€¯accessibility for non-Chinese users. Klingâ€™s official interface is primarily in
Chinese (though Pollo AI, a third-party site, provides an English gateway to try Kling models). Payment and support
might be geared towards Chinese market, so international enterprise adoption could be tricky at the moment. Another
factor: while Kling leads in some metrics, Googleâ€™s Veo 3 has the edge in integrated audio and possibly in overall
â€œatmosphereâ€ (color grading, etc.). In a direct face-off, each had scenarios where it did better: Kling was better at
strictly following the action described, whereas Veoâ€™s clip with audio delivered a more cinematic feel even if it
deviated slightly. Also, Klingâ€™s Master mode, though high-quality, isâ€¯compute-heavyâ€¯â€“ some users might find the wait
times (and costs) for the highest quality prohibitive for very iterative work. Lastly, as an AI that can do
deepfake-like outputs (given meme usage and lip-sync), one must ensure ethical use. Itâ€™s likely Kuaishou has moderation
on their platform, but if one is using the API, compliance with any deepfake laws (like requiring disclaimers for
AI-generated realistic people) is necessary.
In summary, Kling is a powerhouse in AI video and a direct competitor to Sora and Veo. For our global company, Kling
demonstrates thatâ€¯the Chinese market has an opportunity to leverage models not accessible to all geographies. Minimally,
itâ€™s a model to benchmark against. Lilly should keep an eye on whether Kuaishou or partners make Kling available
globally, and on the kinds of content it enables (e.g. itâ€™s very adept at producingâ€¯short-form, eye-catching clipsâ€¯which
is exactly the content driving social media engagement).
Pika Labs â€“â€¯Pika Video
â€‹
Pika Labsâ€¯is a startup that has quickly risen in the AI video field. Co-founded by two PhD students in 2023, Pika Labs
made waves by mid-2024 when it secured aâ€¯US$135 million funding round, valuing the company at $470M.â€¯This substantial
backing (notably involving investors from both the US and Asia) signals that Pika is viewed as a serious contender.
Pikaâ€™s AI video generator â€“ often just calledâ€¯Pika AIâ€¯â€“ is known for beingâ€¯user-friendlyâ€¯and creatively versatile. Itâ€™s
offered via a web interface and API, and has even been integrated into third-party creative apps (e.g. theâ€¯Captionsâ€¯app
integrated Pika to allow users to generate b-roll clips from text inside a video editing workflow).
Pika supports bothâ€¯text-to-video and image-to-video, similar to others. It can generate approximatelyâ€¯5-second clips by
default, and the team has been extending that (the platform notes up to ~2 minutes by chaining or iterating, likely in
their newer version 1.5+). The resolution it works with is aroundâ€¯720p (1296Ã—720)â€¯in current versions for generation.
Pikaâ€™s style flexibility is a strong suit â€“ it can doâ€¯â€œvarious styles, such as cinematic, animated, or cartoonishâ€â€¯just
by adjusting the prompt or selecting presets. They have been updating the model rapidly (the Pollo AI interface shows
Pika v2.2 as latest, implying several version jumps). One unique feature Pika introduced isâ€¯â€œPika Effectsâ€
(dubbedâ€¯Pikaffects). These are essentially AI-drivenâ€¯video effects that manipulate objectsâ€¯in the generated scene. For
example, users can apply effects likeâ€¯Inflate, Melt, Explode, Squash, Crush,â€¯or evenâ€¯â€œCake-ifyâ€â€¯to objects. This
suggests if you generate a video of, say, a statue, you could then have it melt or explode in a physically plausible way
via the AI â€“ an exciting tool for dynamic visuals and likely very popular for fun content creation. This focus
onâ€¯object-level controlâ€¯is somewhat unique to Pika and caters to short-form video creativity (imagine TikTok-style
visual gags, etc.).
Pikaâ€™s interface is noted to beâ€¯intuitive for newcomers. They likely have templates or guided workflows (enter your
prompt, choose a style, etc.). Also, Pika Labs has emphasizedâ€¯continuityâ€¯features: much like others, you can use the
last frame of one video as the first of the next to string together longer scenes with a persistent character. A
community of users have tested Pikaâ€™s capability on things like B-roll (it does well in generating generic footage like
tech product pans), cartoon animation (smooth results), and abstract visuals.
Strengths:
Pika appears to combineâ€¯research-grade tech with a product mindset.â€¯Raising $135M so early means they
have resources to push the modelâ€™s quality and scale quickly. Already by June 2024, Pikaâ€™s results were impressing many
â€“ it producesâ€¯smooth camera movements and lightingâ€¯effects, and in some comparisons was among the top for certain tests.
For instance, a Medium reviewer noted Minimax and Pika as two standout platforms that â€œactually workâ€ for creators
needing reliable output. Pikaâ€™s multi-style capability means marketers or creators can use it for a range of content: a
realistic stock-footage-like clip for one project, a whimsical cartoon for another. Theâ€¯Pikaffectsâ€¯give an extra
dimension of creativity, allowing users to generate not just static scenes but scenes with anâ€¯effect or
transformationâ€¯(great for grabbing viewer attention). Pika also supportsâ€¯API integration, which means businesses can
build it into their own tools or pipelines (for example, an e-learning platform could call Pikaâ€™s API to generate custom
video illustrations on the fly for course content). Finally, Pikaâ€™s background â€“ co-founders with strong academic
credentials (one profile mentions a co-founder, Wenjing Guo, is a Harvard CS grad lauded as a â€œgenius girlâ€ in China) â€“
suggests a talent advantage and possibly cross-collaboration between US and Chinese AI communities.
Weaknesses:
While Pika is promising, itâ€™s still anâ€¯upstartâ€¯in a field with emerging giants. Its model may not yet
match the absolute fidelity of OpenAI or Googleâ€™s latest â€“ for example, resolution being 720p vs others pushing 1080p,
and itâ€™s unclear if Pika has audio generation (likely not at the moment; most mention is about visuals). Some features
in Pika are marked as â€œv1â€ or â€œv1.5â€ in comparison tables, indicating they are in active development (e.g. perhaps
motion brush or mid-frame control is not fully there yet). Also, as a smaller company, Pika might not have as extensive
content filtering or enterprise support as OpenAI, Adobe, and Google, so Lilly would need to vet its outputs carefully
for IP or moderation issues. The competitive landscape is another challenge â€“ multiple competitors means Pika needs to
differentiate; it seems to do so with effects and ease-of-use, but others can emulate those too. Pikaâ€™s strength in
â€œvarious stylesâ€ could also mean it might not yet dominate any one style in quality (jack of all trades risk).
For Lilly, Pika Labs is a startup to watch or possibly partner/invest/acquire. Its agility means new features (like
those quirky video effects) come out quickly, which could be leveraged for marketing novelty. For instance, our creative
teams might use Pika to generate an attention-grabbing visual clip (imagine a pill that â€œexplodesâ€ into confetti to
symbolize treatment success â€“ an effect Pika might handle well). With its sizable funding, Pika could also become a
takeover target or major player globally. The key will be how they scale their service and continue improving model
performance.
MiniMax â€“â€¯Hailuo Video-01
â€‹
MiniMaxâ€¯is a Chinese startup (behind the model namedâ€¯Hailuo Video-01) that has also made a name for itself among AI
video creators. Often just referred to as â€œMiniMax video generator,â€ it gained recognition in late 2023 for delivering
impressivelyâ€¯realistic motion continuityâ€¯at 720p resolution. MiniMaxâ€™s approach appears to prioritize the fundamental
challenges of video generation: physics, continuity, and camera work.
Hailuo Video-01, as offered by MiniMax, generatesâ€¯high-definition (720p) videos at 25 fpsâ€¯and supports
bothâ€¯text-to-video and image-to-videoâ€¯modes. A typical output is ~5 seconds long, but users have successfully stitched
clips to create longer sequences (one user made a 30-second video by concatenating 5s segments, with seamless flow).
MiniMax particularly shines withâ€¯cinematic camera movementsâ€¯â€“ reviewers were impressed by how well it handled complex
pans and drone-like aerial shots while keeping the scene coherent. For example, flying over a beach then tilting toward
a waterfall in one continuous motion was executed smoothly. The model also excelled atâ€¯B-roll style footage: generating
a professional-looking slow-motion laptop product shot with correct focus and lighting transitions. This indicates
strong understanding of focus depth, blur, and other cinematographic details.
In terms of content, MiniMax did very well withâ€¯cartoon animations and abstract visuals. It produces smooth,
high-quality cartoon motion, making it great for creators of animated shorts. And for abstract or surreal scenes, it
maintained immersive and stunning visuals. When it comes to continuity, MiniMax has a feature where you can take the
last frame of a clip and feed it as the first frame of the next generation â€“ this allowed that 30-second continuous
video with a character, where the character stayed consistent throughout. That consistency is a major plus; the user
essentially stiched longer stories by manual continuity, and the tool worked nicely.
Strengths:
MiniMaxâ€™s biggest strength isâ€¯realistic motion physics. Movements in its videos look real and believable
â€“ the AI seems to have a better grasp of inertia and momentum, avoiding the jerky or floaty feel others sometimes have.
In side-by-side tests, motions that looked robotic in Sora were more lifelike in MiniMax, suggesting their model
architecture may explicitly model physics or was trained on plenty of real video. The continuity (the ability to chain
clips) is another strength, enabling content creators to tell longer stories if needed. Also, MiniMaxâ€™s output quality
for its resolution is high â€“ 720p might sound lower than 1080p, but given many AI videos are still in that ballpark,
MiniMaxâ€™s effective quality is top-tier within that. It is also reportedlyâ€¯free or low-cost to tryâ€¯(it was available on
platforms like Anakin.ai for users to experiment), which has helped it gain traction among the community.
Weaknesses:
MiniMax isnâ€™t as globally known as some others, and documentation in English is sparse.
Itsâ€¯image-to-video with humansâ€¯was noted as a weak spot â€“ when given a real human photo to animate, the results were not
very convincing (unnatural transitions, lack of detail). It performed better withâ€¯cartoonish imagesâ€¯than real human
photos. So for tasks like animating a real personâ€™s photo, it may not be the best (other specialized avatar tools or
Synthesia might do better). Also, MiniMax had difficulty withâ€¯very complex movement scenariosâ€¯â€“ e.g. a football player
kicking a ball, where precise interaction is needed, looked clunky. This suggests limits in handling intricate
multi-object physics (ball interacting with foot etc.). Another limitation is that, at least as of late 2024, MiniMax
outputs were capped to short durations and required manual effort to chain longer videos â€“ it doesnâ€™t inherently
generate a 30s story in one go; you have to curate it scene by scene.
For Lilly, MiniMaxâ€™s existence is proof that there are multiple players beyond the most hyped. If we want to focus on
experimenting with as many options as possible, MiniMax would be a good model to assess for scenarios needingâ€¯believable
movementâ€¯(say you want an AI video of a pill bottle gently rotating and tilting â€“ physics heavy â€“ MiniMax might nail
that). However, since itâ€™s less of a full product and more of a model accessible via certain interfaces or APIs
(Segmind, Replicate, etc.), leveraging it requires engineering or technical integrations if not working with a provider
that offers the model as part of a service. Itâ€™s one of the â€œbehind the scenesâ€ engines that some AI video platforms
could incorporate for their motion strengths.
Luma Labs â€“â€¯Dream Machine (Ray 2)
â€‹
Luma AIâ€¯is known for its 3D scanning and NeRF (neural radiance field) technology, which allows creating 3D models from
phone captures. In 2023, Luma expanded into generative AI with itsâ€¯Dream Machineâ€¯app, aiming to be a â€œvisual thought
partnerâ€ for both images and videos. Lumaâ€™s Dream Machine is unique in that it blends 2D and 3D AI capabilities: it
includes an image generation model (Photon) and a video model (Ray 2) working in tandem. The focus is on giving
creatorsâ€¯fluid controlâ€¯andâ€¯fast iterationâ€¯in making spectacular visuals without needing prompt engineering skills.
Dream Machine allows you to generate bothâ€¯images and videosâ€¯and move seamlessly between them. Notably, you can
useâ€¯natural languageâ€¯to not only create but alsoâ€¯editâ€¯andâ€¯refineâ€¯outputs (e.g. â€œmake it a 90s vibeâ€ or â€œadd a fisheye
lens effectâ€ to modify an image/video). For videos, Lumaâ€™s Ray 2 model is designed forâ€¯fast, coherent motion and
ultra-realistic details. Dream Machine includes robust features forâ€¯controlling outputs: you canâ€¯â€œDirect the perfect
shot with start/end framesâ€â€¯â€“ meaning keyframe control similar to Adobeâ€™s and Googleâ€™s approach. You can alsoâ€¯â€œloopâ€â€¯a
video seamlessly if desired. It emphasizes ability to createâ€¯unique, consistent characters from a single imageâ€¯â€“ so you
provide an image of a character and the AI can generate new scenes with that character consistently present. Thereâ€™s
also support forâ€¯visual style references: you can upload or select reference images for style or specific elements,
guiding the generation.
Dream Machineâ€™s interface touts features likeâ€¯Brainstormâ€¯(the AI suggests ideas or variations if youâ€™re not sure where
to go next), andâ€¯Share & Remixâ€¯which encourages a community aspect (users can share their creations and even the â€œbehind
the scenesâ€ prompt+refs so others can iterate on them). Under the hood,â€¯Photonâ€¯(image model) helps ensure any single
frame or detail is high-quality, whileâ€¯Ray 2â€¯(video model) handles the temporal aspect â€“ this dual approach likely
contributes to theâ€¯sharp, detailed frames and accurate text renderingâ€¯that Luma claims (they specifically listâ€¯â€œsharp
and accurate text renderingâ€â€¯as a feature, implying their model can place legible text in video). They also
highlightâ€¯â€œaccurate lighting and physicsâ€â€¯andâ€¯â€œclean and consistent animation styleâ€, indicating the model was trained
to respect physical realism and maintain a coherent style throughout a clip.
Strengths:
Lumaâ€™s Dream Machine is praised for itsâ€¯polished user experience. Tomâ€™s Guide in late 2024 called it â€œone
of the best interfacesâ€ among AI video platforms. This ease and fluidity could reduce the learning curve for non-AI
experts. Theâ€¯range of controlâ€¯is also a key strength: Dream Machine basically offers every control feature weâ€™ve
discussed â€“ keyframes, reference images, style control, camera movement, looping, region modifications â€“ within one app.
This is very powerful for artists or marketing teams who want to fine-tune outputs to match brand guidelines (e.g.
ensuring a brand mascot stays on-model, or a scene has the exact color palette desired). Moreover,â€¯Ray 2â€¯being a
â€œlarge-scale video modelâ€ indicates it has been trained extensively, likely yielding veryâ€¯high success ratesâ€¯of usable
generations (Luma claims Ray2 outputs are â€œsubstantially more production-readyâ€ than prior gen video AI). Dream
Machineâ€™s ability to generate both images and videos means users can prototype a concept with still images (faster) and
then seamlessly switch to video mode to animate it â€“ this interoperability can save time and ensure consistency between
campaign imagery and videos.
Weaknesses:
Luma Dream Machine might be slightly under the radar in enterprise circles compared to OpenAI,
Microsoft, Google or Adobe. Itâ€™s a newer entrant and was initially iOS-only (though now on web too). Being an app that
targets creatives, it might not yet have the integrations into enterprise tech stacks or the compliance features
businesses need (no mention of watermarking or governance, for instance). Also, itsâ€¯output lengthâ€¯is not explicitly
stated â€“ likely similar short durations, possibly extendable with looping or chaining. If someone wants a 30-second
polished piece, they may have to puzzle-piece multiple generations. Additionally, while having lots of control is great
for power users, it could overwhelm others; Luma tries to mitigate this with the Brainstorm auto-suggestions, but using
all features effectively might require training/experience.
For Lilly, Luma could be a fantastic prototyping tool â€“ e.g. you could create a complex scene of a molecular world or
patient journey, iterating quickly with text prompts and tweaks until itâ€™s right, then generate final video frames. Its
strength inâ€¯â€œexceptional adherence to detailed instructionâ€â€¯means if you have a very specific storyboard or shot list,
Dream Machine might follow it closely. Itâ€™s also worth noting Lumaâ€™s heritage in 3D: they might eventually integrate
true 3D scene generation or AR content, which could be useful for pharma (think interactive 3D MOA visuals). At present
though, Dream Machine is a strong sign thatâ€¯user experience and granular control are becoming differentiatorsâ€¯in AI
video platforms.
Stability AI â€“â€¯Stable Video Diffusion
â€‹
Stability AI, known for open-source image generator Stable Diffusion, has also ventured into video. In late 2023, they
releasedâ€¯Stable Video Diffusion (SVD)â€¯in research preview. This model is based on Stable Diffusionâ€™s image generation
but extended to produce short videos. True to Stabilityâ€™s mission, the model was open-sourced (code on GitHub and
weights on HuggingFace) for researchers and developers. This makes Stable Video Diffusion an important project for those
who wantâ€¯custom or self-hosted AI video solutions.
The initial release of Stable Video Diffusion can generate very short clips â€“ specificallyâ€¯14 or 25 framesâ€¯of video. At
common frame rates, thatâ€™s roughly 0.5 to 1 second of footage, so extremely brief, although one can run it sequentially
to make a few seconds. It allows setting aâ€¯frame rate between 3 and 30 fpsâ€¯for those frames. The focus was on proving
the concept and providing a foundation to build on, rather than competing on length/quality with commercial models.
Stability mentioned the model could handle different frame rates and aspect ratios in principle, and that with
finetuning it could do tasks likeâ€¯multi-view (novel view) synthesisâ€¯â€“ e.g. generating different angles of a scene from
one input image. They positioned it as a base model on top of which many specialized video models might be built,
similar to how Stable Diffusion spawned many fine-tuned image models for various styles.
Stability also launched aâ€¯beta web interfaceâ€¯(as a waitlist) for text-to-video using this model, and an API on their
developer platform. By 2024â€™s end, they announcedâ€¯Stable Video 4D 2.0, which was geared towards â€œ4D generation and novel
view synthesis from a single video inputâ€. This suggests a branch of their work focusing on turning a single video into
a 3D scene or generating new angles (useful for AR/VR or VFX where you need more camera coverage).
Strengths:
The big advantage of Stable Video Diffusion isâ€¯openness and customizability. Researchers have full access
to the model to fine-tune on their own data, which could be useful for a company wanting a model trained specifically
on, say, pharmaceutical TV commercials or lab experiment videos (ensuring output is in-domain). Also, because itâ€™s based
on Stable Diffusion, it benefits from a huge open-source community. Already enthusiasts have built demos like
image-to-video â€œanimationâ€ notebooks and integrated SVD into tools (one example: a free web tool offered
stable-video-diffusion image-to-video with trial runs). Stability claimed that at release, their modelâ€¯â€œsurpass[ed] the
leading closed models in user preference studiesâ€â€¯â€“ if true, thatâ€™s a strong endorsement of quality, though it might
refer to comparisons with earlier or simpler models. Theâ€¯speedâ€¯is decent; Stability said it can create videos inâ€¯â€œ2
minutes or lessâ€â€¯for those short clips, and being lightweight means it could run on accessible hardware eventually (they
often optimize models for consumer GPUs). For a company with skilled ML engineers and low risk tolerance, Stable Video
Diffusion provides a platform to customize without needing to send data to third-party APIs.
Weaknesses:
In terms of raw capability, SVD lags behind the likes of Sora, Veo, or others in this list. The
extremely short length (1 second) and likely lower resolution (their paper suggests training at smaller resolutions)
mean itâ€™s not immediately useful for direct content creation yet â€“ more of a tech demo. It doesnâ€™t produce audio. Also,
using it requires ML expertise; itâ€™s not an off-the-shelf product for end-users. Stabilityâ€™s emphasis on research means
polished features like storyboards or content moderation are left to the implementer. Indeed, one has to be careful: an
open model might produce disallowed content if fine-tuned incorrectly, and the onus is on the user to enforce moderation
or filtering.
Stable Video Diffusion is relevant if you have a strategy aroundâ€¯open-source AIâ€¯and want to possiblyâ€¯build in-house
capability. A pharma company might, for example, fund a project to fine-tune SVD on their library of MOA animation
frames to create a custom model that generates new scientific animations consistent with their style â€“ something you
wouldnâ€™t want to put into a public model for IP reasons. That could be a competitive differentiator long-term. Stability
AI also tends to improve their models iteratively, and with community contributions, so by late 2025 we might see SVD
2.0 that can do several seconds at higher quality. Itâ€™s theâ€¯â€œkeep an eye on this if you want a differentiated AI video
solutionâ€â€¯contender.
Adobe â€“â€¯Firefly Generative Video
â€‹
Adobe is integrating generative AI across its Creative Cloud, and for video content Adobeâ€™s solution is theâ€¯Firefly
Video Modelâ€¯(often just calledâ€¯Generative Video in Firefly). Announced in 2024 and rolled out in beta this April (2025),
Adobeâ€™s tool is a bit different in focus from others: itâ€™s designed toâ€¯enhance video editing workflowsâ€¯(think of it as
an assistant to Premiere Pro users) rather than to replace the video camera entirely. Adobe is leveraging its vast
library of licensed assets (Adobe Stock) to train this model, ensuring the outputs areâ€¯commercially safe for useâ€¯(no
unlicensed content).
As of launch, Fireflyâ€™s generative video supports two main generation modes:â€¯Text-to-Video and Image-to-Video. Users can
enter a descriptive prompt to get aâ€¯5-second, 1080pâ€¯video clip. Crucially, Adobeâ€™s tool allows uploading aâ€¯start frame
and end frameâ€¯to guide the motion. This is essentially keyframe control: for example, you could upload a still image of
a scene as the start and another image for the end, and the AI will animate a transition between them. Firefly also
providesâ€¯drop-down menus for camera shot size and angle, as well as presets for camera motionâ€¯(e.g. pan, zoom, tilt) to
further direct the result. This GUI approach aligns with Adobeâ€™s pro users who may be more comfortable picking settings
than writing long textual prompts for everything.
In addition to generation, Adobe introducedâ€¯Translate Video and Translate Audioâ€¯features with the Firefly launch. These
use an AI voice model toâ€¯dub existing videosâ€¯into different languages (with some lip-sync capability for enterprise
users). While not generation from scratch, itâ€™s a complementary AI feature that can be big for training or marketing â€“
e.g. easily turn an English video into French, Spanish, etc., with the voice and lips aligned. It shows Adobeâ€™s holistic
approach: not just make new content, but modify and repurpose content efficiently.
Adobeâ€™s Firefly video is integrated in the cloud (firefly.adobe.com) as a beta, with plans to bring it into Premiere Pro
later. It emphasizes use cases likeâ€¯filling gaps in footage, generatingâ€¯b-roll of scenery, addingâ€¯atmospheric
elementsâ€¯to existing shots (like generate an overlay of smoke or dust on a green screen). Indeed, the FAQ suggests top
use cases as adding elements, animating static images (e.g. making a waterfall flow), or creating stylized graphics and
text animations.
Strengths:
Adobeâ€™s key advantage isâ€¯seamless integration for creatives. Editors can incorporate generative clips
right inside their familiar tools (eventually in Premiere/AfterEffects), which lowers adoption friction. Also,
theâ€¯ethics and legal safetyâ€¯â€“ every Firefly output is trained on licensed content and can be used commercially with
confidence. This is a huge factor for enterprises worried about copyright (some companies avoid using other AI image
generators due to unclear training data provenance; Adobe solves that). The resolution (1080p) and aspect ratio options
(16:9 and 9:16 supported out of the gate) meet professional standards. Fireflyâ€™sâ€¯camera and keyframe controlsâ€¯mean users
can get precisely the shot they envision â€“ rather than hoping a pure text prompt yields the right camera angle, you can
explicitly say you want a â€œwide shot, overhead angle, slow dolly movement,â€ etc. This dramatically improves reliability
for production use, because it reduces ambiguity for the AI. Additionally, Adobeâ€™s inclusion ofâ€¯translation and voice
AIâ€¯in the suite is a bonus for people who want an all-in-one solution for localizing and creating content.
Weaknesses:
Fireflyâ€™s generative video is currently limited toâ€¯5 secondsâ€¯max for generation, which is shorter than
some competitors like Sora (20s) or Kling (minutes). It is clearly meant for quick clips and fillers, not full scenes.
If a marketing team wants to create a 30-second ad purely from AI, theyâ€™d need to string together many Firefly
generations and likely do some manual stitching. Another limitation is that as of now itâ€™s a separate web beta, not
fully in Premiere â€“ meaning an extra step to use (though this will likely change). The content domain might also be
somewhat constrained: given Adobeâ€™s professional focus, the model might excel at certain types of shots (e.g. landscape
b-roll, simple subject animations) but perhaps not be as wildly imaginative or diverse in style as some others. Early
beta users will certainly find edges to what it can do. Also, because Adobe prioritizes safe training data, the training
set might exclude a lot of web videos â€“ itâ€™s likely biased towards stock footage and professionally created content.
This means it might not have learned some of the â€œinternetâ€™s imaginationâ€ that open models did; whether thatâ€™s a
downside (less breadth) or upside (more reliability) depends on use case.
For Lilly, Adobeâ€™s generative video could be immediately useful inâ€¯post-production enhancement. Imagine you have a live
action commercial but need to add an effect â€“ say a glowing aura around a patient to symbolize improvement â€“ Firefly
could potentially generate that element on a transparent background to overlay. Or if youâ€™re storyboarding a concept,
you can quickly generate video mockups of scenes to show stakeholders, all within the Adobe ecosystem. The translation
feature is directly relevant for global pharma marketing: you could take a video with an English voiceover and get a
multilingual version in minutes, which is often needed for global training or promotion (with the caveat that lip-sync
for different languages is still a work in progress). Adobeâ€™s move also signals thatâ€¯AI video will be a standard tool in
creative departments, not a niche experiment.Shape
Industry Impact and Use Cases
â€‹
The advent of AI-generated video impacts multiple sectors. Here we focus onâ€¯marketing, learning & development
(L&D)/training, and specifically the pharmaceutical industryâ€¯context, while noting broader implications:
Marketing and Advertising
â€‹
Content Volume & Personalization
â€‹
Marketing teams are under pressure to produce more content than ever, tailored to different audiences and channels. AI
video generation can produceâ€¯quick video variantsâ€¯at scale. For example, a global brand campaign normally requires
shooting different versions of an ad for each region â€“ with generative AI, one could create localized versions by
changing the background, actorsâ€™ apparent ethnicity, language of on-screen text, etc., without a reshoot. AI video could
generate the same scene in different hospital settings or swap the text on a prescription bottle label to a different
language, saving huge production costs. Personalized video ads (addressable TV or online ads) could also be
auto-generated: an AI might create thousands of slight variations of a medical device ad, each tuned to a particular
demographic or even individual (with appropriate compliance checks).
Creative Brainstorming & Prototyping
â€‹
AI video is a boon to the creative process. Agencies should be saving time and passing cost savings to their customers
with tools like Sora or Lumaâ€™s Dream Machine toâ€¯storyboard concepts in motion, not just static frames. This makes it
easier for non-technical stakeholders to grasp a concept. Pharma marketing often involves explaining abstract concepts
(like how a drug works in the body). Instead of expensive 3D animations made after weeks of work, a team could prototype
an MOA animation with AI in a day to visualize the idea, then refine it. It accelerates the iteration cycle on creative
ideas. Several companies already use DALL-E or Midjourney for concept art â€“ extending that to video is the next logical
step. Business leaders may appreciate seeing aâ€¯mock commercialâ€¯orâ€¯animated conceptâ€¯early on, which guides
decision-making on whether to invest in a full production or adjust messaging.
Cost and Time Savings
â€‹
Particularly forâ€¯B-roll and filler content, AI can reduce or eliminate the need for stock footage or shoots. Need a
5-second establishing shot of a laboratory at sunrise? Instead of searching stock libraries or setting up a shoot, an AI
could generate one that fits the precise vision. This is what Adobe is targeting â€“ filling timeline gaps. Marketing
videos often have many such moments. Over time, as reliability improves, AI might handle even primary footage for
certain types of ads (especially animated or stylized ones). Lower production cost means marketers can do moreâ€¯A/B
testingâ€¯of video ads by trying multiple versions. Lilly could use AI to make rapid tweaks post-approval (e.g. if a claim
needs a different visual emphasis, an AI could alter the scene accordingly without reshooting, if it doesnâ€™t change the
approved message).
Challenges for Marketing
â€‹
Creatives and brand teams must ensureâ€¯brand consistencyâ€¯â€“ ironically a potential issue with AIâ€™s variability. We likely
need to lock down style guides and possibly train custom models on brand assets to keep outputs on-brand. Thereâ€™s also
reputational risk: poorly made AI videos could reflect badly if they slip through (imagine awkward visuals or errors â€“
it could appear tone-deaf or unprofessional). So likely, in near term, AI video will be used for internal drafts, social
media (where lower fidelity is more acceptable), or supplemental content, while critical campaigns still use high-end
production with AI augmentation. Over time, as quality stabilizes, this will shift.
Learning & Development / Training
â€‹
Scenario Simulations:â€¯In corporate or medical training, video role-plays and scenarios are common (e.g. demonstrating
doctor-patient interactions, or proper use of a device). AI video can produceâ€¯custom training scenarios on demand. For
example, a pharma sales training could generate a video of a physician consultation scenario tailored to a specific
specialty or objection handling â€“ without hiring actors. Tools like Synthesia already generate talking avatar videos
from text (not quite these full scene generators, but adjacent tech). With text-to-video, one could specify the scenario
and have the AI create an original scene. Character consistency becomes key here; a trainer might want the same
AI-generated actor to appear across multiple modules â€“ technologies like DeepMotion or others could even give that AI
actor movement. Googleâ€™s Flow enablingâ€¯Jump to new shot while preserving characterâ€¯is highly relevant. That means you
could maintain the same virtual â€œpersonâ€ in different situations.
Localized and Updated Content:â€¯L&D often needs to deliver content in multiple languages and keep it updated regularly.
Using generative video plus integrated voice translation (as Adobe Firefly offers), a training department can take an
English training video andâ€¯automatically generateâ€¯the French, Spanish, Chinese versions with dubbed voices and adjusted
visuals if needed. The AI can even lip-sync the translated speech to the video (Adobe is working on that for
enterprise). Additionally, if a procedure changes or guidelines update, new training videos can be spun up quickly by
editing the prompt or script, rather than reshooting footage. This is particularly useful in pharma where information
changes (new study data, new regulations requiring a tweak in message, etc.).
Micro-learning and Personalized Learning:â€¯With AI, itâ€™s plausible to generateâ€¯personalized training videosâ€¯for employees
or customers. Imagine an onboarding video where the scenarios depicted are customized to that employeeâ€™s role or common
knowledge gaps. Or patient education videos that adjust explanations to the patientâ€™s literacy level or cultural context
â€“ AI could alter the scenery or analogies in the video accordingly. Such on-demand generation was impossible at scale
with traditional methods. Pharma companies doing patient support programs could benefit by tailoring educational content
(for example, showing a patient of the same demographic in the video for relatability, which AI can swap out easily).
Challenges in Training:â€¯Ensuringâ€¯accuracyâ€¯is paramount â€“ any AI hallucination or incorrect depiction in a training video
could misinform, which in pharma could have serious consequences. Thus, for factual or procedural content, AI outputs
must be carefully reviewed, or possibly a hybrid approach used (AI generates visuals but humans script it tightly and
verify every frame). Thereâ€™s also an emotional quality aspect: human-made training videos use real actors to connect
emotionally. AI avatars are improving, but thereâ€™s still an â€œuncanny valleyâ€ risk if not done well. Over time, as
character generation improves, this will lessen. Another challenge is platform adoption: training departments will need
new workflows to incorporate AI generation tools, and staff will require some upskilling (e.g. learning prompt-writing
and basic editing of AI outputs).
Commercial and Promotional
â€‹
The pharma industry, being highly regulated, will approach AI video with both excitement and caution. Hereâ€™s how it
specifically stands to gain or face challenges:
Mechanism of Action (MOA) and Scientific Visualization
â€‹
Pharma marketing relies on complex animations to show how a drug works in the body. These are expensive 3D animations
today. In the future, a text prompt like â€œShow T-cells attacking a cancer cell, zoomed in at cellular level, cinematic
lightingâ€ could produce a decent MOA animation clip. Already, Soraâ€™s examples (like wooly mammoths in a field, or waves
crashing on cliffs) indicate the ability to create nature and complex textures â€“ extending to microscopic imagery is a
question of training data. If we can fine-tune models on biomedical imagery, we could get AI that generates
scientifically accurate animations quickly. This would let marketing and medical teams experiment with different visual
metaphors for MOA and pick the most effective ones. It could also be used inâ€¯medical educationâ€¯â€“ helping doctors
visualize mechanisms or trial results via generated visuals.
HCP Marketing and Peer Influence
â€‹
Pharma often involves key opinion leaders (KOLs) giving talks or explaining data. In the near future, one can imagine
generatingâ€¯avatar videos of KOLsâ€¯presenting data, in multiple languages. While using a real personâ€™s likeness has
ethical/IP issues (would need permission and careful use), generative tech could create a convincingâ€¯virtual
spokesperson. Alternatively, completely fictional yet authoritative-looking avatars might be used to relay information.
This could help scale expert content delivery (though likely for internal training or markets where the actual speaker
cannot be present â€“ regulations on promotional use of an AI â€œdoctorâ€ would be tricky).
Patient Engagement
â€‹
Using AI, we could create more engaging patient resources â€“ e.g. an AI-generatedâ€¯explainer cartoon for kidsâ€¯about how to
use an inhaler, or a reassuring scenario video for patients starting a new therapy showing what to expect. Emotional
expressiveness is something some models like Sora and Veo are working on. If an AI can portray empathy or excitement
through a characterâ€™s face and voice, patient communication could benefit. However, Lilly should remember to be
transparent if an avatar or voice bot is being leveraged to maintain trust.
Compliance and Moderation
â€‹
Every promo or medical video in pharma must go through approval for claims, safety info, etc. AI introduces a new
variable â€“ the visuals might inadvertently introduce something non-compliant (e.g. showing a use of a drug thatâ€™s
off-label). Content moderation tools will need to extend to video. We may need to adopt a practice ofâ€¯frame-by-frame
reviewâ€¯of AI outputs just like they review every word of a brochure. This slows things, but some AI tools might allow
locking certain parameters to ensure compliance (for instance, an AI could be instructed not to show the pill being
taken with any other medication or not to depict certain patient populations if not approved). As OpenAI noted with
Sora, their deployed model is limited in what it can show realistically (some complex or sensitive scenarios are
filtered). Lilly will need even tighter guardrails when using AI video generation.
Summary
â€‹
If harnessed well, Lilly could significantlyâ€¯increase engagementâ€¯with both physicians and patients. Video content that
was once too costly to produce for smaller audiences (like a rare disease community) could be made cost-effectively with
AI, making communications more visual and relatable. It also opens up possibility forâ€¯interactive contentâ€¯â€“ e.g. a
voicebot experience where a patientâ€™s questions trigger dynamically generated video answers (a far future concept, but
not impossible as tech converges). Another area isâ€¯internal communications: using AI to generate internal announcement
videos or CEO messages (with consent of course) to add a personal touch without pulling executives into studio every
time.
The biggest risks are legal (IP considerations), regulatory, and inaction. Pharma is already under scrutiny for how it
markets â€“ if it came out that an AI generated video inadvertently created a misleading impression (even subtly, via
imagery), or leveraged an a tool associated with copyright infringement, it could result in legal penalties or public
trust issues.
Strategies for Adoption
â€‹
For anyone planning to leverage AI video generation, here are strategic considerations and steps:
Watch and Experiment (Now):â€¯Itâ€™s important toâ€¯stay informedâ€¯on this fast-moving field (as this report has shown,
updates in 2024-2025 were frequent). Assign a team or innovation leader to experiment with various tools â€“ e.g. a
creative or technology team who can consistently assess models and output across a wide range of features and
capabilities â€“ leveraging the same prompts or end goal in mind. Encourage sharing of results and set up brainstorming
sessions on â€œWhat could we do with this in our business?â€
Determine High-Impact Use Cases (Now):â€¯Not every video needs to be AI-generated. Look forâ€¯pain points or
opportunitiesâ€¯in your current content pipeline: Is it hard to get budget for certain types of videos (maybe training
scenarios or international market content)? Those are prime targets to try AI. For Lilly, maybe start with non-public
content (internal training, mechanism explainer for reps) to avoid regulatory risk while the tech is still new.
Another ideal use case is social media: social teams always need fresh short videos â€“ AI can help create those
quickly and the lower stakes environment (a playful Twitter video) can tolerate slightly less polish.
Draft Policy and Guidelines (Now):â€¯Just as there are guidelines for using stock images or social media conduct,
createâ€¯guidelines for AI-generated content. This includes: what types of content are allowed (e.g. perhaps no AI
generation of real person likenesses without permission to avoid deepfake issues, or avoid certain sensitive health
scenarios to be safe); requirement that every AI video is reviewed by a subject matter expert for accuracy; and
guidelines for disclosure (if needed). For external content, considerâ€¯transparencyâ€¯â€“ for example, some companies
might add a line in the video description that it contains AI-generated imagery, especially if regulations or brand
trust demands it. Given OpenAIâ€™s Sora tags metadata, if an image verification tool sees itâ€™s AI, we donâ€™t want the
company to look like it was hiding that fact.
Give creatives opportunities to Upskill:â€¯AI tools require new skills â€“ prompt writing, iterative refinement, and
basic video editing to polish AI outputs. Invest in developing and onboarding tools so teams can begin learning how
to effectively harness this technology.
Alpha Projects:â€¯Choose a few of low-risk Alpha products to pursue. For example,â€¯create an internal training video
series using AI avatars, orâ€¯generate supplemental social media content for a product campaign. Monitor how long it
takes, the quality feedback from viewers, and iterate. Use these pilots to build a case study: did it save money? Did
it engage better (or worse)? What were the unforeseen hiccups? This builds institutional knowledge and also helps
justify further investment to higher management.
Ethics and Authenticity:â€¯Review/revise our policy and approach toâ€¯ethical AI use (if needed). For pharma especially,
patient trust is key â€“ avoid anything that could be seen as deceptive. If you use an AI avatar of a â€œdoctorâ€ in a
patient video, maybe have a disclaimer like â€œVisuals are computer generated for illustrationâ€ if required. Also avoid
sensitive contexts â€“ e.g. generating a video of a real patient story would be unethical unless clearly fictionalized.
Keep humans in the loop for empathetic or sensitive communications (AI can generate the draft, but let a human review
the tone).
Monitor Regulatory and Legal Developments:â€¯Regulations around AI-generated content (so-called deepfake laws or
required disclosures) are evolving globally. The EU, China, some US states have begun requiring disclosures for
certain AI-generated media. Ensure your compliance team keeps abreast of these so your use of AI video doesnâ€™t run
afoul of any new rule. For instance, if a law says â€œAI-generated realistic videos of humans must be labeledâ€, youâ€™d
incorporate that into your practice. Also, look to ongoing legal conflicts related to AI image generation to
anticipate future issues with video generation.
Consider a Long-term Strategy:â€¯Think how AI video fits into our digital transformation. This will not replace
everything but could augment many processes. Consider organizing aâ€¯center of excellenceâ€¯for generative media, pooling
expertise from IT, creative, legal. Long-term, also consider investing in custom model development for commercial
needs â€“ e.g. partner with a vendor to fine-tune a model on Lilly video content (especially relevant for pharma with
lots of scientific visuals). Owning or heavily customizing a model could give competitive edge, if it could generate
content that is unique to Lilly IP and brand style, which others using generic models cannot.
Challenges and Risks
â€‹
Weâ€™ve touched on several challenges in context, but letâ€™s consolidate the majorâ€¯risks and challengesâ€¯of AI video
generation adoption:
Quality and Consistency
â€‹
While rapidly improving, AI-generated video can sometimes produceâ€¯glitchesâ€¯â€“ a personâ€™s face warping for a frame, odd
background artifacts, inconsistent lighting continuity, etc. In a professional setting, these can be jarring. Ensuring
every frame is clean might require frame-by-frame editing or re-generation cycles. Thereâ€™s also the continuity over time
issue â€“ making sure the AI doesnâ€™t â€œforgetâ€ what it was showing. Until models are truly robust, teams must budget time
for QC and maybe manually fixing AI output (via editing or inpainting tools).
Misrepresentation and Deepfakes
â€‹
AI can createâ€¯very realistic fake people or events. In pharma, imagine an AI video of a â€œpatientâ€ giving a testimonial â€“
if itâ€™s not clearly fictional, thatâ€™s ethically problematic (real patient testimonials require consent and actual
patient stories). Deepfakes of public figures endorsing a drug (even if done jokingly) would be bad. Lilly should set
strict lines: e.g. never impersonate real individuals or fabricate quotes without prior consent and compensation. The
threat of malicious deepfakes (outside actors using AI to create fake news about a company or fake exec statements) is
also real â€“ another reason transparency and detection are important. On the flip side, we do not want Lilly to
accidentally beâ€¯accusedâ€¯of using a deepfake without disclosure.
Regulatory Compliance
â€‹
For pharma, any promotional content must comply with FDA (or EMA, etc.) regulations. AI doesnâ€™t change the requirement,
but it adds complexity to verification. If an AI video shows a patient using a drug, the visual could inadvertently
become aâ€¯â€œclaimâ€. For example, if it shows the patient doing something that implies a benefit not on label, thatâ€™s a
compliance issue. Or if the AI inadvertently generates a medically inaccurate portrayal (like pills of wrong color or a
device used incorrectly), thatâ€™s problematic. Thus, a deep integration of regulatory review is still needed â€“ possibly
even training the AI on whatâ€¯notâ€¯to show. Regulators themselves may start scrutinizing AI-generated content differently;
companies might need to provide more substantiation that the content is accurate and not misleading. There might even be
future guidance specific to AI in pharma advertising.
Intellectual Property and Training Data
â€‹
A lingering question: if an AI video model was trained on, say, thousands of movie clips or YouTube videos without
permission, is its output legally safe to use? The model developers claim yes, itâ€™s transformative. But cases are in
courts (for images, Getty vs. Stability for e.g.). Using output commercially could pose IP risks if the output
unintentionally replicates a copyrighted scene or character. While Adobe avoids this by training only on licensed data,
others might not. So one risk is the potential for a third-party claim: e.g. a background in an AI video looks too
similar to a scene from a known film. The best mitigation is using providers who have clear training sources or
indemnification. Or limiting use of AI for things where this risk is minimal (unique scenarios rather than known
characters).
Ethical Considerations
â€‹
Beyond legal compliance, ethical use concerns include: not reinforcing biases (if models were trained on biased content,
they might output stereotypes â€“ careful prompting or model choice is needed to avoid, say, always depicting doctors as
one gender or certain roles in subservient ways, etc., which could slip into outputs unconsciously). Also, thereâ€™s
aâ€¯human elementâ€¯â€“ if marketing and comms become too AI-heavy, does it lose authenticity? We will have to find the right
balance between automation and human touch. And internally, treating employees openly about use of AI, offering
re-skilling, etc., is important to maintain morale.
Technical Dependency and Evolution
â€‹
If a business builds processes around a third-party AI tool, they have to manage dependency risk. What if that service
changes pricing significantly, or a policy shift (e.g. an AI model decides to disallow certain medical content
generation)? Executing a strategy that involves multiple options or maintaining the ability to switch models quickly is
highly advised. Also, file formats and integration: ensuring these AI outputs can integrate into existing video editing
workflows smoothly (most give MP4 outputs which is fine, but if you want something like masking info for overlays, not
all provide that yet â€“ Adobe likely will for theirs).
Security and Privacy
â€‹
Prompts and any input data sent to AI cloud services could contain sensitive info (especially if youâ€™re generating
something based on confidential product data or an unreleased device design). Thereâ€™s the usual cloud risk â€“ ensure the
vendor has enterprise agreements if needed. Alternatively, do sensitive work on-prem with open models and sticking to
green data for experimentation.
Public Perception
â€‹
As AI-generated content becomes more common, there may be public skepticism (â€œIs this video real or AI?â€). If misused by
bad actors, there could be a consumer backlash or calls for regulation. If we begin using it in consumer-facing ways
should be prepared to address questions about authenticity. Being on the honest, transparent side will guard brand
reputation. For instance, if an AI is used to simulate a patient story, one might label it as a â€œdramatizationâ€ which is
a term already used in ads for reenactments. Itâ€™s about not breaking trust.
In summary, the opportunities and challenges are significant. They manageable with a proactive approach. The key is
toâ€¯not treat AI video as magic; itâ€™s a powerful new tool that augments human creativity but still needs human oversight
and strategic implementation.
Future Outlook
â€‹
The trajectory of AI video generation suggests that what is cutting-edge today could become commonplace and vastly more
capable in the next 2â€“3 years. Lilly should anticipate and prepare for the following likely developments:
Longer & Real-Time Generation
â€‹
Models will continue scaling (OpenAI hinted that bigger models = longer videos). We can expect by mid-2026 some AI
systems will routinely generate a few minutes of video with coherent storylines. There is even the possibility
ofâ€¯real-timeâ€¯or streaming generation â€“ imagine AI creating video on the fly as you watch (some research is headed there
for live avatar conversations, etc.). This could revolutionize live customer service (AI video agent responding in real
time) or live training sessions with dynamic visuals. While true real-time high-res generation might be a bit farther,
real-time at lower res may come sooner.
Improved Fidelity
â€‹
Already Veo 3â€™s people are highly realistic; with more specialized training (and perhaps hybrid approaches like grafting
AI-generated elements into real video backgrounds), AI videos will reach the point where an average viewer canâ€™t tell AI
vs real footage, at least for short sequences. This will blur the line of what content is â€œshotâ€ vs â€œgeneratedâ€. For
creators, that means tremendous creative freedom (any idea can be visualized without practical constraints) but also
greater need forâ€¯ethical guidelinesâ€¯to maintain trust.
Interactive Video & Multimodality
â€‹
Combining video generation with interactivity (powered by LLMs like GPT-4/5) could yield interactive media. For example,
a user could talk to an AI character who responds with generated video and audio. This could be a new form of engaging
content or education tool. In pharma, a patient could ask an AI â€œWhat will the surgery be like?â€ and get a custom visual
explanation video. Weâ€™re moving toward AI that canâ€¯understand scene contextâ€¯deeply (Gemini is multimodal â€“ it might
align narrative with visuals tightly). The merging of text, images, audio, and video AI means future tools will handle
whole multimedia generation. Googleâ€™s Flow already hints at that synergy (Gemini assisting with consistency across
shots).
Industry-Specific Marketing Models/Tools
â€‹
We will likely see models fine-tuned for specific domains: e.g. aâ€¯Medical VideoGenâ€¯that knows how to accurately depict
anatomy, or anâ€¯Architectural VideoGenâ€¯for real estate walkthroughs. These specialized models could be offered by
vertical SaaS companies or open communities. For pharma, a model trained on, say, all publicly available MOA animations
and medical footage could become the go-to for generating regulatory-compliant medical visuals (with knowledge of what
not to show because of how the body actually works).
Regulatory Frameworks
â€‹
Governments and industry bodies will establish more formal guidelines. We might see something like an â€œAI Content
Certificationâ€ where companies can submit AI-generated ads for a special review or to get a certification mark that itâ€™s
been vetted for accuracy. Thereâ€™s also likely to be technology solutions for provenance: cryptographic signing of AI
content so that it can be traced. OpenAI and Adobe already tag content; this could become standardized so that any
screen can potentially alert viewers â€œThis video is AI-generated.â€ Lilly should anticipate that transparency may become
not just best practice but mandated.
Talent and Workflow Shifts
â€‹
Roles will shift â€“ we might seeâ€¯â€œAI video prompt scriptwritersâ€,â€¯â€œAI ethics reviewersâ€, andâ€¯â€œcontent curatorsâ€â€¯as common
jobs. Teams might reorganize such that an AI tool is part of every content meeting (e.g. brainstorm sessions always
involve someone quickly prototyping ideas with AI to show the room). The speed of execution will increase, meaning
competition in marketing might revolve around who can ideate and deploy creative concepts fastest (with AI doing the
heavy lifting). The flip side is potential content glut â€“ so focusing on strategy and creativity (the human part)
remains vital to stand out.
Competitive Landscape
â€‹
Itâ€™s possible that only a few foundation models end up dominating (like we see in LLMs, a handful of leaders). But many
companies might build on those via fine-tuning. We may see consolidation where big players acquire some startups
(perhaps OpenAI buys a Pika, or Adobe buys an Runway, purely speculative). Or big tech might all have their offerings
and split the market (like cloud providers). Keeping flexibility is wise â€“letâ€™s not get too locked into since another
will likely surpass it. It could be analogous to the early days of web browsers or mobile OS â€“ eventually a stable set
of widely used platforms emerged. Likely a mix of open (Stability) and closed (OpenAI, Google) will persist, each with
pros and cons.
New Content Formats
â€‹
As AI lowers production costs, we might see entirely new forms of media. For instance, personalized short films as a
service (you input some info and get a short film about you). Or dynamic video content on websites that adjusts to
viewer profile. In pharma, maybe dynamic visual aids for reps that change based on the doctor theyâ€™re speaking to (if
the doctor is more visual, the AI generates more mechanism animation; if they care about data, it shows charts, etc.,
all in real time). The boundaries between video, animation, and software could blur.
Conclusion
â€‹
In conclusion, AI video generation is poised to become a standard tool in the content creation toolbox â€“ much like
desktop publishing in the 80s or digital video editing in the 2000s. It will not outright replace humans, but those
whoâ€¯harness it effectivelyâ€¯will outpace those who donâ€™t, by producing more tailored and creative content with less
effort. For the pharmaceutical industry and similar fields, the key will be to integrate these capabilities in a way
thatâ€¯enhances communication and understandingâ€¯(of complex science, of patient stories) whileâ€¯maintaining the rigorous
standardsâ€¯of accuracy and ethics that the industry demands.
Lilly should take a proactive but careful approach: embrace the innovation, experiment with it, and begin with low-risk
use cases with some oversight eyeing the entire spectrum of use cases. By doing so, we can significantly boost its
content innovation capacity and be ready for the future where AI-driven media is ubiquitous. The companies that succeed
will likely be those that combineâ€¯the creative imagination of their teamsâ€¯withâ€¯the generative power of AI, in a governed
and purposeful manner â€“ turning what used to be costly visual dreams into vivid (and responsible) reality.
Sources
â€‹
Video generation models as world simulators | OpenAI
https://openai.com/index/video-generation-models-as-world-simulators/
ï¿¼
Sora is here | OpenAI]
https://openai.com/index/sora-is-here/
ï¿¼
Runway's Gen-2 shows the limitations of today's text-to-video tech | TechCrunch
https://techcrunch.com/2023/06/09/runways-gen-2-shows-the-limitations-of-todays-text-to-video-tech/
Best Short-Form AI Video Generator? Kling 2.1 vs Google Veo 3 - Decrypt
https://decrypt.co/323056/best-short-form-ai-video-generator-kling-google-veo
Generative Video. Now in Adobe Firefly. : r/premiere
https://www.reddit.com/r/premiere/comments/1inrroo/generative_video_now_in_adobe_firefly/
Flow
https://labs.google/fx/tools/flow/faq
Video generation models as world simulators | OpenAI
https://openai.com/index/video-generation-models-as-world-simulators/
Runway's Gen-2 Text-to-Video Tool Now Available to Everyone for Free | Tom's Hardware
https://www.tomshardware.com/news/runway-gen-2-text-to-video-available-to-all
Runway's Gen-2 shows the limitations of today's text-to-video tech | TechCrunch
https://techcrunch.com/2023/06/09/runways-gen-2-shows-the-limitations-of-todays-text-to-video-tech/
I Tried Minimax AI Video Generator to Generate These Insane Videos: Here is My Honest Review | by Amdad H | Towards AGI
| Medium
https://medium.com/towards-agi/i-tried-minimax-ai-video-generator-to-generate-these-insane-videos-here-is-my-honest-review-832f5a6e8b7c
Runway's Gen-2 shows the limitations of today's text-to-video tech | TechCrunch
https://techcrunch.com/2023/06/09/runways-gen-2-shows-the-limitations-of-todays-text-to-video-tech/
Luma Dream Machine: New Freedoms of Imagination | Luma AI
https://lumalabs.ai/dream-machine
Generative Video. Now in Adobe Firefly. : r/premiere
https://www.reddit.com/r/premiere/comments/1inrroo/generative_video_now_in_adobe_firefly/
Bringing generative AI to video with Adobe Firefly Video Model | Adobe Blog
https://blog.adobe.com/en/publish/2024/09/11/bringing-gen-ai-to-video-adobe-firefly-video-model-coming-soon
Video generation models as world simulators | OpenAI
https://openai.com/index/video-generation-models-as-world-simulators/
Runway's Gen-2 shows the limitations of today's text-to-video tech | TechCrunch
https://techcrunch.com/2023/06/09/runways-gen-2-shows-the-limitations-of-todays-text-to-video-tech/
Runway's Gen-2 shows the limitations of today's text-to-video tech | TechCrunch
https://techcrunch.com/2023/06/09/runways-gen-2-shows-the-limitations-of-todays-text-to-video-tech/
Google launches Veo 3, an AI video generator that incorporates audio
https://www.cnbc.com/2025/05/20/google-ai-video-generator-audio-veo-3.html
AI video just took a startling leap in realism. Are we doomed?
https://arstechnica.com/ai/2025/05/ai-video-just-took-a-startling-leap-in-realism-are-we-doomed/
Video generation API - MiniMax
https://www.minimaxi.com/en/news/video-generation-api
Generate videos with Minimax's Hailuo video-01 model - Replicate
https://replicate.com/minimax/video-01
Kling AI Free: Try This AI Video Generator Now! | Pollo AI
https://pollo.ai/m/kling-ai
Pika AI (Pika Art) free: Try Pika Labs AI Video Generator Now | Pollo AI
https://pollo.ai/m/pika-ai
Profile | China â€˜genius girlâ€™ Guo Wenjing, Harvard graduate, co-founder of tech firm backed by US$135 million funding |
South China Morning Post
https://www.scmp.com/news/people-culture/china-personalities/article/3268811/china-genius-girl-guo-wenjing-harvard-graduate-co-founder-tech-firm-backed-us135-million-funding
Pika AI Video Generator: Create B-Roll From Text - Captions
https://www.captions.ai/features/pika-ai-video-generator
Best AI Video Generator - Comparison â€¢ Quality & Price Comparison
https://aianimation.com/best-ai-video-generation-platforms/
Pika AI (Pika Art) free: Try Pika Labs AI Video Generator Now | Pollo AI
https://pollo.ai/m/pika-ai
The Ultimate Guide to MiniMax Hailuo AI Video Models - Getimg.ai
https://getimg.ai/blog/the-ultimate-guide-to-minimax-hailuo-ai-video-models
Pika AI (Pika Art) free: Try Pika Labs AI Video Generator Now | Pollo AI
https://pollo.ai/m/pika-ai
Best AI Video Generator - Comparison â€¢ Quality & Price Comparison
https://aianimation.com/best-ai-video-generation-platforms/
Stable Video Diffusion is awesome! : r/StableDiffusion - Reddit
https://www.reddit.com/r/StableDiffusion/comments/183ync6/stable_video_diffusion_is_awesome/
Stable diffusion video tutorial â€” generate AI video for free - Medium
https://medium.com/@codeandbird/stable-diffusion-video-tutorial-generate-ai-video-for-free-29538ca45ef5
Bringing generative AI to video with Adobe Firefly Video Model | Adobe Blog\
https://blog.adobe.com/en/publish/2024/09/11/bringing-gen-ai-to-video-adobe-firefly-video-model-coming-soon
Generative Video. Now in Adobe Firefly. : r/premiere
https://www.reddit.com/r/premiere/comments/1inrroo/generative_video_now_in_adobe_firefly/
Runway's Gen-2 shows the limitations of today's text-to-video tech | TechCrunch
https://techcrunch.com/2023/06/09/runways-gen-2-shows-the-limitations-of-todays-text-to-video-tech/
Bringing generative AI to video with Adobe Firefly Video Model | Adobe Blog
https://blog.adobe.com/en/publish/2024/09/11/bringing-gen-ai-to-video-adobe-firefly-video-model-coming-soon
Was this helpful?
Tags:
innovation
Edit this page
Previous
AI Speech
Executive Summary
Introduction
AI Video Generation Technology Overview
Major Players and Platforms in AI Video Generation
OpenAI â€“â€¯Sora
Runway â€“â€¯Gen-2â€¯(and beyond)
Google â€“â€¯Veo 3â€¯andâ€¯Flow
Flow
Kuaishou â€“â€¯Kling
Pika Labs â€“â€¯Pika Video
MiniMax â€“â€¯Hailuo Video-01
Luma Labs â€“â€¯Dream Machine (Ray 2)
Stability AI â€“â€¯Stable Video Diffusion
Adobe â€“â€¯Firefly Generative Video
Industry Impact and Use Cases
Marketing and Advertising
Learning & Development / Training
Commercial and Promotional
Strategies for Adoption
Challenges and Risks
Quality and Consistency
Misrepresentation and Deepfakes
Regulatory Compliance
Intellectual Property and Training Data
Ethical Considerations
Technical Dependency and Evolution
Security and Privacy
Public Perception
Future Outlook
Longer & Real-Time Generation
Improved Fidelity
Interactive Video & Multimodality
Industry-Specific Marketing Models/Tools
Regulatory Frameworks
Talent and Workflow Shifts
Competitive Landscape
New Content Formats
Conclusion
Sources
Community
EBA Viva Engage
EBA SharePoint
Questions?
Reach us on Viva Engage!
Copyright Â© 2026 Eli Lilly and Company