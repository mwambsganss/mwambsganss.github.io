{
  "url": "https://techhq.dc.lilly.com/docs/solution/ai/ai_submission_guides/",
  "title": "AI Submission Guide Template | Tech HQ",
  "description": "Product Definition",
  "h1": [
    "AI Submission Guide Template"
  ],
  "h2": [
    "Product Definition‚Äã",
    "Architecture Diagram C0: Tech Stack‚Äã",
    "Architecture Diagram C1: High-Level Architecture‚Äã",
    "Architecture Diagram C2: Detailed Component Architecture‚Äã",
    "Data Flow Diagram‚Äã",
    "LLM/ML Model Details‚Äã",
    "Data Governance & Privacy‚Äã",
    "Evaluation & Validation‚Äã",
    "Usage Tips‚Äã",
    "Questions or Feedback?‚Äã"
  ],
  "h3": [
    "Template‚Äã",
    "Template‚Äã",
    "Template‚Äã",
    "Template Option 1: Sequence Diagram (Best for User Interactions)‚Äã",
    "Template Option 2: Graph Diagram (Best for Multiple Paths/Scenarios)‚Äã",
    "GenAI Workflow Diagram (for Agentic Solutions)‚Äã"
  ],
  "text_content": "AI Submission Guide Template | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nEcosystem\nPatterns\nPositioning\nExamples\nCoding Tools\nStandards\nAI Submission Guide Template\nüóÑÔ∏è AI & Intelligent Agents BLT\nü§ñ Agentic AI\nüß≠ Conversational AI\nüß≠ Knowledge Bases\nüß≠ Multimodal AI\nüß≠ Translation Services\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nü§ñ AI & Intelligent Agents\nAI Submission Guide Template\nOn this page\nAI Submission Guide Template\nProduct Definition\n‚Äã\nDescription for Product Definition.\nWhat is this?\nBrief description of the AI solution and its purpose\nWho are the users?\nTarget audience and user personas\nWhat functions are using this?\nBusiness units, departments, or systems\nWhat are they using this for?\nUse cases and business outcomes\nArchitecture Diagram C0: Tech Stack\n‚Äã\nPurpose:\nHigh-level view of the technology stack organized into four layers: Experience, AI, Model, and Data.\nLayer Guidelines:\nExperience\n: User interfaces (web apps, mobile, APIs)\nAI\n: Orchestration frameworks (LangChain, LangGraph, CrewAI, Cortex)\nModel\n: LLM/ML services (AWS Bedrock, LLM Gateway, Sagemaker)\nData\n: Data sources and storage (S3, EDB, data lakes)\nTemplate\n‚Äã\nLayers\nData\nAWS S3\nAzure Data Lake\nEDB\nResearch Data\nEdge Device Data\nExternal Resources\nCLUWE\nRIM\nModel\nCortex\nLLM Gateway\nAWS Bedrock\nMagTrain\nAWS Sagemaker\nCATS\nAI\nCortex\nLangGraph\nLangChain\nCrewAI\nSwarmAI\nExperience\nNext.js\nLDS 2.0\nStreamlit\nExperience\nAI\nModel\nData\nInstructions:\nReplace the example technologies with your actual stack. Remove layers that don't apply.\nArchitecture Diagram C1: High-Level Architecture\n‚Äã\nPurpose:\nShow how your tech stack components (from C0) connect together at a high level. Think of this as a \"zoomed-in\" view of your C0 diagram showing\nwhich components talk to which components\nand how they integrate.\nWhat to show:\nComponents from your C0 diagram (e.g., Streamlit, LangGraph, Bedrock, S3)\nHow Experience layer connects to AI layer\nHow AI layer connects to Model layer\nHow components access Data layer\nExternal integrations and APIs\nKeep it component-level (no infrastructure details like Lambda/ECS - save that for C2)\nüìã Requirements Checklist & Color Guide\nChecklist:\nUse component names from your C0 (Streamlit, LangGraph, AWS Bedrock, etc.)\nShow connections between layers (Experience ‚Üí AI ‚Üí Model ‚Üí Data)\nLabel connection types (REST API, SDK, HTTP, etc.)\nInclude caching or state management if applicable\nShow external system integrations\nKeep it high-level - no AWS service details yet\nStandard Colors (map to C0 layers):\nBlue\n#2196F3\n- Experience layer components (Streamlit, Next.js, API Gateway)\nGreen\n#4CAF50\n- AI layer components (LangGraph, LangChain, CrewAI, Cortex)\nPurple\n#9C27B0\n- Model layer components (Bedrock, LLM Gateway, Sagemaker)\nOrange\n#FF9800\n- Data layer components (S3, EDB, Pinecone, RDS)\nGray\n#607D8B\n- External systems\nTemplate\n‚Äã\nHTTPS\nREST\nCheck cache\nCache miss\nVector search\nFetch docs\nQuery metadata\nRoute request\nResponse\nStore\nAPI call\nStreamlit App\nAPI Gateway\nLangGraph\nRedis Cache\nLLM Gateway\nAWS Bedrock\nPinecone\nAWS S3\nRDS PostgreSQL\nEnterprise System\nEDB or External API\nSee real examples:\nClinical Programming Assistant\n¬∑\nHR Talent Acquisition\nArchitecture Diagram C2: Detailed Component Architecture\n‚Äã\nPurpose:\nZoom into each component from C1 and show\nhow it's implemented\nwith specific AWS/Azure services, infrastructure, and networking details.\nWhat to show:\nSpecific cloud services (AWS Lambda, Azure Functions, ECS, etc.)\nHow each C1 component is deployed (Lambda for agents, ECS for orchestrator, etc.)\nNetwork architecture (VPCs, subnets, security groups)\nService-level capabilities (3-5 bullet points each)\nDecision nodes for routing logic\nMonitoring and observability integration\nüìã Requirements Checklist & Color Guide\nChecklist:\nShow all architectural layers (UI, Orchestration, Model, Data)\nUse nested subgraphs for related services\nSpecify exact service names (AWS Lambda, Azure Functions, etc.)\nInclude 3-5 bullet points per service\nShow network boundaries (VPCs, accounts) if applicable\nIndicate data flow directions\nShow integration with external systems\nStandard Layer Colors:\nBlue\n#2196F3\n- UI Layer\nGreen\n#4CAF50\n- Orchestration/AI Layer\nPurple\n#9C27B0\n- Model/LLM Layer\nOrange\n#FF9800\n- Data Layer\nPurple dashed\n- External Services\nRed-orange\n#FF5722\n- Decision Nodes\nTemplate\n‚Äã\nüíæ Data Layer\nü§ñ Model Layer\n‚öôÔ∏è Orchestration Layer\nüñ•Ô∏è User Interface Layer\nAI Models\nAgents/Services\nFrontend Application\n‚Ä¢ Feature 1\n‚Ä¢ Feature 2\n‚Ä¢ Feature 3\nOrchestration Service\n‚Ä¢ Workflow Management\n‚Ä¢ State Management\n‚Ä¢ Error Handling\nService 1\nService 2\nLLM Gateway\n‚Ä¢ Load Balancing\n‚Ä¢ Rate Limiting\nModel Name 1\nModel Name 2\nStorage Service\n‚Ä¢ Data Type\n‚Ä¢ Retention Policy\nDatabase Service\n‚Ä¢ Data Model\n‚Ä¢ Access Pattern\nExternal API\n‚Ä¢ Service Description\nSee real examples:\nClinical Programming Assistant\n¬∑\nAMIGO\nData Flow Diagram\n‚Äã\nPurpose:\nShow different scenarios of how data and requests flow through your system. For GenAI solutions, this tells the story of\nwhat happens when users interact with the system\n- which agents, LLM chains, knowledge bases, and tools connect together to serve specific inputs/outputs.\nWhen to include:\nUser interaction scenarios\n(chat query, document upload, API request)\nData pipeline scenarios\n(ETL, batch processing, real-time streams)\nComplex workflows\nwith multiple paths or decision points\nChoose the style that best fits your scenario:\nSequence diagram\nfor temporal/step-by-step flows (best for user interactions)\nGraph diagram\nfor spatial/branching flows (best for showing multiple paths)\nFor GenAI/Agentic solutions:\nSee\nGenAI Workflow Diagram\nsubsection below for specialized agent interaction patterns.\nüìã Requirements Checklist & Color Guide\nChecklist:\nChoose 1-3 key scenarios to document (e.g., \"User asks a question\", \"Batch data ingestion\")\nShow which components from C1 are involved\nFor GenAI: show which agents, LLMs, KBs, and tools are used\nLabel decision points (routing logic, validation gates)\nInclude data formats at key stages\nShow error/exception paths\nAdd timing or volume annotations where relevant\nStandard Colors:\nBlue\n#2196F3\n- Data Sources / User Input\nGreen\n#4CAF50\n- Processing / Agents\nOrange\n#FF9800\n- Validation / Decision Points\nPurple\n#9C27B0\n- Storage / Knowledge Bases\nGray\n#607D8B\n- External Systems\nTeal\n#009688\n- Outputs / Results\nTemplate Option 1: Sequence Diagram (Best for User Interactions)\n‚Äã\nResponse\nClaude 3.5\nPinecone KB\nResearch Agent\nLangGraph\nStreamlit UI\nüë§ User\nResponse\nClaude 3.5\nPinecone KB\nResearch Agent\nLangGraph\nStreamlit UI\nüë§ User\nScenario: User asks a question\nalt\n[Needs context]\n[Simple query]\nTotal time: 2-3 seconds\nSubmit query\nRoute query\nParse intent\nInvoke research agent\nVector search\nTop 5 chunks\nQuery with context\nDirect query\nGenerated response\nFormat response\nDisplay answer\nTemplate Option 2: Graph Diagram (Best for Multiple Paths/Scenarios)\n‚Äã\nSimple\nNeeds context\nCode request\nUser Query\nNatural language\nIntent Classifier\nCategorize query type\nSimple Q&A Agent\nDirect GPT-4 call\nRAG Agent\nRetrieve + Generate\nCode Agent\nGenerate & validate code\nVector DB\nPinecone\n10K docs\nCode Repository\nGitHub examples\nGPT-4o\nFast responses\nClaude 3.5\nLong context\nFinal Response\nFormatted answer\nSee real examples:\nAMIGO Data Pipeline\n¬∑\nClinical Programming Assistant\nGenAI Workflow Diagram (for Agentic Solutions)\n‚Äã\nPurpose:\nFor solutions using agentic frameworks (LangGraph, CrewAI, AutoGen, Cortex), show how agents collaborate, which tools they use, and how they orchestrate LLM calls. This is a specialized view that goes deeper than general data flows. To learn more about agentic patterns and architectural approaches, see\nAgentic Patterns\n.\nWhen to include:\nSolutions with\nmultiple specialized agents\nworking together\nTool-using agents\nthat call external APIs, databases, or code interpreters\nMulti-step reasoning workflows\nwith agent handoffs\nHierarchical agent architectures\n(supervisor ‚Üí worker agents)\nWhat to show:\nEach agent's name, role, and responsibilities\nTools available to each agent (search, calculator, SQL, API calls)\nWhich LLM(s) each agent uses\nCommunication patterns (sequential, parallel, hierarchical, collaborative)\nState management and memory (conversation history, shared state)\nDecision points (routing logic, delegation, escalation)\nüìã Requirements Checklist & Color Guide\nChecklist:\nShow all agents with clear roles (e.g., \"Research Agent\", \"Code Agent\", \"Validator\")\nList tools each agent can use (e.g., \"Vector Search\", \"SQL Query\", \"Web Search\")\nIndicate which LLM(s) each agent uses (GPT-4o, Claude 3.5, etc.)\nShow agent interaction patterns (who talks to whom)\nInclude supervisor/orchestrator if present\nShow state/memory management approach\nLabel handoff conditions (when does one agent call another?)\nIndicate parallel vs sequential execution\nStandard Colors:\nPurple\n#9C27B0\n- Supervisor/Orchestrator Agent\nBlue\n#2196F3\n- Research/Retrieval Agents\nGreen\n#4CAF50\n- Processing/Analysis Agents\nOrange\n#FF9800\n- Code/Tool-Execution Agents\nTeal\n#009688\n- Validation/QA Agents\nGray\n#607D8B\n- External Tools/APIs\nRed-orange\n#FF5722\n- Decision/Routing Nodes\nTemplate Option 1: Hierarchical Agent System (Supervisor Pattern)\n‚Äã\nNeeds research\nNeeds code\nNeeds analysis\nFinal answer\nUser Query\nSupervisor Agent\nLLM: GPT-4o\nRole: Route & orchestrate\nRoute Query\nResearch Agent\nLLM: Claude 3.5\nTools: Vector Search, Web Search\nCode Agent\nLLM: GPT-4o\nTools: Python REPL, GitHub Search\nAnalyst Agent\nLLM: Claude 3.5\nTools: SQL Query, Data Viz\nPinecone\nVector Search\nTavily\nWeb Search\nPython REPL\nCode Execution\nPostgreSQL\nSQL Queries\nGPT-4o\nFast reasoning\nClaude 3.5\nLong context\nShared State\nConversation history\nAgent outputs\nAgent can call\nother agents\nTemplate Option 2: Collaborative Agent System (Peer-to-Peer)\n‚Äã\nCreate plan\nUse tools\nAdd findings\nDraft output\nFeedback\nNeeds revision\nNeeds more info\nApproved\nUser Query\nAgent 1: Planner\nLLM: GPT-4o\nBreaks down task\nAgent 2: Researcher\nLLM: Claude 3.5\nGathers information\nAgent 3: Writer\nLLM: Claude 3.5\nCreates output\nAgent 4: Critic\nLLM: GPT-4o\nValidates quality\nAvailable Tools:\n‚Ä¢ Vector Search\n‚Ä¢ Web Search\n‚Ä¢ Code REPL\n‚Ä¢ SQL Query\nShared Memory\nTask plan\nResearch findings\nDraft outputs\nFeedback\nTemplate Option 3: Sequential Agent Pipeline\n‚Äã\nExternal Tools\nValidator Agent\n(GPT-4o)\nGenerator Agent\n(Claude 3.5)\nPinecone\nRetrieval Agent\n(Claude 3.5)\nRouter Agent\n(GPT-4o)\nUser\nExternal Tools\nValidator Agent\n(GPT-4o)\nGenerator Agent\n(Claude 3.5)\nPinecone\nRetrieval Agent\n(Claude 3.5)\nRouter Agent\n(GPT-4o)\nUser\nAgentic RAG Workflow\nalt\n[Simple query]\n[Complex query]\nopt\n[Needs external data]\nalt\n[Validation fails]\n[Validation passes]\nTotal: 3-5 seconds\n3-4 LLM calls\nSubmit query\nClassify intent\nSelect strategy\nDirect generation\nInvoke retrieval\nVector search\nTop 5 chunks\nRerank results\nContext + query\nGenerate response\nAPI call / Web search\nExternal data\nValidate response\nCheck citations\nVerify accuracy\nSafety check\nRequest revision\nRevised response\nFinal response\nKey Documentation Points:\nAgent Roles & Responsibilities:\nAgent 1 (Supervisor): Routes queries, orchestrates workflow, decides which agents to invoke\nAgent 2 (Researcher): Performs vector search, web search, returns relevant context\nAgent 3 (Analyst): Analyzes data, generates insights, creates visualizations\nLLM Selection Rationale:\nGPT-4o for fast routing and classification (low latency)\nClaude 3.5 for long-context research and generation (200K window)\nTool Usage:\nVector Search (Pinecone): Semantic search over knowledge base\nPython REPL: Execute code for calculations, data transformations\nSQL Query: Structured data retrieval from PostgreSQL\nState Management:\nShared state stores conversation history and intermediate outputs\nAgents read/write to shared state for coordination\nState persisted in Redis for session continuity\nHandoff Logic:\nSupervisor evaluates query complexity ‚Üí routes to appropriate agent\nAgents can invoke other agents when needed (e.g., Researcher ‚Üí Analyst)\nMaximum 3 agent hops to prevent infinite loops\nSee real examples:\nClinical Programming Assistant Agents\n¬∑\nHR Talent Acquisition Workflow\nLLM/ML Model Details\n‚Äã\nDescription for LLM/ML Model Details.\nDocument the AI/ML models and how they're used in your solution:\nTopic\nWhat to Document\nExample\nModels Used\nName, version, provider, use case, context window\nGPT-4o via Azure OpenAI for Q&A (128K tokens, 2.5s avg)\nPrompting Strategy\nSystem prompts, few-shot examples, chain-of-thought, structured outputs\n\"You are a clinical analyst...\" with 3 examples, JSON schema output\nOrchestration Logic\nSingle vs multi-model, routing logic, fallback strategy\nRoute complex queries to Claude, simple to GPT-3.5\nAgentic Framework\nMaster agent, sub-agents, tools, LLMs used, interaction patterns\nSupervisor orchestrates 3 agents (Researcher, Analyst, Writer)\nAI Logic\nHow AI components meet requirements\nRAG retrieves top-5 chunks ‚Üí Claude generates ‚Üí Validator checks citations\nüìã Detailed Guidance\nModels Used - Include:\nModel name/version (GPT-4o, Claude 3.5 Sonnet, Llama 3.1 70B, custom models)\nProvider/platform (Azure OpenAI, AWS Bedrock, Cortex, Sagemaker)\nSpecific use case (Q&A, code generation, classification, reasoning)\nContext window size (128K, 200K tokens)\nPerformance metrics (latency, throughput, cost per request)\nAgentic Framework (if applicable):\nMaster/Supervisor agent role and responsibilities\nSub-agents: name, role, tools available, LLM used\nInteraction pattern (sequential, parallel, hierarchical)\nState management approach\nAI Logic & Requirements Mapping:\nShow how AI capabilities deliver business value:\nRequirement: \"Provide accurate, cited answers to regulatory questions\"\nImplementation: Vector DB ‚Üí Claude 200K ‚Üí Citation validator ‚Üí User feedback loop\nSee real examples:\nClinical Programming Assistant\n¬∑\nHR Talent Acquisition\nData Governance & Privacy\n‚Äã\nDescription for Data Governance & Privacy.\nDocument data handling, privacy, and compliance:\nCategory\nWhat to Document\nData Description\nType collected, classification (CI/PI), sources, volume, sensitivity (PII/PHI)\nStorage & Architecture\nPrimary location (S3/Azure/EDB), region, encryption (at rest/in transit), backup strategy, network security\nData Ownership\nWho owns data (for vendor solutions), license terms, data residency, subprocessors\nTraining Data Governance\nUsed for training? RAG? Fine-tuning? How is Lilly data handled? Anonymization approach\nInteraction Data\nAre prompts/responses stored? Where? For how long? Sent to LLM providers? Zero-retention agreements?\nRetention & Deletion\nPolicy by data type (raw: 30d, processed: 90d, outputs: 2y, logs: 7y), deletion process, archival strategy\nCompliance\nFrameworks (GDPR, HIPAA, SOC 2, GxP), security controls (Auth, monitoring, DLP, scanning), assessments completed\nVendor Due Diligence\nCertifications (SOC 2, ISO 27001), DPA/BAA in place, audit rights, incident response SLA\nüìã Critical Questions for AI Solutions\nTraining Data:\nIs Lilly data used for model training? If yes: What data? How anonymized? Who has access?\nIs Lilly data used for RAG/retrieval? If yes: How chunked/embedded? Access controls?\nAre models fine-tuned on Lilly data? If yes: Where hosted? Who can access?\nPrompts & Responses:\nAre user prompts stored? If yes: Where? How long? Who accesses?\nAre AI responses stored? If yes: Retention policy?\nAre prompts/responses sent to third-party LLM providers? Zero-retention agreements?\nHow long is conversation history maintained? Where is state stored?\nCompliance (for regulated data or external exposure):\nWhich frameworks apply? (GDPR, HIPAA, SOC 2, GxP, 21 CFR Part 11)\nWhat security controls are in place? (Azure AD, MFA, RBAC, monitoring, DLP)\nHas a cyber risk assessment been completed?\nHas legal counsel reviewed data practices?\nEvaluation & Validation\n‚Äã\nDescription for Evaluation & Validation.\nDocument performance measurement and output validation:\nCategory\nWhat to Document\nGround Truth\nHow created (human annotation, expert review, gold standard comparison), who creates it, sample size, maintenance\nAccuracy Metrics\nClassification: precision/recall/F1. Generation: BLEU/ROUGE. RAG: context relevance/faithfulness. Domain-specific metrics\nPerformance Metrics\nResponse time (p50/p95/p99), throughput (req/sec), availability (SLA), cost efficiency\nUser Experience\nSatisfaction (ratings/NPS), task completion rate, engagement, error rate\nBaseline Comparison\nCurrent state without AI, manual process time, existing system performance\nHuman-in-the-Loop\nWho validates (experts, users)? When (100%, sample %, confidence-triggered)? Review workflow? Feedback capture?\nAutomated Validation\nConfidence scoring, semantic validation (second LLM), rule-based checks (format, logic, safety), test suites\nFeedback Loops\nCollection (thumbs up/down, corrections, flags), processing (aggregation, labeling), model improvement (prompt refinement, fine-tuning, RAG optimization)\nMonitoring\nReal-time dashboards, drift detection, alerting rules (error spikes, latency increases), incident response\nValidation Frequency\nPre-deployment testing, continuous production monitoring, periodic reviews (monthly/quarterly), post-change regression tests\nüìã ML Optimization Strategies\nFeedback Collection:\nUser feedback: Thumbs up/down, star ratings, corrections, flags\nImplicit feedback: Click-through rates, time spent, task completion\nSystem logs: Error logs, performance metrics, usage patterns\nModel Improvement:\nPrompt refinement: A/B test different prompts based on feedback\nFine-tuning: How often? (weekly/monthly/quarterly) What's the pipeline?\nRAG optimization: Improve chunking, embedding model, ranking algorithms\nAgent optimization: Adjust responsibilities, retrain routing, update tool usage\nMonitoring & Alerting:\nReal-time dashboards for key metrics\nDrift detection for data/model degradation\nAlert triggers: Error rate spike, latency increase, quality drop\nIncident response process when quality degrades\nUsage Tips\n‚Äã\nStart minimal\n- Copy starter templates and fill in your specifics\nReference exemplars\n- See complete examples in the exemplar files\nRemove what doesn't apply\n- Not every solution needs all sections\nKeep it current\n- Update as architecture evolves\nUse tables\n- For requirements and checklists (more scannable)\nLink to detailed docs\n- Don't overload this template, reference separate docs\nQuestions or Feedback?\n‚Äã\nReach out to the TechHQ team or submit feedback through standard channels.\nWas this helpful?\nEdit this page\nPrevious\nKnowledge Base Standards\nNext\nüóÑÔ∏è AI & Intelligent Agents BLT\nProduct Definition\nArchitecture Diagram C0: Tech Stack\nTemplate\nArchitecture Diagram C1: High-Level Architecture\nTemplate\nArchitecture Diagram C2: Detailed Component Architecture\nTemplate\nData Flow Diagram\nTemplate Option 1: Sequence Diagram (Best for User Interactions)\nTemplate Option 2: Graph Diagram (Best for Multiple Paths/Scenarios)\nGenAI Workflow Diagram (for Agentic Solutions)\nLLM/ML Model Details\nData Governance & Privacy\nEvaluation & Validation\nUsage Tips\nQuestions or Feedback?\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
  "links_found": 18,
  "depth": 3,
  "crawled_at": "2026-02-25T10:07:40.191315"
}