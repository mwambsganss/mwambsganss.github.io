{
  "https://techhq.dc.lilly.com": {
    "url": "https://techhq.dc.lilly.com",
    "title": "Tech HQ",
    "description": "A one-stop-shop for Tech@Lilly's Tech Strategy",
    "h1": [
      "TechHQ - Empowering Tech Leadership & Architects"
    ],
    "h2": [],
    "h3": [
      "Core Pillars",
      "Enabler",
      "Key Outcomes"
    ],
    "text_content": "Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nTechHQ - Empowering Tech Leadership & Architects\nInnovate\nPlan\nSolution\nAccess\nLearn\nInnovate\n:\nExplore how to align innovation with business goals while staying ahead of emerging technologies through shared exploration and purposeful design thinking.\nCore Pillars\nSpeed to Value\n: Deliver solutions quickly and adapt to changing business needs\nEnterprise Scale\n: Ensure consistency and resilience through standard platforms\nTech Innovation\n: Embrace emerging tech to unlock new opportunities\nEnabler\nArchitecture & Products\n: The foundation that connects and supports all three pillars\nKey Outcomes\nCustomer Outcomes\n: Deliver trusted, life-changing solutions\nTeam Lilly\n: Build a purpose-driven, empowered culture\nTechnical Excellence\n: Achieve high-quality execution and innovation\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 13,
    "depth": 0,
    "crawled_at": "2026-02-25T10:05:39.626412"
  },
  "https://techhq.dc.lilly.com/": {
    "url": "https://techhq.dc.lilly.com/",
    "title": "Tech HQ",
    "description": "A one-stop-shop for Tech@Lilly's Tech Strategy",
    "h1": [
      "TechHQ - Empowering Tech Leadership & Architects"
    ],
    "h2": [],
    "h3": [
      "Core Pillars",
      "Enabler",
      "Key Outcomes"
    ],
    "text_content": "Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nTechHQ - Empowering Tech Leadership & Architects\nInnovate\nPlan\nSolution\nAccess\nLearn\nInnovate\n:\nExplore how to align innovation with business goals while staying ahead of emerging technologies through shared exploration and purposeful design thinking.\nCore Pillars\nSpeed to Value\n: Deliver solutions quickly and adapt to changing business needs\nEnterprise Scale\n: Ensure consistency and resilience through standard platforms\nTech Innovation\n: Embrace emerging tech to unlock new opportunities\nEnabler\nArchitecture & Products\n: The foundation that connects and supports all three pillars\nKey Outcomes\nCustomer Outcomes\n: Deliver trusted, life-changing solutions\nTeam Lilly\n: Build a purpose-driven, empowered culture\nTechnical Excellence\n: Achieve high-quality execution and innovation\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 12,
    "depth": 1,
    "crawled_at": "2026-02-25T10:05:41.743747"
  },
  "https://techhq.dc.lilly.com/docs/innovate/": {
    "url": "https://techhq.dc.lilly.com/docs/innovate/",
    "title": "üí° Tech@Lilly Innovation Pipeline | Tech HQ",
    "description": "Tech Innovation Splash Banner",
    "h1": [
      "üí° Tech@Lilly Innovation Pipeline"
    ],
    "h2": [
      "The Why: A Key Growth Enabler‚Äã",
      "The How: Innovation Enablement‚Äã",
      "The What: Catalyzing Tech Innovation‚Äã"
    ],
    "h3": [],
    "text_content": "üí° Tech@Lilly Innovation Pipeline | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüí° Tech@Lilly Innovation Pipeline\nüõû Our Innovation Stages\nüèéÔ∏è Fast Start\nüì° Emerging Tech\nAI Avatars / Digital People\nAI Speech\nAI Video Generation\nüí° Tech@Lilly Innovation Pipeline\nOn this page\nüí° Tech@Lilly Innovation Pipeline\nOur Innovation Pipeline is a powerful framework and set of enablers for all of us across Tech@Lilly to bring our ideas\nfrom concept to reality and navigate through barriers to tech-driven transformation.\nStart your innovation journey at\ntechpipeline.lilly.com\nStarting a new tech innovation team?\nNeed to build a new Alpha?\nCurious about Tech@Lilly's Innovation Council?\nThen check out the\nInnovation Playbook\n!\nThe Why: A Key Growth Enabler\n‚Äã\nInnovation is part of the human experience. It connects us all. Everyone should innovate.\nOur Innovation Pipeline will help simply our work so we can innovate faster with focus and purpose.\nLilly‚Äôs objectives compel us to Increase Our Lead and Embrace Technology.\nThe How: Innovation Enablement\n‚Äã\nOur Innovation Pipeline is a structured pathway that helps identify, validate, and collaborate on innovations with\npotential to transform, eliminate work, or lead the industry.\nAdopting our common framework and terminology enables efficient and effective communication and decisions.\nOur Innovation Pipeline is powered by several enablers:\nPipeline Accelerator\nIncubation Lab\nInnovation Challenges\nThe Garage\nThe What: Catalyzing Tech Innovation\n‚Äã\nOur Innovation Pipeline captures, nurtures, and supports big ideas.\nSharing and validating our ideas is a catalyst to transitioning from concept to value.\nVisibility and transparency into the innovation process creates opportunities to collaborate and promote reuse.\nTags:\ninnovation\nEdit this page\nNext\nüõû Our Innovation Stages\nThe Why: A Key Growth Enabler\nThe How: Innovation Enablement\nThe What: Catalyzing Tech Innovation\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 15,
    "depth": 1,
    "crawled_at": "2026-02-25T10:05:43.901454"
  },
  "https://techhq.dc.lilly.com/docs/plan/": {
    "url": "https://techhq.dc.lilly.com/docs/plan/",
    "title": "üìÖ Plan Overview | Tech HQ",
    "description": "Welcome to Plan ‚Äî your starting point for aligning strategy, managing technology lifecycles, and enabling continuous",
    "h1": [
      "üìÖ Plan Overview"
    ],
    "h2": [
      "Purpose‚Äã"
    ],
    "h3": [],
    "text_content": "üìÖ Plan Overview | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüìÖ Plan Overview\n‚ôªÔ∏è Lifecycle Management\nüìÖ Plan Overview\nüìÖ Plan Overview\nWelcome to\nPlan\n‚Äî your starting point for aligning strategy, managing technology lifecycles, and enabling continuous\nimprovement across the organization.\nPurpose\n‚Äã\nUse this space to:\nDevelop strategies aligned to shared objectives, expectations, and roadmaps\n(planned)\nManage the full lifecycle of digital assets to maintain operational excellence and adaptability\nExplore and evaluate emerging technologies to inform future planning\n(planned)\nWas this helpful?\nEdit this page\nNext\n‚ôªÔ∏è Lifecycle Management\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 7,
    "depth": 1,
    "crawled_at": "2026-02-25T10:05:46.031329"
  },
  "https://techhq.dc.lilly.com/docs/solution/": {
    "url": "https://techhq.dc.lilly.com/docs/solution/",
    "title": "üß© Solution Overview | Tech HQ",
    "description": "Welcome to Solution ‚Äî your space for designing, assembling, and delivering technology solutions that drive",
    "h1": [
      "üß© Solution Overview"
    ],
    "h2": [
      "Purpose‚Äã",
      "Our Reference Architecture‚Äã",
      "Use Cases In Action‚Äã",
      "Tech Positioning‚Äã"
    ],
    "h3": [
      "Problem Exploration‚Äã",
      "SaaS Integration‚Äã",
      "Analytics & Compute‚Äã",
      "Custom AI & Web Tools‚Äã",
      "Internal Custom Development‚Äã",
      "Third-Party Integration‚Äã"
    ],
    "text_content": "üß© Solution Overview | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nüß© Solution Overview\nOn this page\nüß© Solution Overview\nWelcome to\nSolution\n‚Äî your space for designing, assembling, and delivering technology solutions that drive\ninnovation and business success.\nPurpose\n‚Äã\nUse this space to:\nDesign solutions composed of capabilities, technologies, and components\nAssemble and integrate technology to deliver value and enable innovation\nLeverage solution guides to accelerate decision-making, increase enterprise scale, and enhance value\nNeed Help or Want to Contribute?\nIf you're looking for a guide that isn't currently available, please contact Karl Mayer.\nWe also need your contributions - please see our\nContribute page\nOur Reference Architecture\n‚Äã\nIn today‚Äôs complex solutioning landscape, teams face a wide spectrum of needs‚Äîfrom ambiguous problem statements to\nhighly specialized technical implementations. Our Reference Architecture exists to bring clarity, consistency, and\nalignment across that spectrum.\nIt‚Äôs more than a technical blueprint‚Äîit‚Äôs a thinking model that helps teams:\nUnderstand where they are in the solutioning journey\nAlign on the right architectural patterns\nNavigate toward the appropriate solution categories\nThis architecture is modular, scalable, and built to evolve‚Äîjust like the problems we‚Äôre solving.\nUse Cases In Action\n‚Äã\nProblem Exploration\n‚Äã\n\"I'm not sure what my problem is.\"\n\"I know my problem and need to express it.\"\nThese are early-stage innovation needs. The architecture supports teams with tools for problem framing and collaborative\ndesign. These often lead into categories like\nAI & Intelligent Agents\nor\nBusiness Enablement\n, where intelligent\nsearch, automation, or knowledge systems help shape the solution.\nSaaS Integration\n‚Äã\n\"I've purchased a SaaS solution I want to implement.\"\nThis is a clear implementation scenario. The architecture provides identity, data integration, and orchestration\nplatforms to ensure secure and scalable onboarding. These needs typically align with\nCloud & Infrastructure\n,\nCybersecurity\n, and\nEngineering Enablement\n.\nAnalytics & Compute\n‚Äã\n\"I need high-performance analytics without specialized infrastructure.\"\n\"I need high-performance analytics with custom infrastructure.\"\nThe architecture supports both managed and custom compute paths:\nFor managed needs, analytics-as-a-service platforms offer speed and scale.\nFor custom needs, utility architecture enables tailored infrastructure design.\nThese use cases align with\nData & Analytics\nand\nCloud & Infrastructure\n.\nCustom AI & Web Tools\n‚Äã\n\"I want to implement a specialized AI solution in a custom web tool.\"\nThis blends innovation with delivery. The architecture supports this through a Dev Factory model with AI and web\ndevelopment specialties, and close collaboration with UX and business design. These efforts span\nAI & Intelligent\nAgents\n,\nUser Experience & Design\n, and\nEngineering Enablement\n.\nInternal Custom Development\n‚Äã\n\"I want to build a custom solution with my dev team.\"\nThis is a full-stack build scenario. The architecture provides DevX services, platforms, and API enablement to empower\ninternal teams. These efforts typically draw from\nEngineering Enablement\n,\nCloud & Infrastructure\n, and\nObservability & Reliability\n.\nThird-Party Integration\n‚Äã\n\"A partner needs to use our identity, AE checking, and consent models.\"\nThis is a governance-heavy integration. The architecture supports it through secure APIs, workflow orchestration, and\nbusiness services. These needs intersect with\nCybersecurity\n,\nCloud & Infrastructure\n, and\nBusiness\nEnablement\n‚Äîensuring compliance, control, and auditability.\nTech Positioning\n‚Äã\nTech positioning is the strategic process of aligning technology (including IaaS, PaaS, SaaS, tools, accelerators,\ncomponents, and features) with Lilly‚Äôs goals and business needs. This ensures that every technology investment supports\nbroader objectives, such as enhancing customer experience, improving operational efficiency, and driving innovation.\nTechnology Classifications\nWe use the following\ntechnology classifications\nto describe the maturity, adoption, and strategic relevance of\ntechnologies in Tech@Lilly:\nEmerging\n‚Äì Technologies or capabilities in the early stages of exploration and experimentation. These are\ntypically undergoing\nproof-of-value\n,\npilot programs\n, or\nviability testing\nto assess potential business impact\nand feasibility.\nSpecialized\n‚Äì Solutions that are\ntailored to specific use cases\n, departments, or business units. They are\nnot\nbroadly adopted\nacross the enterprise but deliver high value in their niche applications.\nStandard\n‚Äì Technologies or capabilities that are\napproved for enterprise-wide use\n. They are\nwell-supported,\nstable, and scalable\n, with established governance, documentation, and integration patterns.\nStrategic\n‚Äì Foundational technologies or platforms that are\ncritical to the enterprise‚Äôs long-term strategy\nand operations\n. These receive\nongoing investment\n, are\ndeeply embedded\nin business processes, and are\ncentral to\ninnovation, scalability, and resilience\n.\nDeclining\n‚Äì Technologies or capabilities that are\nno longer aligned with strategic direction\n. Investment is\nbeing reduced, and a\nretirement or replacement plan\nis being developed or considered.\nRetiring\n‚Äì Solutions that are in the\nactive process of being phased out\n. Replacement technologies have been\nidentified, and\nmigration or decommissioning efforts\nare underway.\nExited\n‚Äì Technologies or capabilities that have been\nfully retired\n. They are\nno longer supported, approved, or\nin use\nwithin the enterprise environment.\nProhibited\n‚Äì Technologies or practices that are\nexplicitly blocked or restricted\ndue to\nsubstantial risk\n,\ncompliance issues\n, or a\nformal enterprise architecture decision\n. Use is not permitted under any circumstances.\nTech positioning plays a key role in optimizing our technology landscape, mitigate risks, and minimizing the cost of\nchange. It is designed for architects and technical leaders responsible for delivering products and solutions and for\naligning technology with strategic business goals.\nYou can search and browse tech positioning records on\nLillyFlow\nusing the\ntech-positioning\ntag, which are maintained by EBA as articles. To browse records for a specific capability\nor category, use the following links:\nAsk your tech positioning questions on LillyFlow\nfor\nothers to reuse the answers. Use the\ntech-positioning\ntag and, if applicable, include links to the related articles in\nyour question. :::\nCapability\nTag\nAI\nai-platforms\nAutomation\nenterprise-automation\nCloud\naiops\ncloud-computing\ncontent-management\nobservability\nCollaboration\ncollaboration-tools\nevent-broadcasting\nlow-code-platforms\nCybersecurity\ncybersecurity\nData\nbusiness-intelligence\ndata-science-tools\ndatabase-platforms\nintegration-platforms\nsearch-platforms\nMobility\napp-delivery-platforms\nSoftware Development\ndev-tools\nUX\nux-tools\nWas this helpful?\nEdit this page\nNext\nü§ñ AI & Intelligent Agents\nPurpose\nOur Reference Architecture\nUse Cases In Action\nProblem Exploration\nSaaS Integration\nAnalytics & Compute\nCustom AI & Web Tools\nInternal Custom Development\nThird-Party Integration\nTech Positioning\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 15,
    "depth": 1,
    "crawled_at": "2026-02-25T10:05:48.661235"
  },
  "https://techhq.dc.lilly.com/docs/access/": {
    "url": "https://techhq.dc.lilly.com/docs/access/",
    "title": "üß∞ Tech@Lilly Toolbox | Tech HQ",
    "description": "Need the right tool to boost your team's productivity and delivery? Explore these ready-to-use resources across AI, automation, collaboration, development, and more.",
    "h1": [
      "üß∞ Tech@Lilly Toolbox",
      "Accessibility Tools",
      "AI Tools",
      "Automation Tools",
      "Collaboration Tools",
      "Data Analytics and Wrangling Tools",
      "Developer Tools",
      "External Collab and Meeting Tools",
      "Software Requests",
      "UX Tools"
    ],
    "h2": [],
    "h3": [],
    "text_content": "üß∞ Tech@Lilly Toolbox | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß∞ Tech@Lilly Toolbox\nüß∞ Tech@Lilly Toolbox\nüß∞ Tech@Lilly Toolbox\nNeed the right tool to boost your team's productivity and delivery? Explore these ready-to-use resources across AI, automation, collaboration, development, and more.\nIf you cannot find a specific tool, please ask on\nLillyFlow\n.\nAccessibility Tools\nEnsure that digital materials are accessible to all employees and customers.\nAccess Lilly\nAI Tools\nInnovative AI solutions to improve workflows and drive efficiency!\nAI Products\nAutomation Tools\nCheck out our ready-to-use automation and workflow tools.\nAutomation Tech Zone\nCollaboration Tools\nLearn about all our available collaboration tools at tech.lilly.com.\nDiscovering Tech@Lilly\nData Analytics and Wrangling Tools\nEnterprise Data's tools, including Analytics Workbench, work great with or without Marketplace data.\nData Marketplace Tools\nDeveloper Tools\nEnterprise DevOps, Automation and Tools (EDAT)'s developer front door is your one-stop shop for developer tools.\nDeveloper Front Door\nExternal Collab and Meeting Tools\nSecure and efficient ways to share and collaborate with third parties.\nDigital Workplace\nSoftware Requests\nYou'll find desktop tools here, including most Adobe products.\nGlobal Software Request\nUX Tools\nTake advantage of our UX tools when you build your next human-centered user interface!\nBrowse UX Tools\nWas this helpful?\nEdit this page\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 3,
    "depth": 1,
    "crawled_at": "2026-02-25T10:05:50.835992"
  },
  "https://techhq.dc.lilly.com/docs/learn/": {
    "url": "https://techhq.dc.lilly.com/docs/learn/",
    "title": "üìö Learn | Tech HQ",
    "description": "Welcome to Learn ‚Äî a space to grow your architectural expertise and stay sharp in a fast-moving tech landscape.",
    "h1": [
      "üìö Learn"
    ],
    "h2": [
      "Purpose‚Äã"
    ],
    "h3": [],
    "text_content": "üìö Learn | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüìö Learn\nüõ§Ô∏è EBA Guiding Principles\nüìê Tech@Lilly Well-architected\nüìö Learn\nüìö Learn\nWelcome to\nLearn\n‚Äî a space to grow your architectural expertise and stay sharp in a fast-moving tech landscape.\nPurpose\n‚Äã\nUse this space to:\nBuild knowledge through guiding principles, frameworks, and hands-on learning\nStrengthen your architectural craft to stay relevant and impactful\nExplore well-architected practices and prepare for what‚Äôs next in your learning journey\n(coming soon)\nWas this helpful?\nEdit this page\nNext\nüõ§Ô∏è EBA Guiding Principles\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 5,
    "depth": 1,
    "crawled_at": "2026-02-25T10:05:52.997730"
  },
  "https://techhq.dc.lilly.com/docs/contribute/": {
    "url": "https://techhq.dc.lilly.com/docs/contribute/",
    "title": "ü§≤ TechHQ Contributions | Tech HQ",
    "description": "Thank you for your interest in contributing to Tech@Lilly‚Äôs technical guidance and AI knowledge base ‚Äî your input is",
    "h1": [
      "ü§≤ TechHQ Contributions"
    ],
    "h2": [
      "Solution Guide Contributions‚Äã",
      "Source Code Contributions‚Äã",
      "Running TechHQ Locally‚Äã"
    ],
    "h3": [
      "Prerequisites‚Äã",
      "Development Builds‚Äã",
      "Production Testing‚Äã"
    ],
    "text_content": "ü§≤ TechHQ Contributions | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nü§≤ TechHQ Contributions\nüìù Markdown Extensions\nüß≠ Solution Guide Template\nüìÖ Solution Guide Timeline\nü§≤ TechHQ Contributions\nOn this page\nü§≤ TechHQ Contributions\nThank you for your interest in contributing to Tech@Lilly‚Äôs technical guidance and AI knowledge base ‚Äî your input is\ntruly appreciated!\nTechHQ embraces a collaborative model inspired by wikis and open source communities. In our case, we follow an\ninner source\napproach, where everyone is\nencouraged to share, improve, and build together across teams.\nWhat should you put on TechHQ?\nTechnical guidance on TechHQ should be sufficiently complete to aid architects and detailed enough for AI usage. To keep\nour technical knowledge DRY (Don't Repeat Yourself), avoid repeating content already published elsewhere; instead, link\nto important external resources.\nSolution Guide Contributions\n‚Äã\nSolution guides are now synced daily from Stack Overflow articles.\nTo contribute:\nSign up to own a guide in our\nTechHQ Solution Guides spreadsheet\n, which is on our\nTechHQ Teams site\n.\nUse the\nTechHQ Solution Guide Drafter\n(Copilot AI) to start your first draft.\nMost TechHQ Solution Guides have a Use Case / Technology table;\nContainer Services\nis a good example. Other guides are\nsimply a high-level overview of what architects should know, such as\nResiliency & Disaster Recovery‚Äã\n.\nCopy the draft markdown into a\nnew Stack Overflow article\n.\nFor consistency, prefix the guide name with üß≠, and be sure to use the INFO block at the top, as seen in the\nSolution Guide Template\n.\nAdd three tags to the article:\ntechhq\n,\nsolution-guide\n, and one the following:\nai\n- Guidance for generative AI\nand LLMs\nbusiness-enablement\n-\nGuidance for our line-of-business platforms and capabilities, e.g. clinical operations, regulatory, manufacturing,\nsupply chain, commercial\ncloud\n- Guidance for cloud\n(public, private, IaaS, PaaS, & SaaS)\ncybersecurity\n- Guidance for\ncybersecurity, information security, and security best practices\ndata\n- Guidance for data to\ninsights guides, analytics, databases, and data science\nsoftware-engineering\n-\nGuidance for CI/CD, automation, infrastructure as code, and deployment practices\nobservability\n-\nGuidance for monitoring, logging, tracing, and application performance management\nproductivity\n-\nGuidance for tools and practices that enhance individual and team efficiency\nux\n- Guidance for user\nexperience design, usability, accessibility, and interface best practices\nYour solution guide article will be automatically be organized into TechHQ.\nSource Code Contributions\n‚Äã\nMinor, individual file edits\ncan be made directly to the dev branch. These edits will be automatically deployed to\nhttps://techhq-d.dc.lilly.com\n.\nAll edits\nwill be reviewed prior to prod deployments. Reviewers will vary based on\nthe change. Commit messages must capture the change rationale.\nFor new docs drafts and multi-doc edits\n, create a branch with the prefix\ndraft/\n. Open a PR to dev when ready.\nFor new site features\n, create a branch with the prefix\nfeatures/\n. Open a PR to dev when ready.\nNeither draft/ nor feature/ branches are automatically deployed. 'Automatically delete head branches' is not enabled;\ndelete your branch if it is complete or inactive.\nRunning TechHQ Locally\n‚Äã\nPrerequisites\n‚Äã\nNode.js\npnpm\nDevelopment Builds\n‚Äã\nThe dev server automatically reflects most changes without restart.\n# build & run dev\npnpm install\npnpm dev\n# refresh content from Stack Overflow\npnpm stackoverflow-sync\n# lint & typecheck your code!\npnpm eslint\npnpm tsc\nProduction Testing\n‚Äã\nThe prod server has indexed search and caching for performance.\npnpm install\npnpm build\npnpm serve\nWas this helpful?\nEdit this page\nNext\nüìù Markdown Extensions\nSolution Guide Contributions\nSource Code Contributions\nRunning TechHQ Locally\nPrerequisites\nDevelopment Builds\nProduction Testing\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 6,
    "depth": 1,
    "crawled_at": "2026-02-25T10:05:55.208653"
  },
  "https://techhq.dc.lilly.com/blog": {
    "url": "https://techhq.dc.lilly.com/blog",
    "title": "Blog | Tech HQ",
    "description": "Blog",
    "h1": [],
    "h2": [
      "Summer 2025 We've expanded our [Solution Guide Library](/docs/solution/ai/) with new guides including Knowledge Sharing,pdates",
      "February Updates Roll-up",
      "Translations Solution Guide",
      "Welcome!"
    ],
    "h3": [
      "2025",
      "2024"
    ],
    "text_content": "Blog | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nRecent posts\n2025\nSummer 2025 We've expanded our [Solution Guide Library](/docs/solution/ai/) with new guides including Knowledge Sharing,pdates\nFebruary Updates Roll-up\n2024\nTranslations Solution Guide\nWelcome!\nSummer 2025 We've expanded our [Solution Guide Library](/docs/solution/ai/) with new guides including Knowledge Sharing,pdates\nJune 25, 2025\n¬∑\n2 min read\nKarl Mayer\nDigital Core Lead Architect\nIt‚Äôs been an eventful few months since our February updates roll-up! The TechHQ platform has evolved significantly with\nmajor additions to our AI architecture ecosystem, enhanced innovation pipeline framework, and expanded solution guide\nlibrary. Here's what's new this summer:\nTags:\nTech HQ\nRead more\nFebruary Updates Roll-up\nFebruary 19, 2025\n¬∑\n2 min read\nKarl Mayer\nDigital Core Lead Architect\nOur priorities for TechHQ are currently decision guidance and user experience. 5 guides are in progress, and more are queued.\nWork items, including solution guides, are now tracked in a\nJira project\nopen to read to all Jira users.\nOur\nTranslation tech solution guide\ntransitions from Request For Comment to Living Document.\nTechHQ dev\nreceives a\nContribute\nmenu item, describing how to make TechHQ contributions. In the future, this may be available in TechHQ prod.\nOther TechHQ changes include:\nTags:\nTech HQ\nRead more\nTranslations Solution Guide\nDecember 16, 2024\n¬∑\nOne min read\nKarl Mayer\nDigital Core Lead Architect\nOur third solution guide,\nTranslation technologies\n, is now available in Draft status. In addition, we've revised the guides to include:\nlifecycle status\n(\nDraft\n,\nRequest For Comment\n, or\nLiving Document\n)\ncapability owner, who is also the respective solution guide owner\nEBA lead team representative\nsolution guide contributors & reviewers, i.e. the technology working group\nTags:\nTech HQ\nRead more\nWelcome!\nOctober 7, 2024\n¬∑\nOne min read\nKarl Mayer\nDigital Core Lead Architect\nIt‚Äôs time to cut the ribbon, and throw open the doors to Tech HQ.\nWelcome to a new headquarters for architects and technologists in Tech@Lilly. This site is your starting point for finding the guidance, tools, and practices you need to excel.\nTags:\nTech HQ\nRead more\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 19,
    "depth": 1,
    "crawled_at": "2026-02-25T10:05:57.391199"
  },
  "https://techhq.dc.lilly.com/docs/innovate": {
    "url": "https://techhq.dc.lilly.com/docs/innovate",
    "title": "üí° Tech@Lilly Innovation Pipeline | Tech HQ",
    "description": "Tech Innovation Splash Banner",
    "h1": [
      "üí° Tech@Lilly Innovation Pipeline"
    ],
    "h2": [
      "The Why: A Key Growth Enabler‚Äã",
      "The How: Innovation Enablement‚Äã",
      "The What: Catalyzing Tech Innovation‚Äã"
    ],
    "h3": [],
    "text_content": "üí° Tech@Lilly Innovation Pipeline | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüí° Tech@Lilly Innovation Pipeline\nüõû Our Innovation Stages\nüèéÔ∏è Fast Start\nüì° Emerging Tech\nAI Avatars / Digital People\nAI Speech\nAI Video Generation\nüí° Tech@Lilly Innovation Pipeline\nOn this page\nüí° Tech@Lilly Innovation Pipeline\nOur Innovation Pipeline is a powerful framework and set of enablers for all of us across Tech@Lilly to bring our ideas\nfrom concept to reality and navigate through barriers to tech-driven transformation.\nStart your innovation journey at\ntechpipeline.lilly.com\nStarting a new tech innovation team?\nNeed to build a new Alpha?\nCurious about Tech@Lilly's Innovation Council?\nThen check out the\nInnovation Playbook\n!\nThe Why: A Key Growth Enabler\n‚Äã\nInnovation is part of the human experience. It connects us all. Everyone should innovate.\nOur Innovation Pipeline will help simply our work so we can innovate faster with focus and purpose.\nLilly‚Äôs objectives compel us to Increase Our Lead and Embrace Technology.\nThe How: Innovation Enablement\n‚Äã\nOur Innovation Pipeline is a structured pathway that helps identify, validate, and collaborate on innovations with\npotential to transform, eliminate work, or lead the industry.\nAdopting our common framework and terminology enables efficient and effective communication and decisions.\nOur Innovation Pipeline is powered by several enablers:\nPipeline Accelerator\nIncubation Lab\nInnovation Challenges\nThe Garage\nThe What: Catalyzing Tech Innovation\n‚Äã\nOur Innovation Pipeline captures, nurtures, and supports big ideas.\nSharing and validating our ideas is a catalyst to transitioning from concept to value.\nVisibility and transparency into the innovation process creates opportunities to collaborate and promote reuse.\nWas this helpful?\nTags:\ninnovation\nEdit this page\nNext\nüõû Our Innovation Stages\nThe Why: A Key Growth Enabler\nThe How: Innovation Enablement\nThe What: Catalyzing Tech Innovation\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 9,
    "depth": 1,
    "crawled_at": "2026-02-25T10:05:59.577620"
  },
  "https://techhq.dc.lilly.com/docs/plan": {
    "url": "https://techhq.dc.lilly.com/docs/plan",
    "title": "üìÖ Plan Overview | Tech HQ",
    "description": "Welcome to Plan ‚Äî your starting point for aligning strategy, managing technology lifecycles, and enabling continuous",
    "h1": [
      "üìÖ Plan Overview"
    ],
    "h2": [
      "Purpose‚Äã"
    ],
    "h3": [],
    "text_content": "üìÖ Plan Overview | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüìÖ Plan Overview\n‚ôªÔ∏è Lifecycle Management\nüìÖ Plan Overview\nüìÖ Plan Overview\nWelcome to\nPlan\n‚Äî your starting point for aligning strategy, managing technology lifecycles, and enabling continuous\nimprovement across the organization.\nPurpose\n‚Äã\nUse this space to:\nDevelop strategies aligned to shared objectives, expectations, and roadmaps\n(planned)\nManage the full lifecycle of digital assets to maintain operational excellence and adaptability\nExplore and evaluate emerging technologies to inform future planning\n(planned)\nWas this helpful?\nEdit this page\nNext\n‚ôªÔ∏è Lifecycle Management\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 2,
    "depth": 1,
    "crawled_at": "2026-02-25T10:06:01.753897"
  },
  "https://techhq.dc.lilly.com/docs/solution": {
    "url": "https://techhq.dc.lilly.com/docs/solution",
    "title": "üß© Solution Overview | Tech HQ",
    "description": "Welcome to Solution ‚Äî your space for designing, assembling, and delivering technology solutions that drive",
    "h1": [
      "üß© Solution Overview"
    ],
    "h2": [
      "Purpose‚Äã",
      "Our Reference Architecture‚Äã",
      "Use Cases In Action‚Äã",
      "Tech Positioning‚Äã"
    ],
    "h3": [
      "Problem Exploration‚Äã",
      "SaaS Integration‚Äã",
      "Analytics & Compute‚Äã",
      "Custom AI & Web Tools‚Äã",
      "Internal Custom Development‚Äã",
      "Third-Party Integration‚Äã"
    ],
    "text_content": "üß© Solution Overview | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nüß© Solution Overview\nOn this page\nüß© Solution Overview\nWelcome to\nSolution\n‚Äî your space for designing, assembling, and delivering technology solutions that drive\ninnovation and business success.\nPurpose\n‚Äã\nUse this space to:\nDesign solutions composed of capabilities, technologies, and components\nAssemble and integrate technology to deliver value and enable innovation\nLeverage solution guides to accelerate decision-making, increase enterprise scale, and enhance value\nNeed Help or Want to Contribute?\nIf you're looking for a guide that isn't currently available, please contact Karl Mayer.\nWe also need your contributions - please see our\nContribute page\nOur Reference Architecture\n‚Äã\nIn today‚Äôs complex solutioning landscape, teams face a wide spectrum of needs‚Äîfrom ambiguous problem statements to\nhighly specialized technical implementations. Our Reference Architecture exists to bring clarity, consistency, and\nalignment across that spectrum.\nIt‚Äôs more than a technical blueprint‚Äîit‚Äôs a thinking model that helps teams:\nUnderstand where they are in the solutioning journey\nAlign on the right architectural patterns\nNavigate toward the appropriate solution categories\nThis architecture is modular, scalable, and built to evolve‚Äîjust like the problems we‚Äôre solving.\nUse Cases In Action\n‚Äã\nProblem Exploration\n‚Äã\n\"I'm not sure what my problem is.\"\n\"I know my problem and need to express it.\"\nThese are early-stage innovation needs. The architecture supports teams with tools for problem framing and collaborative\ndesign. These often lead into categories like\nAI & Intelligent Agents\nor\nBusiness Enablement\n, where intelligent\nsearch, automation, or knowledge systems help shape the solution.\nSaaS Integration\n‚Äã\n\"I've purchased a SaaS solution I want to implement.\"\nThis is a clear implementation scenario. The architecture provides identity, data integration, and orchestration\nplatforms to ensure secure and scalable onboarding. These needs typically align with\nCloud & Infrastructure\n,\nCybersecurity\n, and\nEngineering Enablement\n.\nAnalytics & Compute\n‚Äã\n\"I need high-performance analytics without specialized infrastructure.\"\n\"I need high-performance analytics with custom infrastructure.\"\nThe architecture supports both managed and custom compute paths:\nFor managed needs, analytics-as-a-service platforms offer speed and scale.\nFor custom needs, utility architecture enables tailored infrastructure design.\nThese use cases align with\nData & Analytics\nand\nCloud & Infrastructure\n.\nCustom AI & Web Tools\n‚Äã\n\"I want to implement a specialized AI solution in a custom web tool.\"\nThis blends innovation with delivery. The architecture supports this through a Dev Factory model with AI and web\ndevelopment specialties, and close collaboration with UX and business design. These efforts span\nAI & Intelligent\nAgents\n,\nUser Experience & Design\n, and\nEngineering Enablement\n.\nInternal Custom Development\n‚Äã\n\"I want to build a custom solution with my dev team.\"\nThis is a full-stack build scenario. The architecture provides DevX services, platforms, and API enablement to empower\ninternal teams. These efforts typically draw from\nEngineering Enablement\n,\nCloud & Infrastructure\n, and\nObservability & Reliability\n.\nThird-Party Integration\n‚Äã\n\"A partner needs to use our identity, AE checking, and consent models.\"\nThis is a governance-heavy integration. The architecture supports it through secure APIs, workflow orchestration, and\nbusiness services. These needs intersect with\nCybersecurity\n,\nCloud & Infrastructure\n, and\nBusiness\nEnablement\n‚Äîensuring compliance, control, and auditability.\nTech Positioning\n‚Äã\nTech positioning is the strategic process of aligning technology (including IaaS, PaaS, SaaS, tools, accelerators,\ncomponents, and features) with Lilly‚Äôs goals and business needs. This ensures that every technology investment supports\nbroader objectives, such as enhancing customer experience, improving operational efficiency, and driving innovation.\nTechnology Classifications\nWe use the following\ntechnology classifications\nto describe the maturity, adoption, and strategic relevance of\ntechnologies in Tech@Lilly:\nEmerging\n‚Äì Technologies or capabilities in the early stages of exploration and experimentation. These are\ntypically undergoing\nproof-of-value\n,\npilot programs\n, or\nviability testing\nto assess potential business impact\nand feasibility.\nSpecialized\n‚Äì Solutions that are\ntailored to specific use cases\n, departments, or business units. They are\nnot\nbroadly adopted\nacross the enterprise but deliver high value in their niche applications.\nStandard\n‚Äì Technologies or capabilities that are\napproved for enterprise-wide use\n. They are\nwell-supported,\nstable, and scalable\n, with established governance, documentation, and integration patterns.\nStrategic\n‚Äì Foundational technologies or platforms that are\ncritical to the enterprise‚Äôs long-term strategy\nand operations\n. These receive\nongoing investment\n, are\ndeeply embedded\nin business processes, and are\ncentral to\ninnovation, scalability, and resilience\n.\nDeclining\n‚Äì Technologies or capabilities that are\nno longer aligned with strategic direction\n. Investment is\nbeing reduced, and a\nretirement or replacement plan\nis being developed or considered.\nRetiring\n‚Äì Solutions that are in the\nactive process of being phased out\n. Replacement technologies have been\nidentified, and\nmigration or decommissioning efforts\nare underway.\nExited\n‚Äì Technologies or capabilities that have been\nfully retired\n. They are\nno longer supported, approved, or\nin use\nwithin the enterprise environment.\nProhibited\n‚Äì Technologies or practices that are\nexplicitly blocked or restricted\ndue to\nsubstantial risk\n,\ncompliance issues\n, or a\nformal enterprise architecture decision\n. Use is not permitted under any circumstances.\nTech positioning plays a key role in optimizing our technology landscape, mitigate risks, and minimizing the cost of\nchange. It is designed for architects and technical leaders responsible for delivering products and solutions and for\naligning technology with strategic business goals.\nYou can search and browse tech positioning records on\nLillyFlow\nusing the\ntech-positioning\ntag, which are maintained by EBA as articles. To browse records for a specific capability\nor category, use the following links:\nAsk your tech positioning questions on LillyFlow\nfor\nothers to reuse the answers. Use the\ntech-positioning\ntag and, if applicable, include links to the related articles in\nyour question. :::\nCapability\nTag\nAI\nai-platforms\nAutomation\nenterprise-automation\nCloud\naiops\ncloud-computing\ncontent-management\nobservability\nCollaboration\ncollaboration-tools\nevent-broadcasting\nlow-code-platforms\nCybersecurity\ncybersecurity\nData\nbusiness-intelligence\ndata-science-tools\ndatabase-platforms\nintegration-platforms\nsearch-platforms\nMobility\napp-delivery-platforms\nSoftware Development\ndev-tools\nUX\nux-tools\nWas this helpful?\nEdit this page\nNext\nü§ñ AI & Intelligent Agents\nPurpose\nOur Reference Architecture\nUse Cases In Action\nProblem Exploration\nSaaS Integration\nAnalytics & Compute\nCustom AI & Web Tools\nInternal Custom Development\nThird-Party Integration\nTech Positioning\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 11,
    "depth": 1,
    "crawled_at": "2026-02-25T10:06:03.969350"
  },
  "https://techhq.dc.lilly.com/docs/access": {
    "url": "https://techhq.dc.lilly.com/docs/access",
    "title": "üß∞ Tech@Lilly Toolbox | Tech HQ",
    "description": "Need the right tool to boost your team's productivity and delivery? Explore these ready-to-use resources across AI, automation, collaboration, development, and more.",
    "h1": [
      "üß∞ Tech@Lilly Toolbox",
      "Accessibility Tools",
      "AI Tools",
      "Automation Tools",
      "Collaboration Tools",
      "Data Analytics and Wrangling Tools",
      "Developer Tools",
      "External Collab and Meeting Tools",
      "Software Requests",
      "UX Tools"
    ],
    "h2": [],
    "h3": [],
    "text_content": "üß∞ Tech@Lilly Toolbox | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß∞ Tech@Lilly Toolbox\nüß∞ Tech@Lilly Toolbox\nüß∞ Tech@Lilly Toolbox\nNeed the right tool to boost your team's productivity and delivery? Explore these ready-to-use resources across AI, automation, collaboration, development, and more.\nIf you cannot find a specific tool, please ask on\nLillyFlow\n.\nAccessibility Tools\nEnsure that digital materials are accessible to all employees and customers.\nAccess Lilly\nAI Tools\nInnovative AI solutions to improve workflows and drive efficiency!\nAI Products\nAutomation Tools\nCheck out our ready-to-use automation and workflow tools.\nAutomation Tech Zone\nCollaboration Tools\nLearn about all our available collaboration tools at tech.lilly.com.\nDiscovering Tech@Lilly\nData Analytics and Wrangling Tools\nEnterprise Data's tools, including Analytics Workbench, work great with or without Marketplace data.\nData Marketplace Tools\nDeveloper Tools\nEnterprise DevOps, Automation and Tools (EDAT)'s developer front door is your one-stop shop for developer tools.\nDeveloper Front Door\nExternal Collab and Meeting Tools\nSecure and efficient ways to share and collaborate with third parties.\nDigital Workplace\nSoftware Requests\nYou'll find desktop tools here, including most Adobe products.\nGlobal Software Request\nUX Tools\nTake advantage of our UX tools when you build your next human-centered user interface!\nBrowse UX Tools\nWas this helpful?\nEdit this page\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 0,
    "depth": 1,
    "crawled_at": "2026-02-25T10:06:06.143490"
  },
  "https://techhq.dc.lilly.com/docs/learn": {
    "url": "https://techhq.dc.lilly.com/docs/learn",
    "title": "üìö Learn | Tech HQ",
    "description": "Welcome to Learn ‚Äî a space to grow your architectural expertise and stay sharp in a fast-moving tech landscape.",
    "h1": [
      "üìö Learn"
    ],
    "h2": [
      "Purpose‚Äã"
    ],
    "h3": [],
    "text_content": "üìö Learn | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüìö Learn\nüõ§Ô∏è EBA Guiding Principles\nüìê Tech@Lilly Well-architected\nüìö Learn\nüìö Learn\nWelcome to\nLearn\n‚Äî a space to grow your architectural expertise and stay sharp in a fast-moving tech landscape.\nPurpose\n‚Äã\nUse this space to:\nBuild knowledge through guiding principles, frameworks, and hands-on learning\nStrengthen your architectural craft to stay relevant and impactful\nExplore well-architected practices and prepare for what‚Äôs next in your learning journey\n(coming soon)\nWas this helpful?\nEdit this page\nNext\nüõ§Ô∏è EBA Guiding Principles\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 3,
    "depth": 1,
    "crawled_at": "2026-02-25T10:06:08.279180"
  },
  "https://techhq.dc.lilly.com/docs/innovate/innovation_stages": {
    "url": "https://techhq.dc.lilly.com/docs/innovate/innovation_stages",
    "title": "üõû Our Innovation Stages | Tech HQ",
    "description": "Pipeline Overview",
    "h1": [
      "üõû Our Innovation Stages"
    ],
    "h2": [
      "Pipeline Overview‚Äã",
      "Pre-Alpha‚Äã",
      "Alpha‚Äã",
      "Beta‚Äã",
      "Early Release‚Äã",
      "General Release‚Äã"
    ],
    "h3": [
      "Examples‚Äã",
      "Examples‚Äã",
      "Examples‚Äã",
      "Examples‚Äã"
    ],
    "text_content": "üõû Our Innovation Stages | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüí° Tech@Lilly Innovation Pipeline\nüõû Our Innovation Stages\nüèéÔ∏è Fast Start\nüì° Emerging Tech\nAI Avatars / Digital People\nAI Speech\nAI Video Generation\nüõû Our Innovation Stages\nOn this page\nüõû Our Innovation Stages\nPipeline Overview\n‚Äã\nOur Tech Innovation Pipeline is a structured pathway for innovations across Tech@Lilly with focused enablers and\nenhanced visibility into the progression of breakthrough technologies and disruptive solutions. The pipeline's five\nstages go from Conceptualization to Realization to Scale.\nPre-Alpha\nAlpha\nBeta\nEarly Release\nGeneral Release\nDescription\nExploratory stage to discover and define possibilities\nInitial development stage to prove feasibility\nInitial user feedback stage to guide improvement\nPilot stage; initiate real-world use with controlled risk\nScaling stage; deploying to full intended user base\nIntent\nExplore possibilities\nDevelop demo\nHands-on preview\nLaunch system\nScale system\nAudience\nInnovator(s)\nProduct Team\nRepresentative Users\nSubset of Real Users\nAll Users\nEnvironment\nNone / Local Host / Dev\nDev\nDev / QA\nProd\nProd\nSponsorship\nNone required\nInnovation or Functional\nTech@Lilly M5\nTech@Lilly M5\nTech@Lilly M5\nExample Criteria\nProblem statement(s)\n*\nValue proposition\n*\nSolution concepts\n*\nTech feasibility assessment\nLandscape assessment\nVendor comparisons\nTech research findings / Spikes\nDemo video\n*\nInitial UI / UX designs\nRepresentative prototype(s)\nStakeholder interest / support\nInitial architecture diagram\nFunctional solution\n*\nUser feedback and confirmation\nRefined UI/UX designs\nBusiness case\nRefined architecture diagrams\nInitial product backlog\nValidation / Qualification package\n*\nProduct roadmap\nScalable architecture\nSupport & maintenance plan\nRefined business case\nBusiness value confirmation\nOngoing product lifecycle\nIncremental improvements\nRelease schedule\nMonitoring and support\nSunsetting/Rationalization plan\n*\nrequired for stage exit\nPre-Alpha\n‚Äã\nThink of this as the brainstorming stage where we're gathering sketches, wireframes, and maybe even napkin doodles. This\nisn't about clarity but about exploring and sharing potential. It‚Äôs more than just an idea.\nDefinition\nPre-alpha is the EXPLORATORY stage centered around technological and experiential possibilities. The clarity of the\ninnovation's potential is still evolving, and it includes diverse artifacts and research materials. The pre-alpha stage\nis where an idea becomes a fledgling innovation with investigative work that discovers signals of feasibility,\ndesirability, and/or viability.\nPurpose\nThe goal of this stage is to gather and communicate ideas, identify potential features, and validate core concepts. It\nsets the groundwork for the upcoming, more defined stages of development.\nExamples\n‚Äã\nRocIT Submission\nDocIT Submission\nAlpha\n‚Äã\nNow, we're onto building a rough draft or prototype. It might be a piece of software, a design blueprint, or another\nkind of tangible representation. It's not complete and its OK to have glitches or missing parts, but it's the first real\nlook at our innovation. The primary objective here? Refine our vision, iron out the kinks, and prepare for the stages\nahead.\nDefinition\nAlpha is the stage where the innovation takes its first TANGIBLE form. This may be functional software or a UX design\nthat has a fair degree of fidelity, but neither are usually ‚Äòfeature-complete‚Äô. It is usually tested internally within\nthe team involved in the early stages. It likely has bugs and incomplete features.\nPurpose\nThe primary goal of the alpha stage is to identify key features and begin to define Product scope, strategy and vision\nmore closely with a demo. This stage allows for rapid testing of the product's functionality with primary stakeholders\nand serves as a preparation for the beta stage, where a larger contingent of resources and people are be needed to\nadvance it further.\nExamples\n‚Äã\nRocIT Alpha\nDocIT Alpha\nBeta\n‚Äã\nWe're getting there! Our innovation is now usable for a small audience but might still have a few rough edges. We are\nready to provide it to a select group of users and gather their feedback. Their insights will be crucial in making sure\nour product isn't just functional, but also user- friendly and ready for a larger set of users.\nDefinition\nBeta is the stage where the innovation is ready to be used for its primary intended purpose. It is the first time the\ninnovation is provided to a select group of users in a controlled environment. A beta release provides the stability\nrequired for end users to regularly exercise functionality with minimal disruption but does not guarantee service levels\nor support agreements.\nPurpose\nThe purpose of the beta stage is to gather FEEDBACK from real-world users who can identify major bugs, usability issues,\nor other potential problems that were not apparent during the alpha stage. This helps ensure the product is becoming a\nvaluable solution heading for Early Release.\nExamples\n‚Äã\nRocIT Beta\nDocIT Beta\nEarly Release\n‚Äã\nConsider this a trial run. Before we go all out, we try our innovation in a limited, real-world setting. When the stakes\nare high, it's a smart move to ensure the technology and processes are ready to scale.\nDefinition\nEarly Release is the CONFIRMATION stage where the innovation is tried in a limited real- world, supported environment.\nEarly Release limits risk when a product‚Äôs intended vision and scale includes a very large number of users/customers or\ncould negatively impact business functions if unforeseen challenges arise. This stage establishes a market test of an\ninnovation to help justify the larger investment needed to achieve full scale.\nPurpose\nThe purpose of the Early Release stage is to ensure we are ready to scale. It validates the product's functionality,\nusability, and reliability in a real-world setting before it is released to a larger audience. We learn from data to\ninform decisions and ensure scalability. This stage allows for the identification and resolution of operational issues\nbefore scaling to the full intended user/customer base.\nGeneral Release\n‚Äã\nTime to roll out! Our innovation is ready to face the world. It's polished, tested, and has the stamp of approval. From\nhere on, our focus pivots from initial innovation to promotion, continuous improvements, and a roadmap of future\nfeatures and capabilities.\nDefinition\nGeneral Release marks the official introduction of the innovation to its full intended audience, whether that's a\ndepartment, the entire company, or the broader public. At this point, the innovation is fully supported and equipped to\nSCALE.\nPurpose\nThe purpose of the General Release stage is to achieve the value promised by the innovation framework.\nExamples\n‚Äã\nCSA Launchpad Demo\nWas this helpful?\nTags:\ninnovation\nEdit this page\nPrevious\nüí° Tech@Lilly Innovation Pipeline\nNext\nüèéÔ∏è Fast Start\nPipeline Overview\nPre-Alpha\nExamples\nAlpha\nExamples\nBeta\nExamples\nEarly Release\nGeneral Release\nExamples\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 7,
    "depth": 2,
    "crawled_at": "2026-02-25T10:06:12.268802"
  },
  "https://techhq.dc.lilly.com/docs/innovate/fast-start/": {
    "url": "https://techhq.dc.lilly.com/docs/innovate/fast-start/",
    "title": "üèéÔ∏è Fast Start | Tech HQ",
    "description": "Introduction",
    "h1": [
      "üèéÔ∏è Fast Start"
    ],
    "h2": [
      "Introduction‚Äã",
      "Key Outcomes‚Äã",
      "Fast Start Approach‚Äã",
      "Leading Your Next Fast Start‚Äã",
      "Recap‚Äã"
    ],
    "h3": [
      "Prep‚Äã",
      "Define‚Äã",
      "Design‚Äã"
    ],
    "text_content": "üèéÔ∏è Fast Start | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüí° Tech@Lilly Innovation Pipeline\nüõû Our Innovation Stages\nüèéÔ∏è Fast Start\nüì° Emerging Tech\nAI Avatars / Digital People\nAI Speech\nAI Video Generation\nüèéÔ∏è Fast Start\nOn this page\nüèéÔ∏è Fast Start\nIntroduction\n‚Äã\nOur Product Fast Start process, initially called Strategic Product Architecture, is purposely designed to accelerate the initiation and development of new products by leveraging strategic alignment, mobilization of resources, and proactive acceleration. This process ensures that product teams are aligned with business goals and can deliver value quickly and efficiently.\nIn addition to this guide, see our\nProduct Fast Start Overview\nto learn more.\nThere are also two\nCopilot Agent accelerators\nto help:\nFast Start Product Starter\n- this agent helps you get started with questions you need to ask/answer and examples\nFast Start Product Accelerator\n- this agent helps you take your existing documents and artifacts and put them in a product strategy context\nKey Outcomes\n‚Äã\nAlignment\n: Ensuring that the product strategy is aligned with business objectives, providing a clear purpose and outcomes for the team.\nMobilization\n: Connecting and engaging the right Tech@Lilly teams for faster response and resource mobilization.\nAcceleration\n: Proactively accelerating the development and value delivery for early and future releases.\nPlatforms\n: Identifying technology synergies and reusability opportunities earlier in the process.\nFast Start Approach\n‚Äã\nPrep\n‚Äã\nEngage Key Stakeholders\n: Identify and involve business sponsors, Tech@Lilly product sponsors, product managers/owners, and product architects early in the process.\nDefine Clear Objectives\n: Set clear objectives and key results (OKRs) that align with the company's goals and ensure that the team is working towards a meaningful purpose.\nLeverage Existing Resources\n: Gather and utilize existing artifacts such as product vision/strategy, personas/journey maps, architecture diagrams, and business processes.\nDefine\n‚Äã\nProduct Definition\n: Collaborate upfront with product teams to define strategy and strategic architecture for priority initiatives.\nTech@Lilly Mobilization\n: Facilitate collaboration with Tech@Lilly teams to mobilize resources and leverage enterprise platforms/services.\nDesign\n‚Äã\nProduct Strategy\n: Align vision, roadmap, and OKRs to ensure delivery against clear outcomes and priorities.\nUser Experience\n: Define user journeys and personas to maintain a relentless focus on customer outcomes.\nLeading Your Next Fast Start\n‚Äã\nLeading your next Fast Start involves a commitment to collaboration, strategic planning, and continuous improvement. To get ready for your next product Fast Start, you should:\nEngage Key Stakeholders\n: Identify and involve business sponsors, Tech@Lilly product sponsors, product managers/owners, and product architects early in the process.\nDefine Clear Objectives\n: Set clear objectives and key results (OKRs) that align with the company's goals and ensure that the team is working towards a meaningful purpose.\nLeverage Existing Resources\n: Gather and utilize existing artifacts such as product vision/strategy, personas/journey maps, architecture diagrams, and business processes.\nFocus on Alignment and Mobilization\n: Ensure alignment across the organization and enhance engagement from Tech@Lilly teams for faster response.\nPlan for Continuous Improvement\n: Establish a continuous learning and action plan to address opportunity areas and ensure the success and sustainability of the initiative.\nRecap\n‚Äã\nThe Fast Start process is a comprehensive initiative aimed at enhancing our digital presence and improving customer engagement. By aligning product strategy, mobilizing Tech@Lilly teams, and creating a strategic architecture, we can ensure the success of our priority initiatives and provide a better user experience. Connect with Enterprise Business Architecture for further guidance and recommendations on taking full advantage of Fast Start.\nWas this helpful?\nEdit this page\nPrevious\nüõû Our Innovation Stages\nNext\nüì° Emerging Tech\nIntroduction\nKey Outcomes\nFast Start Approach\nPrep\nDefine\nDesign\nLeading Your Next Fast Start\nRecap\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 5,
    "depth": 2,
    "crawled_at": "2026-02-25T10:06:14.649485"
  },
  "https://techhq.dc.lilly.com/docs/innovate/emerging_tech/": {
    "url": "https://techhq.dc.lilly.com/docs/innovate/emerging_tech/",
    "title": "üì° Emerging Tech | Tech HQ",
    "description": "Emerging Tech is where we explore the technologies that are starting to shape the future. From early signals to",
    "h1": [
      "üì° Emerging Tech"
    ],
    "h2": [],
    "h3": [],
    "text_content": "üì° Emerging Tech | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüí° Tech@Lilly Innovation Pipeline\nüõû Our Innovation Stages\nüèéÔ∏è Fast Start\nüì° Emerging Tech\nAI Avatars / Digital People\nAI Speech\nAI Video Generation\nüì° Emerging Tech\nüì° Emerging Tech\nEmerging Tech is where we explore the technologies that are starting to shape the future. From early signals to\nfast-moving trends, this space highlights what‚Äôs gaining traction, why it matters, and how it could influence the way we\ndesign, build, and operate. It‚Äôs a practical, forward-looking take on innovation‚Äîgrounded in what‚Äôs real and what‚Äôs\nnext.\nWas this helpful?\nEdit this page\nPrevious\nüèéÔ∏è Fast Start\nNext\nAI Avatars / Digital People\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 4,
    "depth": 2,
    "crawled_at": "2026-02-25T10:06:16.851767"
  },
  "https://techhq.dc.lilly.com/docs/innovate/emerging_tech/ai-avatars": {
    "url": "https://techhq.dc.lilly.com/docs/innovate/emerging_tech/ai-avatars",
    "title": "AI Avatars / Digital People | Tech HQ",
    "description": "- Last Update: 2025-08-05",
    "h1": [
      "AI Avatars / Digital People"
    ],
    "h2": [
      "Executive Summary‚Äã",
      "Introduction‚Äã",
      "How AI Avatars Work‚Äã",
      "AI Training Techniques‚Äã",
      "Traditional Avatars vs AI Avatars‚Äã",
      "Capabilities and Features of Modern AI Avatars‚Äã",
      "Top Vendors and Key Players‚Äã",
      "Secondary ‚ÄúWrapper‚Äù Vendors: Pattern & Pitfalls‚Äã",
      "Applications in Pharma (and Other Industries)‚Äã",
      "Opportunities and Benefits‚Äã",
      "Challenges and Risks‚Äã",
      "Adoption Strategies‚Äã",
      "Future Outlook‚Äã",
      "Conclusion‚Äã",
      "Sources‚Äã"
    ],
    "h3": [],
    "text_content": "AI Avatars / Digital People | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüí° Tech@Lilly Innovation Pipeline\nüõû Our Innovation Stages\nüèéÔ∏è Fast Start\nüì° Emerging Tech\nAI Avatars / Digital People\nAI Speech\nAI Video Generation\nüì° Emerging Tech\nAI Avatars / Digital People\nOn this page\nAI Avatars / Digital People\nEmerging Tech Report\nLast Update: 2025-08-05\nTech Innovation Pipeline\n, Enterprise Business Architecture, Tech@Lilly Enterprise\nAuthor: Doug Gorr\nHeyGen Examples\nExecutive Summary\nGerman\nSpanish\nHindi\nJapanese\nHeyGen Examples\nExecutive Summary\n1 of 6\nExecutive Summary\n‚Äã\nAI Avatars\n‚Äì the highly realistic, AI-generated digital humans ‚Äì are emerging as a transformative technology for corporate communications, training, marketing and more. Instead of filming live actors, organizations can now use AI to generate videos or interactive agents that look and talk like real people. These avatars can speak in dozens of languages, clone voices and likenesses, and even emulate human mannerisms and emotions. Major enterprises are already embracing this trend: for example, over 70% of Fortune 100 companies are using AI avatar platforms like Synthesia to create training and internal communications videos at scale. In pharma, Merck KGaA uses AI avatars for multilingual product training and updates, cutting the cost and time of producing content across regions.\nSeveral categories of AI avatar solutions have matured: AI video generators (e.g. Synthesia, HeyGen, D-ID) enable anyone to turn a script into a polished video of a human presenter; meanwhile, interactive ‚Äúdigital people‚Äù (e.g. Soul Machines, UneeQ) allow real-time conversations with a lifelike virtual agent. These technologies rely on advanced AI models ‚Äì combining deep learning for speech synthesis, computer vision, and natural language processing ‚Äì to produce believable performances. Unlike traditional avatars (static cartoons or pre-recorded actors), AI avatars are dynamic and data-driven, capable of automatically lip-syncing to new dialogue, switching languages on the fly, and even responding to user input.\nFor Lilly, AI avatars present exciting opportunities in marketing, training, HR, and patient engagement. In marketing, personalized AI-driven videos can deliver consistent product messages to healthcare providers or consumers worldwide. In learning & development, training modules can be rapidly produced in multiple languages with a friendly virtual instructor, improving engagement and retention. HR and corporate communications teams can use avatars modeled from executives to broadcast updates in a relatable, cost-effective way. Early adopters report significant efficiency gains ‚Äì ServiceNow cut video production time by 50% for global learning programs using AI avatars, and companies like Mondelƒìz produced tens of thousands of videos in a year via AI avatars.\nKey vendors in this space include Synthesia and HeyGen (leading AI video creation platforms), Soul Machines and UneeQ (interactive 3D digital humans), D-ID (AI avatar and dubbing technology), and tech giants like Microsoft and NVIDIA providing avatar generation services and tools. Their offerings already support features like cloning a person‚Äôs voice and face with consent, adding emotional expressions, 2D or full-body 3D avatars, and integration with chatbots or VR/AR environments. These capabilities will continue to expand. The technology has progressed remarkably over the past 3 years (fueled by advances in deepfake algorithms and generative AI), and recent developments point to even more realistic and controllable avatars on the horizon ‚Äì with more life-like gestures, interactivity, and integration into everyday workflows.\nTo capitalize on AI avatars, innovators should consider a phased adoption strategy (from Alpha experimentation to General Release) outlined by the company‚Äôs Tech Innovation Pipeline. Early on, focus on internal, low-risk use cases like training videos or a virtual assistant for HR FAQs. As the tech and internal comfort grows, scale up to customer-facing applications (with appropriate oversight and compliance checks, given pharma‚Äôs regulatory environment). Throughout, maintain a focus on ethical guidelines ‚Äì ensure avatars are used with consent, clearly identified as AI where appropriate, and content is medically accurate and approved. With right-sized governance, AI avatars can unlock cost-effective content at scale, enhance engagement (especially for video-preferring younger generations), and offer personalized support to both employees and customers. This report provides a deep dive into the technologies, top vendors, capabilities, use cases, and recommendations to help a global pharma company navigate the AI avatar landscape.\n[Notice: While the research and companies included in this report are primarily focused on AI Avatars (a talking digital person visible on a screen), it is important to note that many of the same use cases, opportunities, risks, and outlooks apply to voice-only interactions.]\nIntroduction\n‚Äã\nAI Avatars are computer-generated characters that look and sounds like real people. They leverage AI-driven animation and voice synthesis to speak and gesture in realistic ways, often indistinguishable from a video of a human. Companies adopt AI avatars to streamline video content creation and create interactive virtual agents (aka Digital People, Digital Humans) without the cost and time of using human actors, studios, and multilingual voiceover teams. The goal is to move beyond text-based communication to more engaging, human-like experiences, which deepens user engagement knowledge retention.\nEvolution (Last ~10 Years)\nThe concept of digital avatars isn‚Äôt new ‚Äì video game characters and virtual assistants have existed for decades ‚Äì but the last 2‚Äì5 years saw a revolution in realism thanks to AI. Early academic breakthroughs in deep learning-based face animation (often dubbed ‚Äúdeepfakes‚Äù) demonstrated that an AI model could make one person appear to speak words they never uttered. By the late 2010s, startups began harnessing this for business: Synthesia (founded in 2017) pioneered turning written text into videos of people speaking, and Soul Machines (founded in 2016) applied neural networks to create emotionally responsive 3D avatars. Initially, these technologies were experimental, but around 2020‚Äì2021, they hit an inflection point of quality and usability. The COVID-19 pandemic also accelerated demand for digital communication tools. By 2022, notable deployments like the World Health Organization‚Äôs ‚ÄúFlorence‚Äù digital health worker (built by Soul Machines) showed avatars being used to disseminate health guidance interactively. In 2023‚Äì2024, generative AI and large language models (like GPT-4) further boosted avatars‚Äô intelligence (for example, giving them conversational abilities) and big tech players entered the arena (Microsoft‚Äôs Azure AI introduced an avatar service, and NVIDIA launched its Avatar Cloud Engine). Today in 2025, AI avatars are increasingly mainstream in enterprise settings, with rapidly improving fidelity and capabilities. Major updates in the past year alone include full-body avatar animations, real-time interactive video experiences, and easier creation of custom avatars from a few photos or brief recordings.\nHow AI Avatars Work\n‚Äã\nAt a high level, an AI avatar system combines several AI components:\nText/Dialogue Generation:\nMany avatars start with a script or real-time generated text (which could come from a human-written script or an AI chatbot brain). This text is the content the avatar will speak.\nText-to-Speech (TTS):\nThe text is converted to spoken audio using advanced TTS models. Modern neural TTS can produce very natural, human-like speech, and can be trained to mimic specific voices.\nVisual Rendering (Lip Sync & Animation):\nThe core unique step: the avatar‚Äôs face (and possibly body) must be animated to speak the audio convincingly. Different platforms use different techniques:\nSome use 2D video-based models: e.g. Synthesia, HeyGen and D-ID leverage deep neural networks trained on real photos or video footage. These models take the generated speech audio (plus a base visage of the avatar) and predict corresponding lip movements and facial expressions frame-by-frame. Essentially, they ‚Äúpuppet‚Äù a human face to match the new audio. The underlying tech often extends from GANs or similar deep learning models that have learned how human lips move for each phoneme.\nOthers use 3D animated characters: e.g. Soul Machines, Convai, and UneeQ create a full 3D digital character (with a detailed face rig and body) and drive it with AI. Here, the system might use an animation engine (Unreal Engine, Unity, or proprietary) and AI to control the avatar‚Äôs facial muscles, eye gaze, and gestures in real time based on the speech and the context/emotion of the dialogue. These avatars can allow dynamic camera angles or be placed in AR/VR scenes because they exist as 3D models.\nSome hybrid approaches use single-image animation: e.g. D-ID can take a single photo of a person and animate it talking, by using a pretrained model that infers the 2D facial movements from audio. This is a fast way to create a talking head from one picture (though with less realism than multi-angle video training).\nIntelligence & Interactivity (optional):\nThe more exciting and advanced capabilities include emotional awareness and emotionally adaptive responses. These interactive avatar systems (aka Digital People, Digital Humans) take video and audio inputs, assess a user‚Äôs emotional context and respond like a human would. To make this possible, other AI capabilities, like sentiment analysis, need to observe, assess, and animate on the fly in order to drive the avatar‚Äôs emotional responses. For example, an avatar acting as a virtual concierge would include speech recognition to hear the user‚Äôs question, decipher tone and tenor, and adjust animation in real-time. This loop happens continuously so the avatar can converse fluidly. Leading frameworks like NVIDIA‚Äôs ACE provide pipeline components such as speech-to-text, NLP, and Audio2Face animation to support such real-time interactions.\nDespite the complexity under the hood, many modern platforms abstract this away, offering a simple studio interface or API. Users might only need to select an avatar style, input a script (or integrate their chatbot), and the system handles the rest ‚Äì from generating a voice to producing a video or live avatar stream.\nAI Training Techniques\n‚Äã\nCreating lifelike avatars requires training AI on large datasets of human speech and video. For instance, Synthesia has reportedly trained large video/audio foundation models on recordings of thousands of people to improve avatar realism. These models learn the nuances of facial movement, pronunciation, and emotional expression. D-ID‚Äôs newest avatar models can learn an individual‚Äôs specific mannerisms from just a short video: their ‚ÄúExpress Avatar‚Äù needs ~1 minute of footage to learn your head movements, while the premium model uses a few minutes of video to capture your face and even hand gestures for full-body shots. This is a significant reduction in training data compared to earlier years, thanks to more efficient AI architectures. Many systems also use pretrained models as starting points ‚Äì e.g. starting from a general face-animation model and fine-tuning it to a specific person. Voice models are similarly trained on vast speech datasets and then fine-tuned to mimic a target voice via transfer learning on a smaller voice sample. The result is that with surprisingly little input data, an AI avatar platform can clone a person‚Äôs likeness (with permission). Even TIME Magazine has noted that D-ID‚Äôs avatars can replicate a person‚Äôs voice, expressions and subtle mannerisms from only a brief video input ‚Äì a testament to the efficiency of modern training techniques.\nTraditional Avatars vs AI Avatars\n‚Äã\nIt‚Äôs useful to distinguish these new AI avatars from the ‚Äútraditional‚Äù avatars or digital characters of the past:\nTraditional avatars (in corporate use) might include cartoon profile images, 3D game-like characters, or video footage of actors that is manually edited. They require human animators or video shoots to create or update content. For example, a classic e-learning module might have a pre-recorded video of an instructor or a manually animated cartoon character ‚Äì if the script changes, you must reshoot or re-animate.\nAI avatars, by contrast, are generative. They can be instantly updated or completely created by feeding new text (for speech) or new data (to train a custom persona). There‚Äôs no need for a studio re-shoot to change what the avatar says or the language it speaks ‚Äì the AI system generates the new performance on the fly. Traditional avatars also tended to be less realistic (purposefully cartoonish to avoid the uncanny valley), whereas today‚Äôs AI avatars strive for photorealism or believable human-like presence.\nAnother key differentiator is interactivity: Traditional virtual agents (like earlier chatbots with a face) often had fixed, scripted interactions. Modern AI avatars can leverage AI models to respond flexibly to users, giving a far more natural conversational experience. For instance, Soul Machines‚Äô digital people are not pre-scripted ‚Äì they react to conversational context, even recognizing user emotions via camera. They ‚Äúsee, hear, react, and emote like a real human,‚Äù creating a natural experience. Such fluid behavior was not possible with older rule-based avatars.\nFinally, scale and personalization: AI avatars enable automatically generating thousands of variant videos (say, one per customer with personalized greeting) ‚Äì something infeasible with manually made content. This opens new use cases like personalized marketing messages delivered by a lifelike spokesperson just for you. As cost and technologies continue to improve, we are fast approaching a world where everyone will have their own interactive avatars.\nIn summary, AI avatars represent the convergence of video production, animation, and AI. They allow digital content to reach people in a humanized form, but with the speed and scalability of the digital world.\nCapabilities and Features of Modern AI Avatars\n‚Äã\nToday‚Äôs state-of-the-art AI avatar platforms boast a rich array of features. Below we outline key capabilities, along with their significance:\nLifelike Visuals (2D and 3D):\nAI avatars can appear as talking heads in a 2D video frame or as full-body 3D characters. 2D avatars look like real humans filmed with a camera, typically from the chest up. 3D avatars have a virtual body and environment, allowing more camera angles and movement. Recently, platforms have begun expanding from static torso-up shots to full-body movement. In 2024 full-body avatars emerged, capable of a range of motions and gestures, enabling them to use hand movements and body language in videos. This adds realism and engagement, as body language is a huge part of human communication.\nVoice Cloning and Multilingual Speech:\nAI avatars can be given almost any voice. Many platforms provide a library of synthetic voices in numerous languages. Increasingly, they also offer custom voice cloning, where you can create a unique voice font (e.g. to use your own voice for your avatar). Synthesia and HeyGen both advertise voice cloning features, and Microsoft‚Äôs Azure AI specifically allows training a custom neural voice. This means an avatar could speak with the tone of a specific executive or brand ambassador (with consent). The multilingual abilities are also striking ‚Äì most leading avatars support dozens of languages. Synthesia supports 140+ languages for text-to-speech, and D-ID‚Äôs video translate tool can convert videos into over 120 languages with accurate cloned voices and lip-sync. For a global company, this feature is invaluable: one master video can be automatically localized for regional audiences, maintaining the same visual but speaking the local language. This vastly reduces the need for subtitles or separate video shoots per language.\nRealistic Facial Expressions and Emotions:\nEarlier AI avatars sometimes appeared emotionless or ‚Äúwooden.‚Äù Now, there‚Äôs a big emphasis on making them expressive and emotionally engaging. Providers use two approaches: (1) have the avatar detect sentiment in the script or conversation and adjust expression/tone accordingly, and (2) allow users to specify an emotion or speaking style. Synthesia‚Äôs new Expressive model (EXPRESS-1) enables avatars to understand context and sentiment in text and change their tone and facial expression appropriately. HeyGen likewise launched ‚ÄúAvatar 3.0‚Äù with emotion-aware animation, where the avatar‚Äôs movements reflect the emotional cues in the script (for example, smiling, frowning, or gesturing more dramatically for emphasis). This makes the avatars more engaging and human-like, rather than speaking in a flat monotone. It‚Äôs particularly useful in training or marketing content to convey enthusiasm, empathy, or seriousness as needed.\nPersonalized Avatar Creation (Custom Avatars):\nWhile there are many stock avatars available (Synthesia alone offers 230+ diverse avatar personas), organizations often want their own people represented or a custom character aligning with their brand. Modern platforms enable this in a few ways:\nSelf-Service Avatar Creation:\nSome solutions now let a user create a custom avatar by simply providing a few images or a short video of a person. Synthesia‚Äôs experimental ‚ÄúSelfie Avatars‚Äù feature allows creating a custom avatar from just a few images of yourself. D-ID‚Äôs ‚ÄúPhoto Avatar‚Äù similarly can animate an uploaded photo. HeyGen has taken it further, offering a pipeline to create ‚ÄúCommunity Avatars‚Äù and even generate avatar faces from text prompts (AI-generated faces). This means you can get a unique virtual presenter without hiring an actor ‚Äì either based on a real person‚Äôs likeness or an AI-designed face.\nCustom Avatar with Your Own Video:\nFor higher fidelity, enterprise-oriented platforms allow training the avatar on a professionally recorded video of a person speaking. Microsoft‚Äôs Azure Avatar service, for instance, supports custom avatars by uploading your own video recordings, which it uses to train a neural avatar that closely resembles that person (especially if combined with a custom voice). D-ID‚Äôs Premium+ avatars require a few minutes of footage of the person but in return can even emulate their torso and hand movements for more dynamic shots.\nThe result of these innovations is that a company can clone an executive, instructor, or actor as an avatar (with their permission). That avatar can then deliver any message you need, saving that person from having to be on camera each time. It‚Äôs already used in practice ‚Äì many Synthesia customers have their own CEO or spokesperson avatar. This raises interesting questions (addressed later) about disclosure and authenticity, but technologically it‚Äôs a game-changer for scalable content creation.\nInteractive and Conversational Abilities:\nOne of the most exciting capabilities turns avatars from one-way, pre-recorded video presenters into two-way, interactive agents. Interactive avatars can listen to users (via text or voice input) and respond in real time with speech and appropriate facial reactions. Soul Machines specializes in this ‚Äì their avatars are essentially AI assistants with a face, capable of free-form conversation. They even possess memory and computer vision: a Soul Machines avatar can recognize objects or people it sees through a camera and remember past interactions to personalize the dialogue. This enables use cases like virtual coaches or advisors. For example, Soul Machines reports that their AI wellness coach can watch a user practice a task (like a job interview answer) and give real-time feedback, noticing if the user appears nervous or if their attire is appropriate. On the more business side, HeyGen introduced Interactive Avatar characters that can serve as a ‚ÄúZoom assistant‚Äù or product expert ‚Äì effectively the avatar is tied to a chatbot/LLM that answers questions, while the avatar provides the face and voice for the AI‚Äôs answers. D-ID also has a chat API where you can hook an avatar up to systems like ChatGPT to create a conversational agent; their avatars were recognized by TIME as an innovative way to make chatbot interactions feel more human. This capability opens doors for virtual customer service reps, virtual patient guides, or interactive training simulations (more on use cases later).\nPhysics and Environment Integration:\nIn more advanced or 3D scenarios, avatars can be placed in virtual or real environments and respond to those contexts. For instance, in VR/AR, an avatar might need to match lighting and physics of the world so it appears believable next to real objects. Some platforms allow adjusting lighting on the avatar or use graphics engines to ensure the avatar casts shadows, etc., if composited into real scenes. While this is a niche feature mostly for AR/VR or high-end video production, it‚Äôs worth noting. NVIDIA‚Äôs toolkit, for example, provides ways to embed avatars in game engines so they have spatial awareness ‚Äì e.g. an NPC (non-player character) avatar can navigate around and pick up virtual objects. In enterprise settings, this could translate to AR training where a digital human guides a technician through a task, appearing anchored in the physical space.\nCamera Movement and Shot Control:\nInitially, most AI avatar videos were simple ‚Äúlocked-off‚Äù shots (avatar centered, direct-to-camera). Now, tools are adding more cinematographic control. Users can choose camera zoom levels, angles, or even have the avatar change position between cuts. HeyGen‚Äôs platform, for example, offers an ‚ÄúAI Studio‚Äù editor with the ability to add motion ‚Äì so you could start with a close-up and then switch to a wider shot with the avatar in a different background, all within one generated video. They tout features like the ability to change backgrounds, outfits, stances, and camera angles with the same avatar footage. This is achieved by their ‚ÄúUnlimited Looks‚Äù technology ‚Äì essentially re-synthesizing the avatar with different attire or environment without needing new video shoots. Such control allows more dynamic, multi-scene videos (akin to a normal video edit) entirely through AI synthesis.\nStoryboarding and Workflow Tools:\nTo make avatar video creation accessible, many platforms include template libraries and storyboard tools. Users can script a sequence of scenes ‚Äì e.g. first scene: avatar A delivers intro, second scene: switch to a screen-share or slides, third scene: avatar B concludes. Tools like Synthesia and HeyGen provide pre-built templates for common video types (e.g. corporate training, marketing pitch) and allow inserting other stock media (images, charts, screen recordings) alongside the avatar‚Äôs footage. This workflow integration is crucial for enterprise use, letting a non-video-specialist create complete, polished videos. For example, Synthesia‚Äôs platform now includes an AI screen recorder (record your screen with voice, and it will generate an avatar presenting it) and one-click translation/localization of an entire video. D-ID recently introduced a Scenes feature to build video sequences with multiple avatars, backgrounds and dynamic text overlays for storytelling. These workflow features mean AI avatars are not a standalone novelty, but part of a broader video production solution that fits into corporate content pipelines (often with collaboration, review, and versioning support).\nContent Moderation and Safety Features:\nGiven the potential misuse of hyper-realistic avatars (e.g. deepfake concerns), responsible vendors have built-in controls and moderation guardrails. Content moderation includes preventing the generation of disallowed content (hate speech, sexual or violent content, etc.) and ensuring avatars of real people are only made with consent. Synthesia, for instance, has stringent policies ‚Äì it does not allow public figures to be cloned and politically oriented content is restricted to enterprise accounts with disclosure. They even participated in a public red-team test of their moderation, showing resilience against generating deepfake abuse. Many platforms tag or watermark the AI videos in metadata as synthetic. Another aspect is ethical use of actors‚Äô likeness ‚Äì Synthesia has a talent program to compensate the real actors who have provided their faces for stock avatars, including a revenue share and early insight into how their avatar is used. These measures, while not ‚Äúfeatures‚Äù in the traditional sense, are important capabilities of a mature enterprise-ready avatar platform: to ensure safe, consensual and transparent use of the technology.\nCost Efficiency and Scale:\nThough not a feature per se, it‚Äôs a key selling point ‚Äì AI avatars dramatically reduce costs of video production. Once a custom avatar is made, producing a new video is essentially generating data (no studios, cameras, travel, or on-camera talent needed). The cost is typically a software subscription or per-video fee. Many companies report saving significantly. For instance, what used to require a film crew and weeks of work can now be done by a single employee in a day with an AI platform. The marginal cost of additional videos is low, enabling large-scale video content strategies (such as personalized videos for thousands of recipients). Additionally, some vendors highlight energy efficiency ‚Äì Synthesia published that AI video generation can have a smaller carbon footprint than traditional video production, aligning with sustainability goals.\nIntegration and API Access:\nFinally, modern avatar services offer APIs and integrations so that they can plug into other systems. For example, Synthesia and D-ID provide API endpoints where another software can programmatically generate videos. This could allow, say, a learning management system to automatically generate an avatar video for each new policy update, or a CRM to send a personalized avatar video to a customer when they sign up. For example, D-ID‚Äôs enterprise suite integrates with tools like Canva and PowerPoint for easy content import, and can connect with CRMs/marketing automation to insert avatar videos into campaigns. Microsoft‚Äôs Azure avatar service integrates naturally with Azure‚Äôs bot framework and other cognitive services (one can imagine an Azure bot with an avatar front-end in Teams). These integrations indicate the technology is maturing into an ecosystem component rather than a standalone novelty.\nIn summary, AI avatars today can clone voices and faces, express emotion, deliver content in 100+ languages, interact with users, and be flexibly customized ‚Äì all while offering enterprise-grade workflow integration and moderation. This wide feature set underpins their broad applicability across business functions.\nTop Vendors and Key Players\n‚Äã\nThe AI avatar landscape features a mix of specialized startups and large technology companies. Below we provide a balanced overview of major players, their offerings, and recent developments, with a focus on those relevant to Lilly use. In addition to these overviews and list of features, many additional factors must be considered prior to beginning a project with any AI Avatar vendor.\nContact your Enterprise Business Architect for the latest in vendor positioning and availability for Lilly use.\nSynthesia\nWidely regarded as a market leader in AI avatar videos, Synthesia is a U.K.-based company that provides a platform to create videos with photorealistic avatars from text. It offers a web studio with many stock avatars (diverse ethnicities, ages, professional looks) and the ability to create custom avatars. Synthesia has a strong enterprise focus ‚Äì boasting over 60,000 customers by 2023 and serving 70% of Fortune 100 firms. Use cases center on corporate training, how-to videos, and internal communications. What sets Synthesia apart is its creator platform with notable polish and reliability: it produces high-quality lip-sync, supports 130+ languages, and provides features like one-click video translation and an AI script assistant. Recent innovations from Synthesia include:\nSynthesia 2.0 Platform: Launched in mid-2024, bringing full-body avatars with gestures and an interactive video player. The interactive video feature will let viewers click on elements in the video (e.g. a calendar pop-up to book a meeting or a quiz question). This blurs the line between passive video and e-learning app, all within an avatar-led video.\nExpressive Avatars: A new ‚ÄúExpress‚Äù model that gives avatars context awareness to modulate tone. An avatar can sound upbeat or concerned depending on the content of the script, without the user manually specifying it ‚Äì making videos feel less robotic.\nPersonal Avatars and Selfie Avatars: Synthesia opened up easier custom avatar creation. Instead of requiring a studio filming session, users can record themselves with a normal webcam (with a natural background) to create a personal avatar. This lowers the barrier for organizations to let many employees create their own avatars for internal use. The avatars‚Äô lip sync and voice can then be auto-generated, including translation of the user‚Äôs own voice into other languages.\nScale and Governance: Synthesia has focused on responsible AI ‚Äì implementing rigorous content moderation (e.g., no public figure avatars, no political disinformation usage) and even achieving an ISO 42001 certification for AI trust and security (a first in the industry). They also partner with major tech (Adobe invested in them, and they use NVIDIA GPUs on Google Cloud for training). With a $180M Series D funding in 2023, Synthesia is well-capitalized to drive innovation.\nFor Lilly, Synthesia‚Äôs extensive enterprise adoption and emphasis on compliance are reassuring ‚Äì it‚Äôs a stable choice for large-scale deployment of avatar tech for training or comms.\nHeyGen\nHeyGen (formerly Movio) is another prominent AI video platform, often seen as a competitor to Synthesia with a blend of similar and unique features. It provides a diverse set of 500+ avatars (including not just photoreal actors but also some stylized ones) and supports 70+ languages. HeyGen‚Äôs recent Fall 2024 release introduced cutting-edge features:\nGenerative Avatars from Text: Users can create avatar ‚Äúlooks‚Äù from prompts ‚Äì essentially generating a new digital actor via AI. For example, one could type ‚Äúan older doctor with kind eyes‚Äù and get a unique avatar face to use. This is still experimental but showcases the ability to tailor avatar appearance to niche audiences or brand image without hiring an actor.\nEmotion-Aware and Motion Improvements: HeyGen‚Äôs ‚ÄúAvatar 3.0‚Äù avatars respond to emotion cues in the script, adding expressive depth and natural motions to deliver the message convincingly. They also rolled out 50+ new filmed actors with 400+ outfit and background combinations to cover more scenarios (e.g., an avatar at a hospital vs. in an office). The ‚ÄúUnlimited Looks‚Äù feature allows a custom avatar to be shown in different clothing and settings by uploading additional reference footage, which is valuable for continuity (your avatar can change attire like a real person would on different days).\nInteractive AI Agents: Notably, HeyGen has launched an Interactive Avatar feature ‚Äì these avatars can literally hold conversations, acting as support agents or even sitting in on video calls on your behalf. They listen to user queries and respond, presumably by integrating with an AI like GPT-4 for the brain. This positions HeyGen not just for video creation but also for real-time customer service or employee assistant use cases.\nVideo Translation and other tools: Like competitors, HeyGen offers video dubbing (translate an existing video into other languages with the same speaker‚Äôs avatar/voice) and a Chrome extension that turns a voice recording into an avatar video automatically.\nHeyGen has been aggressive in feature development (shipping updates ‚Äúat lightning speed‚Äù), making it an attractive, often slightly more consumer-friendly option. It also tends to market flexible pricing and a free tier, which helped it gain popularity among individual creators and small businesses.\nFor a Lilly, HeyGen could be useful for quick-turnaround projects or creative marketing experiments, especially if the goal is to prototype capabilities like interactive rep avatars or generate a variety of spokesperson styles via prompt.\nD-ID\nIsraeli startup, D-ID started with ‚Äútalking photo‚Äù technology and has grown into a platform for both asynchronous video creation and real-time conversational avatars. D-ID is known for its ease of turning a single image into a video of a person speaking, which gained fame through products like MyHeritage‚Äôs ‚ÄúDeep Nostalgia‚Äù (animating old photos). Key features of D-ID:\nCreative Reality Studio: D-ID‚Äôs web studio allows users to create videos by uploading an image (or choosing a stock avatar) and entering text. It has a library of voices and supports numerous languages. A big draw is the ability to animate any face ‚Äì even historical figures or personal photos (with appropriate rights) ‚Äì into a speaking avatar.\nHigh-Quality Avatar Modes: In late 2024, D-ID launched new Express and Premium+ avatar tiers. Express avatars train quickly (‚âà1 minute of video input needed) and capture basic head movement, suited for fast setup. Premium+ avatars require a bit more training footage (a few minutes) but produce much more lifelike results, including upper body and hand gestures. Premium+ avatars are also capable of real-time interactions, which D-ID suggests for use cases like live webinars or live language translation of a speaker. This essentially means a person‚Äôs digital double could speak live in many languages or engage an audience in Q&A.\nEnterprise Marketing Suite: D-ID is positioning strongly for marketing use cases. They report that personalized video campaigns created with their avatars significantly boost engagement (30% higher click-through, 35% higher conversion in their early data). To support this, they introduced an array of tools ‚Äì interactive embedded video (avatar asks the viewer questions or responds to choices), integration with Canva/PPT for content design, and API hooks into CRM and marketing automation. This indicates D-ID‚Äôs focus on letting brands incorporate avatars into customer outreach (from outbound personalized ads to inbound customer service via ‚ÄúAI agents‚Äù).\nPartnerships and Recognition: D-ID partnered with Microsoft in 2025 to bring ‚Äúagentic AI avatars‚Äù into Microsoft‚Äôs ecosystem. One application is expected integration with Microsoft Teams or Azure, meaning you might have D-ID powered avatars for live meetings or virtual assistants in enterprise workflows. Additionally, D-ID‚Äôs tech was named one of TIME‚Äôs best inventions of 2024, underscoring its cutting-edge nature. TIME highlighted how D-ID‚Äôs conversational avatars (with LLM brains) create a more engaging, face-to-face interface for AI.\nUse in Healthcare/Pharma: D-ID has already seen use in healthcare training. A noteworthy example is the Southern Illinois University (SIU) School of Medicine, which used D-ID avatars as virtual patients to simulate clinical scenarios for students. These ‚ÄúAI patients‚Äù converse with students, allowing them to practice history-taking and communication in a realistic but controlled environment. This suggests a direct application to pharma in medical education and sales training (e.g., reps practicing detailing a drug with a lifelike doctor avatar as the audience, or trainees interviewing a virtual patient about side effects).\nOverall, D-ID‚Äôs strength is its versatility (from simple photo animation to enterprise-grade interactive avatars) and its focus on making avatar tech accessible (they even have a mobile app). It might appeal to marketing teams for quick content and to innovation teams for building interactive AI agent prototypes. Ensuring proper licensing and consent for any real faces used would be a consideration, which D-ID addresses through its ethics policy and partnerships (for example, they partner with stock image libraries and have usage guidelines).\nSoul Machines\nIn contrast to the above ‚Äúvideo generator‚Äù companies, Soul Machines is all about autonomous animated digital people. A New Zealand-based company co-founded by Oscar-winning animator Mark Sagar, Soul Machines builds fully 3D avatars that are driven by AI in real time, aiming for unprecedented realism and empathy in interactions. Key highlights:\nDigital DNA and ‚ÄúBiological‚Äù AI: Soul Machines created a platform that combines neural networks with models of human biology (they call it ‚ÄúExperiential AI‚Äù). This means their avatars don‚Äôt just lip-sync to text; they have a sort of digital nervous system that simulates things like micro-expressions, eye movements, and emotional responses. They often refer to their avatars having a ‚ÄúDigital Brain‚Äù that can process inputs (vision, audio) and maintain an evolving emotional state.\nInteractivity and Sensing: A hallmark is that Soul Machines avatars can see through the user‚Äôs webcam, hear through the microphone, and respond with appropriate facial reactions. For example, if the user smiles or frowns, the avatar may adapt its demeanor. They can also detect objects (as of their 2024 update) ‚Äì the avatar could recognize a pill bottle the user holds up, for instance, and incorporate that into the conversation. Memory features allow them to recall user details across sessions.\nUse Cases and Industries: Soul Machines targets high-value interactions like coaching, customer experience, healthcare, etc. The WHO‚Äôs Florence 2.0 digital health worker is a flagship example ‚Äì Florence (an avatar of a female health advisor) can talk about COVID-19, mental health, nutrition, and more, and is being developed to speak multiple languages to serve global populations. Florence is envisioned to help fill gaps in healthcare education where human resources are scarce. In the corporate realm, Soul Machines has created digital twins of celebrities (e.g., a digital Will.i.am as a customer engagement avatar) and works with companies like HSBC, Nestle, and Daimler for customer service pilots.\nFor Lilly, one could imagine Soul Machines avatars as digital health coaches (guiding patients through medication management with empathy), or as virtual sales reps that can train or even interact with doctors in a human-like way.\nRecent Advancements: As cited earlier, Soul Machines recently added object recognition and long-term memory to their avatars, making interactions more context-aware. They also launched a self-service ‚ÄúSoul Studio‚Äù where clients can design and deploy their own digital people relatively quickly ‚Äì lowering the barrier from a fully custom build (which used to involve Soul Machines‚Äô in-house team modeling an avatar) to a more configurable SaaS approach. Additionally, partnerships like the one with ServiceNow integrate their avatars into existing enterprise workflow systems (imagine an avatar that can not only talk to you but also create a support ticket or pull up your account info via backend integration).\nSoul Machines‚Äô avatars are perhaps the most technologically sophisticated in simulating human interaction. However, they are also resource-intensive (typically requiring GPU cloud rendering) and currently less common than video-based avatars for things like simple training videos. They shine in live, interactive scenarios that benefit from emotional engagement.\nRecent exits by all founding members and new executive team has shifted Soul Machine‚Äôs product strategy, focusing attention on subscription-based life coach avatars for the consumer market.\nFor Lilly, using Soul Machines might be part of a high-touch digital initiative ‚Äì for example, a virtual patient companion for a chronic disease program that checks in on patients, or an internal coaching avatar for employee wellness. The ROI would need to justify the complexity (as these are not as turnkey as, say, Synthesia videos), but the differentiation in user experience is significant. A patient might respond more to a compassionate digital human that listens and remembers, than to a static chatbot or a series of videos.\nUneeQ\nAnother player in interactive digital humans, UneeQ (originating from New Zealand as well) offers a platform to create ‚Äúdigital companions‚Äù for customer service, sales, and training. UneeQ‚Äôs avatars are 3D animated characters similar to Soul Machines‚Äô, though UneeQ emphasizes a modular approach, integrating best-of-breed AI services for voice, conversation, etc. They have an authoring platform where clients can choose or design an avatar‚Äôs look, plug in an NLP brain (like IBM Watson or Microsoft Bot Framework), and deploy the avatar on websites, kiosks, or AR/VR devices.\nUneeQ was behind some well-known demos, like the ‚ÄúDigital Einstein‚Äù avatar that one could chat with to learn facts or just experience conversing with a historical figure. In healthcare, UneeQ created ‚ÄúSophie,‚Äù a COVID-19 advice avatar in early 2020 that provided information to the public about the pandemic. They have case studies such as a digital human for a diabetes charity to answer questions, and a virtual shopping assistant for a retail brand.\nUneeQ highlights its ability to deploy avatars across channels ‚Äì from web to even holographic displays for events. They also tout integrations: for instance, a digital human concierge that ties into hotel databases, or a training coach that connects to LMS content. Their Synapse (or Synanim) engine handles the coordination of speech, gestures, expressions, and remembering conversation context.\nFor enterprise usage, UneeQ has packaged solutions like a ‚ÄúSales Trainer‚Äù digital human for employee training role-plays, and customer-facing digital greeters. They might not have the massive customer count of Synthesia, but UneeQ is a strong option for bespoke interactive avatar projects. In a pharma setting, UneeQ could be leveraged to create, say, a virtual medical representative to handle common doctor questions on demand, or a digital human resources assistant to help employees navigate benefits (in a more engaging way than reading FAQs). The choice between Soul Machines and UneeQ often comes down to the style and complexity of interaction desired, business relationships, and potentially cost ‚Äì UneeQ markets a more scalable cloud platform which might be more accessible for mid-tier use.\nMicrosoft (Azure AI Services)\nMicrosoft entered the avatar arena by leveraging its strengths in speech and cloud integration. Azure‚Äôs Text-to-Speech Avatar service became publicly available in late 2023. This service allows Azure customers to generate photorealistic talking head videos from text, using either Microsoft‚Äôs pre-made avatars or a custom one:\nPrebuilt avatars are provided by Microsoft out-of-box, each with a certain look, and they can speak any text in supported languages using Azure‚Äôs neural voices.\nCustom avatars enable an organization to upload video of a person (and optionally create a custom voice) to train an avatar that looks and sounds like that person. Microsoft emphasizes that this is a Limited Access feature ‚Äì you must apply and describe your use case, to ensure it‚Äôs used ethically. They have guardrails in place as part of their Responsible AI commitment (e.g., presumably ensuring the person has consented if you clone them).\nMicrosoft‚Äôs entry is significant because of integration: these avatars can be tied into Azure‚Äôs AI ecosystem. For example, Lilly could use Azure OpenAI (GPT-4) to generate dynamic dialogue, Azure Speech to handle input/output, and the Avatar to present it. A cited scenario is a ‚Äúvirtual HR assistant‚Äù where an avatar answers employees‚Äô HR questions, or a ‚ÄúCEO‚Äôs digital twin presenting at a conference‚Äù in multiple languages. Being able to swap a live appearance with a generated one that still feels personal is an attractive use case (particularly in a world of virtual meetings).\nMicrosoft has also hinted at using avatars in Teams (their enterprise meeting app). Although currently Teams offers cartoon-like 3D avatars, the partnership with D-ID suggests they might bring more realistic avatars for certain meeting or training scenarios. Also, Azure‚Äôs avatar is available via API, meaning developers can embed it in custom apps (e.g., a telehealth portal could show a doctor avatar explaining how to take a medication).\nFor Lilly, already invested in Azure platform, the Microsoft AI Avatar service could be appealing. It would allow leveraging existing Azure security, scalability, and perhaps even pricing benefits under enterprise agreements. It‚Äôs relatively new and part of a recent startup acquisition, so it lacks many features (e.g., the studio interface is likely more utilitarian than any other). But Microsoft‚Äôs investment in both real-time bots and content generation with avatars is a strong sign this tech is considered important in the future of communication. One could imagine a near future where every corporate employee has a little AI avatar helper (instead of just a chat icon), powered by these services.\nNVIDIA\nNvidia does not directly offer an end-user avatar app but provides the tools to build avatars, especially for gaming, simulation, and high-end interactive use. Their Omniverse Avatar Cloud Engine (ACE) is a toolkit of AI models and microservices for developers to create interactive avatars. This includes:\nAudio2Face ‚Äì which generates facial animation from audio input.\nRiva ‚Äì for speech-to-text and text-to-speech, supporting multiple languages.\nNeMo / LLM services ‚Äì for conversational AI (NVIDIA offers customizable large language models that can run locally or in cloud).\nBy combining these, a developer can give an avatar the ability to listen, think (via an LLM), and speak with realistic lip-sync. NVIDIA demonstrated this at CES 2024 showing AI-powered NPCs in video games that have unscripted dialogues and react to player behavior. Game studios like Ubisoft and companies like Convai and Inworld (which create AI NPCs) are using ACE. Even enterprise avatar companies such as UneeQ are also listed as partners leveraging NVIDIA‚Äôs tech.\nWhy this matters to Lilly\n: If we want to build something very custom ‚Äì say a proprietary virtual trainer with a unique look integrated deeply into a simulation or our own creator platform ‚Äì we may want to use NVIDIA‚Äôs stack under the hood. NVIDIA also pushes the envelope in innovation: for example, their research in eye contact simulation, voice modulation, and instant animation could filter into commercial products soon. They also ensure these capabilities run efficiently on GPUs, which is crucial for real-time avatars. Additionally, NVIDIA‚Äôs tech can be used on-device (for privacy) or via cloud.\nAs a basic building block for AI Avatar animation, NVIDIA could likely act as a major catalyst that continues to drive the entire AI Avatar market forward.\nFor Lilly, a ready-made solution from the above vendors will be sufficient. But it‚Äôs worth noting that NVIDIA‚Äôs ACE is enabling a new wave of AI Avatar startups offering immersive experiences (especially in AR/VR). If, for instance, Lilly innovators envision an advanced VR training lab where trainees talk to virtual patients, the developers building that could use ACE components to achieve the realism and interactivity required.\nConvai\nConvai is a company that focuses on creating AI-driven avatars, also known as digital humans, that can simulate human-like behavior, speech, and expressions in real-time. These avatars leverage advances in AI to move beyond pre-defined animations, enabling more natural and personalized interactions. Convai offers a Unity SDK to add conversational AI characters into websites and AR applications, making it relevant for training simulations. This technology allows for the creation of virtual patients or doctors in a simulation, like the SIU case but embedded in a 3D training environment.\nConvai's avatars are dynamic and can hold conversations, answer questions, and even show context-appropriate emotions. They can be used in various applications, including marketing, learning and development, HR, training, and corporate communications. The key differences between traditional and AI avatars include automation of behavior, intelligence and interactivity, personalization, multilingual and scalable content, and real-time responsiveness.\nInterestingly for Lilly\n, Convai offers large customers the opportunity to self-host, which often reduces/eliminates many confidentiality and security risks typically found with third-party integrations.\nOthers and Emerging Players\nA myriad startups are coming up in specific domains: e.g., some focus on avatars for education (a virtual tutor that looks like a friendly teacher), some on virtual influencers for social media, and so on. The ecosystem is vibrant, but the ones outlined above are the key players likely to be relevant to a large global company‚Äôs needs today.\nMeta (Facebook) is researching ‚ÄúCodec Avatars‚Äù (ultra-realistic telepresence avatars) and deploying simpler avatars in Horizon Workrooms (though those are cartoonish). Apple is using a face-scanning approach for its Vision Pro ‚ÄúPersona‚Äù avatar (not AI-generated, but potentially Apple could incorporate AI for expressions or automated behaviors in future). Alibaba and other Chinese firms have their own avatar initiatives (for virtual influencers on e-commerce, etc.), but those are region-specific.\nIn summary, the field has both platform players (Synthesia, HeyGen, D-ID ‚Äì competing to be the go-to video avatar tool) and interactive avatar specialists (Soul Machines, UneeQ ‚Äì offering rich real-time digital humans). Additionally, big tech (Microsoft, Nvidia) provides core infrastructure that may either underpin those platforms or be used directly for custom solutions. When evaluating vendors, innovators should consider factors like: quality of lip-sync and visuals, language/voice support, ease of use, data privacy (where is the avatar generation happening ‚Äì on vendor cloud or Lilly-hosted), integration capabilities, and of course cost and licensing.\nSecondary ‚ÄúWrapper‚Äù Vendors: Pattern & Pitfalls\n‚Äã\nEvery time a major technology platform reaches the Peak of Inflated Expectations, a long tail of fast-moving service shops and startups springs up to ‚Äúwrap‚Äù the underlying APIs with a lighter UX or vertical pitch. Gartner flagged this behavior in the early cloud era, noting that cloud resellers would soon ‚Äútip over the peak and experience disillusionment‚Äù once enterprises realized they could contract AWS or Azure directly.\nThis same dynamic is repeating with AI Avatars, indicated by the following points:\nHigh failure‚Äêto-scale rate. A Forbes analysis of 230 gen-AI pilots found ‚âà 90 % never progress beyond proof-of-concept; IDC places the attrition rate at 88 % in production settings\nCommodity risk. TechCrunch notes that many ‚ÄúGPT-for-X‚Äù firms are thin wrappers ‚Äúcreating little more than UI glue around someone else‚Äôs model,‚Äù leaving them exposed the instant the platform owner launches a native feature\nPlatform consolidation. Analyst reporting shows large model providers ‚Äúaggressively launching native features that steam-roll wrapper startups‚Äù\nDue-Diligence Checklist for Assessing Avatar Vendors\nDisclosure of underlying stack: Demand the contract list which foundation model, TTS engine, and rendering pipeline generates the avatar.\nDirect-license availability: Verify you cannot obtain the same capability straight from Synthesia, Hey Gen, D-ID, Microsoft Azure, or NVIDIA at enterprise terms.\nScalability proof: Request production metrics (QPS, concurrent sessions, regional fail-over) tied to their own infrastructure, not screenshots of the core platform‚Äôs SLA dashboard.\nSecurity boundary review: Map OAuth scopes, data residency, and retention policies; extra hops often double the blast radius of a breach.\nExit & portability clause: Insert a 30-day transition provision and insist that all model artifacts, custom avatar assets, and scripts be exportable in a standard format.\nStrategic Guidance for Business Stakeholders\nPrioritize core-platform partnerships for production work (key players listed in this report).\nLeverage boutique ‚Äúwrapper‚Äù vendors only for examples and time-boxed engagements when they provide domain expertise your teams lack, then seek to fill the team gaps. Ensure agreements include a pre-agreed sunset or hand-off plan.\nRetain the vertical value layer internally. Internal functions (Medical Affairs, L&D, HR, etc.) should own scriptwriting, compliance, and standards; third parties should supply technology plumbing, not the industry narrative.\nMaintain a consolidated vendor scorecard to monitor financial health, release cadence, and security posture; thin-wrapper firms often plateau once feature overlap grows with the underlying platform.\nOverall, minimize exploration into intermediaries. In the generative AI ‚Äúgold-rush‚Äù, longevity and compliance will belong to vendors with control over the core AI models and infrastructure. Think of these pop-up offerings as riders in someone else‚Äôs car - useful for conversations and insights, hazardous for mission-critical scale.\nApplications in Pharma (and Other Industries)\n‚Äã\nAI avatars have cross-industry applicability wherever communication, instruction, or personal interaction is involved. Below we explore use cases and examples, focusing on pharmaceuticals and healthcare, but also drawing from other industries to illustrate possibilities and spur thought.\nMarketing and Sales Communication\nProduct Marketing Videos: Pharma companies can use AI avatars to create explainer videos for new drug launches, mechanism-of-action animations with an avatar narrator, or patient education videos about a disease. Traditionally, creating multilingual promotional videos or KOL (key opinion leader) testimonial videos is expensive. With avatars, one could, for example, generate a video of a ‚Äúvirtual doctor‚Äù explaining the benefits of a treatment in 10 different languages, all without a single physical shoot. Merck KGaA‚Äôs team already replaced many live-recorded product update videos with Synthesia avatars to rapidly localize content for internal sales teams. Similarly, marketing departments can A/B test different avatar personas (perhaps one avatar resonates better with cardiologists vs another with oncologists) by simply switching the digital actor ‚Äì something impractical with real actors.\nPersonalized Outreach: Imagine sending a personalized video message to HCPs where an avatar addresses the doctor by name and provides tailored info (e.g., ‚ÄúDr. Smith, here‚Äôs data relevant to your practice from our new clinical trial‚Äù). Companies like Vidyard or BombBomb have shown that personalized video in sales outreach increases engagement; AI avatars can deliver this personalization at scale without individual reps recording hundreds of videos.\nVirtual Sales Reps (Interactive): In some cases, an avatar could act as a stand-in for a human sales rep, especially for ‚Äúno-see‚Äù accounts or during odd hours. For instance, a doctor could click a button to ‚ÄúChat with an expert now‚Äù and a friendly avatar appears (perhaps a digital twin of their medical science liaison), ready to answer questions via an AI backend honed on a particular drug product. This avatar could show slides or data on prompt and talk through them. While it won‚Äôt replace important face-to-face rep visits, it can provide on-demand support and product information. GSK reportedly piloted a virtual sales rep avatar for some product inquiries (hypothetical example to illustrate, as many companies are considering hybrid models). This is analogous to how some banks use digital avatar tellers in kiosks ‚Äì the pharma version would need to be closely governed (ensuring accurate, approved responses only).\nDigital Brand Ambassadors: Outside of HCPs, avatars could appear in consumer-facing campaigns. For example, Lilly could create a virtual influencer avatar (perhaps a character representing a patient advocate) to raise awareness on social media about a condition or wellness tips, answering questions in comments using an AI brain. This has been tried in other industries ‚Äì e.g., cosmetic brands have virtual influencer avatars ‚Äì and though experimental, it can humanize the brand in a controlled way (since the company scripts or approves the avatar‚Äôs messaging).\nLearning & Development (L&D) and Training\nEmployee Training Videos: Compliance training, safety protocols, new SOP rollouts ‚Äì all staples in pharma ‚Äì require frequent training refreshers across global teams. AI avatars excel here by producing engaging training videos quickly and in many languages. Instead of dense documents or slides, an avatar can walk employees through a new code-of-conduct policy with visual aids. Internal communications were among the first widespread uses of Synthesia at companies like SAP. The University College London even studied AI-generated video learning and found benefits for adult learners, indicating that well-designed avatar videos can be as effective as live video in learning contexts.\nSales Training and Role-Play: Pharmaceutical sales reps undergo extensive training for product knowledge and handling doctor interactions. Avatars can play the role of a ‚Äúvirtual doctor‚Äù for reps to practice on. For instance, UneeQ offers a sales trainer avatar that can simulate customer personalities and questions. A rep could speak (or type) to the avatar, and the AI doctor might interject with tough questions or objections in a realistic manner, helping the rep practice responses in a safe environment. Given the consistency of AI, every rep can be measured on the same scenarios, and the avatar can provide feedback or even scoring (especially if hooked to speech analytics that assess confidence or accuracy). This kind of training augmentation can complement human role-play sessions, and it‚Äôs available 24/7 for practice.\nClinical Training & Medical Education: AI avatars can be used in e-learning modules for clinicians. More interestingly, as seen with the SIU School of Medicine‚Äôs AI patient simulations, avatars can take on the role of patients with certain conditions for med students or HCPs to practice diagnostic interviewing. For example, an avatar could present as a patient with migraine ‚Äì describing symptoms, possibly even showing pain via facial expression ‚Äì and the learner has to ask the right questions. The avatar (driven by an LLM with a preset case profile) can answer in a natural, non-scripted way, making the scenario realistic. This can help HCPs build communication skills and empathy. Lilly could support such training as part of educational grants or internal training for medical staff.\nOnboarding and HR Training: Welcoming new employees to Lilly can include lots of orientation videos. An avatar could serve as an onboarding buddy, introducing the company‚Äôs values, giving office virtual tours, etc., in a personalized way. And when policies update (like IT security protocols), an avatar can quickly be deployed in an explainer video that might be more watched than a long memo. Some companies have even used AI avatars to answer common HR questions for employees, freeing up HR staff from repetitive queries. Microsoft‚Äôs scenario of a ‚Äúvirtual HR assistant‚Äù avatar is relevant here. This can be particularly useful in a large company where employees might have questions about benefits, travel policy, etc., and prefer a quick, spoken answer over digging through an intranet site.\nHuman Resources and Corporate Communications\nLeadership Messages: It‚Äôs a challenge for top executives to communicate frequently with a global workforce in their native languages. AI avatars allow creation of leadership video messages with minimal time from the executive. For instance, our CEO could type out or dictate a message about the company strategy, and have their own avatar deliver it in multiple languages to all employees. This ensures consistency of message and a personal touch (seeing Dave Ricks ‚Äúspeak‚Äù) without scheduling video shoots across time zones. Lilly must balance authenticity (some employees may prefer the real Dave Ricks despite language barriers) with efficiency, but this tool can dramatically increase the reach of leadership comms. We have already examples of townhalls that leverage a mix of real person/avatar presentations. Notably, Microsoft‚Äôs avatar service explicitly lists ‚ÄúCEO digital twin for a conference‚Äù as a use case.\nRecruitment: In recruitment marketing, avatars can be the face of the company ‚Äì e.g., a friendly avatar on the careers page that tells prospective candidates about company culture and answers FAQs (‚ÄúWhat is it like to work in R&D at Lilly?‚Äù). This can be interactive or simply a video. It provides a novel, engaging way to consume info compared to text. Additionally, for initial candidate screening, some firms have experimented with avatar interviewers that pose questions and record answers (to avoid human interviewer bias in facial reactions). Though the ethics of AI in interviews are debated, using a consistent avatar interviewer could standardize the process for all candidates. At minimum, avatars might guide candidates through an application process or pre-interview prep (like a virtual recruiter giving tips).\nInternal Communications: Avatars might also serve roles in employee resource groups or internal campaigns. For instance, a wellness campaign could feature an avatar ‚Äúcoach‚Äù that shares daily health tips or mindfulness exercises via short videos to employees. Because avatars can reflect diversity (you can choose avatars of different genders, ethnic backgrounds, and even create ones that represent persons with disabilities), they can be selected to resonate with certain audiences or to be inclusive in company messaging. Some companies have used AI avatars to provide sign-language interpretation or simplified explanations alongside main content, improving accessibility of communications.\nCustomer Service and Patient Engagement\nVirtual Patient Assistants: AI avatars can humanize these digital support tools. For example, instead of a text FAQ, a patient could interact with a virtual nurse avatar available on a patient portal or via a mobile app. This service could explain how to inject a medication, remind patients about refill schedules, and answer common questions about managing side effects ‚Äì all in a comforting, visual manner. Because avatars can be available 24/7 and handle unlimited simultaneous sessions, they complement human-based call centers by handling routine inquiries. Importantly, an avatar can also escalate to a human or encourage contacting a doctor when needed. Alzheimer‚Äôs Foundation of America introduced an avatar named ‚ÄúAllison‚Äù to assist caregivers with information ‚Äì a sign that healthcare organizations see promise in avatar companions for support.\nClinical Trial Education: Recruiting and retaining patients in clinical trials is challenging. Avatars could help explain trial procedures to patients in a consistent way, improving understanding and trust. A trial might use an avatar to walk a participant through the informed consent document, ensuring they grasp key points (the avatar can even quiz them interactively to confirm understanding). During the trial, avatars could be part of digital follow-ups (‚ÄúHow are you feeling after week 2? Here‚Äôs what to expect next‚Ä¶‚Äù). Since trials often happen across different countries, using an avatar that speaks the patient‚Äôs language natively is a big plus.\nTelehealth and Virtual Care: Healthcare providers might integrate avatars as front-end greeters or initial assessors in telehealth platforms. For minor or routine issues, an avatar could gather symptoms from a patient in conversational form (‚ÄúWhat brings you in today?‚Äù) and either provide basic guidance or prepare a summary for the human doctor. Some telehealth apps already use chatbots for triage; adding an avatar face could make the experience less sterile. In sensitive areas like mental health, there have been studies showing some patients feel more comfortable initially talking to a virtual ‚Äúperson‚Äù than a real one (no fear of judgment). An empathetic-looking avatar could encourage patients to open up and then later transition to human care when needed. Woebot (a mental health chatbot) and similar tools use cartoon avatars; it‚Äôs conceivable that with advances in emotional AI, more realistic avatars could play a role in digital therapeutics (though caution is needed to avoid uncanny valley effects that could backfire in emotional situations).\nOther Industry Use Case (for reference)\nEducation: Schools and e-learning platforms are using avatars as tutors or lecture presenters (a professor can scale herself via an avatar to give basic lectures so she can focus on advanced interactive sessions in person).\nFinancial Services: Banks employ avatars in branches or apps for customer questions (e.g., ‚ÄúWhat‚Äôs the status of my loan?‚Äù answered by a smiling digital banker who can also visually show charts or forms).\nRetail & Hospitality: Hotel chains have tested lobby avatar kiosks for concierge info, and e-commerce sites have virtual sales agents that help customers find products.\nEntertainment: Virtual TV anchors, game streamers, and of course NPCs in games ‚Äì avatars are increasingly content creators or characters. There are even AI-generated pop music idols (virtual singers).\nThe common thread is scalability of human touch ‚Äì avatars bring a face and voice to AI, which can increase engagement across many domains.\nFor Lilly, the most immediate high-impact uses likely lie in the internal and HCP-facing realm (training, internal comms, HCP engagement) where regulatory risk is lower compared to direct patient interactions. However, as comfort and evidence build, patient-facing avatars could become part of patient support programs or disease awareness campaigns (with compliance and careful messaging oversight).\nOpportunities and Benefits\n‚Äã\nImplementing AI avatars offers several compelling benefits outlined below.\nScalability and Efficiency:\nAs noted, avatars allow you to produce content or handle interactions at scale, without linear cost increases. A single avatar can generate personalized videos for 100 or 100,000 people with minimal extra effort. This is ideal for pharma, which often needs to reach global audiences in many languages. Instead of maintaining large teams of translators, video producers, trainers in each region, a central team can deploy avatar content globally. This lowers cost per video/interaction dramatically ‚Äì companies have seen cost reductions on the order of 10x for video production. In training, consistent avatar-based modules ensure everyone gets the same quality of instruction, avoiding variability of different trainers.\nEngagement and Learning Impact:\nHumans are wired to respond to faces and voices. Avatars, even if known to be AI, often garner more attention than text on a page. Early career professionals particularly prefer video content. So using avatars in internal communications can increase engagement rates ‚Äì more people watching a policy update instead of ignoring an email. In external use, video messages can break through information overload better than PDF attachments. Studies in education show video with a human presenter can improve understanding and retention, and when that presenter is relatable (e.g., speaks the viewer‚Äôs language or has a comforting demeanor), it‚Äôs even more effective. Thus, avatars can improve training outcomes (e.g., better compliance adherence because employees absorbed the training), and marketing outcomes (better message recall among doctors who saw an avatar video about a drug vs. just reading a brochure).\nPersonalization and Localization:\nAvatars let you easily localize content ‚Äì not just language but potentially cultural style (you might pick different avatar personas for different regions if appropriate). Personalization, as discussed, can make communications feel tailored. For example, an avatar video addressing a patient by name and referencing their specific treatment plan could make them feel seen and cared for by the company, potentially improving adherence. In internal settings, onboarding could include an avatar that mentions the new hire‚Äôs name and department, creating a more personalized welcome than a generic video.\nConsistency and Compliance:\nThis is a subtle but important benefit in regulated industries: when you use an AI system to deliver information, it will stick to the script. There‚Äôs less risk of an off-the-cuff remark or deviation that could be non-compliant. Once the medical/legal team approves the avatar‚Äôs script, guardrails, and/or behavior, you can deploy it widely confident that every viewer gets the approved message exactly. It‚Äôs easier to update content as well ‚Äì if a safety warning needs adding, you edit the script and regenerate; you don‚Äôt need to schedule new shoots with an actor. Avatars also inherently will use the exact phrasing required (e.g., including all the necessary disclaimers verbatim). Of course, one must ensure the AI doesn‚Äôt hallucinate or go off-script ‚Äì which is why most one-way avatar videos are tightly scripted, and interactive ones should be anchored to approved knowledge bases for sensitive info. But compared to human agents or trainers who might paraphrase or get questions wrong, avatars can provide consistent, controlled messaging.\nInnovation and Brand Image:\nUsing AI avatars ‚Äì if done thoughtfully ‚Äì can position Lilly as an innovator. It signals that the company is tech-forward and exploring new ways to communicate. This can have an employer branding benefit (attracting talent excited about innovation) and even an investor/public relations angle (showcasing adoption of digital transformation). For example, if Lilly uses a virtual digital human for a digital-only product launch event, it may garner positive media coverage for creativity. However, this must be balanced with sensitivity to how the audience will perceive it (one doesn‚Äôt want to appear as though replacing human touch with ‚Äúbots‚Äù in patient care ‚Äì messaging has to emphasize augmentation, not replacement).\n24/7 Availability and Speed:\nAvatars, unlike people, don‚Äôt require sleep or schedules. A virtual medical info avatar on a website can answer a doctor‚Äôs question at 2 AM when no MSL is available. An internal IT support avatar could handle routine questions over the weekend. This instant response potential increases productivity (employees aren‚Äôt stuck waiting for answers) and customer satisfaction (HCPs or patients get help when they need it). Furthermore, creating a new avatar video can be done in hours, meaning communications can go out faster. For instance, when there‚Äôs an urgent update (like new drug approval news or a safety bulletin), an avatar video can be generated and disseminated company-wide or to stakeholders on the same day ‚Äì much faster than arranging a global conference call or subtitled video from an executive.\nCost Savings:\nWe touched on production cost savings. To quantify: making a 5-minute professional training video with live actors could cost tens of thousands of dollars (considering scriptwriting, filming crew, editing, talent fees, etc.), whereas with an avatar platform, it might just be the subscription cost or a few hundred dollars equivalent in usage. Over a year, if the company produces hundreds of videos, the savings are substantial. Additionally, travel costs can be saved if some training that used to be done via flying trainers around is replaced with avatar modules. Avatars can also reduce burden on human support staff by handling first-line queries (deflecting some volume away from call centers, which translates to cost savings or ability to redeploy staff to more complex tasks).\nNew Capabilities:\nAvatars can do things humans can‚Äôt, in a way. They can appear simultaneously in many places, as mentioned. They can also have instant knowledge of enormous datasets (if connected to AI). For example, an avatar medical assistant could instantly recall data from thousands of clinical trials to answer a question, something a human would have to research. In interactive training, an avatar could adapt the scenario on the fly to the learner‚Äôs performance (thanks to AI), providing a level of adaptive training that one human role-play partner might not manage consistently. These new capabilities ‚Äì such as an avatar noticing that a learner is struggling and dynamically adjusting difficulty ‚Äì could improve efficacy of programs.\nChallenges and Risks\n‚Äã\nAlong with opportunities come challenges and risks. Implementing AI avatars is not without potential pitfalls. It‚Äôs crucial to be aware of these challenges and plan mitigation strategies.\nAccuracy and Hallucination:\nIf an avatar is powered by AI (especially generative models like GPT) for live conversation, it might hallucinate incorrect answers or say something that is not desired. In pharma, an avatar giving wrong medical information or an unapproved off-label suggestion would be a serious issue. This risk is mitigated by keeping sensitive applications script-based or tightly controlling the AI‚Äôs knowledge domain. For instance, one would use a closed QA database for a medical FAQ, rather than a free-roaming internet model. Testing and validation of avatar responses is needed, similar to validating a chatbot. For one-way videos, the risk is low since content is pre-approved, but for interactive avatars, a robust content moderation and fail-safe mechanism is needed (e.g., the avatar should defer or escalate to human if unsure or asked something out of scope).\nUncanny Valley and User Acceptance:\nWhile avatars are highly realistic now, they can sometimes fall into the ‚Äúuncanny valley‚Äù ‚Äì where they look almost human but something is off, causing discomfort. Early experiences with stiff or emotionless avatars can turn off users. Even when well done, some users might find it creepy to interact with a fake human, especially if they are not informed. Transparency is key: users should know they are interacting with an AI avatar, not a real person (e.g., an on-screen label or introduction). Over time, as people get accustomed (just as everyone got used to voice assistants), this concern may fade. But for now, change management is needed. Internally, explain why the company is using avatars (‚Äúto save time, to provide multilingual support, etc.‚Äù) to avoid rumors of ‚Äúrobots replacing us‚Äù or pushback that it‚Äôs impersonal. Externally, gauge audience reactions through pilots ‚Äì maybe HCPs love the convenience of an avatar rep, or maybe they feel it‚Äôs too impersonal compared to a human rep. Adjust strategy accordingly. The design of the avatar also matters; some companies deliberately choose a more obviously animated style to avoid uncanny valley, while others go for full realism. Both pre-recorded avatar videos and interactive Digital People can be considered regulated content, so be sure to follow all existing policies and procedures for content, especially externally-facing content.\nConsent and Likeness Rights:\nIf you create an avatar of a real person (e.g., an employee or actor), you must have their explicit consent and probably a contract outlining usage. There have been cases of actors worrying about losing control of their digital likeness. Synthesia addressed this by compensating actors and requiring consent for any custom avatars. Lilly should ensure any patient or HCP figures used in avatar form have signed off. Using a popular public figure as an avatar without permission is ethically wrong and may be illegal (and most platforms prevent it).\nDeepfake Misuse:\nThe same tech that powers avatars can be misused for disinformation. While our company would never intend that, being associated with ‚Äúdeepfake tech‚Äù can draw scrutiny. It‚Äôs important to position the use as ‚Äúsynthetic media for positive and authorized purposes‚Äù. Following industry ethics guidelines (there are emerging frameworks for synthetic media transparency) is wise. For example, including an unobtrusive watermark or a statement at the end like ‚ÄúThis video was generated using AI‚Äù can be considered for transparency, especially in external comms.\nData Privacy:\nInteractive avatars will collect data (e.g., video of users if the avatar ‚Äúsees‚Äù them, conversation logs, etc.). All this needs to be handled under privacy laws (HIPAA if any health info, GDPR in EU, etc.). Data should be secured and not used beyond the stated purpose. If patients talk to an avatar about their condition, that‚Äôs sensitive personal health information requiring proper safeguards.\nBias and Inclusion:\nAI models might carry biases (e.g., an avatar‚Äôs speech recognition might struggle with certain accents, or its responses might inadvertently be skewed). It‚Äôs important to test with diverse user groups. Also, offering a variety of avatar ethnicities and genders for user choice can be a way to be inclusive (some research in customer service bots suggests people prefer an avatar they identify with). However, one must avoid stereotypes (e.g., not always making the nurse avatar female or the executive avatar male ‚Äì break stereotypical representations).\nEmployee concerns:\nIn some cases, new technology can cause distraction and concern. While the intent of a use case may be to augment existing staff, always consider proactively communicating how roles will evolve.\nQuality and Technical Limitations:\nAlthough improving rapidly, avatars can still have occasional glitches ‚Äì maybe a pronunciation error for a new drug name, or slightly mismatched lip sync on a difficult word. Visual quality might not perfectly match a professional studio video (especially if internet bandwidth is an issue for streaming interactive avatars). In cases where top-notch polish is required (e.g., a broadcast TV commercial), current avatar tech might not fully replace human footage (though it‚Äôs getting close). Also, generating very long videos can be time-consuming or costly on some platforms. There might be limitations like avatars not being able to show highly dynamic movements (running, complex interactions with objects) ‚Äì they are mostly standing or sitting and gesturing. For most corporate needs these are fine, but we must set expectations that avatars have a ‚Äúpresentation style‚Äù which might be somewhat stiff if overused. Keeping avatar videos concise and to the point helps. On the technical side, integration into legacy systems can be a challenge ‚Äì for example, making an avatar appear within an existing mobile app might require custom dev work or using an SDK, which may require custom development support and possibly waiting on vendor capabilities (e.g., an offline mode if internet is down, etc.). Ensuring you have support and possibly fallback options (text or human chat) is prudent when deploying new tech.\nInfrastructure and Cost Considerations:\nWhile individual video generation is cheap, every technology has some runtime costs. High-quality avatars, especially interactive ones, often require GPU services. If using a cloud API extensively, those costs can add up (though likely still less than human labor equivalents). There‚Äôs also the question of whether to centralize avatar generation or allow many users to do it ‚Äì a large company might need governance to avoid a patchwork of disjointed avatar experiences. License costs for enterprise plans (Synthesia, etc.) should be factored. Also, network infrastructure: streaming avatars (like a live conversation) means video data streaming; Tech@Lilly will need to ensure bandwidth and latency are acceptable, especially for global usage. If deploying via Azure or another cloud, ensure compliance with where data is processed (some may require regional data centers, etc., particularly for health data).\nRegulatory and Public Perception:\nIn pharma, any promotional communication is potentially subject to regulatory scrutiny (FDA, EMA, etc.). It‚Äôs uncharted territory how regulators will view AI-generated spokespeople. The content itself still must meet all regulations (fair balance, no off-label, etc.), which is straightforward if we control the script. But one might consider informing regulators or adding disclaimers that ‚ÄúThis is a fictional animated presenter‚Äù in consumer content to avoid any concern of deception. Another angle: if an avatar answers questions live, ensuring it doesn‚Äôt inadvertently create an ‚Äúadverse event report‚Äù that gets missed (e.g., if a patient tells the avatar about a side effect, does that need to be reported?) ‚Äì processes must be in place as they would for any chatbot intake. On public perception, Lilly must avoid missteps like using an avatar to represent a patient without disclosure. A recent example outside pharma: a state media outlet used Synthesia avatars to deliver political propaganda, which caused controversy. Synthesia has since tightened policies. The lesson: always be transparent and truthful in usage. Use avatars to supplement human communication, not to masquerade deceptively. If a commercial, for example, shows an avatar as a ‚Äúdoctor‚Äù testimonial, consider indicating that it‚Äôs a virtual depiction unless one explicitly wants to simulate a fictional scenario (like an actor would in a dramatization).\nMany of these risks can be managed with proper governance, testing, and a human-in-the-loop approach. For instance, keep humans supervising avatar interactions in early deployments (an avatar could flag a live agent to step in for complex queries). Maintain logs of avatar outputs for auditing (especially in medical info contexts). And engage ethicists or compliance early in the project design.\nAdoption Strategies\n‚Äã\nTo successfully adopt AI avatars or Digital People technology, a phased and strategic approach is advisable. We propose aligning with the Tech@Lilly Innovation Pipeline framework ‚Äì moving from early innovation stages like Alpha and Beta, to small-scale real-world deployment, before scaling with a general release. This innovation process is designed to be both rapid and controlled.\nGiven the breadth of possibilities, it‚Äôs wise to prioritize use cases based on risk and business value. Throughout these stages, cross-functional collaboration is important. Involve Tech@Lilly, Quality, Legal, and subject matter experts in the function benefiting from the technology. Also, get feedback from end-users as early as possible.\nFinally, stay aware of emerging trends and rapid improvement as the technology continues to mature. Examples:\nConvergence with AR/VR (AR glasses could project likenesses of digital coaches in front of an employee during some hands on learning of a practice or procedure).\nImprovements in AI agents (avatars becoming more autonomous in performing tasks, not just talking).\nBeing prepared to integrate those when ready will keep the company at the forefront of digital innovation in the industry.\nFuture Outlook\n‚Äã\nDigital People and pre-recorded Avatar video technology is advancing rapidly. Soon it will become nearly impossible to determine if the person you see on a screen is real or synthetic.\nLooking ahead 2‚Äì5 years, we can expect:\nNear-Photorealism and Beyond: Avatars will become indistinguishable from real humans on camera. Companies like Synthesia are already investing in next-gen models (e.g., their partnership with Shutterstock to train **‚ÄúEXPRESS-2‚Äù models for a new generation of avatars). This will improve subtle details like hair, hand movements, clothing physics, and reducing any visual artifacts. Full-body avatars will progress to walking and interacting naturally with props or environments. For corporate use, this simply means higher quality content that viewers will find credible and engaging.\nReal-Time Multilingual Conversations: We will likely see avatars that can instantly translate speech in one language to another while preserving the speaker‚Äôs voice and facial expressions. Early versions exist (e.g., NVIDIA demoed real-time translation of a talking video from English to Spanish with correct lip movements). In a few years, it will likely be common for live speakers to be ‚Äúavatarized‚Äù in real time to deliver the message in all languages simultaneously on screen. This could be huge for global events or multilingual training ‚Äì a single presenter can effectively speak everyone‚Äôs language through their avatar. Microsoft‚Äôs research in speech-to-speech translation and startups like HeyGen working on live dubbing indicate this is close.\nGreater Autonomy and Agency: Future avatars will not just be reactive, but proactive agents. With more integration to enterprise systems and more sophisticated AI agents, avatars could perform tasks. For example, a virtual medical assistant avatar might notice a patient‚Äôs refill is due and proactively come up (via a phone app) to remind them and offer to place an order or schedule a pharmacy delivery, completing the action within the conversation. Digital People, driven by AI agent technology, could coordinate with each other too ‚Äì one can imagine a team of digital agents handling complex workflows (one talks to the user, another fetches data in background, etc.). In games and simulations, multiple AI NPC avatars may converse among themselves, making scenarios more realistic (Convai demonstrated NPCs discussing and collaborating in a game world). In a business, maybe two avatar advisors (one representing finance and one legal) could jointly brief a user on a matter, each with expertise.\nPersonalized Appearance via Generative AI: We will see more user control to create custom avatar appearances with simple inputs. In the future, it may be common practice for each employee to have their own digital clone, which may attend meetings, delivery presentations, or even provide another employee career advice. Also, ‚ÄúPrompt-based‚Äù avatar generation will improve, meaning you can generate a completely unique digital spokesperson that matches your brand persona in seconds (no actors needed at all). These generative avatars will then be animated by the usual pipelines. We are already seeing companies like HeyGen offer ‚Äòcross-modal cloning‚Äô features ‚Äì e.g., creating an avatar of a person using only a sample voice recording and a photo. This could allow quick creation of avatars of real staff with minimal input (but will need careful consent workflows). It could also enable dynamic avatars that change appearance contextually (imagine an avatar that visually appears tired or wearing winter clothes if talking about winter health, etc., done automatically).\nBlending Virtual and Physical (XR): Avatars will increasingly step off the 2D screen. In AR (augmented reality), you might have an avatar appear in your room via glasses or phone, guiding you through a procedure (like assembling a device or injecting a medicine). In VR, training programs will use AI characters as role-players in fully immersive environments (for example, practicing an emergency response in a virtual hospital with AI avatar patients and colleagues). Pharma companies might, in future, use AR avatar reps ‚Äì instead of a Zoom call, a doctor wearing AR glasses could see a life-size avatar of the pharma rep in their office virtually for a discussion. Companies like Meta are heavily researching ultra-realistic avatars for VR telepresence; by the time devices like Apple‚Äôs Vision Pro become common, having your ‚Äúdigital twin‚Äù join a meeting might be normalized. It will be important to ensure these avatars maintain the same compliance messaging in those mediums as well (which is another reason to have them AI-driven rather than entirely controlled by users).\nIndustry Regulations and Standards: We can anticipate that governments and industry bodies will issue guidelines or even regulations on AI-generated media. This might include requirements to disclose AI content in certain contexts (especially in political or public communications). Advertising standards might adapt ‚Äì e.g., requiring a disclaimer if a ‚Äúperson‚Äù endorsing a product is not real. The pharma industry might create best practices specifically for avatar usage in patient interactions to ensure trust (perhaps an adaptation of existing chatbot guidelines). Being ahead of this by self-imposing transparency will be wise. On the flip side, as it becomes mainstream, people will also become more accepting of AI avatars, just as chatbots became ubiquitous. We may also see certifications or seals (maybe an avatar platform gets certified for healthcare information delivery if it meets certain accuracy standards).\nConvergence with Voice and Robotics: While this report focuses on Digital People, note the synergy with voice assistants and even physical robots. The same AI driving a Digital Person could drive a humanoid robot or a customer service animatronic kiosk.\nIncreased Use in Healthcare Delivery: If we look at the WHO‚Äôs Florence in 2022 as an early example, by 2030 we might commonly have ‚Äúdigital health workers‚Äù supplementing human healthcare, endorsed by major organizations. This could help address personnel shortages by handling routine interactions. Pharma companies may eventually partner in creating validated avatars that can guide patients on proper use of medicines or lifestyle changes. It‚Äôs not far-fetched that clinical guidelines or patient education materials might come with an official Digital Person that any clinic can use to educate patients in a standardized way.\nIn preparing for the future, Lilly should remain agile in adopting updates. It might also consider shaping the future ‚Äì e.g., participating in industry consortia on AI avatars, or even developing bespoke in-house capabilities for strategic advantage, if warranted.\nConclusion\n‚Äã\nDigital People and AI avatars are rapidly transitioning from novelty to a practical tool across enterprise functions. For Lilly, they offer a new medium to communicate scientific information, engage with stakeholders, and train and support the workforce more effectively. By starting with targeted use cases and scaling carefully with proper oversight, we can reap the efficiency and engagement benefits while managing risks. This technology will only get more powerful and more prevalent; as industry leaders and early adopters, we will have the advantage of experience and internal capability when avatars become a standard interface in business.\nSources\n‚Äã\nainvest.com\nainvest.com/news/synthesia-ai-avatars-drive-50-efficiency-gains-fortune-100-companies-2506\nbusinesswire.com\nbusinesswire.com/news/home/20240927126293/en/Soul-Machines-Expands-AI-Assistant-Capabilities-With-New-Advanced-Object-Recognition-and-Memory-Features-Powered-by-Its-Patented-Experiential-AI-Technology\ncutter.com/article/vr-applications-healthcare-and-medicine-digital-humans\nd-id.com/news\ndeveloper.nvidia.com/blog/spotlight-uneeq-revolutionizes-customer-engagement-with-ai-powered-digital-humans\ndigitalhumans.com\ndigitalhumans.com/features/integrations-and-deployment\ndigitalhumans.com/features/synanim\ndigitalhumans.com/platform\nforbes.com/sites/gilpress/2025/03/05/microsoft-transforms-communications-with-agentic-ai-avatars-from-d-id\nheygen.com\nheygen.com/news/latest-drops-fall-2024\nintelligenthq.com\nintelligenthq.com/time-recognises-d-ids-ai-avatars-as-best-innovation-of-2024\nnvidianews.nvidia.com\nnvidianews.nvidia.com/news/ace-avatar-cloud-engine-microservices\npharmaphorum.com\npharmaphorum.com/news/meet-florence-whos-ai-powered-digital-health-worker\nsoulmachines.com\nspringfieldherald.news\nsynthesia.io/blog/category/synthesia-news\nsynthesia.io/case-studies\ntechcommunity.microsoft.com\ntechcommunity.microsoft.com/blog/azure-ai-services-blog/azure-ai-speech-announces-public-preview-of-text-to-speech-avatar/3981448\ntechcrunch.com/2024/10/31/d-id-launches-new-high-quality-avatars-capable-of-real-time-conversations\nventurebeat.com/ai/synthesia-announces-platform-update-with-interactive-ai-videos-full-body-avatars\nWas this helpful?\nTags:\ninnovation\nEdit this page\nPrevious\nüì° Emerging Tech\nNext\nAI Speech\nExecutive Summary\nIntroduction\nHow AI Avatars Work\nAI Training Techniques\nTraditional Avatars vs AI Avatars\nCapabilities and Features of Modern AI Avatars\nTop Vendors and Key Players\nSecondary ‚ÄúWrapper‚Äù Vendors: Pattern & Pitfalls\nApplications in Pharma (and Other Industries)\nOpportunities and Benefits\nChallenges and Risks\nAdoption Strategies\nFuture Outlook\nConclusion\nSources\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 4,
    "depth": 2,
    "crawled_at": "2026-02-25T10:06:19.972910"
  },
  "https://techhq.dc.lilly.com/docs/innovate/emerging_tech/ai_speech": {
    "url": "https://techhq.dc.lilly.com/docs/innovate/emerging_tech/ai_speech",
    "title": "AI Speech | Tech HQ",
    "description": "AI Speech",
    "h1": [
      "AI Speech",
      "Speech-to-Speech vs STT/TTS Pipeline Architectures"
    ],
    "h2": [
      "Executive Summary‚Äã",
      "What is AI speech technology‚Äã",
      "How AI speech technology works‚Äã",
      "Direct speech-to-speech models deliver speed but sacrifice pharmaceutical safety requirements‚Äã",
      "Pipeline architectures achieve medical-grade accuracy through specialized components and modular optimization‚Äã",
      "Hybrid approaches combining strengths remain largely theoretical for pharmaceutical applications‚Äã",
      "The Accuracy-Latency Tradeoff‚Äã",
      "Recent S2S Developments‚Äã",
      "Market analysis: vendors, costs, and value proposition‚Äã",
      "Use Case Examples‚Äã",
      "Regulatory Compliance‚Äã"
    ],
    "h3": [
      "Signal processing and acoustic feature extraction‚Äã",
      "Transformer architectures for speech recognition‚Äã",
      "Neural text-to-speech synthesis‚Äã",
      "Training methods and transfer learning‚Äã",
      "Voice biomarker extraction and analysis‚Äã",
      "Integration with multimodal AI systems‚Äã",
      "Cloud Platform Providers‚Äã",
      "Specialized pharmaceutical and healthcare vendors‚Äã",
      "Voice biomarker companies for clinical endpoints‚Äã",
      "High-level Value Indicators‚Äã",
      "Research and Discovery‚Äã",
      "Clinical Development‚Äã",
      "Manufacturing and Quality‚Äã",
      "Commercial and Medical Affairs‚Äã",
      "FDA Framework and Validation Requirements‚Äã",
      "EMA Guidance and EU Implementation‚Äã",
      "HIPAA and Data Protection Requirements‚Äã",
      "GDPR Article 9 and Biometric Data Governance‚Äã",
      "Cross-Border Data Transfer Mechanisms‚Äã",
      "Asia-Pacific Regional Variations‚Äã"
    ],
    "text_content": "AI Speech | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüí° Tech@Lilly Innovation Pipeline\nüõû Our Innovation Stages\nüèéÔ∏è Fast Start\nüì° Emerging Tech\nAI Avatars / Digital People\nAI Speech\nAI Video Generation\nüì° Emerging Tech\nAI Speech\nOn this page\nAI Speech\nTechnologies for Pharmaceutical Operations: A 12-Month Strategic Assessment for Eli Lilly and Company\nEmerging Tech Report\nLast Update: 2025-11-14\nTech Innovation Pipeline\n, Enterprise Business Architecture, Tech@Lilly Enterprise\nAuthor: Doug Gorr\nNote\nThe rapidly evolving nature of AI speech technologies means that capabilities, vendors, pricing, and regulatory frameworks described may change significantly. This report represents a point-in-time assessment based on available information through November 2025.\nINTERNAL USE ONLY - CONTAINS AI GENERATED CONTENT\nThis containts AI generated content and includes insights from over 1,000 publicly available sources including peer-reviewed publications, regulatory documents, industry reports, vendor specifications, and market analyses. The content represents AI-assisted compilation and interpretation of publicly accessible information current as of November 2025. This should not be distributed, reproduced, or shared outside the organization without explicit authorization from the author and appropriate leadership. The insights, analyses, and recommendations contained herein are intended to inform internal strategic discussions and should not be construed as official Eli Lilly positions or commitments.\nExecutive Summary\n‚Äã\nAI Speech has reached 'pharmaceutical-grade' maturity.\nAI speech represents far reaching opportunities, offering Eli Lilly solutions for voice-enabled documentation, clinical voice biomarkers, and conversational AI across the enterprise. The FDA issued groundbreaking guidance in January 2025 establishing risk-based frameworks for AI in drug development, while voice biomarker markets are projected to reach $5.4 billion by 2035. This creates an immediate 12-month window for strategic positioning before widespread industry adoption accelerates.\nWhy Now\nThree converging forces make 2025 the inflection point for pharmaceutical voice AI adoption. First, transformer-based speech models (OpenAI Whisper, Microsoft VALL-E 2) achieved human parity performance in 2024 with 95%+ medical terminology accuracy. Second, regulatory clarity emerged with FDA's January 2025 credibility assessment framework and EMA's September 2024 final guidance on AI in the medicinal product lifecycle. Third, proven ROI metrics demonstrate 40-70% time savings in clinical documentation, 50% reduction in adverse event processing time, and 200-12,000% first-year returns across implementations.\nThe Strategic Imperative\nTakeda Pharmaceuticals is concidered a leader in AI Speech amoungst competitors with a dedicated MIT voice biomarker program for neurological diseases, while Pfizer, GSK, and Sanofi deploy conversational AI for medical information services. Eli Lilly could leapfrog competitors by strategically deploying voice AI across high-value use cases: voice-enabled laboratory notebooks integrating with LIMS (16% data capture increase documented), hands-free GMP manufacturing documentation (99.86% accuracy with IoT integration), voice biomarkers for clinical trial endpoints in neuropsychiatry (80% detection accuracy for depression from 20 seconds of speech), and AI-powered medical information services (85% call deflection rates). The pharmaceutical voice AI market exhibits classic first-mover advantages with 85% of generative AI spend flowing to innovative startups over incumbents, suggesting rapid consolidation ahead.\nRisks and Mitigations\nVoice data constitutes Protected Health Information under HIPAA and special category data under GDPR Article 9, requiring explicit consent frameworks and privacy-by-design architectures. The FDA's August 2024 warning letter for an AI dosing algorithm that caused 10x pediatric overdose underscores the necessity for human-in-the-loop validation. Voice recognition algorithms trained on non-diverse datasets risk bias across accents and demographics, with 94% of depression voice studies having fewer than 100 participants. Mitigation strategies include federated learning architectures preserving data sovereignty (99% of centralized model quality while keeping data local), differential privacy implementations, comprehensive GxP validation protocols aligned with 21 CFR Part 11, and pilot programs with clearly defined success criteria before enterprise scaling.\nRecommendation for Lilly\nLeverage the Tech Innovation Pipeline framework for advanced speech technologies with structured Alpha, Beta, and Early Release phases to systematically prove value before enterprise commitment. Begin with exploratory Alpha experiments across diverse use cases: voice-enabled laboratory documentation systems, ambient clinical intelligence for field medical teams, digital voice biomarkers as exploratory endpoints, and conversational AI for patient engagement. Progress promising technologies through Beta pilots with defined success metrics and risk mitigation protocols, then advance validated solutions to Early Release with controlled user groups before considering enterprise scaling. Leverage a cross-functional review structure spanning Quality, Regulatory Affairs, Legal, Enterprise Business Architecture, and Medical Affairs to oversee the pipeline progression. Allow low-risk initiatives to progress quickly through the stages of innovation, acting as pathfinders for more strategic, higher-risk applications to follow. Allocate innovation funding with stage-gate investment decisions tied to demonstrated value and regulatory feasibility.\nWhat is AI speech technology\n‚Äã\nAI speech technology enables computers to understand human speech and generate human-like voices through machine learning, fundamentally transforming how pharmaceutical companies capture data, engage patients, and conduct research. At its core, the technology works bidirectionally: Speech-to-Text (STT) systems convert spoken words into written text that computers can process, while Text-to-Speech (TTS) systems transform written text into natural-sounding audio that humans can understand. Voice cloning extends these capabilities by capturing and replicating individual voice characteristics, enabling personalized patient communications and preservation of voice for medical conditions affecting speech.\nThe practical significance for pharmaceutical operations becomes clear in daily scenarios. A laboratory scientist wearing gloves in a biosafety cabinet can verbally dictate experimental observations directly into an electronic lab notebook without touching a keyboard, eliminating contamination risks and data transcription errors. A medical science liaison conducting a physician meeting can have the conversation automatically transcribed and structured into CRM-compliant documentation within seconds, reducing post-meeting administrative work from 45 minutes to 5 minutes. A clinical trial participant with Parkinson's disease can complete daily speech assessments on a smartphone, providing continuous disease progression monitoring without traveling to clinical sites. A quality control inspector on a manufacturing floor can verbally confirm batch parameters while maintaining hands-free GMP documentation compliance.\nModern AI speech systems fundamentally differ from older voice recognition technology\nthat required extensive training on individual voices and struggled with medical vocabulary. Today's transformer-based neural networks are pre-trained on millions of hours of diverse speech data, enabling them to understand medical terminology, pharmaceutical nomenclature, and scientific language out-of-the-box with 95-97% accuracy. The systems adapt to different accents, speaking styles, and acoustic environments without individual voice training. They process language contextually, understanding that \"two milligrams\" and \"2mg\" represent the same pharmaceutical dose, or that \"adverse event\" in regulatory context differs from casual usage of those words.\nThe technology stack comprises four integrated layers working seamlessly together. The acoustic layer captures raw audio and converts it to mathematical representations called spectrograms that preserve pitch, tone, and timing information. The linguistic layer applies natural language processing to understand grammar, context, and meaning within pharmaceutical domain-specific vocabularies. The integration layer connects to enterprise systems like LIMS, EHR, CRM, and quality management platforms, automatically populating structured data fields from conversational speech. The compliance layer ensures all voice interactions generate audit trails, electronic signatures, and documentation meeting FDA 21 CFR Part 11 and EU GMP requirements.\nThree deployment models address different pharmaceutical use cases. Cloud-based systems (Microsoft Azure Speech, Google Cloud Speech-to-Text, Amazon Transcribe Medical) offer the highest accuracy and continuous improvement through centralized learning, ideal for non-real-time applications like transcribing investigator meetings or processing adverse event call center recordings. On-premise systems deployed within Lilly data centers provide maximum data control and sovereignty, required for highly sensitive clinical trial data or proprietary research information subject to strict access controls. Edge deployment on mobile devices or manufacturing equipment enables real-time voice interaction without network connectivity, critical for manufacturing floor operations or field-based clinical research where internet access is unreliable.\nThe pharmaceutical industry specifically benefits from voice biomarker capabilities that extract health information directly from voice characteristics beyond the spoken words. Subtle changes in voice quality, pitch stability, speech rate, pause patterns, and articulatory precision correlate with neurological conditions (Parkinson's disease, Alzheimer's disease, ALS), mental health status (depression, anxiety, PTSD), and respiratory function (COPD, asthma, COVID-19). A 30-second speech sample analyzed by AI algorithms can predict mild cognitive impairment progression to Alzheimer's with 80% accuracy, detect depression from voice acoustics with similar accuracy, or screen for respiratory infection before symptoms emerge. These capabilities transform voice from a data input mechanism into a continuous, non-invasive diagnostic and monitoring tool.\nUnderstanding voice AI capabilities and limitations is essential for realistic deployment planning. The technology excels at structured tasks with clear vocabulary and context: transcribing standard operating procedures, documenting manufacturing batch records, capturing clinical trial visit notes, answering common medical information questions from predefined knowledge bases. It struggles with highly ambiguous context, novel drug names never encountered in training data, extreme background noise exceeding 85 decibels, or detecting subtle deception in patient-reported outcomes. Voice AI augments human expertise rather than replacing it, the optimal paradigm combines AI for rapid transcription and initial analysis with human review for final verification and nuanced interpretation.\nHow AI speech technology works\n‚Äã\nThe transformation of human speech into computer-processable information relies on sophisticated neural network architectures that have evolved dramatically over the past 18 months, with pharmaceutical applications benefiting from models specifically trained on medical terminology and clinical conversations. Understanding the technical architecture enables realistic assessment of capabilities, limitations, and integration requirements for pharmaceutical deployments.\nSignal processing and acoustic feature extraction\n‚Äã\nWhen someone speaks into a microphone, the analog sound wave is digitized at standard sampling rates of 16 kHz for telephony or 44-48 kHz for high-fidelity recording. The raw audio signal undergoes preprocessing to remove background noise, normalize volume levels, and enhance speech frequencies. The critical transformation converts time-domain audio waveforms into frequency-domain representations through Fourier analysis, producing spectrograms that visually represent which frequencies have energy at each time point. Modern systems use mel-scale spectrograms that emphasize frequency ranges most relevant to human speech perception, typically extracting 80-128 mel-frequency channels. These spectrograms become the input to neural networks, with each 30-second audio chunk converted to a two-dimensional image-like representation that the AI model processes.\nEarlier generation systems relied on hand-engineered acoustic features like Mel-Frequency Cepstral Coefficients (MFCCs) that captured specific audio characteristics known to be important for speech. Contemporary transformer-based models learn optimal feature representations automatically from raw spectrograms through self-supervised learning on massive unlabeled audio datasets. This architectural shift explains the dramatic accuracy improvements seen in 2024-2025, with models learning subtle patterns in audio that human engineers never explicitly programmed. For pharmaceutical applications, this means the systems automatically adapt to specialized acoustic environments like cleanrooms with HEPA filter background noise or clinical examination rooms with medical equipment sounds.\nTransformer architectures for speech recognition\n‚Äã\nOpenAI Whisper\nrepresents the current state-of-the-art for pharmaceutical speech-to-text applications, with Whisper Large-v3 (released November 2023) and Whisper Turbo (2024 optimization) offering the best combination of accuracy and speed. The architecture employs a transformer encoder-decoder design where the encoder processes the mel-spectrogram input through stacked transformer blocks with self-attention mechanisms, building progressively higher-level representations of the audio content. The decoder generates text output autoregressively, predicting one token at a time while attending to both the encoded audio features and previously generated text. Training on 5 million hours of labeled audio data across 100+ languages enables zero-shot capabilities on pharmaceutical terminology without requiring domain-specific fine-tuning, though custom vocabulary additions improve accuracy for novel drug names.\nWhisper achieves 50% fewer errors than previous generation models across diverse datasets, with Word Error Rates under 5% on clean speech and 10-15% on conversational clinical interactions. The multilingual architecture uses special tokens to specify source language and task (transcription vs. translation), enabling a single model to handle Lilly's global operations across North America, Europe, and Asia-Pacific. The automatic language detection capability eliminates the need to pre-specify languages in multinational clinical trials. However, Whisper exhibits a 1.4% hallucination rate where it generates plausible but non-existent content in silent audio segments -  critical issue for pharmaceutical documentation requiring perfect fidelity.\nConformer architectures\ncombine convolutional neural networks with transformer attention mechanisms, leveraging CNNs to capture local audio patterns (like phoneme transitions) while transformers model long-range dependencies (like sentence-level prosody). Google's Conformer implementations and Apple's edge-optimized versions demonstrate significant efficiency improvements, with Apple achieving 5.26x real-time speed (19% Real-Time Factor) on mobile processors - fast enough for instantaneous transcription on iPads used in clinical trials. AssemblyAI's Universal-1 model built on Conformer architecture provides pharmaceutical-grade medical conversation models with enhanced accuracy on clinical terminology, doctor-patient dialogues, and adverse event descriptions.\nFor real-time applications like voice-enabled manufacturing documentation or live medical information call transcription, streaming architectures process audio in overlapping chunks, producing transcription with under 300 milliseconds of latency. Deepgram Nova-2 achieves sub-100ms recognition latency (faster than human reaction time), enabling natural conversational flows in AI assistants for patient engagement or medical affairs applications. The technical challenge involves look-ahead limitations: streaming models cannot use future audio context when transcribing the current word, slightly reducing accuracy compared to batch processing of complete utterances.\nNeural text-to-speech synthesis\n‚Äã\nRecent breakthroughs in TTS enable pharmaceutical companies to generate natural-sounding patient communications, multilingual training materials, and accessible content from written documentation.\nMicrosoft VALL-E 2\nachieved the first confirmed human parity performance in June 2024, meaning listeners cannot distinguish AI-generated speech from human recordings. The architecture uses neural codec language models that treat audio as a sequence of discrete tokens similar to how language models treat text. Audio is first compressed using EnCodec into hierarchical representations at multiple bitrates, creating 8 quantization layers that capture both coarse structure (phonetics) and fine acoustic details (voice timbre, prosody). A large transformer language model then learns to generate these audio tokens conditioned on input text and a 3-second reference audio sample.\nTwo architectural innovations enable the human parity result. Repetition Aware Sampling refines the standard nucleus sampling algorithm used to select the next audio token, accounting for repetition patterns in the generation history to prevent infinite loops and stuttering artifacts that plagued earlier models. Grouped Code Modeling treats multiple consecutive audio tokens as atomic units, dramatically shortening the sequence length the model must predict, accelerating inference speed while maintaining fidelity. Training on 60,000 hours of English speech provides the model with sufficient coverage of speaking styles, emotions, and acoustic variations to generalize to new speakers with minimal prompting.\nFor pharmaceutical applications, this enables zero-shot voice cloning for personalized patient communications - recording a brief audio sample from a key opinion leader allows generating derivative content in their voice for educational materials, provided appropriate consent and ethical frameworks are in place. More immediately practical, modern TTS generates patient information in 60+ languages with regionally appropriate accents, addressing health equity requirements for diverse clinical trial populations and global commercial markets. The Mean Opinion Score benchmark measuring perceived naturalness shows current systems achieving 4.3-4.5 out of 5.0 (where 4.5+ represents \"sounds like a human\"), compared to 3.0-3.5 for older TTS systems with robotic qualities.\nDiffusion and flow-matching models\nrepresent an emerging alternative to autoregressive generation. These models iteratively refine random noise into high-quality speech spectrograms through a learned denoising process, similar to how diffusion models generate images in DALL-E or Stable Diffusion. DiTTo-TTS and F5-TTS (2024 models) demonstrate that Diffusion Transformer architectures outperform traditional U-Net designs, producing more natural prosody and fewer artifacts. The variable-length modeling capability handles pharmaceutical content of arbitrary duration, from short patient notifications to hour-long training narrations, without quality degradation. Non-autoregressive generation allows parallel processing of the entire utterance simultaneously rather than sequentially, reducing latency from seconds to under 100 milliseconds - crucial for real-time voice interaction in clinical call centers or patient engagement chatbots.\nSpeech-to-Speech vs STT/TTS Pipeline Architectures\nSpeech-to-Speech models dominate over pipleine architectures for natual language conversation interatction, with 2-3x lower latency\n, however challenges abound for regulated applications, primarily because regulated deployments demand auditability, error traceability, and multi-stage safety verification that modular systems uniquely provide. While 2024-2025 brought commercial S2S breakthroughs with GPT RealTime Voice, GPT4o, and Gemini Live, high-risk deployments may continue to favor STT > LLM > TTS pipelines for clinical documentation, medication ordering, and diagnostic applications where regulatory compliance and patient safety outweigh conversational naturalness.\nThis creates a clear architectural divide:\nConsumer voice AI is rapidly adopting end-to-end S2S models for their superior latency and prosody preservation, while pharmaceutical applications continue investing in pipeline systems that achieve 93-99% accuracy on medical terminology through specialized ASR models, enable FDA-mandated audit trails at each processing stage, and integrate clinical decision support to prevent medication errors - features nearly impossible to implement in monolithic S2S architectures.\nThe pharmaceutical sector's unique constraints - 25% of medication errors involve drug name confusion, FDA transparency requirements for AI medical devices, HIPAA compliance mandates, and the life-or-death consequences of transcription mistakes fundamentally favor architectures where errors can be isolated, verified, and corrected at multiple checkpoints rather than generated in a single black-box forward pass.\nDirect speech-to-speech models deliver speed but sacrifice pharmaceutical safety requirements\n‚Äã\nEnd-to-end S2S models represent a paradigm shift from cascade systems, directly mapping source speech to target speech without intermediate text representation. Google's Translatotron 3 (2024) surpassed cascade models on translation benchmarks, while Microsoft's VALL-E 2 achieved human parity on zero-shot text-to-speech with 4.2% word error rate matching ground truth performance. Meta's SeamlessM4T v2 supports 101 - 36 languages with 20% BLEU improvements over previous state-of-the-art, demonstrating the technical maturity of S2S architectures.\nLatency advantages create the strongest S2S value proposition.\nKyutai's Moshi demonstrates 160ms response time compared to 510ms for optimized pipelines, approaching the 230ms human conversation baseline. OpenAI's Realtime API averages 232ms audio-to-audio response, while SpeechGPT 2.0 achieves sub-200ms latency in practical tests - 3x faster than typical pipeline implementations (500-4000ms). This speed advantage enables natural full-duplex conversations with fluid interruption handling impossible in serial pipeline processing.\nParalinguistic preservation represents S2S models' second major strength.\nAudioLM's hierarchical tokenization separates semantic content from acoustic characteristics, maintaining speaker identity, prosody, and emotional tone across 60,000 hours of training data. Translatotron 2 preserves voice characteristics across language translation turns without segmentation, while SpeechGPT 2.0 offers multi-emotion, multi-style control including rap, drama, whisper, and robot modes through 750bps ultra-low bitrate codecs that jointly model semantics and acoustics.\nHowever, pharmaceutical applications reveal critical S2S limitations that overshadow these benefits. The monolithic architecture creates a black box issue, a concern for FDA's 2024 guidance requiring \"transparent, non-complex algorithms\" allowing providers to \"independently review the basis for recommendations.\" When a medication name is misheard - distinguishing \"epinephrine\" from \"ephedrine,\" or \"Celebrex\" from \"Celexa\" S2S systems provide no mechanism to identify whether acoustic confusion or language model error caused the mistake, making root cause analysis and correction impossible.\nHallucinations pose significate risks.\nS2S models can generate plausible-sounding but non-existent information, a well-documented phenomenon where models add phrases from training data like \"subtitles by Jane Doe\" or confabulate medical terms. In medication ordering, where 25% of errors involve drug name confusion and wrong medications can cause patient death, this single-point-of-failure architecture lacks the multiple verification checkpoints pharmaceutical safety demands.\nTraining S2S models for medical applications faces data scarcity challenges. While AudioPaLM trained on thousands of hours across 100+ languages and VALL-E used 60,000 hours of English speech, medical-specific S2S models remain largely theoretical with limited production deployments. Current S2S systems show 35-65% word error rates on medical terminology without specialized fine-tuning - unacceptable when clinical standards require sub-5% error rates for patient safety.\nPipeline architectures achieve medical-grade accuracy through specialized components and modular optimization\n‚Äã\nThe modular STT -> LLM -> TTS pipeline remains the production standard for pharmaceutical voice AI in 2024-2025, with market leaders universally adopting this architecture: Nuance DAX Copilot, Suki Assistant, Abridge, and Nabla Copilot all use pipeline approaches for clinical documentation serving major health systems like Mass General Brigham and Epic EHR integrations.\nMedical-specific ASR models deliver accuracy S2S systems cannot match.\nDeepgram Nova-3 Medical achieves 63.7% improvement in word error rate over competitors with 4% keyword error rate - half the errors of general systems on clinical terminology. Speechmatics Medical Model reaches 93% real-world accuracy with 50% fewer errors on medical terms versus next-best systems. AssemblyAI's Slam-1 demonstrates 66% reduction in missed medical entity rates with 72% preference in blind human evaluations, supporting up to 1,000 domain-specific terms via prompting for pharmaceutical names, procedure codes, and anatomical references.\nThis specialization extends beyond English. Whisper medical adaptations achieve 0.985% WER on LibriSpeech after fine-tuning, while maintaining multilingual capabilities across 98 languages. Companies like Nabla invested $5M and three years on 7,000 hours of annotated medical encounters to suppress hallucinations and improve terminology accuracy - investment levels impossible to replicate for monolithic S2S systems where component-level optimization is impossible.\nStreaming optimizations reduce pipeline latency to competitive levels.\nOptimized implementations using Deepgram Nova (100ms), GPT-4o-mini (200ms first token), and Cartesia Sonic TTS (90ms time-to-first-byte) achieve 465ms end-to-end latency - approaching S2S performance while maintaining modular advantages. Voice AI platforms like Vapi and Synthflow consistently deliver sub-800ms response times through parallel processing where VAD, STT, and API prefetching execute concurrently rather than serially.\nModern pipeline architectures implement sophisticated concurrent execution graphs that eliminate serial bottlenecks: sentence-level streaming sends partial LLM outputs to TTS before generation completes; predictive prefetching loads customer data on caller ID; speculative processing starts likely operations early. This transforms the mental model from serial chains to parallel graphs, with independent tasks executing simultaneously across STT, VAD, LLM reasoning, API calls, retrieval, and audio buffering stages.\nComponent swapping enables continuous improvement impossible in S2S systems.\nWhen Deepgram releases Nova-3 with 30% WER reduction, pipeline architectures upgrade ASR immediately without touching LLM or TTS components. Medical facilities can deploy GPT-4 for complex reasoning while using GPT-4o-mini for simple queries, mixing models by use case. TTS can switch between ElevenLabs (naturalness), Cartesia (speed), or Deepgram (enterprise balance) based on application requirements - flexibility that monolithic S2S architectures fundamentally cannot provide.\nIntegration with existing pharmaceutical infrastructure represents another decisive pipeline advantage. LLMs natively support function calling for CRM lookups, real-time inventory checks, and EHR data retrieval during conversations. RAG integration with medical knowledge bases using FAISS vector search enables evidence-based responses grounded in current medical literature. Clinical decision support systems intercept text outputs to verify medication indications, check dose ranges, flag drug-drug interactions, and alert on allergy contraindications - safety mechanisms requiring structured text that S2S's direct audio-to-audio processing bypasses.\nHybrid approaches combining strengths remain largely theoretical for pharmaceutical applications\n‚Äã\nWhile hybrid architectures theoretically combine S2S speed with pipeline safety, pharmaceutical implementations show minimal adoption of true hybrid systems. The regulatory burden of validating two parallel processing paths, the engineering complexity of fallback mechanisms, and the difficulty determining when to use which approach create practical barriers outweighing theoretical benefits.\nProposed hybrid patterns include risk-based routing\n: using S2S for low-risk patient engagement (appointment scheduling, general health information) while defaulting to pipelines for any clinical decision-making, medication advice, or symptom triage. However, real-world deployments reveal challenges in boundary detection - conversations naturally drift from administrative topics to medical questions, requiring dynamic switching that introduces latency and complexity.\nCascaded systems using S2S for specific sub-tasks\nwithin larger pipelines show more promise. AudioLM could handle voice cloning and speaker identity preservation while pipeline NLP performs medical entity recognition and clinical reasoning. This compartmentalized approach maintains audit trails and safety verification while leveraging S2S strengths for acoustic processing - though current pharmaceutical systems rarely implement this architecture, suggesting practical challenges exceed theoretical benefits.\nFallback mechanisms from S2S to pipeline during uncertainty add robustness but could defeat S2S's primary latency advantage. When an S2S model expresses low confidence on a drug name, falling back to pipeline verification adds 500-1000ms delay could be worse than using pipelines throughout. Some use cases may prove practical for \"hybrid\" approach. However, simply choosing pipeline for pharmaceutical applications and reserving S2S for low-risk voice interfaces, rather than attempting to combine both within single systems may be best for a given implementation.\nThe Accuracy-Latency Tradeoff\n‚Äã\nLatency measurements show\nS2S models deliver 2-3x faster response times\nacross consistent benchmarks. Moshi achieves 160ms, OpenAI Realtime API averages 232ms, and SpeechGPT 2.0 reaches sub-200ms all approaching human conversation's 230ms baseline. In contrast, typical production pipelines range from 500-4000ms depending on optimization level, with best-in-class implementations achieving 465-812ms through aggressive streaming and parallel processing.\nHowever, accuracy metrics favor pipelines by substantial margins for medical applications.* Medical-specific pipeline ASR achieves 5-10% WER (90-95% accuracy) on clinical conversations compared to 35-65% WER for general S2S models on medical content. Specialized systems like Deepgram Nova-3 Medical reach 4% keyword error rate on pharmaceutical terms - critical when distinguishing \"hydroxyzine\" (antihistamine) from \"hydralazine\" (blood pressure medication) can mean life or death.\nComputational requirements differ substantially between architectures.\nS2S models typically require larger unified models (1-2B parameters) with higher GPU memory demands - AudioPaLM uses 600M parameter Conformers, while medical pipeline ASR runs efficiently on 4-8GB consumer GPUs. Training requirements also diverge: S2S needs 60,000+ hours of parallel speech-speech data (VALL-E, AudioLM), while pipeline components fine-tune independently on smaller domain-specific datasets - Whisper medical adaptations use 7,000 hours versus millions required for general S2S.\nVoice quality metrics measured by Mean Opinion Score reveal\nTTS components in pipelines match or exceed S2S output quality\n. ElevenLabs achieves 81.97% pronunciation accuracy versus 77.30% for OpenAI systems, with 44.98% high naturalness scores and only 5% hallucination rates compared to 10% for competitors. VALL-E 2 reaches human parity at 4.5 MOS, while modern pipeline TTS from Cartesia Sonic achieves 40-95ms model latency - faster than many S2S systems' total response time.\nReal-time factor (RTF) measurements for scalability\nshow pipelines handle concurrent users more efficiently through independent component scaling. A single LLM instance serves multiple STT and TTS streams, while S2S models require dedicated resources per conversation. KV cache memory in LLMs scales linearly (10 concurrent requests need ~122GB for LLaMA-2 13B), but pipelines distribute this load across specialized hardware optimized per stage - ASR on inference servers, LLM on GPU clusters, TTS on edge devices impossible in monolithic S2S architectures.\nRecent S2S Developments\n‚Äã\n2024-2025 brought major S2S commercial launches that dramatically advanced consumer voice AI.\nOpenAI's GPT-4o Advanced Voice Mode\n(May 2024 unveiling, September rollout) delivers native multimodal audio sensing emotional intonations with interruption handling, offering 9 voice options and custom instructions via natural language. Google's Gemini Live (August 2024) provides free-tier conversational AI with 10+ emotionally expressive voices, while Gemini 2.5 (February 2025) adds real-time audio-video understanding with style control across 24+ languages and SynthID watermarking.\nMicrosoft Azure AI Speech's 2024-2025 releases\nillustrate enterprise focus on pipeline optimization rather than S2S: Personal Voice 2.1 offers zero-shot TTS requiring only seconds of audio for voice cloning across 100+ languages; Fast Transcription API delivers 10-minute audio transcribed in 15 seconds; Voice Live API (GA 2025) provides all-in-one STT + GenAI + TTS through a single low-latency interface. These remain modular pipeline services despite unified API presentation, maintaining component-level control enterprises demand.\nMeta's SeamlessM4T v2 publication in Nature (2024)\nwith 20% BLEU improvements and 2-second latency represents S2S research breakthroughs, but real-world pharmaceutical implementations remain absent from literature. Searches for \"S2S pharmaceutical deployment\" or \"end-to-end speech clinical documentation\" yield no production case studies, while pipeline systems dominate published healthcare AI implementations - suggesting a research-practice gap where academic S2S advances haven't translated to pharmaceutical adoption.\nTraining methods and transfer learning\n‚Äã\nModern speech AI leverages transfer learning paradigms where large foundation models pre-trained on general domain data are adapted to pharmaceutical contexts with relatively modest amounts of domain-specific training examples.\nSelf-supervised learning\nenables training on unlabeled audio by creating artificial prediction tasks: HuBERT (Hidden Unit BERT) masks portions of audio and trains the model to predict the missing segments, while wav2vec 2.0 learns to discriminate between true future audio frames and contrastive negative samples. These pre-training objectives force the model to learn general acoustic and linguistic representations transferable to downstream tasks.\nFor Lilly-specific customization, several adaptation strategies balance accuracy improvements against resource requirements and ongoing maintenance.\nCustom vocabulary augmentation\nadds pharmaceutical terminology (drug names, disease names, procedure codes, company-specific acronyms) to the model's recognition dictionary with appropriate phonetic representations and prior probabilities. This low-cost intervention (typically 1-2 days of ML engineering work) provides significant gains for novel or proprietary compound names not in the pre-training data.\nFine-tuning\nretrains portions of the neural network on Lilly-specific audio data, such as recordings of investigator meetings, quality review meetings, or sales call transcripts. Full fine-tuning updates all model parameters and requires substantial labeled data (10,000+ hours), while parameter-efficient methods like LoRA (Low-Rank Adaptation) modify only small adapter modules inserted into the base model, achieving 80-90% of full fine-tuning benefits with 100x less training data.\nThe training data quality and diversity directly determines model robustness across Lilly's global operations. Models trained predominantly on North American English struggle with British, Indian, or Australian accents common in multinational clinical trials and research centers. Acoustic diversity matters equally: training datasets skewed toward quiet studio recordings generalize poorly to noisy manufacturing floors, hospital patient rooms, or outdoor clinical sites. The highest-performing pharmaceutical implementations deliberately include training data representing the actual deployment environments: recordings from manufacturing facilities with\nbackground machinery noise, clinical consultations with typical hospital ambient sounds, field-based researchers using smartphones in suboptimal acoustic conditions. This explains why general-purpose speech models require pharmaceutical-specific adaptation despite their strong baseline performance.\nVoice biomarker extraction and analysis\n‚Äã\nBeyond transcribing words, speech AI extracts quantitative features from voice acoustics that correlate with health status, emotional state, and cognitive function - capabilities with profound implications for clinical trials and patient monitoring. Voice biomarker analysis operates on both linguistic content (what is said) and paralinguistic acoustics (how it is said). Linguistic features include semantic coherence, lexical diversity, word-finding difficulty, grammatical complexity, and pause-to-word ratios. Paralinguistic features comprise fundamental frequency (pitch), jitter (pitch variability), shimmer (amplitude variability), formant frequencies (vocal tract resonances), mel-frequency cepstral coefficients (spectral envelope), zero-crossing rate (breathiness), and speech rate.\nMachine learning classifiers trained on clinically validated datasets learn to recognize patterns discriminating between healthy individuals and patients with specific conditions. For Parkinson's disease, characteristic features include reduced pitch variability (monotone speech), slower articulation rate, imprecise consonant production, and vocal tremor. Trained models analyzing 30-60 second speech samples achieve 85-95% sensitivity and specificity for Parkinson's detection, with accuracy improving when combining multiple speech tasks: sustained vowel phonation, rapid syllable repetition (pa-ta-ka), reading standardized passages, spontaneous speech describing a picture. The Framingham Heart Study analysis published in Nature (2024) demonstrated that speech content features predict conversion from mild cognitive impairment to Alzheimer's disease with 80% accuracy six years in advance, by detecting subtle declines in semantic richness and narrative coherence.\nDepression and anxiety detection relies heavily on prosodic features and temporal dynamics. Depressed individuals exhibit slower speech rate, longer pauses, reduced pitch variation (flatter affect), lower speech energy, and less pitch/amplitude modulation. Sonde Health's Mental Fitness Vocal Biomarker achieved relative risk ratios of 2.00 for detecting anxiety using aggregated two-week voice data in a prospective psychiatric cohort study, improving to 8.50 relative risk for frequent users providing 5-6 samples weekly. The technology enables continuous passive monitoring of mental health status from routine phone conversations or voice journaling, providing early warning signals of decompensation for intervention before crisis points. Respiratory conditions like COPD and COVID-19 alter voice quality through mechanisms including reduced airflow (breathiness), mucus accumulation (vocal tract damping), and inflammation (hoarseness), detectable with 70-80% accuracy from 6-second \"ahh\" vocalizations.\nImplementing voice biomarkers in pharmaceutical clinical trials requires rigorous analytical validation following FDA Biomarker Qualification Program standards. The qualification process involves demonstrating: (1) analytical validity (the biomarker reliably and accurately measures what it purports to measure), (2) clinical validity (the biomarker consistently and accurately predicts or correlates with the clinical outcome), and (3) clinical utility (the biomarker meaningfully informs clinical decision-making to improve patient outcomes). For voice biomarkers specifically, this necessitates large diverse datasets (thousands of participants across demographics, languages, and disease severities), longitudinal validation demonstrating stability of the biomarker in healthy individuals and predictable change patterns in disease progression, and prospective studies showing the biomarker improves upon existing endpoints. The Bridge2AI-Voice consortium's 10,000-participant, 50-institution initiative represents the first large-scale effort to establish the standardized protocols, reference datasets, and analytical pipelines required for regulatory qualification.\nIntegration with multimodal AI systems\n‚Äã\nThe most powerful pharmaceutical applications combine voice AI with complementary data modalities in unified machine learning frameworks.\nVoice plus computer vision\nenables ambient documentation in clinical settings: cameras track physician gaze, gestures, and activities (patient examination, screen viewing, writing) while microphones capture the clinical dialogue, with integrated AI models generating complete clinical notes including both observed physical exam findings and patient history. Johnson & Johnson's Polyphonic surgical video analysis system (partnership with NVIDIA, 2024) combines audio from the operating room with video of surgical techniques to generate highlight reels and identify best practices, reducing surgical training time by 50%.\nVoice plus wearable sensors\nenhances remote patient monitoring by correlating voice biomarkers with physiological signals. A clinical trial participant with Parkinson's disease provides daily 60-second speech samples via smartphone while continuously wearing an accelerometer-equipped smartwatch tracking tremor and gait. Machine learning models trained on the combined dataset achieve higher disease progression detection accuracy than either modality alone, learning that specific combinations of voice stability metrics and movement disorder features predict medication efficacy. The 5G network infrastructure and edge computing capabilities emerging in 2025 enable real-time processing of these multimodal data streams with latency under 500 milliseconds, supporting adaptive interventions.\nVoice plus knowledge graphs\npowers intelligent pharmaceutical information systems. A medical science liaison verbally queries \"What are the latest efficacy data for compound 247 in the Phase 3 ATLAS trial?\" The speech recognition system transcribes the question, a large language model interprets the intent and extracts entities (compound name, trial identifier, data type), the knowledge graph traverses relationships connecting the compound to clinical trials to efficacy endpoints to recent data updates, and the text-to-speech system verbalizes a comprehensive answer with appropriate scientific caveats. Microsoft's integration of Azure Speech Services with Azure OpenAI enables these sophisticated question-answering workflows with conversational natural language interfaces, transforming static document repositories into interactive AI assistants accessible by voice.\nMarket analysis: vendors, costs, and value proposition\n‚Äã\nThe AI speech technology market serving pharmaceutical operations exhibits rapid growth, vendor consolidation, and price-performance improvements creating favorable conditions for enterprise adoption. Global conversational AI in healthcare will expand from $13.68 billion in 2024 to $106.67 billion by 2033 at 25.71% compound annual growth rate, with speech recognition and voice generation representing approximately 30-35% of this total. For pharmaceutical-specific applications, voice biomarker markets project growth from $1.08 billion (2024) to $5.40 billion (2035) at 15.81% CAGR, while ambient clinical intelligence - a key use case for medical affairs and clinical operations - will reach $600 million in 2025, growing 2.4x year-over-year.\nCloud Platform Providers\n‚Äã\nMicrosoft Azure Speech Services\noffers the most comprehensive pharmaceutical ecosystem through its 2022 acquisition of Nuance Healthcare for $19.7 billion, combining cloud infrastructure with proven healthcare products. Dragon Medical One serves 550,000+ physicians and operates in 77% of U.S. hospitals, providing the deepest integration with Epic, Cerner, and Meditech electronic health records. For pharmaceutical applications, Azure provides Speech-to-Text in 92+ languages at $1.00 per hour of audio processed for standard models, with custom pharmaceutical vocabulary adaptation available. Neural Text-to-Speech costs $15.00 per million characters, generating natural-sounding patient communications and training content in 215 voices across 60 languages. Enterprise contracts could include HIPAA Business Associate Agreements, GDPR-compliant data processing, and commitment tier discounts for predictable workloads (volume discounts of 20-40% for million-hour annual commitments).\nThe strategic value for Lilly extends beyond speech APIs to complete ambient intelligence solutions. For example, Nuance DAX Copilot integrates GPT-4 with ambient listening to generate clinical documentation from physician-patient conversations within minutes, priced at approximately $600 monthly per provider under enterprise agreements. Independent validation shows 70% reduction in clinician burnout and 50% reduction in documentation time. The Permanente Medical Group reported 15,791 hours saved annually (equivalent to 8 full-time employees) using Nuance ambient intelligence. For Lilly, this translates to medical science liaison documentation automation, clinical trial investigator meeting transcription, and medical monitor note generation with pharmaceutical-grade compliance and audit trails.\nGoogle Cloud Speech-to-Text\nemphasizes accuracy for medical conversations through its Chirp 3 foundation model trained on billions of sentences. Medical conversation models optimized for clinical dialogues cost $0.072 per minute ($4.32 per hour), representing a 4x premium over standard models but delivering superior accuracy on pharmaceutical terminology, drug names, and adverse event descriptions. Google's Healthcare Natural Language API complements speech recognition with entity extraction identifying medications, dosages, symptoms, and diagnoses from transcribed text, enabling structured data capture from unstructured clinical narratives. The platform supports 125+ languages with single-region or multi-region deployment options addressing data sovereignty requirements across Lilly's global operations. Performance benchmarks show Google's medical models achieve 15-20% lower Word Error Rates than generic speech recognition on pharmacology content.\nAmazon Web Services\npositions Transcribe Medical specifically for healthcare use cases at $0.0237 per minute for batch processing and $0.0395 per minute for real-time streaming, including HIPAA-eligible infrastructure with Business Associate Agreements. The service automatically recognizes medical terminology across 31 specialties including oncology, cardiology, and neurology - the therapeutic areas most relevant to Lilly's portfolio. Amazon Polly neural voices generate text-to-speech at $16 per million characters for neural quality, suitable for patient education materials and multilingual clinical trial communications. The compelling value proposition for pharmaceutical companies emerges from tight integration with AWS's broader healthcare data analytics stack: speech data flows into Amazon HealthLake for FHIR-based clinical data warehouses, with Amazon Comprehend Medical extracting structured clinical concepts for downstream analysis.\nCost modeling for enterprise speech AI adoption depends on anticipated monthly volume and use case distribution. AI speech processings ($1-2.5/hour) compares very favorably to human transcription services ($1.50-3.00 per audio minute).\nSpecialized pharmaceutical and healthcare vendors\n‚Äã\nNuance Healthcare\n(Microsoft-owned) dominates ambient clinical intelligence with market-leading products purpose-built for healthcare documentation. Dragon Medical One provides continuous speech recognition for clinical note dictation at custom enterprise pricing. DAX Copilot represents the premium tier provider, offering fully automated visit note generation from ambient conversation capture without requiring specific voice commands or structured dictation. The April 2023 launch of DAX Express provides completely automated workflow with no human review in the transcription loop, though human physician review of the final note remains mandatory for medical-legal reasons. Nuance reports that 300 million patient stories are captured annually through its platform, with 77% of U.S. hospitals using Nuance technology in some capacity.\nFor Lilly medical affairs applications, integration with Veeva CRM could enable medical science liaisons to focus entirely on medical dialogue with healthcare providers while the ambient system automatically generates compliant interaction documentation including medical information exchanged, physician questions, and product discussions. Assuming a 50-70% documentation time reduction directly translates to increased field capacity: a 50-representative MSL team saving 10 hours per week each gains 500 hours of additional physician engagement time weekly, enabling 50-100 additional meaningful medical interactions monthly depending on meeting duration.\nSuki AI\ntargets the mid-market sweet spot with comprehensive ambient documentation, achieving 93.2 KLAS overall performance score and serving 250+ U.S. health systems across 99 medical specialties. The platform provides end-to-end AI assistance including pre-charting (automatically pulling relevant historical information before visits), ambient note-taking during consultations, clinical reasoning support, ICD-10/CPT coding recommendations, and question-answering from medical literature. Deep EHR integrations with Epic, Cerner, Athenahealth, and MEDITECH enable bidirectional data flow, automatically pulling context from the EHR and writing structured notes back to appropriate fields. Suki reports 72% reduction in documentation time, 6 hours reduction in after-hours \"pajama time\" documentation, and 9x first-year ROI.\nFor Lilly clinical operations, this technology could apply to clinical trial site support, medical monitor documentation, and investigator meeting transcription.\nAbridge\ncommands 30% market share in ambient clinical intelligence (second only to Nuance's 33%) with strong Epic integration and adoption by Mayo Clinic, Kaiser Permanente, and UPMC health systems. The platform uniquely captures structured patient instructions and next steps, automatically generating after-visit summaries in patient-friendly language that can be immediately shared, improving patient engagement and protocol adherence - a critical feature for clinical trial applications where participant understanding drives retention and compliance. The 90-day audio retention policy (shorter than Nuance's indefinite retention with patient consent) may limit usefulness for long-term clinical research applications requiring extended review periods or regulatory audits spanning years.\nVoice biomarker companies for clinical endpoints\n‚Äã\nWinterlight Labs\nleads cognitive assessment through speech analysis, with active deployment in 12+ clinical trials for Alzheimer's disease, depression, dementia, bipolar disorder, and schizophrenia. The technology analyzes 30-40 second speech samples using natural language processing to quantify semantic coherence, lexical diversity, syntactic complexity, and acoustic features. Cambridge Cognition acquired Winterlight in January 2023 for approximately $8.6 million, integrating speech biomarkers with their established cognitive testing battery. The combined offering provides pharmaceutical sponsors a validated digital endpoint package: traditional computerized cognitive assessments plus voice biomarkers, enabling comprehensive cognitive function monitoring in Alzheimer's and dementia trials with reduced participant burden compared to lengthy in-person cognitive batteries.\nFor Lilly's neuroscience portfolio, voice biomarkers may offer several strategic advantages over conventional cognitive endpoints.\nContinuous remote monitoring\ncaptures daily or weekly measurements reflecting real-world cognitive fluctuations, rather than snapshot assessments at quarterly clinic visits that miss disease dynamics and treatment response variability.\nReduced participant burden\n(30-60 seconds voice recording versus 45-60 minute cognitive test batteries) improves retention in elderly populations and those with significant impairment who struggle with long assessments.\nIncreased sensitivity\nto subtle changes may detect treatment effects earlier or with smaller sample sizes, potentially reducing Phase 2 trial costs by 20-30% through better go/no-go decision-making.\nEcological validity\nof speech production - a complex real-world task integrating memory, executive function, language, and motor control - arguably better represents functional capacity than abstract computerized tests.\nSonde Health\nspecializes in mental health and respiratory voice biomarkers, with FDA regulatory pathway engagement positioning their Mental Fitness Vocal Biomarker and Respiratory-Responsive Vocal Biomarker for clinical decision support. The MFVB demonstrated in a 104-participant prospective psychiatric cohort that aggregated two-week voice data achieved relative risk ratio of 2.00 for detecting anxiety (p=0.0068), improving to 8.50 for users providing 5-6 samples weekly. The RRVB trained on 3,000+ respiratory disease patients achieved 70% sensitivity and specificity across asthma, COPD, interstitial lung disease, and persistent cough, with validation extended to COVID-19 detection (73.2% sensitivity, 62.9% specificity for symptomatic and asymptomatic cases). Strategic partnerships with Qualcomm enable on-device processing on mobile chipsets, addressing privacy concerns by keeping voice data local rather than transmitting to cloud servers.\nFor Lilly's immunology brands, respiratory voice biomarkers may enable real-world evidence generation showing treatment impact on day-to-day breathing function beyond controlled spirometry at clinic visits. Depression and anxiety voice monitoring supports neuropsychiatry trials and post-market surveillance for CNS compounds where psychiatric adverse events require vigilant monitoring. Commercial deployment could differentiate patient support programs: branded smartphone apps with daily voice check-ins providing patients continuous feedback on symptom trends and automatically alerting care teams to concerning changes.\nCanary Speech\nemerged from former Amazon Alexa neurology and speech AI teams, offering the industry's most comprehensive multi-condition platform extracting 12+ million biomarkers per minute of speech. FDA registration (not full approval) provides credibility for healthcare system adoption. The technology addresses Alzheimer's, Parkinson's, Huntington's disease, depression, anxiety, stress assessment, and readmission risk prediction. The proactive screening capability - identifying individuals at risk before symptoms emerge aligns with pharmaceutical industry interest in earlier diagnosis enabling intervention in milder disease stages where disease-modifying therapies have best chance of success.\nFor Lilly, this suggests potential partnerships embedding Canary Speech screening in primary care workflows to identify patients who might benefit from neuroscience therapies, expanding the addressable patient population.\nThe compelling economics emerge in Phase 2/3 trials where voice biomarkers could serve as secondary or exploratory endpoints. A 200-participant Phase 2 trial adding voice biomarkers at $100 per participant represents a fractional cost of total trial cost while potentially providing crucial supportive evidence of mechanism, guiding dose selection, or enabling patient stratification. If voice biomarker data enables better Phase 3 trial design, reducing the required sample size by even 50 patients, the savings far exceed the voice biomarker investment.\nHigh-level Value Indicators\n‚Äã\nQuantitative ROI analysis across implemented pharmaceutical use cases reveals consistently strong business cases with payback periods of 3-18 months and multi-year returns of 200-1,200%. The most immediate value derives from productivity improvements in high-cost knowledge worker roles where voice AI eliminates low-value documentation tasks, reallocating human capital to higher-value activities.\nMedical Affairs\n: MSLs may spend 35% of time on administrative tasks (call notes, follow-up emails, CRM data entry, expense reports) = ~35 hours per week per MSL\nClinical Operations\n: 10-minute average handling time, most requiring transcription for pharmacovigilance and regulatory documentation\nLRL lab productivity\n: Research scientists may be spending 2 hours a day on documentation and data entry\nData quality improvements\n: Eliminating manual transcription errors reduces protocol deviations (clinical trials), batch failures (manufacturing), and data-cleaning overhead (R&D). Industry estimates suggest 2-5% of clinical trial costs relate to data issues that voice AI reduces by 30-50%.\nRegulatory compliance\n: Complete audit trails, elimination of paper records, improved GMP documentation quality reduce FDA 483 observations and warning letter risk. The business impact of avoided regulatory citations (production delays, consent decree costs, reputational harm) can exceed millions but proves difficult to quantify prospectively.\nEmployee satisfaction and retention\n: Nuance reports 70-94% of physicians report reduced burnout with ambient AI. For pharmaceutical roles with high turnover costs (recruiting, training, productivity ramp), even modest retention improvements generate substantial value. Replacing knowledge workers come at a high cost (months for recruiting, onboarding, training to full productivity).\nUse Case Examples\n‚Äã\nResearch and Discovery\n‚Äã\nVoice-enabled laboratory informatics systems transform research productivity by eliminating the friction between conducting experiments and capturing data. Traditional laboratory workflows interrupt experimental procedures for researchers to remove gloves, type observations into computers, and then 're-glove' - a cycle repeated dozens of times daily that consumes 15-25% of active research time. LabTwin's AI-powered mobile voice assistant provides hands-free data capture allowing scientists to verbally dictate experimental observations, request protocol steps, log sample information, and query previous results while maintaining sterile technique and workflow continuity. Implementation at dsm-firmenich achieved 100% real-time digitization of lab data with 16% increase in data capture rate, attributed to recording observations immediately rather than deferring to end-of-day batch entry where details fade and errors accumulate.\nThe integration architecture connects voice AI with Laboratory Information Management Systems (LIMS) and Electronic Laboratory Notebooks (ELN) through bidirectional APIs. When a researcher verbally reports \"sample concentration 247 micromolar,\" the voice system transcribes the utterance, applies natural language understanding to extract structured data (sample identifier from context, concentration value 247, units micromolar), validates against expected ranges for the assay, and automatically populates the corresponding LIMS field with full audit trail (timestamp, user, voice confidence score). Researchers query existing data conversationally: \"What was the IC50 for compound 181 in the kinase assay?\" The system retrieves relevant records and verbalizes the answer, eliminating time searching through databases or spreadsheets.\nLabVantage LOTTIE (LabVantage Open Talk Interactive Experience), as an example, exemplifies purpose-built LIMS voice integration, with customizable voice commands executing complex workflows. A chromatography technician verbally triggers \"begin sample prep protocol for batch 445\" and the system retrieves the SOP, displays the current step, marks the protocol as in-progress, reserves necessary equipment, and logs the initiation time - actions requiring 8-10 manual clicks compressed into a 3-second voice command. The property-driven configuration language enables lab managers to define facility-specific commands without programming: \"check freezer 7 temperature\" could query IoT sensors and verbalize the current reading, while \"reserve mass spec for Tuesday at 2pm\" enters equipment scheduling systems.\nBeyond convenience, voice interfaces enable novel experimental approaches. Researchers conducting extended observations (cell culture monitoring, crystallization kinetics, reaction progression) can provide continuous verbal running commentary captured as timestamped annotations correlated with automated sensor data (microscope images, spectroscopy readings, temperature profiles). Machine learning algorithms later analyze synchronized multi-modal data streams to identify subtle patterns human observers miss. In chemical synthesis, verbal description of reaction color changes, precipitate formation, or odor characteristics (safety permitting) captures qualitative information that numerical sensors cannot, providing complete experimental records valuable for troubleshooting and knowledge transfer.\nThe accessibility benefits significantly impact researchers with disabilities or repetitive strain injuries. Voice interaction eliminates barriers for scientists with limited hand mobility, visual impairments interfacing with screen readers, or carpal tunnel syndrome aggravated by extensive typing. This aligns with Lilly's diversity and inclusion objectives while tapping talent that might otherwise face challenges in laboratory roles.\nClinical Development\n‚Äã\nClinical trials represent the most transformative application domain for pharmaceutical voice AI, with implications spanning patient recruitment, continuous monitoring, digital endpoints, adverse event capture, and protocol adherence. The convergence of validated voice biomarkers, regulatory acceptance of digital health technologies, and decentralized trial models creates ideal conditions for large-scale deployment in Lilly's clinical programs.\nVoice biomarkers as clinical trial endpoints\naddress limitations of conventional assessment scales that provide sparse infrequent measurements subject to recall bias, practice effects, and limited ecological validity. Consider a Phase 2 trial for a novel Alzheimer's therapy using the Alzheimer's Disease Assessment Scale-Cognitive (ADAS-Cog) as primary endpoint, measured at baseline and then quarterly clinic visits (months 3, 6, 9, 12). This design provides only 5 data points per participant across a year, missing disease variability between visits and potential treatment effects that emerge then plateau between measurements. Voice biomarkers captured weekly through smartphone apps provide 52 data points per participant, revealing treatment response trajectories, durability of effect, and individual variability patterns invisible in sparse quarterly assessments.\nThe Voice as Biomarker project (NIH-funded, Weill Cornell/University of South Florida) released 12,500 voice recordings from 306 cognitively diverse participants in December 2024, representing the first large-scale standardized voice biomarker dataset. The protocol includes 20 voice tasks: reading passages, free speech describing pictures, counting, breathing exercises, and coughing. Analysis demonstrates that specific linguistic features (semantic coherence, pause patterns, word-finding latency) predict progression from mild cognitive impairment to Alzheimer's dementia with 80% accuracy 6 years prospectively, based on Boston University's Framingham Heart Study cohort analysis published in Nature (2024). These results suggest voice biomarkers could serve as prognostic enrichment criteria (enrolling trial participants more likely to progress, increasing power to detect treatment effects) or as surrogate endpoints (early evidence of disease modification before clinical symptoms change).\nFor Lilly's\nneuroscience\npipeline, voice biomarkers may offer immediate application in Parkinson's disease trials.\nMotor function assessment via speech analysis\ncomplements the Movement Disorder Society-Unified Parkinson's Disease Rating Scale (MDS-UPDRS) which requires in-person clinician administration. Smartphone-based speech tasks (sustained vowel \"ahh\" for 10 seconds, rapid syllable repetition \"pa-ta-ka\", reading a standardized paragraph) capture voice quality, pitch stability, articulatory precision, and speech rate - features correlating with MDS-UPDRS motor scores (correlation coefficients 0.6-0.75) and demonstrating sensitivity to dopaminergic medication effects. A 150-participant Phase 2 trial collecting voice biomarkers 3x weekly provides 150 participants x 3 samples/week x 48 weeks = 21,600 data points, versus 750 data points from quarterly MDS-UPDRS assessments. The increased measurement density improves statistical power, potentially reducing required sample size by 20-30% (saving $3-5M in trial costs) while providing richer dataset for dose selection and patient stratification.\nMental health applications parallel neurology. Sonde Health's validation study demonstrating relative risk ratio of 2.00 for anxiety detection from aggregated two-week voice data (p=0.0068) suggests utility for safety monitoring in psychiatric trials or CNS compounds where anxiety represents a potential adverse effect. Daily voice check-ins administered through patient-facing trial apps capture mental health status continuously, providing objective data complementing subjective patient-reported outcomes. The\nTherabot trial\npublished in NEJM AI (March 2025) demonstrated that generative AI conversational agents achieved 51% average depression symptom reduction with engagement levels (6 hours over 8 weeks) comparable to traditional therapy. While Therabot serves therapeutic purposes, the underlying voice analysis capability enables passive monitoring during those interactions, creating data streams for clinical research.\nAdverse event capture from natural language\nrepresents an immediate near-term application requiring less regulatory validation than formal endpoints. Medical information contact centers receiving unsolicited AE reports currently rely on human transcription and coding, consuming 10-15 minutes per call and introducing 24-48 hour delays before pharmacovigilance database entry. ProPharma Medical Information Contact Center reports that AI-powered speech recognition with automated AE identification achieves 40% reduction in case intake processing time, accelerating safety signal detection and regulatory reporting compliance. Natural language processing algorithms trained on pharmacovigilance taxonomy automatically identify verbatim terms (patient-reported symptom descriptions), map to preferred terms (MedDRA coding), and extract metadata (onset date, severity, relationship to study drug, concomitant medications). The system flags serious adverse events (death, hospitalization, life-threatening events) for immediate escalation to medical monitors.\nImplementation architecture for clinical trial voice biomarkers requires consideration of data flow, regulatory compliance, and participant experience. Example implementation pattern:\nPatient-facing mobile app (iOS/Android) administering standardized voice tasks with prompts and instructions, recording high-quality audio locally\nOn-device initial processing using privacy-preserving algorithms extracting voice features without transmitting raw audio (addressing patient privacy concerns)\nEncrypted transmission of derived features (numerical vectors, not audio) to clinical trial data management systems via HTTPS with certificate pinning\nCloud-based advanced analytics in HIPAA/GDPR-compliant infrastructure (AWS/Azure healthcare zones) performing machine learning inference\neCRF integration - writing voice biomarker scores to electronic case report forms as study data\nAudit trail generation - documenting complete data provenance for regulatory inspection\nThe sponsor retains original audio recordings in encrypted archival storage for potential regulatory queries but primarily operates on privacy-preserving features, reducing re-identification risk and simplifying data protection compliance.\nManufacturing and Quality\n‚Äã\nPharmaceutical manufacturing operates under stringent Good Manufacturing Practice (GMP) regulations requiring complete documentation of every production step, environmental condition, material transfer, and quality check. The traditional paradigm demands operators interrupt hands-on work to manually record data on paper batch records or log into manufacturing execution systems (MES), creating productivity bottlenecks and introducing transcription errors. Voice-enabled manufacturing systems allow hands-free documentation while maintaining continuous production flow, improving both efficiency and data quality.\nThe\n21 CFR Part 11 compliance\nframework for electronic records and signatures applies equally to voice-captured data, requiring systems that ensure authenticity, integrity, and confidentiality. Implementation involves: (1) speaker authentication through voice biometrics or secondary credentials (badge scan, PIN), (2) secure timestamping of all voice interactions with NTP-synchronized clocks, (3) audit trail generation recording user identity, timestamp, original voice data, transcribed text, system actions, and approval chains, (4) version control for voice-enabled SOPs and work instructions, (5) electronic signature capture for critical process steps using voice passphrase plus secondary factor.\nQuality control testing\nexemplifies hands-free value proposition. A QC analyst performing pH measurement on 30 samples can verbally report \"sample one zero three pH 6.8\" for each measurement as performed, rather than recording on paper then transcribing to LIMS post-shift. The voice system integrates with laboratory equipment (pH meters, balances, spectrophotometers) via IoT protocols, cross-validating verbal reports against instrument readings. If the analyst reports \"pH 6.8\" but the connected pH meter indicates 6.9, the system flags the discrepancy for immediate resolution, preventing errors from entering permanent records.\nBatch record documentation\ntransforms from paper-intensive to voice-driven workflows. An operator conducting visual inspection of fill-finish vials verbally confirms \"one hundred units inspected, zero rejects, lot number A4739\" and the system automatically populates corresponding batch record fields, attaches electronic signature, marks the step complete, and advances to the next protocol step. For time-critical operations like aseptic processing where operators cannot break sterile field, voice interaction maintains continuous workflow while ensuring complete documentation. Research published in PMC (AI and IoT Integration in Pharmaceutical Manufacturing, 2024) demonstrates that CNN-based visual inspection integrated with voice verification achieves 99.86% accuracy in pharmaceutical quality control, combining AI image analysis with verbal operator confirmation for optimal human-AI collaboration.\nPredictive maintenance\ngains from voice-enabled reporting. When operators notice unusual equipment sounds, vibrations, or performance characteristics not captured by automated sensors (subtleties like slight binding in tablet press tooling or unusual smell from HVAC systems), they verbally report observations immediately through mobile devices or floor-mounted voice terminals. Machine learning algorithms aggregate verbal reports with IoT sensor data (vibration, temperature, current draw) to predict equipment failures before breakdowns occur. The same PMC study reports 30% reduction in unplanned manufacturing downtime through this voice-augmented predictive maintenance approach, directly impacting production throughput and product supply reliability.\nDeviation management\naccelerates through voice-enabled workflows. When operators identify deviations (parameters outside specifications, missed process steps, material discrepancies), immediate verbal reporting initiates the deviation process: the voice system generates a deviation record with preliminary details, notifies quality and production management, and prompts the operator through initial corrective actions. This reduces median deviation reporting time from 4-6 hours (completing paper forms, obtaining management signatures, scanning to quality systems) to 15-30 minutes, enabling faster root cause analysis and corrective action while memories remain fresh.\nValidation of voice-enabled manufacturing systems follows Computer System Validation (CSV) principles per GAMP 5 guidance, with voice AI systems typically classified as GAMP Category 4 (configured products) or Category 5 (custom applications). The validation lifecycle includes:\nInstallation Qualification (IQ)\n: Verifying voice hardware (microphones, network infrastructure) and software (speech recognition engines, MES integration modules) installed per specifications with appropriate security configurations, backup systems, and disaster recovery capabilities.\nOperational Qualification (OQ)\n: Functional testing covering voice recognition accuracy across user population (different voices, accents, speech patterns), environmental conditions (background noise typical of production areas, cleanroom HEPA filters, equipment operation), and failure modes (network interruption, API timeout, ambiguous utterances). Testing validates voice commands execute correct MES functions, data populates proper fields, errors trigger appropriate exception handling, and alternative input methods function when voice unavailable.\nPerformance Qualification (PQ)\n: Real-world validation during actual production runs demonstrates that voice-enabled documentation produces complete, accurate batch records meeting GMP requirements. Statistical analysis confirms voice transcription accuracy exceeds 95% with error rates below manual paper documentation. User acceptance testing with representative operators across shifts confirms usability and identifies workflow refinements.\nThe validation documentation package (Validation Master Plan, protocols, test scripts, test evidence, summary reports, traceability matrices) demonstrates to regulatory inspectors that voice-enabled systems consistently produce GMP-compliant documentation. Revalidation follows any significant changes: software version upgrades, MES integration modifications, or substantive SOP revisions.\nCommercial and Medical Affairs\n‚Äã\nMedical science liaisons represent pharmaceutical companies' most valuable customer-facing asset, with average fully-loaded costs of $250,000-300,000 annually per representative. The strategic imperative involves maximizing high-value medical dialogue time with healthcare providers while minimizing low-value administrative burden. Voice AI transforms MSL effectiveness through three primary mechanisms: automated interaction documentation, intelligent pre-call preparation, and real-time support during engagements.\nIQVIA Field Force Agent exemplifies comprehensive voice-enabled MSL support, integrating ambient intelligence with CRM systems and medical databases. The pre-call workflow begins with voice query: \"Brief me on Dr. Johnson ahead of my 2pm meeting.\" The AI assistant retrieves the physician's profile from IQVIA's OneKey database (25M+ HCPs, 6M+ HCOs across 118 countries), recent interactions from Veeva CRM, published research, clinical trial involvement, formulary position at their institution, and generates a voice summary highlighting key talking points and potential areas of interest. This 2-3 minute briefing replaces 15-20 minutes of manual research, producing the documented 27% reduction in call preparation time.\nDuring the medical engagement, ambient listening captures the conversation without requiring the MSL to take notes or interrupt dialogue flow. Post-meeting, AI generates structured documentation including: medical topics discussed, product information provided, healthcare provider questions and MSL responses, scientific literature exchanged, commitments for follow-up, next interaction recommendations. The system automatically populates Veeva CRM with required fields (contact date, interaction type, products discussed, attendees), generates follow-up email drafts with referenced scientific publications, and creates reminders for promised information delivery. The MSL reviews and approves the automated documentation (human-in-the-loop compliance requirement) before finalizing, reducing post-call documentation from 45 minutes to 5-10 minutes.\nZS Field Force Effectiveness@Scale, for example, adds AI coaching capabilities analyzing MSL conversation patterns against best practices. The system identifies when MSLs effectively address objections, probe for unmet medical needs, or position scientific evidence persuasively. Aggregated across hundreds of interactions, pattern recognition algorithms surface communication techniques correlating with positive HCP engagement outcomes, enabling data-driven coaching: \"MSLs who ask for HCP perspectives on emerging biomarker data before presenting clinical trial results achieve 35% higher follow-up meeting rates.\" This transforms anecdotal coaching into precision skill development.\nFor sales representatives in Lilly's commercial organization, voice technology might address unique compliance challenges. The highly regulated promotion environment requires documented evidence that sales messaging remains within FDA-approved labeling and promotional review board approvals. Voice-enabled CRM systems (integrating tools like Maggie Voice Bot deployed by 500+ pharmaceutical field representatives in India) capture sales call details through natural conversation: \"Visited Dr. Patel at Memorial Hospital, discussed indication in relapsed refractory setting, left dosing guide and patient brochure.\" The system validates the interaction complies with approved promotional claims, automatically generates call notes, tracks sample distribution, and identifies potential off-label promotion for compliance intervention.\nMedical information services examples\nleverage conversational AI to scale inquiry handling while maintaining medical accuracy. Pfizer's multi-country deployment demonstrates the maturity of this approach:\nMedibot (US): Answers temperature stability questions for biologics, critical for pharmacies managing complex storage requirements\nFabi (Brazil): Handles 6,000+ non-technical customer questions monthly, reducing call center volume and providing 24/7 availability\nMaibo (Japan): Delivers medical information to healthcare professionals with local language fluency and cultural appropriateness\nThe architecture employs knowledge graphs connecting products to indications, dosing, adverse events, drug interactions, and supportive scientific evidence. When inquiries arrive via phone, web chat, or voice assistants, NLP extracts intent and entities, the knowledge graph retrieves relevant information verified by medical affairs, and responses undergo safety checks before delivery (screening for potential adverse events or product quality complaints requiring pharmacovigilance reporting). Critical inquiries exceeding the AI's confidence threshold escalate to human medical information specialists, with the AI providing draft response and relevant background information to accelerate human handling.\nThe integration with pharmacovigilance systems represents a critical safety capability. Conversational AI analyzing medical information contacts automatically identifies potential adverse events through pattern matching on symptom descriptions, temporal relationships to product use, and severity indicators. When detected, the system immediately generates a safety case intake form with verbatim patient/provider statements, prompts the medical information specialist for additional required information, and routes to pharmacovigilance for regulatory reporting. ProPharma reports this automation achieves 40% reduction in safety case processing time, directly supporting Lilly's patient safety obligations and regulatory compliance.\nRegulatory Compliance\n‚Äã\nThe regulatory landscape for pharmaceutical AI speech technologies crystallized in 2025 with FDA's draft guidance on AI in drug submissions and EMA's final reflection paper, creating clear pathways for implementation while maintaining stringent data protection and validation requirements.\nVoice data's classification as both Protected Health Information (PHI) under HIPAA and special category biometric data under GDPR Article 9 necessitates comprehensive privacy-by-design architecture with explicit consent mechanisms, data minimization, and sovereignty controls across Lilly's US, EU, and Asia-Pacific operations.\nFDA Framework and Validation Requirements\n‚Äã\nThe FDA's January 2025 draft guidance \"Considerations for the Use of Artificial Intelligence to Support Regulatory Decision-Making\" establishes risk-based credibility assessment criteria for AI systems in drug development.\nVoice AI applications fall into three FDA risk categories:\n(1) Low-risk administrative uses like transcription of non-critical meetings, requiring basic validation; (2) Moderate-risk clinical documentation and adverse event detection, necessitating comprehensive Computer System Validation per 21 CFR Part 11; (3) High-risk clinical trial endpoints using voice biomarkers, demanding full analytical and clinical validation through the Biomarker Qualification Program.\nThe FDA explicitly requires \"fit-for-purpose\" validation demonstrating the AI system reliably performs its intended function within specified contexts. For voice biomarkers as clinical endpoints, this involves establishing analytical validity (consistent measurement across devices, environments, populations), clinical validity (correlation with established clinical outcomes), and clinical utility (improved patient outcomes or trial efficiency). The agency emphasizes continuous monitoring post-deployment, with documented procedures for model drift detection, retraining triggers, and version control maintaining regulatory compliance as AI systems evolve.\nEMA Guidance and EU Implementation\n‚Äã\nEMA's September 2024 final \"Reflection Paper on the Use of Artificial Intelligence in the Medicinal Product Lifecycle\" aligns with FDA principles while adding European-specific requirements. The guidance mandates transparency in AI decision-making processes, with pharmaceutical companies required to document model architecture, training data characteristics, performance metrics, and known limitations. For voice AI processing EU citizen data, GDPR Article 22 grants individuals the right not to be subject to solely automated decision-making, requiring human review loops for any voice-based assessments affecting trial participation or treatment decisions.\nThe EU Medical Device Regulation (MDR) classification of certain voice biomarker software as Class IIa medical devices (active devices for diagnosis/monitoring) triggers CE marking requirements including technical documentation, clinical evaluation, and notified body assessment. This particularly impacts standalone voice assessment apps used in decentralized clinical trials, requiring Lilly to determine whether voice tools constitute investigational medical devices subject to MDR oversight beyond pharmaceutical regulatory frameworks.\nHIPAA and Data Protection Requirements\n‚Äã\nVoice recordings containing patient health discussions or collected during clinical care constitute Protected Health Information under HIPAA, requiring Business Associate Agreements (BAAs) with all technology vendors processing voice data. The minimum necessary standard applies: voice AI systems should access only the audio segments required for their specific function, with automatic redaction of incidental PHI captured in recordings. De-identification following Safe Harbor or Expert Determination methods enables broader research use, though voice characteristics themselves may enable re-identification, limiting true anonymization potential.\nHIPAA Security Rule safeguards for voice data mandate: (1) Access controls with unique user identification and automatic logoff from voice interfaces; (2) Encryption of voice data at rest (AES-256) and in transit (TLS 1.3+); (3) Audit logs capturing all voice data access, modification, and transmission; (4) Integrity controls ensuring voice recordings and transcriptions remain unaltered post-capture; (5) Transmission security for voice data crossing network boundaries, particularly relevant for cloud-based speech processing.\nGDPR Article 9 and Biometric Data Governance\n‚Äã\nVoice recordings constitute special category biometric data under GDPR Article 9 when used for unique identification or health assessment, triggering enhanced protection requirements. Processing requires explicit consent with clear disclosure of: specific purposes (research, clinical assessment, safety monitoring), data retention periods (typically 10 years for clinical trial data), third-party sharing (CROs, technology vendors, regulatory authorities), cross-border transfers (particularly US processing of EU voice data), and individual rights (access, rectification, erasure, portability).\nThe consent withdrawal right creates operational complexity: participants must be able to revoke consent for future voice processing while potentially maintaining already-collected data for scientific integrity. Lilly must implement granular consent management distinguishing primary trial purposes (where withdrawal may require study discontinuation) from secondary research uses (where selective withdrawal preserves trial participation). Data minimization principles favor on-device voice processing extracting only necessary features rather than transmitting full audio to centralized servers, reducing privacy risk and simplifying compliance.\nCross-Border Data Transfer Mechanisms\n‚Äã\nProcessing voice data across Lilly's global operations requires lawful transfer mechanisms addressing Schrems II decision invalidating Privacy Shield. Options include: (1) Standard Contractual Clauses with supplementary measures (encryption, pseudonymization, access controls) demonstrating US government surveillance laws cannot compromise EU voice data protection; (2) Binding Corporate Rules for intra-Lilly transfers, though these require extensive regulatory approval; (3) Explicit consent for transfers, viable for clinical trial participants but insufficient as sole basis for employee voice data; (4) Data localization with region-specific processing infrastructure, increasing cost but simplifying compliance.\nAsia-Pacific Regional Variations\n‚Äã\nChina's Personal Information Protection Law (PIPL) mirrors GDPR principles with stricter localization requirements: voice data from Chinese citizens must be stored within China with government security assessments for any cross-border transfer. Japan's Act on Protection of Personal Information (APPI) requires purpose limitation and consent for voice processing but provides GDPR adequacy, simplifying EU-Japan data flows. Singapore's Personal Data Protection Act permits broader commercial use with opt-out consent for non-sensitive applications, though health-related voice analysis likely requires explicit consent comparable to Western standards.\nWas this helpful?\nEdit this page\nPrevious\nAI Avatars / Digital People\nNext\nAI Video Generation\nExecutive Summary\nWhat is AI speech technology\nHow AI speech technology works\nSignal processing and acoustic feature extraction\nTransformer architectures for speech recognition\nNeural text-to-speech synthesis\nDirect speech-to-speech models deliver speed but sacrifice pharmaceutical safety requirements\nPipeline architectures achieve medical-grade accuracy through specialized components and modular optimization\nHybrid approaches combining strengths remain largely theoretical for pharmaceutical applications\nThe Accuracy-Latency Tradeoff\nRecent S2S Developments\nTraining methods and transfer learning\nVoice biomarker extraction and analysis\nIntegration with multimodal AI systems\nMarket analysis: vendors, costs, and value proposition\nCloud Platform Providers\nSpecialized pharmaceutical and healthcare vendors\nVoice biomarker companies for clinical endpoints\nHigh-level Value Indicators\nUse Case Examples\nResearch and Discovery\nClinical Development\nManufacturing and Quality\nCommercial and Medical Affairs\nRegulatory Compliance\nFDA Framework and Validation Requirements\nEMA Guidance and EU Implementation\nHIPAA and Data Protection Requirements\nGDPR Article 9 and Biometric Data Governance\nCross-Border Data Transfer Mechanisms\nAsia-Pacific Regional Variations\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 2,
    "depth": 2,
    "crawled_at": "2026-02-25T10:06:22.543933"
  },
  "https://techhq.dc.lilly.com/docs/innovate/emerging_tech/ai_video_generation": {
    "url": "https://techhq.dc.lilly.com/docs/innovate/emerging_tech/ai_video_generation",
    "title": "AI Video Generation | Tech HQ",
    "description": "- Last Update: 2025-06-06",
    "h1": [
      "AI Video Generation"
    ],
    "h2": [
      "Executive Summary‚Äã",
      "Introduction‚Äã",
      "AI Video Generation Technology Overview‚Äã",
      "Major Players and Platforms in AI Video Generation‚Äã",
      "Industry Impact and Use Cases‚Äã",
      "Strategies for Adoption‚Äã",
      "Challenges and Risks‚Äã",
      "Future Outlook‚Äã",
      "Sources‚Äã"
    ],
    "h3": [
      "OpenAI ‚Äì‚ÄØSora‚Äã",
      "Runway ‚Äì‚ÄØGen-2‚ÄØ(and beyond)‚Äã",
      "Google ‚Äì‚ÄØVeo 3‚ÄØand‚ÄØFlow‚Äã",
      "Flow‚Äã",
      "Kuaishou ‚Äì‚ÄØKling‚Äã",
      "Pika Labs ‚Äì‚ÄØPika Video‚Äã",
      "MiniMax ‚Äì‚ÄØHailuo Video-01‚Äã",
      "Luma Labs ‚Äì‚ÄØDream Machine (Ray 2)‚Äã",
      "Stability AI ‚Äì‚ÄØStable Video Diffusion‚Äã",
      "Adobe ‚Äì‚ÄØFirefly Generative Video‚Äã",
      "Marketing and Advertising‚Äã",
      "Learning & Development / Training‚Äã",
      "Commercial and Promotional‚Äã",
      "Quality and Consistency‚Äã",
      "Misrepresentation and Deepfakes‚Äã",
      "Regulatory Compliance‚Äã",
      "Intellectual Property and Training Data‚Äã",
      "Ethical Considerations‚Äã",
      "Technical Dependency and Evolution‚Äã",
      "Security and Privacy‚Äã",
      "Public Perception‚Äã",
      "Longer & Real-Time Generation‚Äã",
      "Improved Fidelity‚Äã",
      "Interactive Video & Multimodality‚Äã",
      "Industry-Specific Marketing Models/Tools‚Äã",
      "Regulatory Frameworks‚Äã",
      "Talent and Workflow Shifts‚Äã",
      "Competitive Landscape‚Äã",
      "New Content Formats‚Äã",
      "Conclusion‚Äã"
    ],
    "text_content": "AI Video Generation | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüí° Tech@Lilly Innovation Pipeline\nüõû Our Innovation Stages\nüèéÔ∏è Fast Start\nüì° Emerging Tech\nAI Avatars / Digital People\nAI Speech\nAI Video Generation\nüì° Emerging Tech\nAI Video Generation\nOn this page\nAI Video Generation\nEmerging Tech Report\nLast Update: 2025-06-06\nTech Innovation Pipeline\n, Enterprise Business Architecture, Tech@Lilly Enterprise\nAuthor: Doug Gorr\nExecutive Summary\n‚Äã\nArtificial Intelligence‚ÄØvideo generation‚ÄØis emerging as a transformative technology that enables the automatic creation\nof video content from text prompts, images, or other inputs. Recent breakthroughs in‚ÄØgenerative AI models‚ÄØ‚Äì especially\ndiffusion and transformer-based models ‚Äì have dramatically improved the realism and coherence of AI-generated video\nclips. Today‚Äôs leading AI video tools can produce short high-definition videos (typically a few seconds up to ~20\nseconds) with‚ÄØcinematic visuals, diverse styles, and even synchronized audio. Major AI providers\nlike‚ÄØOpenAI,‚ÄØGoogle,‚ÄØAdobe, and a wave of startups are rapidly advancing this field. For tech and business leaders, AI\nvideo generation promises‚ÄØsignificant opportunities: dramatically lowering the cost and time required to produce\nmarketing videos, training content, or promotional media; enabling mass personalization of video messages; and unlocking\ncreative concepts that would be impractical with traditional filming. Early adopters in marketing (including pharma\ncommercial teams) are already experimenting with AI-generated b-roll footage, product visualizations, and even\nAI-generated spokespeople. At the same time, this technology raises‚ÄØnew challenges and risks‚ÄØ‚Äì from output quality\nlimitations (e.g. surreal physics or inconsistent characters) to ethical concerns like deepfakes and brand safety. The\ncurrent state-of-the-art models still struggle with longer narrative coherence and some realism aspects, but progress is\nrapid. Competitors in the US and China are pushing the boundaries: for example, OpenAI‚Äôs‚ÄØSora‚ÄØmodel can generate up to\n20-second 1080p videos with rich detail, while Google‚Äôs newly unveiled‚ÄØVeo 3‚ÄØmodel goes further by‚ÄØintegrating\naudio‚ÄØ(music, sound effects, even speech) directly into generated videos. Well-funded startups like Pika Labs and\nindustry players like Adobe (with Firefly) are adding features such as‚ÄØstoryboarding tools,‚ÄØkeyframe control, and direct\nintegration into creative workflows. Lilly should‚ÄØpay close attention‚ÄØto this fast-evolving landscape ‚Äì the companies,\nmodels, and capabilities discussed in this report ‚Äì to craft strategies for adoption. In the near term, AI video\ngenerators can augment creative teams, accelerate content production, and enable novel marketing strategies, if\norganizations also institute proper oversight (to manage legal, ethical, and quality concerns). This report provides a\ndeep dive into the technology, key players (and their relative strengths/weaknesses), feature comparisons, industry use\ncases (with a focus on marketing, training, and pharma applications), and recommendations for‚ÄØstrategic adoption‚ÄØof AI\nvideo generation.\nIntroduction\n‚Äã\nThe ability for AI to‚ÄØgenerate video content‚ÄØon demand ‚Äì once a futuristic concept ‚Äì is now becoming reality. Over the\npast few years, generative AI has progressed from creating still images to producing short video clips. In 2022-2023,\nearly text-to-video research prototypes (e.g. Meta‚Äôs Make-A-Video) showed the potential but were limited to a few\nseconds of low-resolution, often jittery footage. Fast-forward to 2024-2025, and we see the emergence of‚ÄØcommercial AI\nvideo generation tools‚ÄØthat anyone can try. These systems take a prompt (text description and optionally reference\nimages or video clips) and synthesize a new video clip matching the description. Under the hood, most utilize‚ÄØdiffusion\nmodels‚ÄØor advanced transformers that have been trained on vast amounts of video data to learn how to generate\nconsecutive frames coherently. Essentially, they extend the techniques that made AI image generators (like DALL¬∑E and\nStable Diffusion) so successful, adding a temporal dimension to handle motion and physics.\nThis technology arrives at an apt time: organizations are hungry for more video content than ever (for social media,\ne-learning, personalized marketing), but traditional video production is time-consuming and costly. AI video generation\npromises to‚ÄØdemocratize video creation, making it almost as easy as writing a script. A marketer could simply ‚Äútell‚Äù an\nAI,‚ÄØ‚ÄúShow a medicine droplet traveling through a bloodstream and destroying viruses, in a cinematic style,‚Äù‚ÄØand get a\ncustom animated clip within minutes ‚Äì something that would have taken a VFX studio weeks to produce. Likewise, a\ntraining team could generate scenario videos with different backgrounds or actors at the click of a button. The\nimplications span many industries: media and entertainment (AI-assisted filmmaking and pre-visualization), advertising\n(rapid content iteration and localization), education (illustrative videos), and more. In pharma marketing specifically,\nwe envision uses like‚ÄØmechanism-of-action animations, doctor-patient roleplay scenarios for training, or personalized\npatient education videos generated for different demographics and languages.\nHowever, it‚Äôs important to approach this innovation with open eyes.‚ÄØCurrent limitations‚ÄØmean AI-generated videos are not\nyet completely indistinguishable from real footage in most cases. Videos are typically short (a few seconds long) and\nmay exhibit artifacts or surreal glitches, especially for complex motions or human anatomy. Ensuring a consistent\ncharacter or narrative across a longer video remains challenging (some models allow chaining multiple short clips, but\ntrue long-form consistency is still an R&D topic). There are also‚ÄØethical considerations: generative video opens the\ndoor to hyper-realistic deepfakes or misleading content, which is a particular concern in regulated industries like\npharmaceuticals. The source of content leveraged, the machine learning processes used to create these capabilities,\nremains an open legal concern. As with any new technology, Lilly must weigh the potential benefits and risks together\nwhen determining a strategy for use and adoption. As we delve into the technology, differences between tools, and\nstrategies for adoption, we will highlight both the‚ÄØopportunities‚ÄØand‚ÄØrisks.\nAI Video Generation Technology Overview\n‚Äã\nAt its core, AI video generation extends image generation with the dimension of time. Early approaches\ninvolved‚ÄØGenerative Adversarial Networks (GANs)‚ÄØand‚ÄØrecurrent models, but today‚Äôs state-of-the-art largely\nuses‚ÄØdiffusion models with transformers‚ÄØtrained on video data. A diffusion model gradually ‚Äúdenoises‚Äù random noise into\na coherent image; for video, this process is applied to a sequence of video frames (often in a latent compressed space\nto reduce computation). For example, OpenAI‚Äôs‚ÄØSora‚ÄØmodel uses a‚ÄØlatent video compressor‚ÄØand represents video as a series\nof ‚Äúspacetime patches‚Äù that a transformer can process. The model is conditioned on a text prompt (and optionally\nstarting frames or images) and learns to predict the next frames of video that match the prompt. With enough training\n(using millions of videos and images), such a model can generate remarkably diverse and complex scenes.\nKey architectural innovations have improved video generation quality recently. One is the use of‚ÄØunified training on\nimages and videos‚ÄØ‚Äì since images are essentially 1-frame videos, including large image datasets helps the model learn\nhigher resolution details. Sora, for instance, is a ‚Äúgeneralist‚Äù trained on both images and videos of variable lengths,\nwhich enables it to output higher fidelity and longer durations (up to ~1 minute internally). Another advance is the\nincorporation of‚ÄØspatial-temporal attention‚ÄØmechanisms (as seen in Google‚Äôs and others‚Äô models) that help preserve\nconsistency over time. Google DeepMind‚Äôs latest‚ÄØVeo 3‚ÄØmodel reportedly uses‚ÄØ3D spatiotemporal attention‚ÄØand a\nspecialized‚ÄØ3D VAE‚ÄØ(Variational Autoencoder) to achieve cinema-quality results. In practice, many systems break the task\ninto two stages: first generate a lower-resolution or lower-frame-rate video with AI, then‚ÄØupscale or interpolate‚ÄØit to\nHD and smooth frame rates (some platforms include built-in AI upscalers and frame interpolation to reach 1080p30 or\nbeyond).\nDespite these advances, current models have constraints. Most‚ÄØclip lengths‚ÄØare short ‚Äì e.g. 4‚Äì5 seconds for many public\ntools, sometimes extendable to ~20 seconds with higher tiers or chaining clips. Maintaining‚ÄØobject permanence and\ncharacter consistency‚ÄØacross even those seconds is non-trivial (e.g. a person‚Äôs face might subtly change between frames\nif not controlled). Each frame must not only look plausible on its own but also flow logically from the previous,\nrespecting physics and causality. This is an area where models still stumble: early-gen systems produced‚ÄØwarping limbs\nand melting objects‚ÄØunder motion. Newer models are improving ‚Äì for example, Minimax‚Äôs video model was noted to handle\nmotion physics (inertia, shadows, momentum) far more naturally than earlier models. Still, complex sequences (e.g. a\nhuman performing a sports move) can appear unnatural or ‚Äúrobotic‚Äù if the AI‚Äôs understanding of physics isn‚Äôt perfect.\nResolution‚ÄØand‚ÄØframe rate‚ÄØare other technical factors. Producing full 1080p or 4K frames is computationally heavy, so\nmany models trade off length or fidelity. Runway‚Äôs Gen-2 model, for instance, initially generated only ~480p-equivalent,\nlow-framerate videos (appearing ‚Äúslideshow-like‚Äù) to conserve compute. Today there are models that directly output HD ‚Äì\nOpenAI‚Äôs Sora Turbo can do 1080p‚ÄØand Adobe‚Äôs Firefly Video generates 1080p by default‚ÄØ‚Äì but usually for very short clips\n(~5‚Äì10 seconds). Some platforms provide‚ÄØflexible aspect ratios and orientations‚ÄØ(portrait, square, etc.), recognizing\nneeds for social media content.\nImportantly, AI video generators are increasingly handling‚ÄØmultimodal inputs: not just plain text prompts, but\nalso‚ÄØimage inputs (for image-to-video)‚ÄØand‚ÄØinitial video clips (video-to-video). Image-to-video means you can give a\nsingle frame or picture and have the model animate it or use it as the scene start. Many tools (Runway, Pika, Sora,\nLuma, Adobe, etc.) support this because it helps with consistency and user control ‚Äì e.g. you can provide a character\nimage to ensure the generated video revolves around that character‚Äôs appearance. Video-to-video allows taking an\nexisting video and transforming it (changing style or continuing it beyond original length). For example, Runway Gen-1\nwould take an input video and apply an AI-generated‚ÄØ‚Äústyle transfer‚Äù‚ÄØto turn real footage into an animated look. Several\nplatforms now let you‚ÄØextend videos‚ÄØby generating new frames after the last frame (both Sora and Google‚Äôs Flow have a\nfeature to‚ÄØ‚Äúextend‚Äù‚ÄØor‚ÄØ‚Äúcontinue‚Äù‚ÄØa clip seamlessly). These workflow features open the door to‚ÄØstoryboard-driven\ngeneration: creators can stitch multiple AI clips into a longer sequence, maintaining some continuity by carrying over\nlast frames as the next clip‚Äôs start.\nAnother frontier is‚ÄØaudio integration. Until recently, AI video tools have only produced silent clips ‚Äì adding sound was\na separate task. Google‚Äôs Veo 3 made headlines by generating synchronized audio along with the video. This includes\nsound effects and even rudimentary voices or music that match the scene, greatly enhancing realism and emotional impact.\nWhile not yet common across all platforms, this indicates a trend toward full multimedia generation (Google achieved\nthis by drawing on its text-to-audio research and perhaps its new Gemini multimodal AI to understand the scene context).\nAdobe‚Äôs approach, meanwhile, integrates an‚ÄØAI voice model for dubbing/translation‚ÄØrather than generating sound effects\nfor imaginary scenes‚Äì an example of focusing on practical editing needs like translating a video‚Äôs narration into\nmultiple languages with matching lip sync. We anticipate more convergence of‚ÄØAI video + audio, enabling one-shot\ncreation of a complete video with soundtrack.\nFinally,‚ÄØcontent moderation and safety‚ÄØare built into the technology stack of responsible providers. Providers like\nOpenAI and Google constrain their models to avoid disallowed content (e.g. extreme violence, pornographic or hate\ncontent), and they add‚ÄØwatermarks or metadata‚ÄØto identify AI-generated videos. OpenAI‚Äôs Sora, for example, attaches C2PA\nmetadata tags to all generated videos for transparency. Adobe trains Firefly on licensed, uncontroversial content to\nmake it ‚Äúcommercially safe‚Äù and avoid IP violations. These safeguards are critical given how easily video can mislead;\nthey are also important for adoption. For example, Lilly would have to ensure any AI-generated visuals do not\ninadvertently create off-label claims or inappropriate imagery. Additional layers of content moderation/control/filters\nwill remain a Lilly-specific need.\nIn summary, the‚ÄØkey capabilities‚ÄØof AI video generation to be aware of include multiple dimensions to be evaluated as we\napproach new models and tools, including but not limited to:\nquality of visual output (resolution, frame rate, fidelity)\nability to portray‚ÄØrealistic physics and lighting\noptions for‚ÄØcamera movement/angles\ndegree of‚ÄØcontrol‚ÄØvia prompts and references (storyboards, keyframes, etc.)\nsupport for‚ÄØaudio‚ÄØoutput and moderation‚ÄØfeatures\nMajor Players and Platforms in AI Video Generation\n‚Äã\nThe AI video generation ecosystem in 2025 spans Big Tech companies, well-funded startups, and open-source projects.\nHaving surveyed these major players ‚Äì OpenAI, Runway, Google, Kuaishou (Kling), Pika, MiniMax, Luma, Stability, Adobe ‚Äì\nit‚Äôs evident that the field is crowded with innovation.\nEach brings something slightly different: OpenAI and Google push the frontier of capability, Adobe and Luma focus on\ncreator workflow integration, startups like Pika and MiniMax carve out specialized strengths or faster iteration, and\nopen projects like Stability ensure there‚Äôs room for custom solutions.\nBelow we break down the notable models/tools and their current state-of-the-art capabilities, as well as how they\ndiffer:\nOpenAI ‚Äì‚ÄØSora\n‚Äã\nOpenAI‚Äôs‚ÄØSora‚ÄØis a cutting-edge‚ÄØtext-to-video‚ÄØmodel introduced in early 2024 and moved out of research preview by the\nend of last year. It is designed as a general-purpose video generator that can take text (and optionally image/video\ninputs) and produce new videos. Sora operates as a consumer product via a web interface (ChaGPT.com and Sora.com) and\nthrough API integration with OpenAI and the Microsoft Azure platform. The latter of which, seems to be the most likely\navenue for Lilly‚Äôs adoption.\nSora was initially capable of up to‚ÄØ1 minute‚ÄØof video in research settings, but for end-users the service currently\nallows generating clips up to‚ÄØ20 seconds‚ÄØlong at up to‚ÄØ1080p‚ÄØresolution. Users can choose aspect ratios (widescreen,\nsquare, vertical) and even‚ÄØbring their own assets‚ÄØ‚Äì e.g. provide an initial or final frame, or a short video to\nbe‚ÄØextended or blended‚ÄØinto new content. OpenAI has emphasized‚ÄØSora‚Äôs‚ÄØprompt adherence and visual quality, aiming to\nclosely match the input description while keeping outputs photorealistic or stylistically high-quality. The model\nleverages a‚ÄØdiffusion transformer‚ÄØarchitecture and was trained on an extremely large dataset of image and video content,\nmaking it a ‚Äúgeneralist‚Äù that can handle everything from cartoon-like clips to natural scenery.\nOne of Sora‚Äôs distinguishing features is a built-in‚ÄØstoryboard tool‚ÄØin its interface. This allows users to script out a\nsequence of scenes or specify key frames, giving more control over each moment in a longer narrative. For example, a\nuser could specify that at second 0 a character is in Location A, and by second 5 the character moves to Location B, and\nSora will try to fill in the transition. Such tools help mitigate the ‚Äúconsistency‚Äù problem by explicitly guiding the\nmodel. Sora also supports‚ÄØimage-to-video‚ÄØ(you can upload an image to influence the first frame or style)\nand‚ÄØvideo-to-video‚ÄØ(upload a short clip to have Sora modify or continue it) generation, making it flexible in creative\nworkflows.\nStrengths:\nSora is backed by OpenAI‚Äôs leading research, so it benefits from state-of-the-art training and\nfine-tuning. It produces impressively‚ÄØdetailed and vivid visuals‚ÄØacross a range of subjects ‚Äì reviewers have showcased\nexample prompts from cinematic city scenes to fantasy creatures, rendered with convincing lighting and depth of field.\nIt can maintain coherence for longer durations than many competitors (20s or more), and prompt alignment is generally\nstrong. The integration with ChatGPT means a large user base can experiment easily, and OpenAI has put in place\nrobust‚ÄØcontent moderation‚ÄØand‚ÄØorigin tagging‚ÄØ(every Sora video is cryptographically tagged as AI-generated). OpenAI‚Äôs\nbrand has also lead the GenAI field and continues to innovation across a variety of AI models and advanced capabilities.\nAdditionally, OpenAI‚Äôs partnership with Microsoft makes it the most viable option for large enterprises, like Lilly.\nWeaknesses:\nDespite its prowess, Sora (like others) still has limitations. OpenAI itself notes that the model can\nstruggle with‚ÄØrealistic physics and long complex actions‚ÄØ‚Äì e.g. if a prompt involves a very intricate movement sequence,\nSora often introduces artifacts or odd motion. Early testers observed that human figures from Sora could have a stiff or\nunnatural motion compared to real video. Also, because of the computational cost, generating 20 seconds of HD video\nis‚ÄØresource intensive.‚ÄØThe‚ÄØcost structure‚ÄØfor heavy use is thus non-trivial; Lilly may want to negotiate custom pricing.\nAnother consideration is that Sora is a‚ÄØclosed model‚ÄØ‚Äì unlike some open-source alternatives, you cannot self-host or\nretrain it on proprietary data, so there is reliance on OpenAI‚Äôs innovation focus and reseller partner, Microsoft.\nOverall, Sora represents the‚ÄØhigh-quality, generalist approach‚ÄØto AI video: it aims to handle as many scenarios as\npossible with good quality and provide a safe, managed platform for users. It‚Äôs a bellwether for where mainstream AI\nvideo tech is, demonstrating realistic scene generation and moderate length, with expectation of rapid improvement as\nmodels scale further.\nRunway ‚Äì‚ÄØGen-2‚ÄØ(and beyond)\n‚Äã\nRunway ML is a startup well-known among creatives for providing accessible AI tools, and they were one of the first to\noffer‚ÄØtext-to-video‚ÄØgeneration to the public. Runway‚Äôs‚ÄØGen-2‚ÄØmodel (launched mid-2023) garnered attention as a\ncommercially available text-to-video system available via a simple web interface. It followed their earlier‚ÄØGen-1, which\nwas limited to video stylization (applying AI-generated styles to existing videos). Gen-2 marked the ability to generate\ncompletely new video frames from a text prompt or an image prompt. Users could sign up on Runway‚Äôs website and generate\nshort clips (initially ~4 seconds long) for free or with a subscription. This early availability made Runway Gen-2 a\npopular choice for designers and artists to experiment with AI video in 2023.\nGen-2 outputs a few seconds of video at‚ÄØlow resolution‚ÄØ(around 720√ó1280 or less) and without sound. It supports‚ÄØtext\nprompts‚ÄØas well as an‚ÄØimage + text combo‚ÄØ‚Äì for example, you can upload a reference photo and ask it to animate that\nscene with some described action. The generation process is cloud-based and quick (often under a minute to get results).\nRunway‚Äôs interface emphasizes ease of use: you enter a prompt, hit generate, and get a preview you can download as an\nMP4. Gen-2 can produce some‚ÄØartistic and creative results, but it has notable limitations. Reviewers found that Gen-2‚Äôs\nvideos were often‚ÄØchoppy in frame rate‚ÄØ(almost like a fast slideshow) and‚ÄØgrainy‚ÄØin appearance. This appears to have\nbeen a trade-off to make the service widely accessible (lowering GPU requirements by limiting quality). Gen-2 also\nstruggled with‚ÄØcomplex prompts‚ÄØ‚Äì it might latch onto one word and ignore nuance, and it failed at precise instructions\nlike ‚Äúa slow zoom-in‚Äù (the output didn‚Äôt execute the camera move). Consistency issues were present: objects could warp,\nand human figures often had surreal artifacts (blended limbs, etc.).\nRunway has been continuously improving its models. By late 2023, they were experimenting with features to increase\ncoherence (some community members referred to a Gen-3 or new features like ‚Äúlast frame continuity‚Äù ‚Äì allowing one\ngeneration‚Äôs last frame to carry into the next, somewhat like what others call extend or storyboard). It‚Äôs reported\nthat‚ÄØRunway Gen-4‚ÄØis on the horizon, aiming for higher quality and longer duration, but as of early 2025 the‚ÄØGen-2‚ÄØmodel\n(with incremental upgrades) is what‚Äôs publicly available. Runway‚Äôs comparative advantage is being‚ÄØcreator-focused: their\nplatform is an all-in-one creative suite, with tools for video editing, composting, and AI effects. For instance, Runway\noffers‚ÄØgreen screen background removal‚ÄØand‚ÄØimage generation‚ÄØin the same app, so a user can mix AI video with other\nediting easily. The‚ÄØprice‚ÄØof Runway is subscription-based, with a free tier that gives some generation credits and paid\ntiers for more usage.\nRunway shows the‚ÄØaccessibility‚ÄØside of AI video. It may not have the absolute best fidelity, but it pioneered getting\nthe tech into users‚Äô hands. This has allowed a community of artists to develop around it (for example, the first AI\nmusic videos were often made with Runway Gen-2, accepting the rough edges as an aesthetic). Runway‚Äôs trajectory suggests\nthey will continue to close the quality gap. Notably, Runway collaborated in the development of Stable Diffusion (image\nmodel) previously, indicating strong AI research chops. We anticipate Runway‚Äôs future models will improve frame\nsmoothness and possibly length (their roadmap mentions working towards 15+ second clips). Businesses might use Runway\ncurrently for‚ÄØquick prototypes or social media content‚ÄØwhere a slightly stylized or surreal look is acceptable. However,\nfor polished, photoreal marketing videos, other solutions (or further model maturity) would be needed.\nGoogle ‚Äì‚ÄØVeo 3‚ÄØand‚ÄØFlow\n‚Äã\nGoogle entered the AI video arena through its DeepMind team, and in May 2025 it announced‚ÄØVeo 3, a next-generation video\nmodel, along with a tool called‚ÄØFlow‚ÄØfor creators. Veo 3 represents one of the most advanced text-to-video models to\ndate, notable for delivering‚ÄØhigh realism and integrated audio. In tests, Veo 3 generated videos of‚ÄØrealistic human\nactors with synchronized sound and music, a leap in overall cinematic quality. For example, an Ars Technica report\ndescribes Veo 3 producing a clip of a person that was startlingly close to real footage, complete with background music\nthat fit the mood. This puts Veo 3 at the cutting edge, potentially ahead of OpenAI‚Äôs and others‚Äô models in certain\naspects as of mid-2025.\nVeo 3 can generate HD video content with accompanying audio in response to a prompt. It excels at‚ÄØtext rendering within\nvideo‚ÄØ(e.g. generating legible signage or captions in the scene ‚Äì something many image AIs fail at); testers found Veo 3\nmore consistently renders written text correctly in frames compared to others. The audio component means if your prompt\nimplies a sound (‚Äúa dog barking in a park‚Äù), the output video might have a barking sound. This dramatically increases\nthe emotional impact and usefulness of the raw generations. Veo 3‚Äôs visual fidelity is extremely high:‚ÄØnatural motion,\nbetter handling of complex scenes (e.g. multiple characters or dynamic camera movements) and fewer obvious glitches have\nbeen reported. It uses heavy-duty AI (likely an evolution of Google‚Äôs‚ÄØImagen Video‚ÄØresearch combined with their Gemini\nmultimodal model) and thus is computationally demanding ‚Äì one test noted ~5 minutes or more to generate a clip with\nGoogle‚Äôs servers.\nFlow\n‚Äã\nTo make Veo 3 accessible, Google launched‚ÄØFlow, an AI filmmaking web app. Flow is essentially the user interface and\nworkflow around Veo (and other models like Google‚Äôs Imagen image model). It‚Äôs available to Google‚Äôs AI Cloud subscribers\n(Pro and Ultra tiers) as a beta tool. Flow is built ‚Äúwith and for creatives‚Äù and introduces robust storytelling\nfeatures: you can generate individual‚ÄØshots‚ÄØusing‚ÄØText to Video,‚ÄØFrames to Video, or‚ÄØIngredients to Video.‚ÄØFrames to\nVideo‚ÄØlets you specify a‚ÄØstart and end frame‚ÄØ(essentially keyframes) and the AI will animate the transition.‚ÄØIngredients\nto Video‚ÄØmeans you can feed the AI specific elements ‚Äì e.g. an image of a character or an art style ‚Äì and it will ensure\nthe output video includes those elements. This modular approach helps keep characters consistent and styles unified\nacross scenes. Flow also has a‚ÄØScene Builder: once you have a clip you like, you can add it to Scene Builder and then\neither ‚ÄúExtend‚Äù the scene (Flow will use the last second of frames to continue the action into a new clip) or ‚ÄúJump to‚Äù\na new scene (preserving context like the main character, but allowing a change of setting or action). This approach\nleverages Google‚Äôs powerful Gemini AI behind the scenes to maintain coherence when transitioning shots. In effect, Flow\nallows the creation of a multi-shot short film completely within the AI tool, something quite revolutionary. When\nfinished, users can upscale their clips to 1080p and download them or even get GIF previews.\nStrengths:\nGoogle‚Äôs solution is arguably the most‚ÄØcomprehensive‚ÄØand advanced: highest quality visuals with‚ÄØaudio,\nand a pro-oriented workflow that tackles many prior limitations (short length, lack of consistency, etc.). The\nintegrated audio is a standout ‚Äì in comparisons, testers found scenes from Veo 3 much more engaging due to background\nsounds and music, even when Kling (a competitor) had slightly better prompt accuracy in some cases. Veo 3 also shows\nstrength in‚ÄØcinematography: it tends to choose cinematic camera angles and color grading that enhance the prompt‚Äôs mood.\nThis may be due to training on film footage or specific tuning. Google‚Äôs cloud infrastructure means it can roll this out\nat scale when ready (currently it‚Äôs limited to certain users but expected to expand). Google will likely integrate these\ncapabilities into Google Cloud offerings (GCP), making it easier to use via API or within Google‚Äôs productivity tools in\nthe future.\nWeaknesses:\nThe main drawbacks are‚ÄØaccessibility and cost. As of now, Flow (with Veo 3) is not open to everyone ‚Äì\nit‚Äôs an exclusive service for paid subscribers, likely with limited invites. The compute intensity also means it‚Äôs slow\n(minutes per generation) and presumably expensive; Google‚Äôs tiered credits system in Flow indicates heavy usage could\nrack up costs (Flow has ‚ÄúFast‚Äù and ‚ÄúHighest Quality‚Äù modes, with Highest using Veo 3 and consuming more credits). In\ntesting, system congestion led to errors for Veo 3 generation, implying the service can be bottlenecked. Another\nconsideration is that while Veo 3 pushes realism, it‚Äôs not infallible ‚Äì some prompts found its‚ÄØprompt adherence‚ÄØweaker\nthan a rival in specific aspects. For example, in a test prompt of ‚Äúa woman runs away from a giant spider,‚Äù Kling‚Äôs\nmodel actually showed the woman running away from the spider (as instructed) while Veo 3‚Äôs output had the elements but\nnot the exact action. So creative control might still need multiple attempts or careful prompt tuning. Also, if using\nFlow‚Äôs image-to-video via ‚ÄúIngredients‚Äù mode, note that those videos apparently come‚ÄØwithout audio‚ÄØ(so the audio\ngeneration only occurs in pure text-to-video mode, at least currently).\nGoogle‚Äôs entry with Veo 3 is a sign that‚ÄØAI video is reaching new heights. Lilly should watch this space because what is\na limited release today could be a widely available tool in a year. If Google integrates Flow into its ecosystem it\ncould accelerate adoption dramatically. In terms of strategy, Google‚Äôs heavy emphasis on‚ÄØstorytelling workflow‚ÄØ(e.g. the\nScene Builder and keyframes) suggests that even as AI gets more powerful,‚ÄØhuman creative direction remains crucial‚ÄØ‚Äì the\nwinners will be those who combine AI capabilities with clear narrative guidance.\nKuaishou ‚Äì‚ÄØKling\n‚Äã\nKling is an AI video generator developed by‚ÄØKuaishou, a major Chinese social media and short-video company (often seen\nas a rival to ByteDance). Kuaishou launched Kling in 2024 and has rapidly iterated on it; by mid-2025 they\nreleased‚ÄØKling 2.1, explicitly targeting the leadership in AI video generation. Kling has gained a reputation for\nproducing‚ÄØcinema-grade short videos‚ÄØand is widely used in China for creating meme videos, animated satires, and more. In\nfact, Kling became popular among meme-makers to animate political satire videos of public figures (e.g. face-swapping\nfamous people into movie scenes). This hints at Kling‚Äôs strengths in generating reasonably‚ÄØrealistic human\ncharacters‚ÄØand its permissiveness (though presumably with some safeguards).\nKling stands out for allowing relatively‚ÄØlong outputs ‚Äì up to 2 minutes‚ÄØof video in its latest version, far surpassing\nmost Western counterparts currently. It also supports‚ÄØhigh resolution (1080p)‚ÄØgeneration and smooth frame rates (25+\nFPS). To handle different use cases, Kling 2.1 is offered in multiple quality tiers. According to a review, there\nare‚ÄØStandard,‚ÄØProfessional, and‚ÄØMaster‚ÄØmodes: Standard produces 720p ~5-second clips, Pro yields 1080p with good\nquality, and Master (the most advanced) also at 1080p but with maximum fidelity and consistency. The Master tier uses\nthe latest model with those 3D spatial-temporal techniques we mentioned, and it is slower and costlier (100 credits for\n5s, vs 35 credits for Pro). Notably,‚ÄØprompt adherence‚ÄØin Kling 2.1 is reported to be excellent ‚Äì it captures complex\ninstructions well and keeps on-script better than many models. For instance, if asked to show specific text on a robot‚Äôs\nbody and a certain action, Kling usually gets the details right (especially in Master mode). It does have some\ndifficulty if the text or element is not the main focus (it might drop minor details when focusing on bigger things),\nbut overall its‚ÄØaccuracy to the prompt is a selling point.\nKling offers a rich set of features akin to others:‚ÄØtext-to-video, image-to-video, video extension, camera control, even\nface and lip-sync capabilities. The platform lists tools like‚ÄØMotion Brush‚ÄØ(perhaps to guide motion in a specific\nregion),‚ÄØStart/End Frame‚ÄØinput (like keyframes),‚ÄØVirtual Try-On‚ÄØ(indicating a specialization in changing outfits on\npeople in video), and‚ÄØLip Sync‚ÄØwhich suggests you can input audio dialogue and have a generated character speak it. This\naligns with Kuaishou‚Äôs needs for user-generated content (imagine an app where users can type a script and have an AI\navatar video speak it). Kling also provides an‚ÄØAPI, meaning developers can integrate it into apps or pipelines. The\nvolume of content generated is huge ‚Äì by one report, Kling had already output over 10 million videos within a short time\nof launch, reflecting its popularity on a large social platform.\nStrengths:\nKling‚Äôs‚ÄØnatural motion and authenticity‚ÄØhave been highlighted. A Decrypt comparison noted Kling 2.1\nproduces footage that looks genuinely cinematic, with characters moving naturally and complex action sequences playing\nout without obvious AI artifacts. Emotions on faces feel more authentic than prior models in some tests. Also,\nKling‚Äôs‚ÄØfast pace of improvement‚ÄØis notable: within one year, Kuaishou jumped from version 1.0 to 2.1, each time\ntightening the gap with the cutting edge. The multi-tier offering is smart for business use ‚Äì one can use cheaper modes\nfor drafts and then upscale final output with Master mode. For an enterprise, Kling could be cost-effective especially\nfor image-to-video tasks; in fact, tests found Kling 2.1‚ÄØexcels at image-to-video conversion‚ÄØ(turning a static image\ninto a moving scene) relative to peers. Its‚ÄØcamera movements and VFX‚ÄØare also advanced; for example, it can handle\ndynamic shots (one test had it do a time-lapse city transformation with camera movement ‚Äì challenging, but it partially\nsucceeded). The presence of specialized effects (like presumably the‚ÄØElements‚ÄØor‚ÄØVFX presets‚ÄØon their site) means it\nmight offer fun creative filters natively.\nWeaknesses:\nOne limitation is‚ÄØaccessibility for non-Chinese users. Kling‚Äôs official interface is primarily in\nChinese (though Pollo AI, a third-party site, provides an English gateway to try Kling models). Payment and support\nmight be geared towards Chinese market, so international enterprise adoption could be tricky at the moment. Another\nfactor: while Kling leads in some metrics, Google‚Äôs Veo 3 has the edge in integrated audio and possibly in overall\n‚Äúatmosphere‚Äù (color grading, etc.). In a direct face-off, each had scenarios where it did better: Kling was better at\nstrictly following the action described, whereas Veo‚Äôs clip with audio delivered a more cinematic feel even if it\ndeviated slightly. Also, Kling‚Äôs Master mode, though high-quality, is‚ÄØcompute-heavy‚ÄØ‚Äì some users might find the wait\ntimes (and costs) for the highest quality prohibitive for very iterative work. Lastly, as an AI that can do\ndeepfake-like outputs (given meme usage and lip-sync), one must ensure ethical use. It‚Äôs likely Kuaishou has moderation\non their platform, but if one is using the API, compliance with any deepfake laws (like requiring disclaimers for\nAI-generated realistic people) is necessary.\nIn summary, Kling is a powerhouse in AI video and a direct competitor to Sora and Veo. For our global company, Kling\ndemonstrates that‚ÄØthe Chinese market has an opportunity to leverage models not accessible to all geographies. Minimally,\nit‚Äôs a model to benchmark against. Lilly should keep an eye on whether Kuaishou or partners make Kling available\nglobally, and on the kinds of content it enables (e.g. it‚Äôs very adept at producing‚ÄØshort-form, eye-catching clips‚ÄØwhich\nis exactly the content driving social media engagement).\nPika Labs ‚Äì‚ÄØPika Video\n‚Äã\nPika Labs‚ÄØis a startup that has quickly risen in the AI video field. Co-founded by two PhD students in 2023, Pika Labs\nmade waves by mid-2024 when it secured a‚ÄØUS$135 million funding round, valuing the company at $470M.‚ÄØThis substantial\nbacking (notably involving investors from both the US and Asia) signals that Pika is viewed as a serious contender.\nPika‚Äôs AI video generator ‚Äì often just called‚ÄØPika AI‚ÄØ‚Äì is known for being‚ÄØuser-friendly‚ÄØand creatively versatile. It‚Äôs\noffered via a web interface and API, and has even been integrated into third-party creative apps (e.g. the‚ÄØCaptions‚ÄØapp\nintegrated Pika to allow users to generate b-roll clips from text inside a video editing workflow).\nPika supports both‚ÄØtext-to-video and image-to-video, similar to others. It can generate approximately‚ÄØ5-second clips by\ndefault, and the team has been extending that (the platform notes up to ~2 minutes by chaining or iterating, likely in\ntheir newer version 1.5+). The resolution it works with is around‚ÄØ720p (1296√ó720)‚ÄØin current versions for generation.\nPika‚Äôs style flexibility is a strong suit ‚Äì it can do‚ÄØ‚Äúvarious styles, such as cinematic, animated, or cartoonish‚Äù‚ÄØjust\nby adjusting the prompt or selecting presets. They have been updating the model rapidly (the Pollo AI interface shows\nPika v2.2 as latest, implying several version jumps). One unique feature Pika introduced is‚ÄØ‚ÄúPika Effects‚Äù\n(dubbed‚ÄØPikaffects). These are essentially AI-driven‚ÄØvideo effects that manipulate objects‚ÄØin the generated scene. For\nexample, users can apply effects like‚ÄØInflate, Melt, Explode, Squash, Crush,‚ÄØor even‚ÄØ‚ÄúCake-ify‚Äù‚ÄØto objects. This\nsuggests if you generate a video of, say, a statue, you could then have it melt or explode in a physically plausible way\nvia the AI ‚Äì an exciting tool for dynamic visuals and likely very popular for fun content creation. This focus\non‚ÄØobject-level control‚ÄØis somewhat unique to Pika and caters to short-form video creativity (imagine TikTok-style\nvisual gags, etc.).\nPika‚Äôs interface is noted to be‚ÄØintuitive for newcomers. They likely have templates or guided workflows (enter your\nprompt, choose a style, etc.). Also, Pika Labs has emphasized‚ÄØcontinuity‚ÄØfeatures: much like others, you can use the\nlast frame of one video as the first of the next to string together longer scenes with a persistent character. A\ncommunity of users have tested Pika‚Äôs capability on things like B-roll (it does well in generating generic footage like\ntech product pans), cartoon animation (smooth results), and abstract visuals.\nStrengths:\nPika appears to combine‚ÄØresearch-grade tech with a product mindset.‚ÄØRaising $135M so early means they\nhave resources to push the model‚Äôs quality and scale quickly. Already by June 2024, Pika‚Äôs results were impressing many\n‚Äì it produces‚ÄØsmooth camera movements and lighting‚ÄØeffects, and in some comparisons was among the top for certain tests.\nFor instance, a Medium reviewer noted Minimax and Pika as two standout platforms that ‚Äúactually work‚Äù for creators\nneeding reliable output. Pika‚Äôs multi-style capability means marketers or creators can use it for a range of content: a\nrealistic stock-footage-like clip for one project, a whimsical cartoon for another. The‚ÄØPikaffects‚ÄØgive an extra\ndimension of creativity, allowing users to generate not just static scenes but scenes with an‚ÄØeffect or\ntransformation‚ÄØ(great for grabbing viewer attention). Pika also supports‚ÄØAPI integration, which means businesses can\nbuild it into their own tools or pipelines (for example, an e-learning platform could call Pika‚Äôs API to generate custom\nvideo illustrations on the fly for course content). Finally, Pika‚Äôs background ‚Äì co-founders with strong academic\ncredentials (one profile mentions a co-founder, Wenjing Guo, is a Harvard CS grad lauded as a ‚Äúgenius girl‚Äù in China) ‚Äì\nsuggests a talent advantage and possibly cross-collaboration between US and Chinese AI communities.\nWeaknesses:\nWhile Pika is promising, it‚Äôs still an‚ÄØupstart‚ÄØin a field with emerging giants. Its model may not yet\nmatch the absolute fidelity of OpenAI or Google‚Äôs latest ‚Äì for example, resolution being 720p vs others pushing 1080p,\nand it‚Äôs unclear if Pika has audio generation (likely not at the moment; most mention is about visuals). Some features\nin Pika are marked as ‚Äúv1‚Äù or ‚Äúv1.5‚Äù in comparison tables, indicating they are in active development (e.g. perhaps\nmotion brush or mid-frame control is not fully there yet). Also, as a smaller company, Pika might not have as extensive\ncontent filtering or enterprise support as OpenAI, Adobe, and Google, so Lilly would need to vet its outputs carefully\nfor IP or moderation issues. The competitive landscape is another challenge ‚Äì multiple competitors means Pika needs to\ndifferentiate; it seems to do so with effects and ease-of-use, but others can emulate those too. Pika‚Äôs strength in\n‚Äúvarious styles‚Äù could also mean it might not yet dominate any one style in quality (jack of all trades risk).\nFor Lilly, Pika Labs is a startup to watch or possibly partner/invest/acquire. Its agility means new features (like\nthose quirky video effects) come out quickly, which could be leveraged for marketing novelty. For instance, our creative\nteams might use Pika to generate an attention-grabbing visual clip (imagine a pill that ‚Äúexplodes‚Äù into confetti to\nsymbolize treatment success ‚Äì an effect Pika might handle well). With its sizable funding, Pika could also become a\ntakeover target or major player globally. The key will be how they scale their service and continue improving model\nperformance.\nMiniMax ‚Äì‚ÄØHailuo Video-01\n‚Äã\nMiniMax‚ÄØis a Chinese startup (behind the model named‚ÄØHailuo Video-01) that has also made a name for itself among AI\nvideo creators. Often just referred to as ‚ÄúMiniMax video generator,‚Äù it gained recognition in late 2023 for delivering\nimpressively‚ÄØrealistic motion continuity‚ÄØat 720p resolution. MiniMax‚Äôs approach appears to prioritize the fundamental\nchallenges of video generation: physics, continuity, and camera work.\nHailuo Video-01, as offered by MiniMax, generates‚ÄØhigh-definition (720p) videos at 25 fps‚ÄØand supports\nboth‚ÄØtext-to-video and image-to-video‚ÄØmodes. A typical output is ~5 seconds long, but users have successfully stitched\nclips to create longer sequences (one user made a 30-second video by concatenating 5s segments, with seamless flow).\nMiniMax particularly shines with‚ÄØcinematic camera movements‚ÄØ‚Äì reviewers were impressed by how well it handled complex\npans and drone-like aerial shots while keeping the scene coherent. For example, flying over a beach then tilting toward\na waterfall in one continuous motion was executed smoothly. The model also excelled at‚ÄØB-roll style footage: generating\na professional-looking slow-motion laptop product shot with correct focus and lighting transitions. This indicates\nstrong understanding of focus depth, blur, and other cinematographic details.\nIn terms of content, MiniMax did very well with‚ÄØcartoon animations and abstract visuals. It produces smooth,\nhigh-quality cartoon motion, making it great for creators of animated shorts. And for abstract or surreal scenes, it\nmaintained immersive and stunning visuals. When it comes to continuity, MiniMax has a feature where you can take the\nlast frame of a clip and feed it as the first frame of the next generation ‚Äì this allowed that 30-second continuous\nvideo with a character, where the character stayed consistent throughout. That consistency is a major plus; the user\nessentially stiched longer stories by manual continuity, and the tool worked nicely.\nStrengths:\nMiniMax‚Äôs biggest strength is‚ÄØrealistic motion physics. Movements in its videos look real and believable\n‚Äì the AI seems to have a better grasp of inertia and momentum, avoiding the jerky or floaty feel others sometimes have.\nIn side-by-side tests, motions that looked robotic in Sora were more lifelike in MiniMax, suggesting their model\narchitecture may explicitly model physics or was trained on plenty of real video. The continuity (the ability to chain\nclips) is another strength, enabling content creators to tell longer stories if needed. Also, MiniMax‚Äôs output quality\nfor its resolution is high ‚Äì 720p might sound lower than 1080p, but given many AI videos are still in that ballpark,\nMiniMax‚Äôs effective quality is top-tier within that. It is also reportedly‚ÄØfree or low-cost to try‚ÄØ(it was available on\nplatforms like Anakin.ai for users to experiment), which has helped it gain traction among the community.\nWeaknesses:\nMiniMax isn‚Äôt as globally known as some others, and documentation in English is sparse.\nIts‚ÄØimage-to-video with humans‚ÄØwas noted as a weak spot ‚Äì when given a real human photo to animate, the results were not\nvery convincing (unnatural transitions, lack of detail). It performed better with‚ÄØcartoonish images‚ÄØthan real human\nphotos. So for tasks like animating a real person‚Äôs photo, it may not be the best (other specialized avatar tools or\nSynthesia might do better). Also, MiniMax had difficulty with‚ÄØvery complex movement scenarios‚ÄØ‚Äì e.g. a football player\nkicking a ball, where precise interaction is needed, looked clunky. This suggests limits in handling intricate\nmulti-object physics (ball interacting with foot etc.). Another limitation is that, at least as of late 2024, MiniMax\noutputs were capped to short durations and required manual effort to chain longer videos ‚Äì it doesn‚Äôt inherently\ngenerate a 30s story in one go; you have to curate it scene by scene.\nFor Lilly, MiniMax‚Äôs existence is proof that there are multiple players beyond the most hyped. If we want to focus on\nexperimenting with as many options as possible, MiniMax would be a good model to assess for scenarios needing‚ÄØbelievable\nmovement‚ÄØ(say you want an AI video of a pill bottle gently rotating and tilting ‚Äì physics heavy ‚Äì MiniMax might nail\nthat). However, since it‚Äôs less of a full product and more of a model accessible via certain interfaces or APIs\n(Segmind, Replicate, etc.), leveraging it requires engineering or technical integrations if not working with a provider\nthat offers the model as part of a service. It‚Äôs one of the ‚Äúbehind the scenes‚Äù engines that some AI video platforms\ncould incorporate for their motion strengths.\nLuma Labs ‚Äì‚ÄØDream Machine (Ray 2)\n‚Äã\nLuma AI‚ÄØis known for its 3D scanning and NeRF (neural radiance field) technology, which allows creating 3D models from\nphone captures. In 2023, Luma expanded into generative AI with its‚ÄØDream Machine‚ÄØapp, aiming to be a ‚Äúvisual thought\npartner‚Äù for both images and videos. Luma‚Äôs Dream Machine is unique in that it blends 2D and 3D AI capabilities: it\nincludes an image generation model (Photon) and a video model (Ray 2) working in tandem. The focus is on giving\ncreators‚ÄØfluid control‚ÄØand‚ÄØfast iteration‚ÄØin making spectacular visuals without needing prompt engineering skills.\nDream Machine allows you to generate both‚ÄØimages and videos‚ÄØand move seamlessly between them. Notably, you can\nuse‚ÄØnatural language‚ÄØto not only create but also‚ÄØedit‚ÄØand‚ÄØrefine‚ÄØoutputs (e.g. ‚Äúmake it a 90s vibe‚Äù or ‚Äúadd a fisheye\nlens effect‚Äù to modify an image/video). For videos, Luma‚Äôs Ray 2 model is designed for‚ÄØfast, coherent motion and\nultra-realistic details. Dream Machine includes robust features for‚ÄØcontrolling outputs: you can‚ÄØ‚ÄúDirect the perfect\nshot with start/end frames‚Äù‚ÄØ‚Äì meaning keyframe control similar to Adobe‚Äôs and Google‚Äôs approach. You can also‚ÄØ‚Äúloop‚Äù‚ÄØa\nvideo seamlessly if desired. It emphasizes ability to create‚ÄØunique, consistent characters from a single image‚ÄØ‚Äì so you\nprovide an image of a character and the AI can generate new scenes with that character consistently present. There‚Äôs\nalso support for‚ÄØvisual style references: you can upload or select reference images for style or specific elements,\nguiding the generation.\nDream Machine‚Äôs interface touts features like‚ÄØBrainstorm‚ÄØ(the AI suggests ideas or variations if you‚Äôre not sure where\nto go next), and‚ÄØShare & Remix‚ÄØwhich encourages a community aspect (users can share their creations and even the ‚Äúbehind\nthe scenes‚Äù prompt+refs so others can iterate on them). Under the hood,‚ÄØPhoton‚ÄØ(image model) helps ensure any single\nframe or detail is high-quality, while‚ÄØRay 2‚ÄØ(video model) handles the temporal aspect ‚Äì this dual approach likely\ncontributes to the‚ÄØsharp, detailed frames and accurate text rendering‚ÄØthat Luma claims (they specifically list‚ÄØ‚Äúsharp\nand accurate text rendering‚Äù‚ÄØas a feature, implying their model can place legible text in video). They also\nhighlight‚ÄØ‚Äúaccurate lighting and physics‚Äù‚ÄØand‚ÄØ‚Äúclean and consistent animation style‚Äù, indicating the model was trained\nto respect physical realism and maintain a coherent style throughout a clip.\nStrengths:\nLuma‚Äôs Dream Machine is praised for its‚ÄØpolished user experience. Tom‚Äôs Guide in late 2024 called it ‚Äúone\nof the best interfaces‚Äù among AI video platforms. This ease and fluidity could reduce the learning curve for non-AI\nexperts. The‚ÄØrange of control‚ÄØis also a key strength: Dream Machine basically offers every control feature we‚Äôve\ndiscussed ‚Äì keyframes, reference images, style control, camera movement, looping, region modifications ‚Äì within one app.\nThis is very powerful for artists or marketing teams who want to fine-tune outputs to match brand guidelines (e.g.\nensuring a brand mascot stays on-model, or a scene has the exact color palette desired). Moreover,‚ÄØRay 2‚ÄØbeing a\n‚Äúlarge-scale video model‚Äù indicates it has been trained extensively, likely yielding very‚ÄØhigh success rates‚ÄØof usable\ngenerations (Luma claims Ray2 outputs are ‚Äúsubstantially more production-ready‚Äù than prior gen video AI). Dream\nMachine‚Äôs ability to generate both images and videos means users can prototype a concept with still images (faster) and\nthen seamlessly switch to video mode to animate it ‚Äì this interoperability can save time and ensure consistency between\ncampaign imagery and videos.\nWeaknesses:\nLuma Dream Machine might be slightly under the radar in enterprise circles compared to OpenAI,\nMicrosoft, Google or Adobe. It‚Äôs a newer entrant and was initially iOS-only (though now on web too). Being an app that\ntargets creatives, it might not yet have the integrations into enterprise tech stacks or the compliance features\nbusinesses need (no mention of watermarking or governance, for instance). Also, its‚ÄØoutput length‚ÄØis not explicitly\nstated ‚Äì likely similar short durations, possibly extendable with looping or chaining. If someone wants a 30-second\npolished piece, they may have to puzzle-piece multiple generations. Additionally, while having lots of control is great\nfor power users, it could overwhelm others; Luma tries to mitigate this with the Brainstorm auto-suggestions, but using\nall features effectively might require training/experience.\nFor Lilly, Luma could be a fantastic prototyping tool ‚Äì e.g. you could create a complex scene of a molecular world or\npatient journey, iterating quickly with text prompts and tweaks until it‚Äôs right, then generate final video frames. Its\nstrength in‚ÄØ‚Äúexceptional adherence to detailed instruction‚Äù‚ÄØmeans if you have a very specific storyboard or shot list,\nDream Machine might follow it closely. It‚Äôs also worth noting Luma‚Äôs heritage in 3D: they might eventually integrate\ntrue 3D scene generation or AR content, which could be useful for pharma (think interactive 3D MOA visuals). At present\nthough, Dream Machine is a strong sign that‚ÄØuser experience and granular control are becoming differentiators‚ÄØin AI\nvideo platforms.\nStability AI ‚Äì‚ÄØStable Video Diffusion\n‚Äã\nStability AI, known for open-source image generator Stable Diffusion, has also ventured into video. In late 2023, they\nreleased‚ÄØStable Video Diffusion (SVD)‚ÄØin research preview. This model is based on Stable Diffusion‚Äôs image generation\nbut extended to produce short videos. True to Stability‚Äôs mission, the model was open-sourced (code on GitHub and\nweights on HuggingFace) for researchers and developers. This makes Stable Video Diffusion an important project for those\nwho want‚ÄØcustom or self-hosted AI video solutions.\nThe initial release of Stable Video Diffusion can generate very short clips ‚Äì specifically‚ÄØ14 or 25 frames‚ÄØof video. At\ncommon frame rates, that‚Äôs roughly 0.5 to 1 second of footage, so extremely brief, although one can run it sequentially\nto make a few seconds. It allows setting a‚ÄØframe rate between 3 and 30 fps‚ÄØfor those frames. The focus was on proving\nthe concept and providing a foundation to build on, rather than competing on length/quality with commercial models.\nStability mentioned the model could handle different frame rates and aspect ratios in principle, and that with\nfinetuning it could do tasks like‚ÄØmulti-view (novel view) synthesis‚ÄØ‚Äì e.g. generating different angles of a scene from\none input image. They positioned it as a base model on top of which many specialized video models might be built,\nsimilar to how Stable Diffusion spawned many fine-tuned image models for various styles.\nStability also launched a‚ÄØbeta web interface‚ÄØ(as a waitlist) for text-to-video using this model, and an API on their\ndeveloper platform. By 2024‚Äôs end, they announced‚ÄØStable Video 4D 2.0, which was geared towards ‚Äú4D generation and novel\nview synthesis from a single video input‚Äù. This suggests a branch of their work focusing on turning a single video into\na 3D scene or generating new angles (useful for AR/VR or VFX where you need more camera coverage).\nStrengths:\nThe big advantage of Stable Video Diffusion is‚ÄØopenness and customizability. Researchers have full access\nto the model to fine-tune on their own data, which could be useful for a company wanting a model trained specifically\non, say, pharmaceutical TV commercials or lab experiment videos (ensuring output is in-domain). Also, because it‚Äôs based\non Stable Diffusion, it benefits from a huge open-source community. Already enthusiasts have built demos like\nimage-to-video ‚Äúanimation‚Äù notebooks and integrated SVD into tools (one example: a free web tool offered\nstable-video-diffusion image-to-video with trial runs). Stability claimed that at release, their model‚ÄØ‚Äúsurpass[ed] the\nleading closed models in user preference studies‚Äù‚ÄØ‚Äì if true, that‚Äôs a strong endorsement of quality, though it might\nrefer to comparisons with earlier or simpler models. The‚ÄØspeed‚ÄØis decent; Stability said it can create videos in‚ÄØ‚Äú2\nminutes or less‚Äù‚ÄØfor those short clips, and being lightweight means it could run on accessible hardware eventually (they\noften optimize models for consumer GPUs). For a company with skilled ML engineers and low risk tolerance, Stable Video\nDiffusion provides a platform to customize without needing to send data to third-party APIs.\nWeaknesses:\nIn terms of raw capability, SVD lags behind the likes of Sora, Veo, or others in this list. The\nextremely short length (1 second) and likely lower resolution (their paper suggests training at smaller resolutions)\nmean it‚Äôs not immediately useful for direct content creation yet ‚Äì more of a tech demo. It doesn‚Äôt produce audio. Also,\nusing it requires ML expertise; it‚Äôs not an off-the-shelf product for end-users. Stability‚Äôs emphasis on research means\npolished features like storyboards or content moderation are left to the implementer. Indeed, one has to be careful: an\nopen model might produce disallowed content if fine-tuned incorrectly, and the onus is on the user to enforce moderation\nor filtering.\nStable Video Diffusion is relevant if you have a strategy around‚ÄØopen-source AI‚ÄØand want to possibly‚ÄØbuild in-house\ncapability. A pharma company might, for example, fund a project to fine-tune SVD on their library of MOA animation\nframes to create a custom model that generates new scientific animations consistent with their style ‚Äì something you\nwouldn‚Äôt want to put into a public model for IP reasons. That could be a competitive differentiator long-term. Stability\nAI also tends to improve their models iteratively, and with community contributions, so by late 2025 we might see SVD\n2.0 that can do several seconds at higher quality. It‚Äôs the‚ÄØ‚Äúkeep an eye on this if you want a differentiated AI video\nsolution‚Äù‚ÄØcontender.\nAdobe ‚Äì‚ÄØFirefly Generative Video\n‚Äã\nAdobe is integrating generative AI across its Creative Cloud, and for video content Adobe‚Äôs solution is the‚ÄØFirefly\nVideo Model‚ÄØ(often just called‚ÄØGenerative Video in Firefly). Announced in 2024 and rolled out in beta this April (2025),\nAdobe‚Äôs tool is a bit different in focus from others: it‚Äôs designed to‚ÄØenhance video editing workflows‚ÄØ(think of it as\nan assistant to Premiere Pro users) rather than to replace the video camera entirely. Adobe is leveraging its vast\nlibrary of licensed assets (Adobe Stock) to train this model, ensuring the outputs are‚ÄØcommercially safe for use‚ÄØ(no\nunlicensed content).\nAs of launch, Firefly‚Äôs generative video supports two main generation modes:‚ÄØText-to-Video and Image-to-Video. Users can\nenter a descriptive prompt to get a‚ÄØ5-second, 1080p‚ÄØvideo clip. Crucially, Adobe‚Äôs tool allows uploading a‚ÄØstart frame\nand end frame‚ÄØto guide the motion. This is essentially keyframe control: for example, you could upload a still image of\na scene as the start and another image for the end, and the AI will animate a transition between them. Firefly also\nprovides‚ÄØdrop-down menus for camera shot size and angle, as well as presets for camera motion‚ÄØ(e.g. pan, zoom, tilt) to\nfurther direct the result. This GUI approach aligns with Adobe‚Äôs pro users who may be more comfortable picking settings\nthan writing long textual prompts for everything.\nIn addition to generation, Adobe introduced‚ÄØTranslate Video and Translate Audio‚ÄØfeatures with the Firefly launch. These\nuse an AI voice model to‚ÄØdub existing videos‚ÄØinto different languages (with some lip-sync capability for enterprise\nusers). While not generation from scratch, it‚Äôs a complementary AI feature that can be big for training or marketing ‚Äì\ne.g. easily turn an English video into French, Spanish, etc., with the voice and lips aligned. It shows Adobe‚Äôs holistic\napproach: not just make new content, but modify and repurpose content efficiently.\nAdobe‚Äôs Firefly video is integrated in the cloud (firefly.adobe.com) as a beta, with plans to bring it into Premiere Pro\nlater. It emphasizes use cases like‚ÄØfilling gaps in footage, generating‚ÄØb-roll of scenery, adding‚ÄØatmospheric\nelements‚ÄØto existing shots (like generate an overlay of smoke or dust on a green screen). Indeed, the FAQ suggests top\nuse cases as adding elements, animating static images (e.g. making a waterfall flow), or creating stylized graphics and\ntext animations.\nStrengths:\nAdobe‚Äôs key advantage is‚ÄØseamless integration for creatives. Editors can incorporate generative clips\nright inside their familiar tools (eventually in Premiere/AfterEffects), which lowers adoption friction. Also,\nthe‚ÄØethics and legal safety‚ÄØ‚Äì every Firefly output is trained on licensed content and can be used commercially with\nconfidence. This is a huge factor for enterprises worried about copyright (some companies avoid using other AI image\ngenerators due to unclear training data provenance; Adobe solves that). The resolution (1080p) and aspect ratio options\n(16:9 and 9:16 supported out of the gate) meet professional standards. Firefly‚Äôs‚ÄØcamera and keyframe controls‚ÄØmean users\ncan get precisely the shot they envision ‚Äì rather than hoping a pure text prompt yields the right camera angle, you can\nexplicitly say you want a ‚Äúwide shot, overhead angle, slow dolly movement,‚Äù etc. This dramatically improves reliability\nfor production use, because it reduces ambiguity for the AI. Additionally, Adobe‚Äôs inclusion of‚ÄØtranslation and voice\nAI‚ÄØin the suite is a bonus for people who want an all-in-one solution for localizing and creating content.\nWeaknesses:\nFirefly‚Äôs generative video is currently limited to‚ÄØ5 seconds‚ÄØmax for generation, which is shorter than\nsome competitors like Sora (20s) or Kling (minutes). It is clearly meant for quick clips and fillers, not full scenes.\nIf a marketing team wants to create a 30-second ad purely from AI, they‚Äôd need to string together many Firefly\ngenerations and likely do some manual stitching. Another limitation is that as of now it‚Äôs a separate web beta, not\nfully in Premiere ‚Äì meaning an extra step to use (though this will likely change). The content domain might also be\nsomewhat constrained: given Adobe‚Äôs professional focus, the model might excel at certain types of shots (e.g. landscape\nb-roll, simple subject animations) but perhaps not be as wildly imaginative or diverse in style as some others. Early\nbeta users will certainly find edges to what it can do. Also, because Adobe prioritizes safe training data, the training\nset might exclude a lot of web videos ‚Äì it‚Äôs likely biased towards stock footage and professionally created content.\nThis means it might not have learned some of the ‚Äúinternet‚Äôs imagination‚Äù that open models did; whether that‚Äôs a\ndownside (less breadth) or upside (more reliability) depends on use case.\nFor Lilly, Adobe‚Äôs generative video could be immediately useful in‚ÄØpost-production enhancement. Imagine you have a live\naction commercial but need to add an effect ‚Äì say a glowing aura around a patient to symbolize improvement ‚Äì Firefly\ncould potentially generate that element on a transparent background to overlay. Or if you‚Äôre storyboarding a concept,\nyou can quickly generate video mockups of scenes to show stakeholders, all within the Adobe ecosystem. The translation\nfeature is directly relevant for global pharma marketing: you could take a video with an English voiceover and get a\nmultilingual version in minutes, which is often needed for global training or promotion (with the caveat that lip-sync\nfor different languages is still a work in progress). Adobe‚Äôs move also signals that‚ÄØAI video will be a standard tool in\ncreative departments, not a niche experiment.Shape\nIndustry Impact and Use Cases\n‚Äã\nThe advent of AI-generated video impacts multiple sectors. Here we focus on‚ÄØmarketing, learning & development\n(L&D)/training, and specifically the pharmaceutical industry‚ÄØcontext, while noting broader implications:\nMarketing and Advertising\n‚Äã\nContent Volume & Personalization\n‚Äã\nMarketing teams are under pressure to produce more content than ever, tailored to different audiences and channels. AI\nvideo generation can produce‚ÄØquick video variants‚ÄØat scale. For example, a global brand campaign normally requires\nshooting different versions of an ad for each region ‚Äì with generative AI, one could create localized versions by\nchanging the background, actors‚Äô apparent ethnicity, language of on-screen text, etc., without a reshoot. AI video could\ngenerate the same scene in different hospital settings or swap the text on a prescription bottle label to a different\nlanguage, saving huge production costs. Personalized video ads (addressable TV or online ads) could also be\nauto-generated: an AI might create thousands of slight variations of a medical device ad, each tuned to a particular\ndemographic or even individual (with appropriate compliance checks).\nCreative Brainstorming & Prototyping\n‚Äã\nAI video is a boon to the creative process. Agencies should be saving time and passing cost savings to their customers\nwith tools like Sora or Luma‚Äôs Dream Machine to‚ÄØstoryboard concepts in motion, not just static frames. This makes it\neasier for non-technical stakeholders to grasp a concept. Pharma marketing often involves explaining abstract concepts\n(like how a drug works in the body). Instead of expensive 3D animations made after weeks of work, a team could prototype\nan MOA animation with AI in a day to visualize the idea, then refine it. It accelerates the iteration cycle on creative\nideas. Several companies already use DALL-E or Midjourney for concept art ‚Äì extending that to video is the next logical\nstep. Business leaders may appreciate seeing a‚ÄØmock commercial‚ÄØor‚ÄØanimated concept‚ÄØearly on, which guides\ndecision-making on whether to invest in a full production or adjust messaging.\nCost and Time Savings\n‚Äã\nParticularly for‚ÄØB-roll and filler content, AI can reduce or eliminate the need for stock footage or shoots. Need a\n5-second establishing shot of a laboratory at sunrise? Instead of searching stock libraries or setting up a shoot, an AI\ncould generate one that fits the precise vision. This is what Adobe is targeting ‚Äì filling timeline gaps. Marketing\nvideos often have many such moments. Over time, as reliability improves, AI might handle even primary footage for\ncertain types of ads (especially animated or stylized ones). Lower production cost means marketers can do more‚ÄØA/B\ntesting‚ÄØof video ads by trying multiple versions. Lilly could use AI to make rapid tweaks post-approval (e.g. if a claim\nneeds a different visual emphasis, an AI could alter the scene accordingly without reshooting, if it doesn‚Äôt change the\napproved message).\nChallenges for Marketing\n‚Äã\nCreatives and brand teams must ensure‚ÄØbrand consistency‚ÄØ‚Äì ironically a potential issue with AI‚Äôs variability. We likely\nneed to lock down style guides and possibly train custom models on brand assets to keep outputs on-brand. There‚Äôs also\nreputational risk: poorly made AI videos could reflect badly if they slip through (imagine awkward visuals or errors ‚Äì\nit could appear tone-deaf or unprofessional). So likely, in near term, AI video will be used for internal drafts, social\nmedia (where lower fidelity is more acceptable), or supplemental content, while critical campaigns still use high-end\nproduction with AI augmentation. Over time, as quality stabilizes, this will shift.\nLearning & Development / Training\n‚Äã\nScenario Simulations:‚ÄØIn corporate or medical training, video role-plays and scenarios are common (e.g. demonstrating\ndoctor-patient interactions, or proper use of a device). AI video can produce‚ÄØcustom training scenarios on demand. For\nexample, a pharma sales training could generate a video of a physician consultation scenario tailored to a specific\nspecialty or objection handling ‚Äì without hiring actors. Tools like Synthesia already generate talking avatar videos\nfrom text (not quite these full scene generators, but adjacent tech). With text-to-video, one could specify the scenario\nand have the AI create an original scene. Character consistency becomes key here; a trainer might want the same\nAI-generated actor to appear across multiple modules ‚Äì technologies like DeepMotion or others could even give that AI\nactor movement. Google‚Äôs Flow enabling‚ÄØJump to new shot while preserving character‚ÄØis highly relevant. That means you\ncould maintain the same virtual ‚Äúperson‚Äù in different situations.\nLocalized and Updated Content:‚ÄØL&D often needs to deliver content in multiple languages and keep it updated regularly.\nUsing generative video plus integrated voice translation (as Adobe Firefly offers), a training department can take an\nEnglish training video and‚ÄØautomatically generate‚ÄØthe French, Spanish, Chinese versions with dubbed voices and adjusted\nvisuals if needed. The AI can even lip-sync the translated speech to the video (Adobe is working on that for\nenterprise). Additionally, if a procedure changes or guidelines update, new training videos can be spun up quickly by\nediting the prompt or script, rather than reshooting footage. This is particularly useful in pharma where information\nchanges (new study data, new regulations requiring a tweak in message, etc.).\nMicro-learning and Personalized Learning:‚ÄØWith AI, it‚Äôs plausible to generate‚ÄØpersonalized training videos‚ÄØfor employees\nor customers. Imagine an onboarding video where the scenarios depicted are customized to that employee‚Äôs role or common\nknowledge gaps. Or patient education videos that adjust explanations to the patient‚Äôs literacy level or cultural context\n‚Äì AI could alter the scenery or analogies in the video accordingly. Such on-demand generation was impossible at scale\nwith traditional methods. Pharma companies doing patient support programs could benefit by tailoring educational content\n(for example, showing a patient of the same demographic in the video for relatability, which AI can swap out easily).\nChallenges in Training:‚ÄØEnsuring‚ÄØaccuracy‚ÄØis paramount ‚Äì any AI hallucination or incorrect depiction in a training video\ncould misinform, which in pharma could have serious consequences. Thus, for factual or procedural content, AI outputs\nmust be carefully reviewed, or possibly a hybrid approach used (AI generates visuals but humans script it tightly and\nverify every frame). There‚Äôs also an emotional quality aspect: human-made training videos use real actors to connect\nemotionally. AI avatars are improving, but there‚Äôs still an ‚Äúuncanny valley‚Äù risk if not done well. Over time, as\ncharacter generation improves, this will lessen. Another challenge is platform adoption: training departments will need\nnew workflows to incorporate AI generation tools, and staff will require some upskilling (e.g. learning prompt-writing\nand basic editing of AI outputs).\nCommercial and Promotional\n‚Äã\nThe pharma industry, being highly regulated, will approach AI video with both excitement and caution. Here‚Äôs how it\nspecifically stands to gain or face challenges:\nMechanism of Action (MOA) and Scientific Visualization\n‚Äã\nPharma marketing relies on complex animations to show how a drug works in the body. These are expensive 3D animations\ntoday. In the future, a text prompt like ‚ÄúShow T-cells attacking a cancer cell, zoomed in at cellular level, cinematic\nlighting‚Äù could produce a decent MOA animation clip. Already, Sora‚Äôs examples (like wooly mammoths in a field, or waves\ncrashing on cliffs) indicate the ability to create nature and complex textures ‚Äì extending to microscopic imagery is a\nquestion of training data. If we can fine-tune models on biomedical imagery, we could get AI that generates\nscientifically accurate animations quickly. This would let marketing and medical teams experiment with different visual\nmetaphors for MOA and pick the most effective ones. It could also be used in‚ÄØmedical education‚ÄØ‚Äì helping doctors\nvisualize mechanisms or trial results via generated visuals.\nHCP Marketing and Peer Influence\n‚Äã\nPharma often involves key opinion leaders (KOLs) giving talks or explaining data. In the near future, one can imagine\ngenerating‚ÄØavatar videos of KOLs‚ÄØpresenting data, in multiple languages. While using a real person‚Äôs likeness has\nethical/IP issues (would need permission and careful use), generative tech could create a convincing‚ÄØvirtual\nspokesperson. Alternatively, completely fictional yet authoritative-looking avatars might be used to relay information.\nThis could help scale expert content delivery (though likely for internal training or markets where the actual speaker\ncannot be present ‚Äì regulations on promotional use of an AI ‚Äúdoctor‚Äù would be tricky).\nPatient Engagement\n‚Äã\nUsing AI, we could create more engaging patient resources ‚Äì e.g. an AI-generated‚ÄØexplainer cartoon for kids‚ÄØabout how to\nuse an inhaler, or a reassuring scenario video for patients starting a new therapy showing what to expect. Emotional\nexpressiveness is something some models like Sora and Veo are working on. If an AI can portray empathy or excitement\nthrough a character‚Äôs face and voice, patient communication could benefit. However, Lilly should remember to be\ntransparent if an avatar or voice bot is being leveraged to maintain trust.\nCompliance and Moderation\n‚Äã\nEvery promo or medical video in pharma must go through approval for claims, safety info, etc. AI introduces a new\nvariable ‚Äì the visuals might inadvertently introduce something non-compliant (e.g. showing a use of a drug that‚Äôs\noff-label). Content moderation tools will need to extend to video. We may need to adopt a practice of‚ÄØframe-by-frame\nreview‚ÄØof AI outputs just like they review every word of a brochure. This slows things, but some AI tools might allow\nlocking certain parameters to ensure compliance (for instance, an AI could be instructed not to show the pill being\ntaken with any other medication or not to depict certain patient populations if not approved). As OpenAI noted with\nSora, their deployed model is limited in what it can show realistically (some complex or sensitive scenarios are\nfiltered). Lilly will need even tighter guardrails when using AI video generation.\nSummary\n‚Äã\nIf harnessed well, Lilly could significantly‚ÄØincrease engagement‚ÄØwith both physicians and patients. Video content that\nwas once too costly to produce for smaller audiences (like a rare disease community) could be made cost-effectively with\nAI, making communications more visual and relatable. It also opens up possibility for‚ÄØinteractive content‚ÄØ‚Äì e.g. a\nvoicebot experience where a patient‚Äôs questions trigger dynamically generated video answers (a far future concept, but\nnot impossible as tech converges). Another area is‚ÄØinternal communications: using AI to generate internal announcement\nvideos or CEO messages (with consent of course) to add a personal touch without pulling executives into studio every\ntime.\nThe biggest risks are legal (IP considerations), regulatory, and inaction. Pharma is already under scrutiny for how it\nmarkets ‚Äì if it came out that an AI generated video inadvertently created a misleading impression (even subtly, via\nimagery), or leveraged an a tool associated with copyright infringement, it could result in legal penalties or public\ntrust issues.\nStrategies for Adoption\n‚Äã\nFor anyone planning to leverage AI video generation, here are strategic considerations and steps:\nWatch and Experiment (Now):‚ÄØIt‚Äôs important to‚ÄØstay informed‚ÄØon this fast-moving field (as this report has shown,\nupdates in 2024-2025 were frequent). Assign a team or innovation leader to experiment with various tools ‚Äì e.g. a\ncreative or technology team who can consistently assess models and output across a wide range of features and\ncapabilities ‚Äì leveraging the same prompts or end goal in mind. Encourage sharing of results and set up brainstorming\nsessions on ‚ÄúWhat could we do with this in our business?‚Äù\nDetermine High-Impact Use Cases (Now):‚ÄØNot every video needs to be AI-generated. Look for‚ÄØpain points or\nopportunities‚ÄØin your current content pipeline: Is it hard to get budget for certain types of videos (maybe training\nscenarios or international market content)? Those are prime targets to try AI. For Lilly, maybe start with non-public\ncontent (internal training, mechanism explainer for reps) to avoid regulatory risk while the tech is still new.\nAnother ideal use case is social media: social teams always need fresh short videos ‚Äì AI can help create those\nquickly and the lower stakes environment (a playful Twitter video) can tolerate slightly less polish.\nDraft Policy and Guidelines (Now):‚ÄØJust as there are guidelines for using stock images or social media conduct,\ncreate‚ÄØguidelines for AI-generated content. This includes: what types of content are allowed (e.g. perhaps no AI\ngeneration of real person likenesses without permission to avoid deepfake issues, or avoid certain sensitive health\nscenarios to be safe); requirement that every AI video is reviewed by a subject matter expert for accuracy; and\nguidelines for disclosure (if needed). For external content, consider‚ÄØtransparency‚ÄØ‚Äì for example, some companies\nmight add a line in the video description that it contains AI-generated imagery, especially if regulations or brand\ntrust demands it. Given OpenAI‚Äôs Sora tags metadata, if an image verification tool sees it‚Äôs AI, we don‚Äôt want the\ncompany to look like it was hiding that fact.\nGive creatives opportunities to Upskill:‚ÄØAI tools require new skills ‚Äì prompt writing, iterative refinement, and\nbasic video editing to polish AI outputs. Invest in developing and onboarding tools so teams can begin learning how\nto effectively harness this technology.\nAlpha Projects:‚ÄØChoose a few of low-risk Alpha products to pursue. For example,‚ÄØcreate an internal training video\nseries using AI avatars, or‚ÄØgenerate supplemental social media content for a product campaign. Monitor how long it\ntakes, the quality feedback from viewers, and iterate. Use these pilots to build a case study: did it save money? Did\nit engage better (or worse)? What were the unforeseen hiccups? This builds institutional knowledge and also helps\njustify further investment to higher management.\nEthics and Authenticity:‚ÄØReview/revise our policy and approach to‚ÄØethical AI use (if needed). For pharma especially,\npatient trust is key ‚Äì avoid anything that could be seen as deceptive. If you use an AI avatar of a ‚Äúdoctor‚Äù in a\npatient video, maybe have a disclaimer like ‚ÄúVisuals are computer generated for illustration‚Äù if required. Also avoid\nsensitive contexts ‚Äì e.g. generating a video of a real patient story would be unethical unless clearly fictionalized.\nKeep humans in the loop for empathetic or sensitive communications (AI can generate the draft, but let a human review\nthe tone).\nMonitor Regulatory and Legal Developments:‚ÄØRegulations around AI-generated content (so-called deepfake laws or\nrequired disclosures) are evolving globally. The EU, China, some US states have begun requiring disclosures for\ncertain AI-generated media. Ensure your compliance team keeps abreast of these so your use of AI video doesn‚Äôt run\nafoul of any new rule. For instance, if a law says ‚ÄúAI-generated realistic videos of humans must be labeled‚Äù, you‚Äôd\nincorporate that into your practice. Also, look to ongoing legal conflicts related to AI image generation to\nanticipate future issues with video generation.\nConsider a Long-term Strategy:‚ÄØThink how AI video fits into our digital transformation. This will not replace\neverything but could augment many processes. Consider organizing a‚ÄØcenter of excellence‚ÄØfor generative media, pooling\nexpertise from IT, creative, legal. Long-term, also consider investing in custom model development for commercial\nneeds ‚Äì e.g. partner with a vendor to fine-tune a model on Lilly video content (especially relevant for pharma with\nlots of scientific visuals). Owning or heavily customizing a model could give competitive edge, if it could generate\ncontent that is unique to Lilly IP and brand style, which others using generic models cannot.\nChallenges and Risks\n‚Äã\nWe‚Äôve touched on several challenges in context, but let‚Äôs consolidate the major‚ÄØrisks and challenges‚ÄØof AI video\ngeneration adoption:\nQuality and Consistency\n‚Äã\nWhile rapidly improving, AI-generated video can sometimes produce‚ÄØglitches‚ÄØ‚Äì a person‚Äôs face warping for a frame, odd\nbackground artifacts, inconsistent lighting continuity, etc. In a professional setting, these can be jarring. Ensuring\nevery frame is clean might require frame-by-frame editing or re-generation cycles. There‚Äôs also the continuity over time\nissue ‚Äì making sure the AI doesn‚Äôt ‚Äúforget‚Äù what it was showing. Until models are truly robust, teams must budget time\nfor QC and maybe manually fixing AI output (via editing or inpainting tools).\nMisrepresentation and Deepfakes\n‚Äã\nAI can create‚ÄØvery realistic fake people or events. In pharma, imagine an AI video of a ‚Äúpatient‚Äù giving a testimonial ‚Äì\nif it‚Äôs not clearly fictional, that‚Äôs ethically problematic (real patient testimonials require consent and actual\npatient stories). Deepfakes of public figures endorsing a drug (even if done jokingly) would be bad. Lilly should set\nstrict lines: e.g. never impersonate real individuals or fabricate quotes without prior consent and compensation. The\nthreat of malicious deepfakes (outside actors using AI to create fake news about a company or fake exec statements) is\nalso real ‚Äì another reason transparency and detection are important. On the flip side, we do not want Lilly to\naccidentally be‚ÄØaccused‚ÄØof using a deepfake without disclosure.\nRegulatory Compliance\n‚Äã\nFor pharma, any promotional content must comply with FDA (or EMA, etc.) regulations. AI doesn‚Äôt change the requirement,\nbut it adds complexity to verification. If an AI video shows a patient using a drug, the visual could inadvertently\nbecome a‚ÄØ‚Äúclaim‚Äù. For example, if it shows the patient doing something that implies a benefit not on label, that‚Äôs a\ncompliance issue. Or if the AI inadvertently generates a medically inaccurate portrayal (like pills of wrong color or a\ndevice used incorrectly), that‚Äôs problematic. Thus, a deep integration of regulatory review is still needed ‚Äì possibly\neven training the AI on what‚ÄØnot‚ÄØto show. Regulators themselves may start scrutinizing AI-generated content differently;\ncompanies might need to provide more substantiation that the content is accurate and not misleading. There might even be\nfuture guidance specific to AI in pharma advertising.\nIntellectual Property and Training Data\n‚Äã\nA lingering question: if an AI video model was trained on, say, thousands of movie clips or YouTube videos without\npermission, is its output legally safe to use? The model developers claim yes, it‚Äôs transformative. But cases are in\ncourts (for images, Getty vs. Stability for e.g.). Using output commercially could pose IP risks if the output\nunintentionally replicates a copyrighted scene or character. While Adobe avoids this by training only on licensed data,\nothers might not. So one risk is the potential for a third-party claim: e.g. a background in an AI video looks too\nsimilar to a scene from a known film. The best mitigation is using providers who have clear training sources or\nindemnification. Or limiting use of AI for things where this risk is minimal (unique scenarios rather than known\ncharacters).\nEthical Considerations\n‚Äã\nBeyond legal compliance, ethical use concerns include: not reinforcing biases (if models were trained on biased content,\nthey might output stereotypes ‚Äì careful prompting or model choice is needed to avoid, say, always depicting doctors as\none gender or certain roles in subservient ways, etc., which could slip into outputs unconsciously). Also, there‚Äôs\na‚ÄØhuman element‚ÄØ‚Äì if marketing and comms become too AI-heavy, does it lose authenticity? We will have to find the right\nbalance between automation and human touch. And internally, treating employees openly about use of AI, offering\nre-skilling, etc., is important to maintain morale.\nTechnical Dependency and Evolution\n‚Äã\nIf a business builds processes around a third-party AI tool, they have to manage dependency risk. What if that service\nchanges pricing significantly, or a policy shift (e.g. an AI model decides to disallow certain medical content\ngeneration)? Executing a strategy that involves multiple options or maintaining the ability to switch models quickly is\nhighly advised. Also, file formats and integration: ensuring these AI outputs can integrate into existing video editing\nworkflows smoothly (most give MP4 outputs which is fine, but if you want something like masking info for overlays, not\nall provide that yet ‚Äì Adobe likely will for theirs).\nSecurity and Privacy\n‚Äã\nPrompts and any input data sent to AI cloud services could contain sensitive info (especially if you‚Äôre generating\nsomething based on confidential product data or an unreleased device design). There‚Äôs the usual cloud risk ‚Äì ensure the\nvendor has enterprise agreements if needed. Alternatively, do sensitive work on-prem with open models and sticking to\ngreen data for experimentation.\nPublic Perception\n‚Äã\nAs AI-generated content becomes more common, there may be public skepticism (‚ÄúIs this video real or AI?‚Äù). If misused by\nbad actors, there could be a consumer backlash or calls for regulation. If we begin using it in consumer-facing ways\nshould be prepared to address questions about authenticity. Being on the honest, transparent side will guard brand\nreputation. For instance, if an AI is used to simulate a patient story, one might label it as a ‚Äúdramatization‚Äù which is\na term already used in ads for reenactments. It‚Äôs about not breaking trust.\nIn summary, the opportunities and challenges are significant. They manageable with a proactive approach. The key is\nto‚ÄØnot treat AI video as magic; it‚Äôs a powerful new tool that augments human creativity but still needs human oversight\nand strategic implementation.\nFuture Outlook\n‚Äã\nThe trajectory of AI video generation suggests that what is cutting-edge today could become commonplace and vastly more\ncapable in the next 2‚Äì3 years. Lilly should anticipate and prepare for the following likely developments:\nLonger & Real-Time Generation\n‚Äã\nModels will continue scaling (OpenAI hinted that bigger models = longer videos). We can expect by mid-2026 some AI\nsystems will routinely generate a few minutes of video with coherent storylines. There is even the possibility\nof‚ÄØreal-time‚ÄØor streaming generation ‚Äì imagine AI creating video on the fly as you watch (some research is headed there\nfor live avatar conversations, etc.). This could revolutionize live customer service (AI video agent responding in real\ntime) or live training sessions with dynamic visuals. While true real-time high-res generation might be a bit farther,\nreal-time at lower res may come sooner.\nImproved Fidelity\n‚Äã\nAlready Veo 3‚Äôs people are highly realistic; with more specialized training (and perhaps hybrid approaches like grafting\nAI-generated elements into real video backgrounds), AI videos will reach the point where an average viewer can‚Äôt tell AI\nvs real footage, at least for short sequences. This will blur the line of what content is ‚Äúshot‚Äù vs ‚Äúgenerated‚Äù. For\ncreators, that means tremendous creative freedom (any idea can be visualized without practical constraints) but also\ngreater need for‚ÄØethical guidelines‚ÄØto maintain trust.\nInteractive Video & Multimodality\n‚Äã\nCombining video generation with interactivity (powered by LLMs like GPT-4/5) could yield interactive media. For example,\na user could talk to an AI character who responds with generated video and audio. This could be a new form of engaging\ncontent or education tool. In pharma, a patient could ask an AI ‚ÄúWhat will the surgery be like?‚Äù and get a custom visual\nexplanation video. We‚Äôre moving toward AI that can‚ÄØunderstand scene context‚ÄØdeeply (Gemini is multimodal ‚Äì it might\nalign narrative with visuals tightly). The merging of text, images, audio, and video AI means future tools will handle\nwhole multimedia generation. Google‚Äôs Flow already hints at that synergy (Gemini assisting with consistency across\nshots).\nIndustry-Specific Marketing Models/Tools\n‚Äã\nWe will likely see models fine-tuned for specific domains: e.g. a‚ÄØMedical VideoGen‚ÄØthat knows how to accurately depict\nanatomy, or an‚ÄØArchitectural VideoGen‚ÄØfor real estate walkthroughs. These specialized models could be offered by\nvertical SaaS companies or open communities. For pharma, a model trained on, say, all publicly available MOA animations\nand medical footage could become the go-to for generating regulatory-compliant medical visuals (with knowledge of what\nnot to show because of how the body actually works).\nRegulatory Frameworks\n‚Äã\nGovernments and industry bodies will establish more formal guidelines. We might see something like an ‚ÄúAI Content\nCertification‚Äù where companies can submit AI-generated ads for a special review or to get a certification mark that it‚Äôs\nbeen vetted for accuracy. There‚Äôs also likely to be technology solutions for provenance: cryptographic signing of AI\ncontent so that it can be traced. OpenAI and Adobe already tag content; this could become standardized so that any\nscreen can potentially alert viewers ‚ÄúThis video is AI-generated.‚Äù Lilly should anticipate that transparency may become\nnot just best practice but mandated.\nTalent and Workflow Shifts\n‚Äã\nRoles will shift ‚Äì we might see‚ÄØ‚ÄúAI video prompt scriptwriters‚Äù,‚ÄØ‚ÄúAI ethics reviewers‚Äù, and‚ÄØ‚Äúcontent curators‚Äù‚ÄØas common\njobs. Teams might reorganize such that an AI tool is part of every content meeting (e.g. brainstorm sessions always\ninvolve someone quickly prototyping ideas with AI to show the room). The speed of execution will increase, meaning\ncompetition in marketing might revolve around who can ideate and deploy creative concepts fastest (with AI doing the\nheavy lifting). The flip side is potential content glut ‚Äì so focusing on strategy and creativity (the human part)\nremains vital to stand out.\nCompetitive Landscape\n‚Äã\nIt‚Äôs possible that only a few foundation models end up dominating (like we see in LLMs, a handful of leaders). But many\ncompanies might build on those via fine-tuning. We may see consolidation where big players acquire some startups\n(perhaps OpenAI buys a Pika, or Adobe buys an Runway, purely speculative). Or big tech might all have their offerings\nand split the market (like cloud providers). Keeping flexibility is wise ‚Äìlet‚Äôs not get too locked into since another\nwill likely surpass it. It could be analogous to the early days of web browsers or mobile OS ‚Äì eventually a stable set\nof widely used platforms emerged. Likely a mix of open (Stability) and closed (OpenAI, Google) will persist, each with\npros and cons.\nNew Content Formats\n‚Äã\nAs AI lowers production costs, we might see entirely new forms of media. For instance, personalized short films as a\nservice (you input some info and get a short film about you). Or dynamic video content on websites that adjusts to\nviewer profile. In pharma, maybe dynamic visual aids for reps that change based on the doctor they‚Äôre speaking to (if\nthe doctor is more visual, the AI generates more mechanism animation; if they care about data, it shows charts, etc.,\nall in real time). The boundaries between video, animation, and software could blur.\nConclusion\n‚Äã\nIn conclusion, AI video generation is poised to become a standard tool in the content creation toolbox ‚Äì much like\ndesktop publishing in the 80s or digital video editing in the 2000s. It will not outright replace humans, but those\nwho‚ÄØharness it effectively‚ÄØwill outpace those who don‚Äôt, by producing more tailored and creative content with less\neffort. For the pharmaceutical industry and similar fields, the key will be to integrate these capabilities in a way\nthat‚ÄØenhances communication and understanding‚ÄØ(of complex science, of patient stories) while‚ÄØmaintaining the rigorous\nstandards‚ÄØof accuracy and ethics that the industry demands.\nLilly should take a proactive but careful approach: embrace the innovation, experiment with it, and begin with low-risk\nuse cases with some oversight eyeing the entire spectrum of use cases. By doing so, we can significantly boost its\ncontent innovation capacity and be ready for the future where AI-driven media is ubiquitous. The companies that succeed\nwill likely be those that combine‚ÄØthe creative imagination of their teams‚ÄØwith‚ÄØthe generative power of AI, in a governed\nand purposeful manner ‚Äì turning what used to be costly visual dreams into vivid (and responsible) reality.\nSources\n‚Äã\nVideo generation models as world simulators | OpenAI\nhttps://openai.com/index/video-generation-models-as-world-simulators/\nÔøº\nSora is here | OpenAI]\nhttps://openai.com/index/sora-is-here/\nÔøº\nRunway's Gen-2 shows the limitations of today's text-to-video tech | TechCrunch\nhttps://techcrunch.com/2023/06/09/runways-gen-2-shows-the-limitations-of-todays-text-to-video-tech/\nBest Short-Form AI Video Generator? Kling 2.1 vs Google Veo 3 - Decrypt\nhttps://decrypt.co/323056/best-short-form-ai-video-generator-kling-google-veo\nGenerative Video. Now in Adobe Firefly. : r/premiere\nhttps://www.reddit.com/r/premiere/comments/1inrroo/generative_video_now_in_adobe_firefly/\nFlow\nhttps://labs.google/fx/tools/flow/faq\nVideo generation models as world simulators | OpenAI\nhttps://openai.com/index/video-generation-models-as-world-simulators/\nRunway's Gen-2 Text-to-Video Tool Now Available to Everyone for Free | Tom's Hardware\nhttps://www.tomshardware.com/news/runway-gen-2-text-to-video-available-to-all\nRunway's Gen-2 shows the limitations of today's text-to-video tech | TechCrunch\nhttps://techcrunch.com/2023/06/09/runways-gen-2-shows-the-limitations-of-todays-text-to-video-tech/\nI Tried Minimax AI Video Generator to Generate These Insane Videos: Here is My Honest Review | by Amdad H | Towards AGI\n| Medium\nhttps://medium.com/towards-agi/i-tried-minimax-ai-video-generator-to-generate-these-insane-videos-here-is-my-honest-review-832f5a6e8b7c\nRunway's Gen-2 shows the limitations of today's text-to-video tech | TechCrunch\nhttps://techcrunch.com/2023/06/09/runways-gen-2-shows-the-limitations-of-todays-text-to-video-tech/\nLuma Dream Machine: New Freedoms of Imagination | Luma AI\nhttps://lumalabs.ai/dream-machine\nGenerative Video. Now in Adobe Firefly. : r/premiere\nhttps://www.reddit.com/r/premiere/comments/1inrroo/generative_video_now_in_adobe_firefly/\nBringing generative AI to video with Adobe Firefly Video Model | Adobe Blog\nhttps://blog.adobe.com/en/publish/2024/09/11/bringing-gen-ai-to-video-adobe-firefly-video-model-coming-soon\nVideo generation models as world simulators | OpenAI\nhttps://openai.com/index/video-generation-models-as-world-simulators/\nRunway's Gen-2 shows the limitations of today's text-to-video tech | TechCrunch\nhttps://techcrunch.com/2023/06/09/runways-gen-2-shows-the-limitations-of-todays-text-to-video-tech/\nRunway's Gen-2 shows the limitations of today's text-to-video tech | TechCrunch\nhttps://techcrunch.com/2023/06/09/runways-gen-2-shows-the-limitations-of-todays-text-to-video-tech/\nGoogle launches Veo 3, an AI video generator that incorporates audio\nhttps://www.cnbc.com/2025/05/20/google-ai-video-generator-audio-veo-3.html\nAI video just took a startling leap in realism. Are we doomed?\nhttps://arstechnica.com/ai/2025/05/ai-video-just-took-a-startling-leap-in-realism-are-we-doomed/\nVideo generation API - MiniMax\nhttps://www.minimaxi.com/en/news/video-generation-api\nGenerate videos with Minimax's Hailuo video-01 model - Replicate\nhttps://replicate.com/minimax/video-01\nKling AI Free: Try This AI Video Generator Now! | Pollo AI\nhttps://pollo.ai/m/kling-ai\nPika AI (Pika Art) free: Try Pika Labs AI Video Generator Now | Pollo AI\nhttps://pollo.ai/m/pika-ai\nProfile | China ‚Äògenius girl‚Äô Guo Wenjing, Harvard graduate, co-founder of tech firm backed by US$135 million funding |\nSouth China Morning Post\nhttps://www.scmp.com/news/people-culture/china-personalities/article/3268811/china-genius-girl-guo-wenjing-harvard-graduate-co-founder-tech-firm-backed-us135-million-funding\nPika AI Video Generator: Create B-Roll From Text - Captions\nhttps://www.captions.ai/features/pika-ai-video-generator\nBest AI Video Generator - Comparison ‚Ä¢ Quality & Price Comparison\nhttps://aianimation.com/best-ai-video-generation-platforms/\nPika AI (Pika Art) free: Try Pika Labs AI Video Generator Now | Pollo AI\nhttps://pollo.ai/m/pika-ai\nThe Ultimate Guide to MiniMax Hailuo AI Video Models - Getimg.ai\nhttps://getimg.ai/blog/the-ultimate-guide-to-minimax-hailuo-ai-video-models\nPika AI (Pika Art) free: Try Pika Labs AI Video Generator Now | Pollo AI\nhttps://pollo.ai/m/pika-ai\nBest AI Video Generator - Comparison ‚Ä¢ Quality & Price Comparison\nhttps://aianimation.com/best-ai-video-generation-platforms/\nStable Video Diffusion is awesome! : r/StableDiffusion - Reddit\nhttps://www.reddit.com/r/StableDiffusion/comments/183ync6/stable_video_diffusion_is_awesome/\nStable diffusion video tutorial ‚Äî generate AI video for free - Medium\nhttps://medium.com/@codeandbird/stable-diffusion-video-tutorial-generate-ai-video-for-free-29538ca45ef5\nBringing generative AI to video with Adobe Firefly Video Model | Adobe Blog\\\nhttps://blog.adobe.com/en/publish/2024/09/11/bringing-gen-ai-to-video-adobe-firefly-video-model-coming-soon\nGenerative Video. Now in Adobe Firefly. : r/premiere\nhttps://www.reddit.com/r/premiere/comments/1inrroo/generative_video_now_in_adobe_firefly/\nRunway's Gen-2 shows the limitations of today's text-to-video tech | TechCrunch\nhttps://techcrunch.com/2023/06/09/runways-gen-2-shows-the-limitations-of-todays-text-to-video-tech/\nBringing generative AI to video with Adobe Firefly Video Model | Adobe Blog\nhttps://blog.adobe.com/en/publish/2024/09/11/bringing-gen-ai-to-video-adobe-firefly-video-model-coming-soon\nWas this helpful?\nTags:\ninnovation\nEdit this page\nPrevious\nAI Speech\nExecutive Summary\nIntroduction\nAI Video Generation Technology Overview\nMajor Players and Platforms in AI Video Generation\nOpenAI ‚Äì‚ÄØSora\nRunway ‚Äì‚ÄØGen-2‚ÄØ(and beyond)\nGoogle ‚Äì‚ÄØVeo 3‚ÄØand‚ÄØFlow\nFlow\nKuaishou ‚Äì‚ÄØKling\nPika Labs ‚Äì‚ÄØPika Video\nMiniMax ‚Äì‚ÄØHailuo Video-01\nLuma Labs ‚Äì‚ÄØDream Machine (Ray 2)\nStability AI ‚Äì‚ÄØStable Video Diffusion\nAdobe ‚Äì‚ÄØFirefly Generative Video\nIndustry Impact and Use Cases\nMarketing and Advertising\nLearning & Development / Training\nCommercial and Promotional\nStrategies for Adoption\nChallenges and Risks\nQuality and Consistency\nMisrepresentation and Deepfakes\nRegulatory Compliance\nIntellectual Property and Training Data\nEthical Considerations\nTechnical Dependency and Evolution\nSecurity and Privacy\nPublic Perception\nFuture Outlook\nLonger & Real-Time Generation\nImproved Fidelity\nInteractive Video & Multimodality\nIndustry-Specific Marketing Models/Tools\nRegulatory Frameworks\nTalent and Workflow Shifts\nCompetitive Landscape\nNew Content Formats\nConclusion\nSources\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 1,
    "depth": 2,
    "crawled_at": "2026-02-25T10:06:24.740441"
  },
  "https://techhq.dc.lilly.com/assets/files/Innovation Playbook Final-b8b1722fed38314dc3d8f751b1b5f3ef.pdf": {
    "url": "https://techhq.dc.lilly.com/assets/files/Innovation Playbook Final-b8b1722fed38314dc3d8f751b1b5f3ef.pdf",
    "title": "",
    "description": "",
    "h1": [],
    "h2": [],
    "h3": [],
    "text_content": "",
    "links_found": 0,
    "depth": 2,
    "crawled_at": "2026-02-25T10:06:27.146358"
  },
  "https://techhq.dc.lilly.com/docs/tags/innovation": {
    "url": "https://techhq.dc.lilly.com/docs/tags/innovation",
    "title": "5 docs tagged with \"innovation\" | Tech HQ",
    "description": "",
    "h1": [
      "5 docs tagged with \"innovation\""
    ],
    "h2": [
      "üí° Tech@Lilly Innovation Pipeline",
      "üõû Our Innovation Stages",
      "üõ§Ô∏è EBA Guiding Principles",
      "AI Avatars / Digital People",
      "AI Video Generation"
    ],
    "h3": [],
    "text_content": "5 docs tagged with \"innovation\" | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\n5 docs tagged with \"innovation\"\nView all tags\nüí° Tech@Lilly Innovation Pipeline\nTech Innovation Splash Banner\nüõû Our Innovation Stages\nPipeline Overview\nüõ§Ô∏è EBA Guiding Principles\nSpeed to Value\nAI Avatars / Digital People\n- Last Update: 2025-08-05\nAI Video Generation\n- Last Update: 2025-06-06\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 2,
    "depth": 2,
    "crawled_at": "2026-02-25T10:06:29.346659"
  },
  "https://techhq.dc.lilly.com/docs/plan/lifecycle-management": {
    "url": "https://techhq.dc.lilly.com/docs/plan/lifecycle-management",
    "title": "‚ôªÔ∏è Lifecycle Management | Tech HQ",
    "description": "The LCM process, owned by EBA, is core to managing our declining and exiting technologies across our technical ecosystem. It is essential that we plan and manage with the following characteristics:‚Äã",
    "h1": [
      "‚ôªÔ∏è Lifecycle Management (LCM)"
    ],
    "h2": [
      "2025 LCM‚Äã",
      "Footnotes‚Äã"
    ],
    "h3": [],
    "text_content": "‚ôªÔ∏è Lifecycle Management | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüìÖ Plan Overview\n‚ôªÔ∏è Lifecycle Management\n‚ôªÔ∏è Lifecycle Management\nOn this page\n‚ôªÔ∏è Lifecycle Management (LCM)\nThe LCM process, owned by EBA, is core to managing our declining and exiting technologies across our technical ecosystem. It is essential that we plan and manage with the following characteristics:‚Äã\nAppropriate Technical Leadership (EBA)‚Äã\nCross-Functional Engagement‚Äã\nAccountable Leaderships‚Äã\nVisible Outcomes‚Äã\nProactive Planning‚Äã\nFor questions about LCM or technologies on the LCM roadmap, use the\nLifecycle Management\nMicrosoft Teams channel.\n2025 LCM\n‚Äã\nFor specifics refer the\nApproved 2025 LCM Roadmap\ndeck, including impacted business areas, remediation steps, and funding requirements.\nOrg\nQ1 2025\nQ2 2025\nQ3 2025\nQ4 2025\nCybersecurity\n‚Ä¢\nLegacy Protocol Burndown ‚Äì Wave 2\n‚Ä¢\nGlobal Network Visibility and Protection (GNVP) ‚Äì Proxy Replacement\n‚Ä¢\nCybersecurity Technology Update ‚Äì ‚ÄãCrowdstrike\nLegacy Protocol Burndown ‚Äì Wave 2.5\nCloud\n‚Ä¢\nUbuntu 20 LCM (Apr-30)\n‚Ä¢\nRed Hat 7 LCM\nCollaboration\nKaltura (InVision) (Dec-31)\nMobility\n‚Ä¢\nWindows 11 Feature update\n‚Ä¢\nMicrosoft Edge/Google Chrome Unload event deprecation (Feb-28)\n1\n‚Ä¢\nAdobe Acrobat and Reader upgrade\n‚Ä¢\nSemiannual Java upgrade\nMicrosoft Project and Visio upgrade\nSemiannual Java upgrade\nEnterprise Data\nInformatica PowerCenter 10.4.1 Extended Support (Mar-30)\n2\nSpotfire 12 (Jun-30)\nAxway API Management SaaS (Sep-1)\n‚Ä¢\nOracle (Dec-31)\n3\n‚Ä¢\nInformatica PowerCenter (Dec-31)\n‚Ä¢\nInformatica Intelligence Cloud Services (IICS) (Dec-31)\n‚Ä¢\nBusiness Object v4.3\n‚Ä¢\nLayer 7 API (SOA) Gateway\nFootnotes\n‚Äã\nhttps://developer.chrome.com/docs/web-platform/deprecating-unload\n‚Ü©\nhttps://www.informatica.com/blogs/powercenter-customer-alert-migrate-now-informatica-powercenter-104-support-ends-on-march-31.html\n‚Ü©\nhttps://collab.lilly.com/sites/Oracleenterprisefootprintreduction\n‚Ü©\nWas this helpful?\nEdit this page\nPrevious\nüìÖ Plan Overview\n2025 LCM\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 0,
    "depth": 2,
    "crawled_at": "2026-02-25T10:06:31.544424"
  },
  "https://techhq.dc.lilly.com/docs/solution/ai/": {
    "url": "https://techhq.dc.lilly.com/docs/solution/ai/",
    "title": "ü§ñ AI & Intelligent Agents | Tech HQ",
    "description": "_The AI Section of TechHQ is the primary tool for making AI technology decisions at Lilly. It defines the layers (the",
    "h1": [
      "ü§ñ AI & Intelligent Agents"
    ],
    "h2": [
      "üèóÔ∏è AI Architecture Ecosystem‚Äã",
      "üìÑÔ∏èüßë‚Äçüíª Experience Layer",
      "üìÑÔ∏èüß∞ AI Layer",
      "üìÑÔ∏èüì¶ Model Layer",
      "üìÑÔ∏èüìä Data Layer",
      "üéØ AI Design Patterns‚Äã",
      "üìÑÔ∏èüß† Agentic Patterns",
      "üìÑÔ∏è‚öôÔ∏è Agents and Workflows",
      "üìÑÔ∏èüß© Context Engineering",
      "üìÑÔ∏èüì° Protocols",
      "üìÑÔ∏èüè≠ Model Foundry",
      "üìÑÔ∏è‚òÅÔ∏è Lilly Cloud MCP",
      "üìç Strategic Positioning‚Äã",
      "üìÑÔ∏èüìä Google Positioning",
      "üìÑÔ∏èüìù Prompt Management",
      "üíº Implementation Examples‚Äã",
      "üìÑÔ∏èüíº Press Release Drafter",
      "üìÑÔ∏èüíº Press Release Miner",
      "AI Architecture Ecosystem‚Äã",
      "AI Design Patterns‚Äã",
      "AI Positioning‚Äã",
      "Implementation Examples‚Äã"
    ],
    "h3": [],
    "text_content": "ü§ñ AI & Intelligent Agents | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nEcosystem\nPatterns\nPositioning\nExamples\nCoding Tools\nStandards\nAI Submission Guide Template\nüóÑÔ∏è AI & Intelligent Agents BLT\nü§ñ Agentic AI\nüß≠ Conversational AI\nüß≠ Knowledge Bases\nüß≠ Multimodal AI\nüß≠ Translation Services\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nü§ñ AI & Intelligent Agents\nOn this page\nü§ñ AI & Intelligent Agents\nAI Architecture Framework\nThe AI Section of TechHQ is the primary tool for making AI technology decisions at Lilly. It defines the layers (the\nstack), capabilities, and platforms that are approved for use.\nOwner:\nBrian Lewis\nPoint of Contact:\nAli Kharazmi\nContributors and Reviewers:\nArchit Kaila, Haitham Maya, Malika Mahoui\nTools and platforms that leverage AI for automation, decision-making, and natural interactions.\nThis area provides comprehensive documentation on Lilly's AI architecture,\npatterns, and implementation examples. Use the diagram below to navigate to specific topics of interest.\nüèóÔ∏è AI Architecture Ecosystem\n‚Äã\nüìÑÔ∏è\nüßë‚Äçüíª Experience Layer\nüìÑÔ∏è\nüß∞ AI Layer\nüìÑÔ∏è\nüì¶ Model Layer\nüìÑÔ∏è\nüìä Data Layer\nüéØ AI Design Patterns\n‚Äã\nüìÑÔ∏è\nüß† Agentic Patterns\nüìÑÔ∏è\n‚öôÔ∏è Agents and Workflows\nüìÑÔ∏è\nüß© Context Engineering\nüìÑÔ∏è\nüì° Protocols\nüìÑÔ∏è\nüè≠ Model Foundry\nüìÑÔ∏è\n‚òÅÔ∏è Lilly Cloud MCP\nüìç Strategic Positioning\n‚Äã\nüìÑÔ∏è\nüìä Google Positioning\nüìÑÔ∏è\nüìù Prompt Management\nüíº Implementation Examples\n‚Äã\nüìÑÔ∏è\nüíº Press Release Drafter\nüìÑÔ∏è\nüíº Press Release Miner\nAI Architecture Ecosystem\n‚Äã\nThe AI Architecture Ecosystem defines the layers, capabilities, and platforms approved for use at Lilly. It consists of\nsix interconnected layers:\nExperience Layer\n: Consistent human interfaces for AI-enabled products\nAI Layer\n: Reusable capabilities merging deterministic and probabilistic approaches\nModel Layer\n: Commercial capabilities for ML/LLM with MLOps features\nData Layer\n: AI-enabled data capabilities like vector databases and semantic layers\nInfrastructure Layer\n: Secure compute and storage foundations for regulated environments\nResponsible AI Layer\n: Guardrails for ethical, regulatory, and security standards\nAI Design Patterns\n‚Äã\nAI Design Patterns provide proven approaches to designing and implementing AI solutions:\nAgentic Patterns\n: Architectural approaches for AI agents (Single Agent, ReAct, Plan-and-Execute, etc.)\nWorkflows\n: Distinctions between agents and workflows, with patterns like Prompt Chaining and Routing\nContext Engineering\n: Strategies for optimizing context windows through Write, Select, Compress, and Isolate techniques\nProtocols\n: Communication standards for agent-to-agent (A2A) and agent-tool (MCP) interactions\nLilly Cloud MCP\n: Implementation of Model Context Protocol servers for Lilly systems\nAI Positioning\n‚Äã\nAI Positioning defines the strategic approach to AI technologies at Lilly:\nGoogle Positioning\n: Strategic approach to Google's AI technologies and services\nImplementation Examples\n‚Äã\nPractical examples of AI implementations across different use cases:\nPress Release Drafter (PRD)\n: Agentic AI system for automating press release generation from clinical trial results\nwith multi-agent reasoning and MLR compliance\nPress Release Miner (PRM)\n: Multi-agent research system for extracting insights from documents and generating\nbriefing materials using ReAct and parallel processing patterns\nWas this helpful?\nTags:\nai\nEdit this page\nPrevious\nüß© Solution Overview\nNext\nEcosystem\nüèóÔ∏è AI Architecture Ecosystem\nüéØ AI Design Patterns\nüìç Strategic Positioning\nüíº Implementation Examples\nAI Architecture Ecosystem\nAI Design Patterns\nAI Positioning\nImplementation Examples\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 36,
    "depth": 2,
    "crawled_at": "2026-02-25T10:06:34.680969"
  },
  "https://techhq.dc.lilly.com/docs/solution/business-enablement/": {
    "url": "https://techhq.dc.lilly.com/docs/solution/business-enablement/",
    "title": "üè¢ Business Enablement | Tech HQ",
    "description": "Solutions that streamline core business processes, enhance compliance, and improve operational efficiency across",
    "h1": [
      "üè¢ Business Enablement"
    ],
    "h2": [],
    "h3": [],
    "text_content": "üè¢ Business Enablement | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nüè¢ Business Enablement\nüóÑÔ∏è Business Enablement BLT\nüß≠ AE/PC Detection\nüß≠ Customer Engagement Reference Architecture\nüß≠ PI & Sensitive Data Detection\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nüè¢ Business Enablement\nüè¢ Business Enablement\nSolutions that streamline core business processes, enhance compliance, and improve operational efficiency across\nenterprise functions.\nComing Soon!\nOur business enablement guides are in progress. Have something to add? Find out how to\nContribute\n!\nWas this helpful?\nEdit this page\nPrevious\nüß≠ Translation Services\nNext\nüóÑÔ∏è Business Enablement BLT\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 14,
    "depth": 2,
    "crawled_at": "2026-02-25T10:06:36.893972"
  },
  "https://techhq.dc.lilly.com/docs/solution/cloud/": {
    "url": "https://techhq.dc.lilly.com/docs/solution/cloud/",
    "title": "‚òÅÔ∏è Cloud & Infrastructure | Tech HQ",
    "description": "Technologies that power scalable, secure, and resilient IT environments, including cloud platforms, networking, storage,",
    "h1": [
      "‚òÅÔ∏è Cloud & Infrastructure"
    ],
    "h2": [],
    "h3": [],
    "text_content": "‚òÅÔ∏è Cloud & Infrastructure | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüóÑÔ∏è Cloud & Infrastructure BLT\nüß≠ Cloud Architecture\nüß≠ Cloud FinOps\nüß≠ Container Services\nüß≠ Networking Services\nüß≠ Workflow & Orchestration\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\n‚òÅÔ∏è Cloud & Infrastructure\n‚òÅÔ∏è Cloud & Infrastructure\nTechnologies that power scalable, secure, and resilient IT environments, including cloud platforms, networking, storage,\nand orchestration.\nMore Coming Soon!\nHave something to add? Find out how to\nContribute\n!\nWas this helpful?\nEdit this page\nPrevious\nüß≠ PI & Sensitive Data Detection\nNext\nüóÑÔ∏è Cloud & Infrastructure BLT\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 15,
    "depth": 2,
    "crawled_at": "2026-02-25T10:06:39.077904"
  },
  "https://techhq.dc.lilly.com/docs/solution/cybersecurity/": {
    "url": "https://techhq.dc.lilly.com/docs/solution/cybersecurity/",
    "title": "üîê Cybersecurity | Tech HQ",
    "description": "Capabilities that protect systems, data, and identities through security controls, threat detection, and compliance",
    "h1": [
      "üîê Cybersecurity"
    ],
    "h2": [],
    "h3": [],
    "text_content": "üîê Cybersecurity | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüóÑÔ∏è Cybersecurity BLT\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nüîê Cybersecurity\nüîê Cybersecurity\nCapabilities that protect systems, data, and identities through security controls, threat detection, and compliance\nmeasures to safeguard the enterprise from cyber risks.\nComing Soon!\nOur cybersecurity guides are in progress. Have something to add? Find out how to\nContribute\n!\nWas this helpful?\nEdit this page\nPrevious\nüß≠ Workflow & Orchestration\nNext\nüóÑÔ∏è Cybersecurity BLT\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 9,
    "depth": 2,
    "crawled_at": "2026-02-25T10:06:41.361931"
  },
  "https://techhq.dc.lilly.com/docs/solution/data/": {
    "url": "https://techhq.dc.lilly.com/docs/solution/data/",
    "title": "üìä Data & Analytics | Tech HQ",
    "description": "Platforms and tools for managing, analyzing, and visualizing data to generate insights, support decision-making, and",
    "h1": [
      "üìä Data & Analytics"
    ],
    "h2": [],
    "h3": [],
    "text_content": "üìä Data & Analytics | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüóÑÔ∏è Data & Analytics BLT\nüß≠ Data Catalogs and Access\nüß≠ Data Visualization\nüß≠ Integration Middleware\nüß≠ Specialized Databases\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nüìä Data & Analytics\nüìä Data & Analytics\nPlatforms and tools for managing, analyzing, and visualizing data to generate insights, support decision-making, and\nenable advanced analytics.\nComing Soon!\nOur data & analytics guides are in progress. Have something to add? Find out how to\nContribute\n!\nWas this helpful?\nEdit this page\nPrevious\nüóÑÔ∏è Cybersecurity BLT\nNext\nüóÑÔ∏è Data & Analytics BLT\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 12,
    "depth": 2,
    "crawled_at": "2026-02-25T10:06:43.519651"
  },
  "https://techhq.dc.lilly.com/docs/solution/engineering/": {
    "url": "https://techhq.dc.lilly.com/docs/solution/engineering/",
    "title": "üõ†Ô∏è Engineering Enablement | Tech HQ",
    "description": "Developer-focused tools and practices that accelerate software delivery, including DevOps, CI/CD, automation, and MLOps.",
    "h1": [
      "üõ†Ô∏è Engineering Enablement"
    ],
    "h2": [],
    "h3": [],
    "text_content": "üõ†Ô∏è Engineering Enablement | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüóÑÔ∏è Engineering Enablement BLT\nüß≠ AI Code Assistants\nAPI Standards\nüß≠ ML Ops\nüß≠ Test Automation\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nüõ†Ô∏è Engineering Enablement\nüõ†Ô∏è Engineering Enablement\nDeveloper-focused tools and practices that accelerate software delivery, including DevOps, CI/CD, automation, and MLOps.\nComing Soon!\nOur software engineering guides are in progress. Have something to add? Find out how to\nContribute\n!\nWas this helpful?\nEdit this page\nPrevious\nüß≠ Specialized Databases\nNext\nüóÑÔ∏è Engineering Enablement BLT\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 11,
    "depth": 2,
    "crawled_at": "2026-02-25T10:06:45.746342"
  },
  "https://techhq.dc.lilly.com/docs/solution/observability/": {
    "url": "https://techhq.dc.lilly.com/docs/solution/observability/",
    "title": "üõ∞Ô∏è Observability & Reliability | Tech HQ",
    "description": "Practices and platforms that ensure system health and resilience, including telemetry (logs, metrics, traces),",
    "h1": [
      "üõ∞Ô∏è Observability & Reliability"
    ],
    "h2": [],
    "h3": [],
    "text_content": "üõ∞Ô∏è Observability & Reliability | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüóÑÔ∏è Observability & Reliability BLT\nInfrastructure Performance Monitoring\nLog Management & Analytics\nNetwork Monitoring\nüß≠ Resiliency & Disaster Recovery\nüöÄ Team Productivity\nüé® User Experience & Design\nüõ∞Ô∏è Observability & Reliability\nüõ∞Ô∏è Observability & Reliability\nPractices and platforms that ensure system health and resilience, including telemetry (logs, metrics, traces),\nSLO-driven reliability engineering, resiliency patterns (HA/DR), and AIOps for automated incident detection and\nresponse.\nMore Coming Soon!\nHave something to add? Find out how to\nContribute\n!\nWas this helpful?\nEdit this page\nPrevious\nüß≠ Test Automation\nNext\nüóÑÔ∏è Observability & Reliability BLT\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 10,
    "depth": 2,
    "crawled_at": "2026-02-25T10:06:47.931004"
  },
  "https://techhq.dc.lilly.com/docs/solution/productivity/": {
    "url": "https://techhq.dc.lilly.com/docs/solution/productivity/",
    "title": "üöÄ Team Productivity | Tech HQ",
    "description": "Solutions to enhance communication, teamwork, and efficiency across the digital workplace, from project management",
    "h1": [
      "üöÄ Team Productivity"
    ],
    "h2": [],
    "h3": [],
    "text_content": "üöÄ Team Productivity | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüóÑÔ∏è Team Productivity BLT\nüß≠ Diagramming\nüé® User Experience & Design\nüöÄ Team Productivity\nüöÄ Team Productivity\nSolutions to enhance communication, teamwork, and efficiency across the digital workplace, from project management\nto document collaboration.\nMore Coming Soon!\nHave something to add? Find out how to\nContribute\n!\nWas this helpful?\nEdit this page\nPrevious\nüß≠ Resiliency & Disaster Recovery\nNext\nüóÑÔ∏è Team Productivity BLT\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 6,
    "depth": 2,
    "crawled_at": "2026-02-25T10:06:50.127705"
  },
  "https://techhq.dc.lilly.com/docs/solution/ux/": {
    "url": "https://techhq.dc.lilly.com/docs/solution/ux/",
    "title": "üé® User Experience & Design | Tech HQ",
    "description": "Frameworks and tools for creating accessible, intuitive, and visually engaging digital experiences, including design",
    "h1": [
      "üé® User Experience & Design"
    ],
    "h2": [],
    "h3": [],
    "text_content": "üé® User Experience & Design | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nüóÑÔ∏è User Experience & Design BLT\nüß≠ Accessibility Enablement\nüé® User Experience & Design\nüé® User Experience & Design\nFrameworks and tools for creating accessible, intuitive, and visually engaging digital experiences, including design\nsystems and prototyping platforms.\nComing Soon!\nOur UX, accessibility, and design guides are in progress. Have something to add? Find out how to\nContribute\nhere!\nWas this helpful?\nEdit this page\nPrevious\nüß≠ Diagramming\nNext\nüóÑÔ∏è User Experience & Design BLT\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 5,
    "depth": 2,
    "crawled_at": "2026-02-25T10:06:52.328488"
  },
  "https://techhq.dc.lilly.com/docs/contribute": {
    "url": "https://techhq.dc.lilly.com/docs/contribute",
    "title": "ü§≤ TechHQ Contributions | Tech HQ",
    "description": "Thank you for your interest in contributing to Tech@Lilly‚Äôs technical guidance and AI knowledge base ‚Äî your input is",
    "h1": [
      "ü§≤ TechHQ Contributions"
    ],
    "h2": [
      "Solution Guide Contributions‚Äã",
      "Source Code Contributions‚Äã",
      "Running TechHQ Locally‚Äã"
    ],
    "h3": [
      "Prerequisites‚Äã",
      "Development Builds‚Äã",
      "Production Testing‚Äã"
    ],
    "text_content": "ü§≤ TechHQ Contributions | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nü§≤ TechHQ Contributions\nüìù Markdown Extensions\nüß≠ Solution Guide Template\nüìÖ Solution Guide Timeline\nü§≤ TechHQ Contributions\nOn this page\nü§≤ TechHQ Contributions\nThank you for your interest in contributing to Tech@Lilly‚Äôs technical guidance and AI knowledge base ‚Äî your input is\ntruly appreciated!\nTechHQ embraces a collaborative model inspired by wikis and open source communities. In our case, we follow an\ninner source\napproach, where everyone is\nencouraged to share, improve, and build together across teams.\nWhat should you put on TechHQ?\nTechnical guidance on TechHQ should be sufficiently complete to aid architects and detailed enough for AI usage. To keep\nour technical knowledge DRY (Don't Repeat Yourself), avoid repeating content already published elsewhere; instead, link\nto important external resources.\nSolution Guide Contributions\n‚Äã\nSolution guides are now synced daily from Stack Overflow articles.\nTo contribute:\nSign up to own a guide in our\nTechHQ Solution Guides spreadsheet\n, which is on our\nTechHQ Teams site\n.\nUse the\nTechHQ Solution Guide Drafter\n(Copilot AI) to start your first draft.\nMost TechHQ Solution Guides have a Use Case / Technology table;\nContainer Services\nis a good example. Other guides are\nsimply a high-level overview of what architects should know, such as\nResiliency & Disaster Recovery‚Äã\n.\nCopy the draft markdown into a\nnew Stack Overflow article\n.\nFor consistency, prefix the guide name with üß≠, and be sure to use the INFO block at the top, as seen in the\nSolution Guide Template\n.\nAdd three tags to the article:\ntechhq\n,\nsolution-guide\n, and one the following:\nai\n- Guidance for generative AI\nand LLMs\nbusiness-enablement\n-\nGuidance for our line-of-business platforms and capabilities, e.g. clinical operations, regulatory, manufacturing,\nsupply chain, commercial\ncloud\n- Guidance for cloud\n(public, private, IaaS, PaaS, & SaaS)\ncybersecurity\n- Guidance for\ncybersecurity, information security, and security best practices\ndata\n- Guidance for data to\ninsights guides, analytics, databases, and data science\nsoftware-engineering\n-\nGuidance for CI/CD, automation, infrastructure as code, and deployment practices\nobservability\n-\nGuidance for monitoring, logging, tracing, and application performance management\nproductivity\n-\nGuidance for tools and practices that enhance individual and team efficiency\nux\n- Guidance for user\nexperience design, usability, accessibility, and interface best practices\nYour solution guide article will be automatically be organized into TechHQ.\nSource Code Contributions\n‚Äã\nMinor, individual file edits\ncan be made directly to the dev branch. These edits will be automatically deployed to\nhttps://techhq-d.dc.lilly.com\n.\nAll edits\nwill be reviewed prior to prod deployments. Reviewers will vary based on\nthe change. Commit messages must capture the change rationale.\nFor new docs drafts and multi-doc edits\n, create a branch with the prefix\ndraft/\n. Open a PR to dev when ready.\nFor new site features\n, create a branch with the prefix\nfeatures/\n. Open a PR to dev when ready.\nNeither draft/ nor feature/ branches are automatically deployed. 'Automatically delete head branches' is not enabled;\ndelete your branch if it is complete or inactive.\nRunning TechHQ Locally\n‚Äã\nPrerequisites\n‚Äã\nNode.js\npnpm\nDevelopment Builds\n‚Äã\nThe dev server automatically reflects most changes without restart.\n# build & run dev\npnpm install\npnpm dev\n# refresh content from Stack Overflow\npnpm stackoverflow-sync\n# lint & typecheck your code!\npnpm eslint\npnpm tsc\nProduction Testing\n‚Äã\nThe prod server has indexed search and caching for performance.\npnpm install\npnpm build\npnpm serve\nWas this helpful?\nEdit this page\nNext\nüìù Markdown Extensions\nSolution Guide Contributions\nSource Code Contributions\nRunning TechHQ Locally\nPrerequisites\nDevelopment Builds\nProduction Testing\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 5,
    "depth": 2,
    "crawled_at": "2026-02-25T10:06:54.531786"
  },
  "https://techhq.dc.lilly.com/docs/learn/guiding-principles": {
    "url": "https://techhq.dc.lilly.com/docs/learn/guiding-principles",
    "title": "üõ§Ô∏è EBA Guiding Principles | Tech HQ",
    "description": "Speed to Value",
    "h1": [
      "üõ§Ô∏è EBA Guiding Principles"
    ],
    "h2": [
      "Speed to Value‚Äã",
      "Enterprise Scale‚Äã",
      "Tech Innovation‚Äã",
      "Footnotes‚Äã"
    ],
    "h3": [],
    "text_content": "üõ§Ô∏è EBA Guiding Principles | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüìö Learn\nüõ§Ô∏è EBA Guiding Principles\nüìê Tech@Lilly Well-architected\nüõ§Ô∏è EBA Guiding Principles\nOn this page\nüõ§Ô∏è EBA Guiding Principles\nSpeed to Value\n‚Äã\nDelightful Customer Outcomes\nWe deliver measurable results that\nexceed customer expectations\n, creating an\nexceptional and delightful experience at every touchpoint.\n1\nEntrepreneurial Spirit\nWe empower every team member to\nthink and act like an entrepreneur\n, envisioning\nfuture opportunities and adapting strategies to changing circumstances.\n2\nUninterrupted Value Flow\nWe deliver\ncontinuous value to our customers\nby using agile methods and iterative\nimprovements to quickly adapt and consistently provide high-quality outcomes.\n3\nEnterprise Scale\n‚Äã\nEnd-to-End Thinking\nWe view our\norganization as an integrated system\n, ensuring that people, processes, and\ntechnology work together harmoniously to enhance collaboration, streamline operations, and achieve our mission.\n4\nComposable & Adaptable\nWe design systems and processes for\neasy reuse and recombination\n, maximizing\nefficiency and accelerating value delivery so we may focus on differentiating outcomes and experiences.\n5\nAntifragile Resilience\nWe build systems and processes that not only\nwithstand and adapt to disruptions\nbut\nalso grow stronger from them, ensuring our organization remains robust, responsive, and flexible to change.\n6\nTech Innovation\n‚Äã\nBold Transformation\nWe embrace boldness in our actions and decisions, encouraging\ninnovative thinking\nand\ntaking calculated risks to drive transformative change and achieve extraordinary outcomes.\n7\nInclusive Innovation\nWe foster an open, collaborative environment where there is no monopoly on innovation,\nencouraging\ndiverse perspectives\nto drive breakthrough solutions and create inclusive, equitable opportunities for\nall participants.\n8\nContinuous Learning\nWe cultivate a\nculture of curiosity\nand ongoing personal development, encouraging our\nteams to seek new knowledge and apply the latest advancements to stay at the forefront of innovation.\n9\nFootnotes\n‚Äã\nUXDesign - Design for Meaningful Outcomes\n‚Ü©\nMIT Sloan - 3 Traits of an Entrepreneurial Mindset\nMedium - Airbnb: A Startling Tale of Reinvention that Embodies the True Spirit of Entrepreneurship\n‚Ü©\nSAFe - Make Value Flow Without Interruptions\n‚Ü©\nSAFe - Apply Systems Thinking\nMedium - Systems Thinking in Design\n‚Ü©\nGartner - The Future of Business is Composable\nContentstack - Define composability: Building adaptable digital ecosystems\n‚Ü©\nDevOps.com - How to Build Anti-Fragile Software Ecosystems\n‚Ü©\nHBR - Become More Comfortable Making Bold Decisions\n‚Ü©\nForbes - Inclusion Drives Innovation: The Power Of Diverse Perspectives\n‚Ü©\nHBR - Make Learning a Part of Your Daily Routine\n‚Ü©\nWas this helpful?\nTags:\ninnovation\nEdit this page\nPrevious\nüìö Learn\nNext\nüìê Tech@Lilly Well-architected\nSpeed to Value\nEnterprise Scale\nTech Innovation\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 2,
    "depth": 2,
    "crawled_at": "2026-02-25T10:06:56.730949"
  },
  "https://techhq.dc.lilly.com/docs/learn/well-architected": {
    "url": "https://techhq.dc.lilly.com/docs/learn/well-architected",
    "title": "üìê Tech@Lilly Well-architected | Tech HQ",
    "description": "Blueprints for Success",
    "h1": [
      "üìê Tech@Lilly Well-architected"
    ],
    "h2": [
      "Blueprints for Success‚Äã",
      "Well-Architected Pillars‚Äã",
      "Industry-leading Frameworks‚Äã"
    ],
    "h3": [
      "AWS Well-Architected‚Äã",
      "Azure Well-Architected Framework‚Äã",
      "Google Cloud Architecture Framework‚Äã",
      "Salesforce Well-architected‚Äã"
    ],
    "text_content": "üìê Tech@Lilly Well-architected | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüìö Learn\nüõ§Ô∏è EBA Guiding Principles\nüìê Tech@Lilly Well-architected\nüìê Tech@Lilly Well-architected\nOn this page\nüìê Tech@Lilly Well-architected\nBlueprints for Success\n‚Äã\nIn today's tech landscape, elevating our systems is crucial to achieve scale, speed, and innovation. A well-architected\nframework helps us design, deploy, and manage our digital workloads effectively, ensuring reliability, security, and\ncost-efficiency. Start evaluating digital architecture against best practices to unlock greater value for our business\nand customers today!\nWell-Architected Pillars\n‚Äã\nOur architecture leverages well-architected frameworks from Microsoft, AWS, and Google. These frameworks share common\npillars that help us design, deploy, and manage systems effectively, whether they are cloud-based, on-premises, hybrid,\nor SaaS. By following these pillars, we ensure our systems are robust, secure, and efficient.\nOperational Excellence\nRun and monitor systems to deliver business value and continuously improve processes. Automate changes, respond to\nevents, and define standards for daily operations.\nSecurity, Privacy & Compliance\nProtect information, systems, and assets. Implement strong identity and access management, safeguard data, maintain\ndata integrity, and establish controls to detect security and compliance events.\nReliability\nEnsure workloads perform their intended functions correctly and consistently. Design systems to recover from\nfailures, meet customer demands, and mitigate disruptions.\nPerformance Efficiency\nOptimize the use of IT and computing resources to maintain performance. Select the right resource types and sizes,\nmonitor performance, and make informed decisions to maintain efficiency as business needs evolve.\nCost Optimization\nManage costs to maximize value delivery. Understand and control spending, select cost-effective resources, and\nscale to meet business needs without overspending.\nSustainability\n(\nan emerging pillar, especially for computationally-intensive AI workloads\n)\nMinimize the environmental impact of cloud workloads. Design and operate systems to reduce energy consumption and\nimprove efficiency, supporting long-term sustainability goals.\nIndustry-leading Frameworks\n‚Äã\nTo evaluate and improve your system's architecture, start with\none\nof these frameworks. Since AWS, Azure, and Google's\nwell-architected frameworks are so similar, select one based on team comfort and interest. If this is your first time,\nwe recommend starting with the\nAWS Well-architected Tool\n, which will\nask you specific questions and record your decisions for future reference.\nAWS Well-Architected\n‚Äã\nAzure Well-Architected Framework\n‚Äã\nGoogle Cloud Architecture Framework\n‚Äã\nSalesforce Well-architected\n‚Äã\nWas this helpful?\nEdit this page\nPrevious\nüõ§Ô∏è EBA Guiding Principles\nBlueprints for Success\nWell-Architected Pillars\nIndustry-leading Frameworks\nAWS Well-Architected\nAzure Well-Architected Framework\nGoogle Cloud Architecture Framework\nSalesforce Well-architected\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 0,
    "depth": 2,
    "crawled_at": "2026-02-25T10:06:58.908479"
  },
  "https://techhq.dc.lilly.com/docs/contribute/markdown_examples": {
    "url": "https://techhq.dc.lilly.com/docs/contribute/markdown_examples",
    "title": "üìù Markdown Extensions | Tech HQ",
    "description": "This page lists examples of extensions to Markdown that TechHQ supports.",
    "h1": [
      "üìù Markdown Extensions"
    ],
    "h2": [
      "Mermaid‚Äã",
      "Markmap‚Äã",
      "Font Awesome icons‚Äã",
      "External link highlights‚Äã",
      "Additional admonitions‚Äã",
      "Columns and cards‚Äã"
    ],
    "h3": [
      "Lorem Ipsum",
      "Lorem Ipsum",
      "Lorem Ipsum",
      "Lorem Ipsum",
      "Lorem Ipsum",
      "Lorem Ipsum"
    ],
    "text_content": "üìù Markdown Extensions | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nü§≤ TechHQ Contributions\nüìù Markdown Extensions\nüß≠ Solution Guide Template\nüìÖ Solution Guide Timeline\nüìù Markdown Extensions\nOn this page\nüìù Markdown Extensions\nThis page lists examples of extensions to Markdown that TechHQ supports.\nnote\nThe page will generate a known error when run in dev mode!\nMermaid\n‚Äã\nMermaid diagramming & charting\nTechHQ\nDocusaurus\nserver\npod\nGitHub\nTechHQ\ncodebase\nGitHub\nActions\nDocker\nimage\nCATS\nArgo\nCD\nUser\nvisits\nTechHQ\nwebsite\nLilly\nAWS\nCATS\nEKS\ncluster\nTechHQ_Prod\nnamespace\nMarkmap\n‚Äã\nMarkmap mindmaps\nKnown Markmap Bug\nMarkmap generates a known exception on live\npage reload in dev environments:\nFailed to read the 'value' property from 'SVGLength': Could not resolve relative\nlength.\nThis bug is not present in prod.\nClick anywhere in this box interact with the mindmap\nDeliver a Digital Core Marketplace with self-service products, reusable components, and cutting-edge platforms to\nimprove cycle time and value\nObjective\n% of Digital Core services covered\nUsed by % of new product initiatives\nInternal NPS\nUsed for % of Digital Core engagements\nLeading indicators & Measures\n40% of Marketplace users report 2 months saved each time they use a Marketplace product, component, or platform\nEnhance partner understanding of available capabilities and tech investments\nPlanned Outcomes\nTechHQ\nGet IT Help on LillyNow\nDiscovering Tech@Lilly\nServiceNow Catalog\nServiceNow Knowledge Base\nChatNow\nEcosystem\nQ1 '25 Priorities - TechHQ Decision Guidance - Digital Core Quarterly Roadmaps & Milestones\nDigital Core Marketplace\nmarkmap\nFont Awesome icons\n‚Äã\nExample FA GitHub icon\nExternal link highlights\n‚Äã\nGitHub\nLillyFlow\nTech@Lilly on Viva Engage\nAdditional admonitions\n‚Äã\nIn addition to Docusaurus' built-in\nadmonitions\n,\ncommunity extensions\nare also\nincluded.\nnote\nBeta is the stage where the innovation\nis ready to be used for its primary intended purpose.\nWarning!\nThis is a security warning\ncaution\nCaution Caution!\nColumns and cards\n‚Äã\nLorem Ipsum\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Quis ipsum suspendisse ultrices gravida.\nSee All\nLorem Ipsum\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Quis ipsum suspendisse ultrices gravida.\nSee All\nLorem Ipsum\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Quis ipsum suspendisse ultrices gravida.\nSee All\nLorem Ipsum\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Quis ipsum suspendisse ultrices gravida.\nSee All\nLorem Ipsum\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Quis ipsum suspendisse ultrices gravida.\nSee All\nLorem Ipsum\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Quis ipsum suspendisse ultrices gravida.\nSee All\nWas this helpful?\nEdit this page\nPrevious\nü§≤ TechHQ Contributions\nNext\nüß≠ Solution Guide Template\nMermaid\nMarkmap\nFont Awesome icons\nExternal link highlights\nAdditional admonitions\nColumns and cards\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 3,
    "depth": 2,
    "crawled_at": "2026-02-25T10:07:01.292042"
  },
  "https://techhq.dc.lilly.com/docs/contribute/solution-guide-template": {
    "url": "https://techhq.dc.lilly.com/docs/contribute/solution-guide-template",
    "title": "üß≠ Solution Guide Template | Tech HQ",
    "description": "INFO Solution Guide",
    "h1": [
      "üß≠ Solution Guide Template"
    ],
    "h2": [
      "[Technology 1]‚Äã",
      "[Technology 2]‚Äã"
    ],
    "h3": [],
    "text_content": "üß≠ Solution Guide Template | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nü§≤ TechHQ Contributions\nüìù Markdown Extensions\nüß≠ Solution Guide Template\nüìÖ Solution Guide Timeline\nüß≠ Solution Guide Template\nOn this page\nüß≠ Solution Guide Template\nINFO Solution Guide\nLifecycle: Draft\nLast Update: yyyy-mm-dd\nCapability Owner: [Name]\nEBA Lead: [Name]\nContributors & Reviewers: [List of names]\nA guide for brief description of the solution guide, and the use cases, technologies and capabilities it supports. This\nis also an example of how to format a solution guide for Stack Overflow that will be synchronized to TechHQ. To support\nDocusaurus admonitions\n, blockquotes are used instead.\nTIP\nYou can use the\nTechHQ Solution Guide Drafter\nto draft these!\nUse Case/Scenario\nTech Recommendation\nPositioning\nComplexity\nKey Integrations\nOwning Org/Team\nNext Step\n[Use_Case_A]\n[Technology 1]\n[Strategic/Standard/Specialized/Emerging/Declining]\nLow/Medium/High\n[Key Integrations]\n[Org/Team Name]\n[Next Steps]\n[Use_Case_B]\n[Technology 2]\n[Strategic/Standard/Specialized/Emerging]\nLow/Medium/High\n[Key Integrations]\n[Org/Team Name]\n[Next Steps]\nWARNING\n[Any cautionary guidance or anti-patterns.]\n[Technology 1]\n‚Äã\nImportant information to know about Technology 1.\n[Technology 2]\n‚Äã\nImportant information to know about Technology 2.\nWas this helpful?\nEdit this page\nPrevious\nüìù Markdown Extensions\nNext\nüìÖ Solution Guide Timeline\n[Technology 1]\n[Technology 2]\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 2,
    "depth": 2,
    "crawled_at": "2026-02-25T10:07:03.491742"
  },
  "https://techhq.dc.lilly.com/docs/contribute/solution-guide-timeline": {
    "url": "https://techhq.dc.lilly.com/docs/contribute/solution-guide-timeline",
    "title": "üìÖ Solution Guide Timeline | Tech HQ",
    "description": "Our goal is to deliver each solution guide in less than 8 weeks, i.e. roughly 4 sprints. We'll use a",
    "h1": [
      "üìÖ Solution Guide Timeline"
    ],
    "h2": [
      "Working Group‚Äã",
      "First Draft‚Äã",
      "Public Draft Refinement‚Äã",
      "Request For Comment‚Äã",
      "Living Document‚Äã"
    ],
    "h3": [],
    "text_content": "üìÖ Solution Guide Timeline | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nü§≤ TechHQ Contributions\nüìù Markdown Extensions\nüß≠ Solution Guide Template\nüìÖ Solution Guide Timeline\nüìÖ Solution Guide Timeline\nOn this page\nüìÖ Solution Guide Timeline\nOur goal is to deliver each solution guide in less than 8 weeks, i.e. roughly 4 sprints. We'll use a\nJira TECHHQ project\nto track\npriorities and progress.\n0w   -   1w\nAssemble   Working\nGroup\nTeam   Mobilized\n1w   -   2w\nFirst   Draft\nOutline   Use   Cases,\nTech,   &   Structure\nPublish   to\ntechhq-d.dc.lilly.com\n2w   -   4w\nPublic   Draft\nRefinement\nSME   Input   &\nIteration\nPromote   to\ntechhq.dc.lilly.com\n4w   -   6w\nRequest   For\nComment\nFeedback   Collection\n6w   -   ongoing\nLiving   Document\nPeriodic   Updates\nSolution Guide Delivery Lifecycle\nWorking Group\n‚Äã\nThe 'Directly Responsible' solution guide owner assembles the team to kickstart the creation of the solution guide. This\nstage involves gathering the necessary experts and stakeholders to ensure a comprehensive and well-rounded guide. We\nrecommend scheduling a standing meeting cadence for the drafting process.\nFirst Draft\n‚Äã\nThe team outlines the use cases and technologies relevant to the solution guide. This stage involves initial research\nand documentation to create a foundational draft. The first draft should be checked in and published to the TechHQ\nnon-prod environment.\nPublic Draft Refinement\n‚Äã\nSubject matter experts (SMEs) provide input and iterate on the draft. This stage focuses on refining the content to\nensure accuracy and relevance. The Public Draft should be promoted to TechHQ Prod by removing the\ndraft:\ntrue\nor\nunlisted: true\nmetadata.\nRequest For Comment\n‚Äã\nCrucial feedback is gathered from stakeholders and other relevant parties. This stage involves collecting and\nincorporating feedback to enhance the guide's comprehensiveness and usability.\nLiving Document\n‚Äã\nOnce all feedback has been incorporated and the guide is finalized, it becomes a Living Document. This status indicates\nthat the guide will be continuously updated and revised as needed to remain current and relevant.\nWas this helpful?\nEdit this page\nPrevious\nüß≠ Solution Guide Template\nWorking Group\nFirst Draft\nPublic Draft Refinement\nRequest For Comment\nLiving Document\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 0,
    "depth": 2,
    "crawled_at": "2026-02-25T10:07:05.662339"
  },
  "https://techhq.dc.lilly.com/blog/2025/06/25/summer-updates": {
    "url": "https://techhq.dc.lilly.com/blog/2025/06/25/summer-updates",
    "title": "Summer 2025 We've expanded our [Solution Guide Library](/docs/solution/ai/) with new guides including Knowledge Sharing,pdates | Tech HQ",
    "description": "It‚Äôs been an eventful few months since our February updates roll-up! The TechHQ platform has evolved significantly with",
    "h1": [
      "Summer 2025 We've expanded our [Solution Guide Library](/docs/solution/ai/) with new guides including Knowledge Sharing,pdates"
    ],
    "h2": [],
    "h3": [
      "2025",
      "2024"
    ],
    "text_content": "Summer 2025 We've expanded our [Solution Guide Library](/docs/solution/ai/) with new guides including Knowledge Sharing,pdates | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nRecent posts\n2025\nSummer 2025 We've expanded our [Solution Guide Library](/docs/solution/ai/) with new guides including Knowledge Sharing,pdates\nFebruary Updates Roll-up\n2024\nTranslations Solution Guide\nWelcome!\nSummer 2025 We've expanded our [Solution Guide Library](/docs/solution/ai/) with new guides including Knowledge Sharing,pdates\nJune 25, 2025\n¬∑\n2 min read\nKarl Mayer\nDigital Core Lead Architect\nIt‚Äôs been an eventful few months since our February updates roll-up! The TechHQ platform has evolved significantly with\nmajor additions to our AI architecture ecosystem, enhanced innovation pipeline framework, and expanded solution guide\nlibrary. Here's what's new this summer:\nThe highlight is the launch of our\nAI Architecture Ecosystem\n, authored by the EBA AI Architecture\nteam. This framework introduces a four-layer stack‚ÄîExperience, AI, Model, and Data‚Äîalong with reusable agentic AI\npatterns and memory architectures. We‚Äôve also clarified our strategic AI platform positioning, with Cortex as our\nprimary GenAI platform.\nThe\nInnovation Pipeline\nis now easier to navigate thanks to a new Innovation Stage\nAssessment Table, helping teams define and assess the maturity of their work‚Äîfrom Pre-Alpha to General Release. Several\ncapabilities are currently in Pre-Alpha, including AI chatbot testing, automated test script generation, and mobile and\nA/V testing frameworks. We‚Äôve also published our first Emerging Technologies write-up on AI video generation, comparing\nplatforms and outlining pharma-specific use cases.\nWe‚Äôve expanded our\nSolution Guide Library\nwith new guides including Knowledge Sharing,\nData Visualization, and Document Processing. Each guide reflects our AI-first approach, with integrated recommendations\nfor tools like Microsoft Copilot, AI-powered test automation, and enhanced translation services. We‚Äôve also improved\nlifecycle management\n.\nTo make all of this easier to explore, we‚Äôve also made improvements to homepage navigation‚Äîso you can find what you need\nfaster. We‚Äôd love to hear what you think and what you'd like to see next.\nYour contributions\nand feedback help shape the\nfuture of TechHQ.\nTags:\nTech HQ\nEdit this page\nOlder post\nFebruary Updates Roll-up\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 5,
    "depth": 2,
    "crawled_at": "2026-02-25T10:07:07.861392"
  },
  "https://techhq.dc.lilly.com/blog/feb-updates-rollup": {
    "url": "https://techhq.dc.lilly.com/blog/feb-updates-rollup",
    "title": "February Updates Roll-up | Tech HQ",
    "description": "Our priorities for TechHQ are currently decision guidance and user experience. 5 guides are in progress, and more are queued.",
    "h1": [
      "February Updates Roll-up"
    ],
    "h2": [],
    "h3": [
      "2025",
      "2024"
    ],
    "text_content": "February Updates Roll-up | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nRecent posts\n2025\nSummer 2025 We've expanded our [Solution Guide Library](/docs/solution/ai/) with new guides including Knowledge Sharing,pdates\nFebruary Updates Roll-up\n2024\nTranslations Solution Guide\nWelcome!\nFebruary Updates Roll-up\nFebruary 19, 2025\n¬∑\n2 min read\nKarl Mayer\nDigital Core Lead Architect\nOur priorities for TechHQ are currently decision guidance and user experience. 5 guides are in progress, and more are queued.\nWork items, including solution guides, are now tracked in a\nJira project\nopen to read to all Jira users.\nOur\nTranslation tech solution guide\ntransitions from Request For Comment to Living Document.\nTechHQ dev\nreceives a\nContribute\nmenu item, describing how to make TechHQ contributions. In the future, this may be available in TechHQ prod.\nOther TechHQ changes include:\nDraft & unlisted\ndocuments are shown in\nTechHQ dev\nand hidden in TechHQ prod\nA\nsolution guide timeline\nand\nsolution guide template\nMarkdown examples\npage to demo document features, e.g.\nMermaid diagrams\nand\nMarkmap mind maps\nAccessibility and aethestic improvements\nTechHQ passes all\naxe DevTools\naccessibility checks\n'Took the red out' of TechHQ's theme\nPer document feedback (üëç üëé) added and Microsoft Forms linkage to capture detailed feedback\nGoogle Analytics integration for usage metrics\nESLint\ncode quality checks\nTags:\nTech HQ\nEdit this page\nNewer post\nSummer 2025 We've expanded our [Solution Guide Library](/docs/solution/ai/) with new guides including Knowledge Sharing,pdates\nOlder post\nTranslations Solution Guide\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 5,
    "depth": 2,
    "crawled_at": "2026-02-25T10:07:12.965698"
  },
  "https://techhq.dc.lilly.com/blog/translations-decision-guide": {
    "url": "https://techhq.dc.lilly.com/blog/translations-decision-guide",
    "title": "Translations Solution Guide | Tech HQ",
    "description": "Our third solution guide, Translation technologies, is now available in Draft status. In addition, we've revised the guides to include:",
    "h1": [
      "Translations Solution Guide"
    ],
    "h2": [],
    "h3": [
      "2025",
      "2024"
    ],
    "text_content": "Translations Solution Guide | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nRecent posts\n2025\nSummer 2025 We've expanded our [Solution Guide Library](/docs/solution/ai/) with new guides including Knowledge Sharing,pdates\nFebruary Updates Roll-up\n2024\nTranslations Solution Guide\nWelcome!\nTranslations Solution Guide\nDecember 16, 2024\n¬∑\nOne min read\nKarl Mayer\nDigital Core Lead Architect\nOur third solution guide,\nTranslation technologies\n, is now available in Draft status. In addition, we've revised the guides to include:\nlifecycle status\n(\nDraft\n,\nRequest For Comment\n, or\nLiving Document\n)\ncapability owner, who is also the respective solution guide owner\nEBA lead team representative\nsolution guide contributors & reviewers, i.e. the technology working group\nTags:\nTech HQ\nEdit this page\nNewer post\nFebruary Updates Roll-up\nOlder post\nWelcome!\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 4,
    "depth": 2,
    "crawled_at": "2026-02-25T10:07:15.175729"
  },
  "https://techhq.dc.lilly.com/blog/welcome": {
    "url": "https://techhq.dc.lilly.com/blog/welcome",
    "title": "Welcome! | Tech HQ",
    "description": "A ribbon cutting ceremony for Tech HQ",
    "h1": [
      "Welcome!"
    ],
    "h2": [],
    "h3": [
      "2025",
      "2024"
    ],
    "text_content": "Welcome! | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nRecent posts\n2025\nSummer 2025 We've expanded our [Solution Guide Library](/docs/solution/ai/) with new guides including Knowledge Sharing,pdates\nFebruary Updates Roll-up\n2024\nTranslations Solution Guide\nWelcome!\nWelcome!\nOctober 7, 2024\n¬∑\nOne min read\nKarl Mayer\nDigital Core Lead Architect\nIt‚Äôs time to cut the ribbon, and throw open the doors to Tech HQ.\nWelcome to a new headquarters for architects and technologists in Tech@Lilly. This site is your starting point for finding the guidance, tools, and practices you need to excel.\nOur menu items ‚Äî Innovate, Plan, Build, Use, & Learn ‚Äî reflect our most frequent and impactful actions. Whether you‚Äôre innovating new solutions, planning your strategy, building robust systems, using the latest tools, or learning best practices, Tech HQ has you covered.\nThis is just the beginning. We aim to keep Tech HQ dynamic and up-to-date with the latest decisions, guidance, and knowledge that matter most to you. Stay tuned for more updates and resources as we grow and evolve.\nStep inside, and let's transform the way we approach tech!\nTags:\nTech HQ\nEdit this page\nNewer post\nTranslations Solution Guide\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 1,
    "depth": 2,
    "crawled_at": "2026-02-25T10:07:17.536547"
  },
  "https://techhq.dc.lilly.com/blog/tags/techhq": {
    "url": "https://techhq.dc.lilly.com/blog/tags/techhq",
    "title": "4 posts tagged with \"Tech HQ\" | Tech HQ",
    "description": "Tech HQ significant updates",
    "h1": [
      "4 posts tagged with \"Tech HQ\""
    ],
    "h2": [
      "Summer 2025 We've expanded our [Solution Guide Library](/docs/solution/ai/) with new guides including Knowledge Sharing,pdates",
      "February Updates Roll-up",
      "Translations Solution Guide",
      "Welcome!"
    ],
    "h3": [
      "2025",
      "2024"
    ],
    "text_content": "4 posts tagged with \"Tech HQ\" | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nRecent posts\n2025\nSummer 2025 We've expanded our [Solution Guide Library](/docs/solution/ai/) with new guides including Knowledge Sharing,pdates\nFebruary Updates Roll-up\n2024\nTranslations Solution Guide\nWelcome!\n4 posts tagged with \"Tech HQ\"\nTech HQ significant updates\nView All Tags\nSummer 2025 We've expanded our [Solution Guide Library](/docs/solution/ai/) with new guides including Knowledge Sharing,pdates\nJune 25, 2025\n¬∑\n2 min read\nKarl Mayer\nDigital Core Lead Architect\nIt‚Äôs been an eventful few months since our February updates roll-up! The TechHQ platform has evolved significantly with\nmajor additions to our AI architecture ecosystem, enhanced innovation pipeline framework, and expanded solution guide\nlibrary. Here's what's new this summer:\nTags:\nTech HQ\nRead more\nFebruary Updates Roll-up\nFebruary 19, 2025\n¬∑\n2 min read\nKarl Mayer\nDigital Core Lead Architect\nOur priorities for TechHQ are currently decision guidance and user experience. 5 guides are in progress, and more are queued.\nWork items, including solution guides, are now tracked in a\nJira project\nopen to read to all Jira users.\nOur\nTranslation tech solution guide\ntransitions from Request For Comment to Living Document.\nTechHQ dev\nreceives a\nContribute\nmenu item, describing how to make TechHQ contributions. In the future, this may be available in TechHQ prod.\nOther TechHQ changes include:\nTags:\nTech HQ\nRead more\nTranslations Solution Guide\nDecember 16, 2024\n¬∑\nOne min read\nKarl Mayer\nDigital Core Lead Architect\nOur third solution guide,\nTranslation technologies\n, is now available in Draft status. In addition, we've revised the guides to include:\nlifecycle status\n(\nDraft\n,\nRequest For Comment\n, or\nLiving Document\n)\ncapability owner, who is also the respective solution guide owner\nEBA lead team representative\nsolution guide contributors & reviewers, i.e. the technology working group\nTags:\nTech HQ\nRead more\nWelcome!\nOctober 7, 2024\n¬∑\nOne min read\nKarl Mayer\nDigital Core Lead Architect\nIt‚Äôs time to cut the ribbon, and throw open the doors to Tech HQ.\nWelcome to a new headquarters for architects and technologists in Tech@Lilly. This site is your starting point for finding the guidance, tools, and practices you need to excel.\nTags:\nTech HQ\nRead more\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 3,
    "depth": 2,
    "crawled_at": "2026-02-25T10:07:20.564845"
  },
  "https://techhq.dc.lilly.com/docs/solution/ai/translation_services": {
    "url": "https://techhq.dc.lilly.com/docs/solution/ai/translation_services",
    "title": "üß≠ Translation Services | Tech HQ",
    "description": "Stack Overflow Article: üß≠ Translation Services",
    "h1": [
      "Translation Services"
    ],
    "h2": [
      "Performance of LLMs and NMT‚Äã",
      "Footnotes‚Äã"
    ],
    "h3": [],
    "text_content": "üß≠ Translation Services | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nEcosystem\nPatterns\nPositioning\nExamples\nCoding Tools\nStandards\nAI Submission Guide Template\nüóÑÔ∏è AI & Intelligent Agents BLT\nü§ñ Agentic AI\nüß≠ Conversational AI\nüß≠ Knowledge Bases\nüß≠ Multimodal AI\nüß≠ Translation Services\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nü§ñ AI & Intelligent Agents\nüß≠ Translation Services\nOn this page\nTranslation Services\nSynced daily from Stack Overflow\nSolution Guide\nLifecycle: Living Document\nLast Update: 2025-04-28\nCapability Owner: Ryan W Miller\nEBA Lead: Chris Blessing\nContributors & Reviewers: Ryan W Miller, Rafael Garcia, Neal Chen, Emily Bartman\nA solution guide for common language translations use cases, using services that can handle all Lilly information\nclassifications.\nTo plan for upcoming Lilly Translation features, please see the\nLilly Translate roadmap\n.\nDiscover announcements and ask questions at\nLilly Translate on Viva Engage\n,\nour Tech@Lilly community for machine translation, speech, and language needs.\nUse Case/Scenario\nTech Recommendation\nPositioning\nComplexity\nNotable Integration/Interop\nOwning Org/Team\nNext Step\nAd hoc plain text translation\nLilly Translate (UI)\nStrategic Core\nLow\nEnterprise Data\nUse\nAd hoc document-to-document translation\nLilly Translate (UI)\nStrategic Core\nLow\nWord, Excel, PowerPoint, PDF, TXT, CSV, Markdown, HTML, or XLIFF\nEnterprise Data\nUse\nProgrammatic translation\nLilly Translate (API)\n1\nStrategic Core\nLow\nAPIs, includes document support\nEnterprise Data\nRequest API access\n(see Lilly Translate page footer)\nTranslation using custom dictionaries/terminology\nLilly Translate (UI & API)\n1\nStandard\nLow\nAPIs\nEnterprise Data\nEngage with Enterprise Data\nTranslation model trained on custom user/domain data\nLilly Translate (UI & API)\n1\nStandard\nLow\nAPIs\nEnterprise Data\nEngage with Enterprise Data\nPrioritizing natural-sounding translation, with public domain knowledge\nLLMs, such as\nAzure OpenAI\nEmerging\nMedium\nAPIs\nAI\nRegister AI Idea\nPost-editing UI for model refinement\nTransPerfect GlobalLink\nSpecialized\nMedium\nEnterprise Data & LRL\nContact\nLilly Translate for onboarding\nTranslation workflow management\nTransPerfect GlobalLink\nSpecialized\nMedium\nEnterprise Data & LRL\nContact\nLilly Translate for onboarding\nCertified translations\nVarious, including\nTransPerfect\noption\nSpecialized\nMedium\nVarious\nContact your Area Architect & Procurement\nDirect Veeva integration\nTransPerfect GlobalLink\nSpecialized\nHigh\nVeeva\nEnterprise Data & LRL\nContact\nLilly Translate for onboarding\nPerformance of LLMs and NMT\n‚Äã\nBoth Large Language Models (LLMs) and neural machine translation (NMT) have strengths and are available at Lilly in\nvarious tools above. It is hard to quantify performance in a way that applies to a wide variety of use cases. Metrics\nfor performance do exist, such as\nBLEU scores\n, but\ncare must be taken to understand the context of the measure. Microsoft has a useful article on AI and LLMs for\ntranslation, and compares the\ngeneral\nstrengths and weaknesses of the 2.\nUsing artificial intelligence and large language models for translation on Microsoft.com\nFootnotes\n‚Äã\nSee the guide in Lilly Translate's left sidebar on training custom models, using dictionaries, and API integrations\n‚Ü©\n‚Ü©\n2\n‚Ü©\n3\nWas this helpful?\nTags:\nai\nsolution-guide\ntechhq\nEdit this page\nPrevious\nüß≠ Multimodal AI\nNext\nüè¢ Business Enablement\nPerformance of LLMs and NMT\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 15,
    "depth": 2,
    "crawled_at": "2026-02-25T10:07:22.819953"
  },
  "https://techhq.dc.lilly.com/docs/tags": {
    "url": "https://techhq.dc.lilly.com/docs/tags",
    "title": "Tags | Tech HQ",
    "description": "",
    "h1": [
      "Tags"
    ],
    "h2": [
      "A‚Äã",
      "B‚Äã",
      "C‚Äã",
      "D‚Äã",
      "E‚Äã",
      "G‚Äã",
      "I‚Äã",
      "K‚Äã",
      "L‚Äã",
      "O‚Äã",
      "P‚Äã",
      "R‚Äã",
      "S‚Äã",
      "T‚Äã",
      "U‚Äã"
    ],
    "h3": [],
    "text_content": "Tags | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nTags\nA\n‚Äã\naccessibility\n1\nagent-workflows\n1\nagentic\n1\nai\n9\napi\n1\nauthentication\n1\nB\n‚Äã\nbusiness-enablement\n3\nC\n‚Äã\nclaude-code\n1\ncloud\n5\ncoding-tools\n2\nD\n‚Äã\ndata\n4\ndata-pipeline\n1\ndeveloper-experience\n1\nE\n‚Äã\nenterprise\n1\nG\n‚Äã\ngithub-copilot\n1\nI\n‚Äã\ninnovation\n5\nK\n‚Äã\nknowledge-bases\n1\nL\n‚Äã\nlilly-code\n2\nO\n‚Äã\nobservability\n4\nP\n‚Äã\nproductivity\n1\nR\n‚Äã\nrust\n1\nS\n‚Äã\nsoftware-engineering\n4\nsolution-guide\n27\nstandards\n1\nT\n‚Äã\ntechhq\n28\nU\n‚Äã\nux\n1\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 25,
    "depth": 3,
    "crawled_at": "2026-02-25T10:07:25.397505"
  },
  "https://techhq.dc.lilly.com/docs/solution/ai/ecosystem/": {
    "url": "https://techhq.dc.lilly.com/docs/solution/ai/ecosystem/",
    "title": "Ecosystem | Tech HQ",
    "description": "Last Updated: Feb 12, 2025",
    "h1": [
      "Ecosystem",
      "AI Architecture Layers"
    ],
    "h2": [
      "Overview‚Äã",
      "AI Ecosystem diagram‚Äã",
      "Reference‚Äã"
    ],
    "h3": [],
    "text_content": "Ecosystem | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nEcosystem\nExperience Layer\nAI Layer\nData Layer\nModel Layer\nPatterns\nPositioning\nExamples\nCoding Tools\nStandards\nAI Submission Guide Template\nüóÑÔ∏è AI & Intelligent Agents BLT\nü§ñ Agentic AI\nüß≠ Conversational AI\nüß≠ Knowledge Bases\nüß≠ Multimodal AI\nüß≠ Translation Services\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nü§ñ AI & Intelligent Agents\nEcosystem\nOn this page\nEcosystem\nDocument Information\nLast Updated:\nFeb 12, 2025\nOwner:\nBrian Lewis\nPoint of Contact:\nAli Kharazmi\nContributors and Reviewers:\nArchit Kaila, Haitham Maya, Malika Mahoui\nOverview\n‚Äã\nThe AI Architecture Ecosystem is the primary tool for making AI technology decisions at Lilly. It defines the layers\n(the stack), capabilities, and platforms that are approved for use.\nThe ecosystem, and supporting documentation, is created and maintained by ML/AI experts. The framework represents a\ncomprehensive reconciliation of existing Lilly AI services, external AI benchmarking, and emerging AI technologies\nappropriate for applying in the pharmaceutical industry. As new capabilities and features are identified, the framework\nis updated accordingly.\nAI Architecture Layers\nThe AI Architecture Ecosystem consists of the following layers:\nAI Architecture Layers\nExperience Layer\nHuman interfaces for AI-enabled products where users are\n‚Üë\nAI Layer\nRe-usable deterministic and probabilistic AI capabilities\n‚Üë\nModel Layer\nCommercial ML capabilities with common controls and MLOps\n‚Üë\nData Layer\nVector databases, semantic layers, and synthetic data\nExperience Layer\n- Consistent human interfaces for bringing AI-enabled products to meet\nusers and customers where they are.\nAI Layer\n- Re-usable capabilities merging deterministic and probabilistic AI approaches; accessible\nto local engineers.\nModel Layer\n- Leading commercial and strategic capabilities for probabilistic approaches,\nproviding common controls, MLOps, and LLMOps features.\nData Layer\n- AI-enabled and enabling capabilities, such as data products, vector databases,\nsemantic layers, and synthetic data.\nEach layer is detailed in its own document, accessible via the links above.\nAI Ecosystem diagram\n‚Äã\nEcosystem\nLayer 1\nComponents\nComponent 1\nComponent 2\nPlatforms\nPlatform 1\nPlatform 2\nLayer 2\nComponents\nComponent 1\nComponent 2\nComponent 3\nPlatforms\nPlatform 1\nPlatform 2\nPlatform 3\nReference\n‚Äã\nOpenAI Frontier\nWas this helpful?\nEdit this page\nPrevious\nü§ñ AI & Intelligent Agents\nNext\nExperience Layer\nOverview\nAI Ecosystem diagram\nReference\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 23,
    "depth": 3,
    "crawled_at": "2026-02-25T10:07:27.615732"
  },
  "https://techhq.dc.lilly.com/docs/solution/ai/patterns/": {
    "url": "https://techhq.dc.lilly.com/docs/solution/ai/patterns/",
    "title": "Patterns | Tech HQ",
    "description": "Last Updated: July 25, 2025",
    "h1": [
      "Patterns"
    ],
    "h2": [
      "Topics‚Äã"
    ],
    "h3": [
      "Workflows‚Äã",
      "Agentic‚Äã",
      "Context Engineering‚Äã",
      "Protocols‚Äã",
      "Model Foundry‚Äã"
    ],
    "text_content": "Patterns | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nEcosystem\nPatterns\nQuantum Computing\nAgentic\nWorkflows\nBuilding Agentic Patterns\nContext Engineering\nDeep Agents\nDeep Research\nModel Foundry\nProtocols\nPositioning\nExamples\nCoding Tools\nStandards\nAI Submission Guide Template\nüóÑÔ∏è AI & Intelligent Agents BLT\nü§ñ Agentic AI\nüß≠ Conversational AI\nüß≠ Knowledge Bases\nüß≠ Multimodal AI\nüß≠ Translation Services\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nü§ñ AI & Intelligent Agents\nPatterns\nOn this page\nPatterns\nDocument Information\nLast Updated:\nJuly 25, 2025\nOwner:\nBrian Lewis\nPoint of Contact:\nAli Kharazmi\nContributors and Reviewers:\nArchit Kaila, Haitham Maya, Malika Mahoui\nWelcome to the AI Patterns section. Here you'll find comprehensive information about architectural patterns, workflows,\nand communication protocols for building effective AI systems. These patterns represent proven approaches to designing\nand implementing AI solutions, with a focus on agent-based architectures and their interactions.\nWorkflows\nAgentic Patterns\nContext Engineering\nProtocols\nModel Foundry\nLilly MCP Servers\nTopics\n‚Äã\nWorkflows\n‚Äã\nLearn about the distinction between agents and workflows, tool calling mechanisms, and how to choose the right approach\nfor your use case. This section covers workflow patterns like Prompt Chaining, Routing, Parallelization,\nOrchestrator-Workers, and Evaluator-Optimizer.\nAgentic\n‚Äã\nExplore various agent architectural patterns including Single Agent, ReAct, Plan-and-Execute, Self-Critique, Tree of\nThought, and more. This section provides detailed diagrams and explanations of when to use each pattern and their key\nadvantages.\nContext Engineering\n‚Äã\nMaster the art and science of filling the context window with just the right information at each step of an agent's trajectory. Learn about the four core strategies: Write (saving context outside the context window), Select (pulling relevant context when needed), Compress (retaining only essential tokens), and Isolate (splitting context into separate components). This section covers practical implementation methods including scratchpads, memory systems, summarization techniques, and multi-agent architectures.\nProtocols\n‚Äã\nUnderstand the communication protocols used for agent-to-agent and agent-tool interactions, including industry standards\nlike REST/HTTP, gRPC, and emerging AI-specific protocols like Model Context Protocol (MCP) and Agent2Agent (A2A).\nLilly MCP Servers\n‚Äã\nExplore planned Model Context Protocol (MCP) servers for Lilly systems, focusing on resource provisioning capabilities and integration with Lilly's infrastructure. This section outlines various MCP servers under consideration, implementation considerations, and next steps.\nModel Foundry\n‚Äã\nDiscover the Model Foundry platform architecture for bringing model development to production. This section covers the current implementation  and evaluation of MLflow vs Weights & Biases for ML lifecycle management, experiment tracking, and model deployment.\nWas this helpful?\nEdit this page\nPrevious\nModel Layer\nNext\nQuantum Computing\nTopics\nWorkflows\nAgentic\nContext Engineering\nProtocols\nModel Foundry\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 26,
    "depth": 3,
    "crawled_at": "2026-02-25T10:07:29.833432"
  },
  "https://techhq.dc.lilly.com/docs/solution/ai/positioning/": {
    "url": "https://techhq.dc.lilly.com/docs/solution/ai/positioning/",
    "title": "Positioning | Tech HQ",
    "description": "Owner: Brian Lewis",
    "h1": [
      "Positioning"
    ],
    "h2": [
      "Topics‚Äã"
    ],
    "h3": [
      "Agentic AI‚Äã",
      "Platforms‚Äã",
      "Agentic Enterprise‚Äã",
      "Google‚Äã",
      "Prompt Management‚Äã"
    ],
    "text_content": "Positioning | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nEcosystem\nPatterns\nPositioning\nAgentic Enterprise\nGoogle\nPlatforms\nPrompt Management\nExamples\nCoding Tools\nStandards\nAI Submission Guide Template\nüóÑÔ∏è AI & Intelligent Agents BLT\nü§ñ Agentic AI\nüß≠ Conversational AI\nüß≠ Knowledge Bases\nüß≠ Multimodal AI\nüß≠ Translation Services\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nü§ñ AI & Intelligent Agents\nPositioning\nOn this page\nPositioning\nDocument Information\nLast Updated:\nFebruary 3, 2026\nOwner:\nBrian Lewis\nPoint of Contact:\nAli Kharazmi\nContributors and Reviewers:\nArchit Kaila, Haitham Maya, Malika Mahoui\nTopics\n‚Äã\nAgentic AI\n‚Äã\nComprehensive solution guide for building autonomous AI agents and multi-agent orchestration systems at Eli Lilly. Learn about strategic technology recommendations including Cortex Landing Zone, Copilot Studio, and Chat-In-A-Box, with guidance on enabling agent-to-agent interoperability.\nPlatforms\n‚Äã\nInteractive visual presentation exploring the architecture and relationships of agentic enterprise platforms. This embedded presentation complements the strategic guidance with visual diagrams showing platform integration patterns and technology positioning.\nAgentic Enterprise\n‚Äã\nComprehensive presentation covering Eli Lilly's transformation into an agentic enterprise, including the Learn-Architect-Build journey for implementing autonomous AI agents across Enterprise and strategic business functions.\nGoogle\n‚Äã\nOutlines Eli Lilly's strategic positioning regarding Google technologies and services, detailing when and how Google's\nofferings should be evaluated, adopted, and governed within our enterprise architecture.\nPrompt Management\n‚Äã\nDefines Eli Lilly's approach to prompt engineering and management, detailing the proposed platform architecture with\nfrontend and backend components for effective prompt cataloging, evaluation, and execution across the enterprise.\nWas this helpful?\nEdit this page\nPrevious\nProtocols\nNext\nAgentic Enterprise\nTopics\nAgentic AI\nPlatforms\nAgentic Enterprise\nGoogle\nPrompt Management\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 19,
    "depth": 3,
    "crawled_at": "2026-02-25T10:07:32.030427"
  },
  "https://techhq.dc.lilly.com/docs/solution/ai/examples/": {
    "url": "https://techhq.dc.lilly.com/docs/solution/ai/examples/",
    "title": "Examples | Tech HQ",
    "description": "-->",
    "h1": [
      "Examples",
      "Enterprise Example",
      "Product Exemplar"
    ],
    "h2": [],
    "h3": [],
    "text_content": "Examples | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nEcosystem\nPatterns\nPositioning\nExamples\nEnterprise\nPress Release Drafter (PRD)\nPress Release Miner (PRM)\nProduct Exemplars\nAMIGO\nChatNow\nChat in a box\nClinical Programming Assistant\nHR Talent Acquisition Agents\nLillyNow\nPrescriberPoint Kisunla AI Pilot\nPromo Maker\nRevenue Defender\nScientific E-Author\nCoding Tools\nStandards\nAI Submission Guide Template\nüóÑÔ∏è AI & Intelligent Agents BLT\nü§ñ Agentic AI\nüß≠ Conversational AI\nüß≠ Knowledge Bases\nüß≠ Multimodal AI\nüß≠ Translation Services\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nü§ñ AI & Intelligent Agents\nExamples\nExamples\nEnterprise Example\nPress release drafter and Press release Miner POCs that led and established best practices in using Agentic AI in developing multi-agent systems for the enterprise.\nProduct Exemplar\nHere are some examples of our select star products:\nChat in a Box\nCortex Landing Zone\nRevenue defender\nPromo Maker\nAmigo\nScentific E-Author\nLillyNow\nChatNow\nWas this helpful?\nEdit this page\nPrevious\nPrompt Management\nNext\nPress Release Drafter (PRD)\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 21,
    "depth": 3,
    "crawled_at": "2026-02-25T10:07:35.727200"
  },
  "https://techhq.dc.lilly.com/docs/solution/ai/coding-tools/": {
    "url": "https://techhq.dc.lilly.com/docs/solution/ai/coding-tools/",
    "title": "AI Coding Tools | Tech HQ",
    "description": "Last Updated: November 7, 2025",
    "h1": [
      "AI Coding Tools"
    ],
    "h2": [
      "Strategic Architecture‚Äã",
      "Available Solutions‚Äã",
      "üìÑÔ∏èLilly Code",
      "Getting Started‚Äã",
      "Related Resources‚Äã"
    ],
    "h3": [
      "Architecture Comparison‚Äã",
      "For Developers‚Äã",
      "For Teams‚Äã"
    ],
    "text_content": "AI Coding Tools | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nEcosystem\nPatterns\nPositioning\nExamples\nCoding Tools\nLilly Code\nStandards\nAI Submission Guide Template\nüóÑÔ∏è AI & Intelligent Agents BLT\nü§ñ Agentic AI\nüß≠ Conversational AI\nüß≠ Knowledge Bases\nüß≠ Multimodal AI\nüß≠ Translation Services\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nü§ñ AI & Intelligent Agents\nCoding Tools\nOn this page\nDocument Information\nLast Updated:\nNovember 7, 2025\nOwner:\nJosh Bloxsome\nPoint of Contact:\nHaitham Maya\nContributors and Reviewers:\nJosh Bloxsome, Brian Lewis, Haitham Maya\nAI Coding Tools\nAI-powered coding tools are transforming how developers work at Lilly, providing intelligent assistance for code\ngeneration, debugging, testing, and documentation. This section covers Lilly's strategic approach to AI coding tools,\nincluding enterprise solutions and vendor options.\nStrategic Architecture\n‚Äã\nThe AI coding tools landscape consists of four key layers, each representing a different level of the technology stack:\nCompute\nCost & Optimization\nModels\nAI Coding Tools\nLilly Code\nClaude Code SaaS\nTool agnostic:\nClaude Code, Cline,\nGemini CLI, OpenCode\nClaude Code\nModel agnostic:\nClaude, Gemini,\nGPT, open source\nClaude models\nPay for model use,\ncustom Lilly user limits and cost optimization\nFixed price per user,\nAnthropic controlled rate limiting\nLilly infrastructure:\nMulti-cloud, HPC, Magtrain,\nprovisioned throughput,\npay as you go\nAnthropic managed\nArchitecture Comparison\n‚Äã\nLayer\nLilly Code\nClaude Code SaaS\nAI Coding Tools\nüî¥ Lilly-controlled interface\nüî¥ Lilly-controlled interface\nModels\nüî¥ Lilly LLM Gateway routing\nüîµ Anthropic-managed models\nOptimization\nüî¥ Lilly-controlled optimization\nüîµ Anthropic-managed optimization\nCompute\nüî¥ Lilly infrastructure (HPC/cloud)\nüîµ Anthropic cloud infrastructure\nKey Benefits of Lilly Code:\nFull Stack Control\n: Manage models, optimization, and compute infrastructure\nData Sovereignty\n: All processing stays within Lilly infrastructure\nCost Optimization\n: Direct control over compute resources and model routing\nCustomization\n: Ability to fine-tune models and optimization strategies\nIntegration\n: Seamless integration with Lilly's existing HPC and cloud resources\nAvailable Solutions\n‚Äã\nüìÑÔ∏è\nLilly Code\nLast Updated: December 2, 2025\nGetting Started\n‚Äã\nFor Developers\n‚Äã\nChoose your tool\n: Select an AI coding assistant based on your workflow\nAuthentication\n: Use Lilly Code for enterprise authentication and access\nIntegration\n: Connect your tool to Lilly's LLM Gateway\nBest Practices\n: Follow Lilly's AI coding standards and guidelines\nFor Teams\n‚Äã\nPilot Programs\n: Start with small team pilots before broad adoption\nTraining\n: Provide team training on effective AI-assisted coding\nGovernance\n: Establish code review processes for AI-generated code\nMonitoring\n: Track usage, costs, and effectiveness metrics\nRelated Resources\n‚Äã\nAI Patterns\n- Design patterns for AI systems\nModel Context Protocol\n- MCP standard for tool\nintegration\nWas this helpful?\nTags:\nai\ncoding-tools\ndeveloper-experience\nclaude-code\nlilly-code\nEdit this page\nPrevious\nScientific E-Author\nNext\nLilly Code\nStrategic Architecture\nArchitecture Comparison\nAvailable Solutions\nGetting Started\nFor Developers\nFor Teams\nRelated Resources\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 16,
    "depth": 3,
    "crawled_at": "2026-02-25T10:07:37.917444"
  },
  "https://techhq.dc.lilly.com/docs/solution/ai/ai_submission_guides/": {
    "url": "https://techhq.dc.lilly.com/docs/solution/ai/ai_submission_guides/",
    "title": "AI Submission Guide Template | Tech HQ",
    "description": "Product Definition",
    "h1": [
      "AI Submission Guide Template"
    ],
    "h2": [
      "Product Definition‚Äã",
      "Architecture Diagram C0: Tech Stack‚Äã",
      "Architecture Diagram C1: High-Level Architecture‚Äã",
      "Architecture Diagram C2: Detailed Component Architecture‚Äã",
      "Data Flow Diagram‚Äã",
      "LLM/ML Model Details‚Äã",
      "Data Governance & Privacy‚Äã",
      "Evaluation & Validation‚Äã",
      "Usage Tips‚Äã",
      "Questions or Feedback?‚Äã"
    ],
    "h3": [
      "Template‚Äã",
      "Template‚Äã",
      "Template‚Äã",
      "Template Option 1: Sequence Diagram (Best for User Interactions)‚Äã",
      "Template Option 2: Graph Diagram (Best for Multiple Paths/Scenarios)‚Äã",
      "GenAI Workflow Diagram (for Agentic Solutions)‚Äã"
    ],
    "text_content": "AI Submission Guide Template | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nEcosystem\nPatterns\nPositioning\nExamples\nCoding Tools\nStandards\nAI Submission Guide Template\nüóÑÔ∏è AI & Intelligent Agents BLT\nü§ñ Agentic AI\nüß≠ Conversational AI\nüß≠ Knowledge Bases\nüß≠ Multimodal AI\nüß≠ Translation Services\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nü§ñ AI & Intelligent Agents\nAI Submission Guide Template\nOn this page\nAI Submission Guide Template\nProduct Definition\n‚Äã\nDescription for Product Definition.\nWhat is this?\nBrief description of the AI solution and its purpose\nWho are the users?\nTarget audience and user personas\nWhat functions are using this?\nBusiness units, departments, or systems\nWhat are they using this for?\nUse cases and business outcomes\nArchitecture Diagram C0: Tech Stack\n‚Äã\nPurpose:\nHigh-level view of the technology stack organized into four layers: Experience, AI, Model, and Data.\nLayer Guidelines:\nExperience\n: User interfaces (web apps, mobile, APIs)\nAI\n: Orchestration frameworks (LangChain, LangGraph, CrewAI, Cortex)\nModel\n: LLM/ML services (AWS Bedrock, LLM Gateway, Sagemaker)\nData\n: Data sources and storage (S3, EDB, data lakes)\nTemplate\n‚Äã\nLayers\nData\nAWS S3\nAzure Data Lake\nEDB\nResearch Data\nEdge Device Data\nExternal Resources\nCLUWE\nRIM\nModel\nCortex\nLLM Gateway\nAWS Bedrock\nMagTrain\nAWS Sagemaker\nCATS\nAI\nCortex\nLangGraph\nLangChain\nCrewAI\nSwarmAI\nExperience\nNext.js\nLDS 2.0\nStreamlit\nExperience\nAI\nModel\nData\nInstructions:\nReplace the example technologies with your actual stack. Remove layers that don't apply.\nArchitecture Diagram C1: High-Level Architecture\n‚Äã\nPurpose:\nShow how your tech stack components (from C0) connect together at a high level. Think of this as a \"zoomed-in\" view of your C0 diagram showing\nwhich components talk to which components\nand how they integrate.\nWhat to show:\nComponents from your C0 diagram (e.g., Streamlit, LangGraph, Bedrock, S3)\nHow Experience layer connects to AI layer\nHow AI layer connects to Model layer\nHow components access Data layer\nExternal integrations and APIs\nKeep it component-level (no infrastructure details like Lambda/ECS - save that for C2)\nüìã Requirements Checklist & Color Guide\nChecklist:\nUse component names from your C0 (Streamlit, LangGraph, AWS Bedrock, etc.)\nShow connections between layers (Experience ‚Üí AI ‚Üí Model ‚Üí Data)\nLabel connection types (REST API, SDK, HTTP, etc.)\nInclude caching or state management if applicable\nShow external system integrations\nKeep it high-level - no AWS service details yet\nStandard Colors (map to C0 layers):\nBlue\n#2196F3\n- Experience layer components (Streamlit, Next.js, API Gateway)\nGreen\n#4CAF50\n- AI layer components (LangGraph, LangChain, CrewAI, Cortex)\nPurple\n#9C27B0\n- Model layer components (Bedrock, LLM Gateway, Sagemaker)\nOrange\n#FF9800\n- Data layer components (S3, EDB, Pinecone, RDS)\nGray\n#607D8B\n- External systems\nTemplate\n‚Äã\nHTTPS\nREST\nCheck cache\nCache miss\nVector search\nFetch docs\nQuery metadata\nRoute request\nResponse\nStore\nAPI call\nStreamlit App\nAPI Gateway\nLangGraph\nRedis Cache\nLLM Gateway\nAWS Bedrock\nPinecone\nAWS S3\nRDS PostgreSQL\nEnterprise System\nEDB or External API\nSee real examples:\nClinical Programming Assistant\n¬∑\nHR Talent Acquisition\nArchitecture Diagram C2: Detailed Component Architecture\n‚Äã\nPurpose:\nZoom into each component from C1 and show\nhow it's implemented\nwith specific AWS/Azure services, infrastructure, and networking details.\nWhat to show:\nSpecific cloud services (AWS Lambda, Azure Functions, ECS, etc.)\nHow each C1 component is deployed (Lambda for agents, ECS for orchestrator, etc.)\nNetwork architecture (VPCs, subnets, security groups)\nService-level capabilities (3-5 bullet points each)\nDecision nodes for routing logic\nMonitoring and observability integration\nüìã Requirements Checklist & Color Guide\nChecklist:\nShow all architectural layers (UI, Orchestration, Model, Data)\nUse nested subgraphs for related services\nSpecify exact service names (AWS Lambda, Azure Functions, etc.)\nInclude 3-5 bullet points per service\nShow network boundaries (VPCs, accounts) if applicable\nIndicate data flow directions\nShow integration with external systems\nStandard Layer Colors:\nBlue\n#2196F3\n- UI Layer\nGreen\n#4CAF50\n- Orchestration/AI Layer\nPurple\n#9C27B0\n- Model/LLM Layer\nOrange\n#FF9800\n- Data Layer\nPurple dashed\n- External Services\nRed-orange\n#FF5722\n- Decision Nodes\nTemplate\n‚Äã\nüíæ Data Layer\nü§ñ Model Layer\n‚öôÔ∏è Orchestration Layer\nüñ•Ô∏è User Interface Layer\nAI Models\nAgents/Services\nFrontend Application\n‚Ä¢ Feature 1\n‚Ä¢ Feature 2\n‚Ä¢ Feature 3\nOrchestration Service\n‚Ä¢ Workflow Management\n‚Ä¢ State Management\n‚Ä¢ Error Handling\nService 1\nService 2\nLLM Gateway\n‚Ä¢ Load Balancing\n‚Ä¢ Rate Limiting\nModel Name 1\nModel Name 2\nStorage Service\n‚Ä¢ Data Type\n‚Ä¢ Retention Policy\nDatabase Service\n‚Ä¢ Data Model\n‚Ä¢ Access Pattern\nExternal API\n‚Ä¢ Service Description\nSee real examples:\nClinical Programming Assistant\n¬∑\nAMIGO\nData Flow Diagram\n‚Äã\nPurpose:\nShow different scenarios of how data and requests flow through your system. For GenAI solutions, this tells the story of\nwhat happens when users interact with the system\n- which agents, LLM chains, knowledge bases, and tools connect together to serve specific inputs/outputs.\nWhen to include:\nUser interaction scenarios\n(chat query, document upload, API request)\nData pipeline scenarios\n(ETL, batch processing, real-time streams)\nComplex workflows\nwith multiple paths or decision points\nChoose the style that best fits your scenario:\nSequence diagram\nfor temporal/step-by-step flows (best for user interactions)\nGraph diagram\nfor spatial/branching flows (best for showing multiple paths)\nFor GenAI/Agentic solutions:\nSee\nGenAI Workflow Diagram\nsubsection below for specialized agent interaction patterns.\nüìã Requirements Checklist & Color Guide\nChecklist:\nChoose 1-3 key scenarios to document (e.g., \"User asks a question\", \"Batch data ingestion\")\nShow which components from C1 are involved\nFor GenAI: show which agents, LLMs, KBs, and tools are used\nLabel decision points (routing logic, validation gates)\nInclude data formats at key stages\nShow error/exception paths\nAdd timing or volume annotations where relevant\nStandard Colors:\nBlue\n#2196F3\n- Data Sources / User Input\nGreen\n#4CAF50\n- Processing / Agents\nOrange\n#FF9800\n- Validation / Decision Points\nPurple\n#9C27B0\n- Storage / Knowledge Bases\nGray\n#607D8B\n- External Systems\nTeal\n#009688\n- Outputs / Results\nTemplate Option 1: Sequence Diagram (Best for User Interactions)\n‚Äã\nResponse\nClaude 3.5\nPinecone KB\nResearch Agent\nLangGraph\nStreamlit UI\nüë§ User\nResponse\nClaude 3.5\nPinecone KB\nResearch Agent\nLangGraph\nStreamlit UI\nüë§ User\nScenario: User asks a question\nalt\n[Needs context]\n[Simple query]\nTotal time: 2-3 seconds\nSubmit query\nRoute query\nParse intent\nInvoke research agent\nVector search\nTop 5 chunks\nQuery with context\nDirect query\nGenerated response\nFormat response\nDisplay answer\nTemplate Option 2: Graph Diagram (Best for Multiple Paths/Scenarios)\n‚Äã\nSimple\nNeeds context\nCode request\nUser Query\nNatural language\nIntent Classifier\nCategorize query type\nSimple Q&A Agent\nDirect GPT-4 call\nRAG Agent\nRetrieve + Generate\nCode Agent\nGenerate & validate code\nVector DB\nPinecone\n10K docs\nCode Repository\nGitHub examples\nGPT-4o\nFast responses\nClaude 3.5\nLong context\nFinal Response\nFormatted answer\nSee real examples:\nAMIGO Data Pipeline\n¬∑\nClinical Programming Assistant\nGenAI Workflow Diagram (for Agentic Solutions)\n‚Äã\nPurpose:\nFor solutions using agentic frameworks (LangGraph, CrewAI, AutoGen, Cortex), show how agents collaborate, which tools they use, and how they orchestrate LLM calls. This is a specialized view that goes deeper than general data flows. To learn more about agentic patterns and architectural approaches, see\nAgentic Patterns\n.\nWhen to include:\nSolutions with\nmultiple specialized agents\nworking together\nTool-using agents\nthat call external APIs, databases, or code interpreters\nMulti-step reasoning workflows\nwith agent handoffs\nHierarchical agent architectures\n(supervisor ‚Üí worker agents)\nWhat to show:\nEach agent's name, role, and responsibilities\nTools available to each agent (search, calculator, SQL, API calls)\nWhich LLM(s) each agent uses\nCommunication patterns (sequential, parallel, hierarchical, collaborative)\nState management and memory (conversation history, shared state)\nDecision points (routing logic, delegation, escalation)\nüìã Requirements Checklist & Color Guide\nChecklist:\nShow all agents with clear roles (e.g., \"Research Agent\", \"Code Agent\", \"Validator\")\nList tools each agent can use (e.g., \"Vector Search\", \"SQL Query\", \"Web Search\")\nIndicate which LLM(s) each agent uses (GPT-4o, Claude 3.5, etc.)\nShow agent interaction patterns (who talks to whom)\nInclude supervisor/orchestrator if present\nShow state/memory management approach\nLabel handoff conditions (when does one agent call another?)\nIndicate parallel vs sequential execution\nStandard Colors:\nPurple\n#9C27B0\n- Supervisor/Orchestrator Agent\nBlue\n#2196F3\n- Research/Retrieval Agents\nGreen\n#4CAF50\n- Processing/Analysis Agents\nOrange\n#FF9800\n- Code/Tool-Execution Agents\nTeal\n#009688\n- Validation/QA Agents\nGray\n#607D8B\n- External Tools/APIs\nRed-orange\n#FF5722\n- Decision/Routing Nodes\nTemplate Option 1: Hierarchical Agent System (Supervisor Pattern)\n‚Äã\nNeeds research\nNeeds code\nNeeds analysis\nFinal answer\nUser Query\nSupervisor Agent\nLLM: GPT-4o\nRole: Route & orchestrate\nRoute Query\nResearch Agent\nLLM: Claude 3.5\nTools: Vector Search, Web Search\nCode Agent\nLLM: GPT-4o\nTools: Python REPL, GitHub Search\nAnalyst Agent\nLLM: Claude 3.5\nTools: SQL Query, Data Viz\nPinecone\nVector Search\nTavily\nWeb Search\nPython REPL\nCode Execution\nPostgreSQL\nSQL Queries\nGPT-4o\nFast reasoning\nClaude 3.5\nLong context\nShared State\nConversation history\nAgent outputs\nAgent can call\nother agents\nTemplate Option 2: Collaborative Agent System (Peer-to-Peer)\n‚Äã\nCreate plan\nUse tools\nAdd findings\nDraft output\nFeedback\nNeeds revision\nNeeds more info\nApproved\nUser Query\nAgent 1: Planner\nLLM: GPT-4o\nBreaks down task\nAgent 2: Researcher\nLLM: Claude 3.5\nGathers information\nAgent 3: Writer\nLLM: Claude 3.5\nCreates output\nAgent 4: Critic\nLLM: GPT-4o\nValidates quality\nAvailable Tools:\n‚Ä¢ Vector Search\n‚Ä¢ Web Search\n‚Ä¢ Code REPL\n‚Ä¢ SQL Query\nShared Memory\nTask plan\nResearch findings\nDraft outputs\nFeedback\nTemplate Option 3: Sequential Agent Pipeline\n‚Äã\nExternal Tools\nValidator Agent\n(GPT-4o)\nGenerator Agent\n(Claude 3.5)\nPinecone\nRetrieval Agent\n(Claude 3.5)\nRouter Agent\n(GPT-4o)\nUser\nExternal Tools\nValidator Agent\n(GPT-4o)\nGenerator Agent\n(Claude 3.5)\nPinecone\nRetrieval Agent\n(Claude 3.5)\nRouter Agent\n(GPT-4o)\nUser\nAgentic RAG Workflow\nalt\n[Simple query]\n[Complex query]\nopt\n[Needs external data]\nalt\n[Validation fails]\n[Validation passes]\nTotal: 3-5 seconds\n3-4 LLM calls\nSubmit query\nClassify intent\nSelect strategy\nDirect generation\nInvoke retrieval\nVector search\nTop 5 chunks\nRerank results\nContext + query\nGenerate response\nAPI call / Web search\nExternal data\nValidate response\nCheck citations\nVerify accuracy\nSafety check\nRequest revision\nRevised response\nFinal response\nKey Documentation Points:\nAgent Roles & Responsibilities:\nAgent 1 (Supervisor): Routes queries, orchestrates workflow, decides which agents to invoke\nAgent 2 (Researcher): Performs vector search, web search, returns relevant context\nAgent 3 (Analyst): Analyzes data, generates insights, creates visualizations\nLLM Selection Rationale:\nGPT-4o for fast routing and classification (low latency)\nClaude 3.5 for long-context research and generation (200K window)\nTool Usage:\nVector Search (Pinecone): Semantic search over knowledge base\nPython REPL: Execute code for calculations, data transformations\nSQL Query: Structured data retrieval from PostgreSQL\nState Management:\nShared state stores conversation history and intermediate outputs\nAgents read/write to shared state for coordination\nState persisted in Redis for session continuity\nHandoff Logic:\nSupervisor evaluates query complexity ‚Üí routes to appropriate agent\nAgents can invoke other agents when needed (e.g., Researcher ‚Üí Analyst)\nMaximum 3 agent hops to prevent infinite loops\nSee real examples:\nClinical Programming Assistant Agents\n¬∑\nHR Talent Acquisition Workflow\nLLM/ML Model Details\n‚Äã\nDescription for LLM/ML Model Details.\nDocument the AI/ML models and how they're used in your solution:\nTopic\nWhat to Document\nExample\nModels Used\nName, version, provider, use case, context window\nGPT-4o via Azure OpenAI for Q&A (128K tokens, 2.5s avg)\nPrompting Strategy\nSystem prompts, few-shot examples, chain-of-thought, structured outputs\n\"You are a clinical analyst...\" with 3 examples, JSON schema output\nOrchestration Logic\nSingle vs multi-model, routing logic, fallback strategy\nRoute complex queries to Claude, simple to GPT-3.5\nAgentic Framework\nMaster agent, sub-agents, tools, LLMs used, interaction patterns\nSupervisor orchestrates 3 agents (Researcher, Analyst, Writer)\nAI Logic\nHow AI components meet requirements\nRAG retrieves top-5 chunks ‚Üí Claude generates ‚Üí Validator checks citations\nüìã Detailed Guidance\nModels Used - Include:\nModel name/version (GPT-4o, Claude 3.5 Sonnet, Llama 3.1 70B, custom models)\nProvider/platform (Azure OpenAI, AWS Bedrock, Cortex, Sagemaker)\nSpecific use case (Q&A, code generation, classification, reasoning)\nContext window size (128K, 200K tokens)\nPerformance metrics (latency, throughput, cost per request)\nAgentic Framework (if applicable):\nMaster/Supervisor agent role and responsibilities\nSub-agents: name, role, tools available, LLM used\nInteraction pattern (sequential, parallel, hierarchical)\nState management approach\nAI Logic & Requirements Mapping:\nShow how AI capabilities deliver business value:\nRequirement: \"Provide accurate, cited answers to regulatory questions\"\nImplementation: Vector DB ‚Üí Claude 200K ‚Üí Citation validator ‚Üí User feedback loop\nSee real examples:\nClinical Programming Assistant\n¬∑\nHR Talent Acquisition\nData Governance & Privacy\n‚Äã\nDescription for Data Governance & Privacy.\nDocument data handling, privacy, and compliance:\nCategory\nWhat to Document\nData Description\nType collected, classification (CI/PI), sources, volume, sensitivity (PII/PHI)\nStorage & Architecture\nPrimary location (S3/Azure/EDB), region, encryption (at rest/in transit), backup strategy, network security\nData Ownership\nWho owns data (for vendor solutions), license terms, data residency, subprocessors\nTraining Data Governance\nUsed for training? RAG? Fine-tuning? How is Lilly data handled? Anonymization approach\nInteraction Data\nAre prompts/responses stored? Where? For how long? Sent to LLM providers? Zero-retention agreements?\nRetention & Deletion\nPolicy by data type (raw: 30d, processed: 90d, outputs: 2y, logs: 7y), deletion process, archival strategy\nCompliance\nFrameworks (GDPR, HIPAA, SOC 2, GxP), security controls (Auth, monitoring, DLP, scanning), assessments completed\nVendor Due Diligence\nCertifications (SOC 2, ISO 27001), DPA/BAA in place, audit rights, incident response SLA\nüìã Critical Questions for AI Solutions\nTraining Data:\nIs Lilly data used for model training? If yes: What data? How anonymized? Who has access?\nIs Lilly data used for RAG/retrieval? If yes: How chunked/embedded? Access controls?\nAre models fine-tuned on Lilly data? If yes: Where hosted? Who can access?\nPrompts & Responses:\nAre user prompts stored? If yes: Where? How long? Who accesses?\nAre AI responses stored? If yes: Retention policy?\nAre prompts/responses sent to third-party LLM providers? Zero-retention agreements?\nHow long is conversation history maintained? Where is state stored?\nCompliance (for regulated data or external exposure):\nWhich frameworks apply? (GDPR, HIPAA, SOC 2, GxP, 21 CFR Part 11)\nWhat security controls are in place? (Azure AD, MFA, RBAC, monitoring, DLP)\nHas a cyber risk assessment been completed?\nHas legal counsel reviewed data practices?\nEvaluation & Validation\n‚Äã\nDescription for Evaluation & Validation.\nDocument performance measurement and output validation:\nCategory\nWhat to Document\nGround Truth\nHow created (human annotation, expert review, gold standard comparison), who creates it, sample size, maintenance\nAccuracy Metrics\nClassification: precision/recall/F1. Generation: BLEU/ROUGE. RAG: context relevance/faithfulness. Domain-specific metrics\nPerformance Metrics\nResponse time (p50/p95/p99), throughput (req/sec), availability (SLA), cost efficiency\nUser Experience\nSatisfaction (ratings/NPS), task completion rate, engagement, error rate\nBaseline Comparison\nCurrent state without AI, manual process time, existing system performance\nHuman-in-the-Loop\nWho validates (experts, users)? When (100%, sample %, confidence-triggered)? Review workflow? Feedback capture?\nAutomated Validation\nConfidence scoring, semantic validation (second LLM), rule-based checks (format, logic, safety), test suites\nFeedback Loops\nCollection (thumbs up/down, corrections, flags), processing (aggregation, labeling), model improvement (prompt refinement, fine-tuning, RAG optimization)\nMonitoring\nReal-time dashboards, drift detection, alerting rules (error spikes, latency increases), incident response\nValidation Frequency\nPre-deployment testing, continuous production monitoring, periodic reviews (monthly/quarterly), post-change regression tests\nüìã ML Optimization Strategies\nFeedback Collection:\nUser feedback: Thumbs up/down, star ratings, corrections, flags\nImplicit feedback: Click-through rates, time spent, task completion\nSystem logs: Error logs, performance metrics, usage patterns\nModel Improvement:\nPrompt refinement: A/B test different prompts based on feedback\nFine-tuning: How often? (weekly/monthly/quarterly) What's the pipeline?\nRAG optimization: Improve chunking, embedding model, ranking algorithms\nAgent optimization: Adjust responsibilities, retrain routing, update tool usage\nMonitoring & Alerting:\nReal-time dashboards for key metrics\nDrift detection for data/model degradation\nAlert triggers: Error rate spike, latency increase, quality drop\nIncident response process when quality degrades\nUsage Tips\n‚Äã\nStart minimal\n- Copy starter templates and fill in your specifics\nReference exemplars\n- See complete examples in the exemplar files\nRemove what doesn't apply\n- Not every solution needs all sections\nKeep it current\n- Update as architecture evolves\nUse tables\n- For requirements and checklists (more scannable)\nLink to detailed docs\n- Don't overload this template, reference separate docs\nQuestions or Feedback?\n‚Äã\nReach out to the TechHQ team or submit feedback through standard channels.\nWas this helpful?\nEdit this page\nPrevious\nKnowledge Base Standards\nNext\nüóÑÔ∏è AI & Intelligent Agents BLT\nProduct Definition\nArchitecture Diagram C0: Tech Stack\nTemplate\nArchitecture Diagram C1: High-Level Architecture\nTemplate\nArchitecture Diagram C2: Detailed Component Architecture\nTemplate\nData Flow Diagram\nTemplate Option 1: Sequence Diagram (Best for User Interactions)\nTemplate Option 2: Graph Diagram (Best for Multiple Paths/Scenarios)\nGenAI Workflow Diagram (for Agentic Solutions)\nLLM/ML Model Details\nData Governance & Privacy\nEvaluation & Validation\nUsage Tips\nQuestions or Feedback?\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 18,
    "depth": 3,
    "crawled_at": "2026-02-25T10:07:40.191315"
  },
  "https://techhq.dc.lilly.com/docs/solution/ai/ai-blt": {
    "url": "https://techhq.dc.lilly.com/docs/solution/ai/ai-blt",
    "title": "üóÑÔ∏è AI & Intelligent Agents BLT | Tech HQ",
    "description": "The AI & Intelligent Agents Big List of Technology (BLT) is maintained using data from the Enterprise Architecture Tech Positioning Database and LillyFlow, with AI-assisted updates. While we aim for accuracy, please verify critical information and contact the EBA team with questions or corrections.",
    "h1": [
      "üóÑÔ∏è AI & Intelligent Agents BLT"
    ],
    "h2": [
      "AWS Textract‚Äã",
      "Azure Bot Composer‚Äã",
      "Azure OpenAI Services‚Äã",
      "IBM Watson Discovery‚Äã",
      "IBM Watson Knowledge Studio‚Äã",
      "IBM Watson Language Translator‚Äã",
      "IBM Watson NLU‚Äã",
      "IBM Watson Speech to Text‚Äã",
      "Jupyter‚Äã",
      "Kore.ai‚Äã",
      "Lilly Enterprise NLP Solutions (LENS)‚Äã",
      "Lilly Translate‚Äã",
      "Microsoft Copilot Studio‚Äã",
      "Tamr‚Äã",
      "Yseop‚Äã",
      "Declining ‚Üí Retired‚Äã"
    ],
    "h3": [
      "Azure LUIS‚Äã",
      "Azure QnA Maker‚Äã"
    ],
    "text_content": "üóÑÔ∏è AI & Intelligent Agents BLT | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nEcosystem\nPatterns\nPositioning\nExamples\nCoding Tools\nStandards\nAI Submission Guide Template\nüóÑÔ∏è AI & Intelligent Agents BLT\nü§ñ Agentic AI\nüß≠ Conversational AI\nüß≠ Knowledge Bases\nüß≠ Multimodal AI\nüß≠ Translation Services\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nü§ñ AI & Intelligent Agents\nüóÑÔ∏è AI & Intelligent Agents BLT\nOn this page\nüóÑÔ∏è AI & Intelligent Agents BLT\nThe AI & Intelligent Agents Big List of Technology (BLT) is maintained using data from the Enterprise Architecture Tech Positioning Database and LillyFlow, with AI-assisted updates. While we aim for accuracy, please verify critical information and contact the EBA team with questions or corrections.\nCan't find a technology?\nUse TechHQ's search feature to look across all technology documentation. For questions or to suggest additions to this list, contact the\nEnterprise Business Architecture team\n.\nAWS Textract\n‚Äã\nPosition\nEmerging\nTags\naws, cloud-computing\nInformation Sensitivity\nArchitect\nTodd Walters\nContact(s)\nDustin Jefford\nUse Cases\n‚Äã\nEthics & Compliance Expense Analysis\n‚Äã\nAADS is using Textract Expense Analysis to pull text from receipts and invoices submitted to Concur Expense Reporting. The complete stack is as follows:\nSource receipt images in S3.\nLambda function to start expense analysis.\nTextract sends notification to SNS Topic when job completes.\nMessage from SNS Topic is written to SQS.\nLambda function listens to SNS Topic. Writes completed analysis output to S3 as a JSON document.\nDynamoDB is used to sync source, Textract job ID, and target.\nAzure Bot Composer\n‚Äã\nPosition\nEmerging\nTags\nenterprise-automation, azure\nArchitect\nGreg Graf\nContact(s)\nJason Yazell\nPreferred for internal chatbots\nUse Cases\n‚Äã\nBot services composer to integrate with other chatbots\nNotes\n‚Äã\nFramework for other teams to have locally developed chatbots integrated wtih. Enterprise teams (automation) own user \"front door\", local / domain specific chatbots will be owned and stewarded by local teams, with integration help provided by enterprise team.\nAzure OpenAI Services\n‚Äã\nPosition\nEmerging\nTags\nazure-ai, openai, ai-platforms\nInformation Sensitivity\nRed\nArchitect\nRubin Wei\nContact(s)\nAndrew R Povinelli, Liping Cai, Malika Mahoui\nOpenAI based Large Language Model services for content generation, summarization, semantic search, image creation, and natural language to code translation. Please refer to the entry on Azure AI (Cognitive) Services to understand other components within Azure AI (Cognitive) Services.\nUse Cases\n‚Äã\nPro-code developed solutions for content generation, summarization, semantic search, image creation, and natural language to code translation.\nNotes\n‚Äã\nGreen data for sandbox environment. Red data for Production environment.\nSee Also\n‚Äã\nhttps://collab.lilly.com/sites/GenerativeAI/\nTech Position: Azure AI Services\nIBM Watson Discovery\n‚Äã\nPosition\nEmerging\nTags\ndata-science-tools\nInformation Sensitivity\nGreen\nArchitect\nMalika Mahoui\nContact(s)\nSurya Prakash Rawat\nNLP AI powered cognitive search engine, text mining and topic modeling  tools using unstructured data\nUse Cases\n‚Äã\nUse this tool to perform cognitive search  from various unstructured data sources, Extract entities from unstructured data without writing a code, This tool is not suitable if you have only structured data\nNotes\n‚Äã\nExtract data from websites and other data sources and allow users to search the content using web interface. It includes Natural Language Understanding capabilities. Cloud Subscription based and charges based on number of documents.\nSee Also\n‚Äã\nhttps://collab.lilly.com/sites/EIP/SitePages/Enterprise-Analytics.aspx\nIBM Watson Knowledge Studio\n‚Äã\nPosition\nEmerging\nTags\ndata-science-tools\nInformation Sensitivity\nGreen\nArchitect\nMalika Mahoui\nContact(s)\nSurya Prakash Rawat\nUse to create and train ML models for Watson Explorer, Watson Discovery and Watson NLU\nUse Cases\n‚Äã\nCreate ML models to deploy it in Watson Discovery and Watson NLU, This is not suitable to develop, train and deploy model in non-Watson services\nNotes\n‚Äã\nCloud Subscription based and charges based on number of users.\nSee Also\n‚Äã\nhttps://collab.lilly.com/sites/EIP/SitePages/Enterprise-Analytics.aspx\nIBM Watson Language Translator\n‚Äã\nPosition\nEmerging\nTags\ndata-science-tools\nInformation Sensitivity\nYellow\nArchitect\nMalika Mahoui\nContact(s)\nSurya Prakash Rawat\nUse to translate text from one language to another and supports customization to create language models\nUse Cases\n‚Äã\nUse this tool if you want to language translations and create domain model to improve translation accuracy.\nNotes\n‚Äã\nit is a on-prem API based service. All operations such as translation, model creation, etc. are done through API calls.\nSee Also\n‚Äã\nhttps://collab.lilly.com/sites/EIP/SitePages/Enterprise-Analytics.aspx\nIBM Watson NLU\n‚Äã\nPosition\nEmerging\nTags\ndata-science-tools\nInformation Sensitivity\nGreen\nArchitect\nMalika Mahoui\nContact(s)\nSurya Prakash Rawat\nUse to extract keywords, concepts, Semantics and emotions from unstructured  data (Text Analytics)\nUse Cases\n‚Äã\nUse this tool if you want to extract entities, sentiments from unstructured data using NLP algorithms using no code\nNotes\n‚Äã\nExact keyworks from pdf, text files and other sources contained human readable data. Use Machine learning model created using Watson Knowledge studio to extract custom information.  Cloud Subscription based and charges based on use.\nSee Also\n‚Äã\nhttps://collab.lilly.com/sites/EIP/SitePages/Enterprise-Analytics.aspx\nIBM Watson Speech to Text\n‚Äã\nPosition\nEmerging\nTags\ndata-science-tools\nArchitect\nMalika Mahoui\nContact(s)\nSurya Prakash Rawat\nConvert speeches/audio data to text\nUse Cases\n‚Äã\nUse this tool when we have have audio files/data need to be converted into text, this support language and acoustic models to improve the accuracy of output.\nNotes\n‚Äã\nit's a on-prem API based service. All operations such as audio transcribe, model (Language, Acoustic) creation and training are done through API calls.\nSee Also\n‚Äã\nhttps://collab.lilly.com/sites/EIP/SitePages/Enterprise-Analytics.aspx\nJupyter\n‚Äã\nPosition\nStandard\nTags\ndata-science-tools\nArchitect\nRubin Wei\nContact(s)\nJeff Kriske\nRecommended interactive computing environment for Python programmers especially for data science/machine learning/visualization projects.\nNotes\n‚Äã\nJupyter Notebook is a web-based interactive computational environment for creating notebook documents for programming languages including Julia, Python and R. JupyterLab is a newer user interface for Project Jupyter and offers the building blocks of the classic Jupyter Notebook in a flexible user interface. Both Jupyter Notebook and JupyterLab are free and open source. You can install Jupyter Notebook or JupyterLab on your local Windows or Mac computer or access JupyterHub (\nhttps://hpc.am.lilly.com/index.php/JupyterHub\n) hosted on Lilly High Performance Computing (\nhttps://hpc.am.lilly.com/index.php/Main_Page\n).  There is no technical support for installation on your Windows or Mac computer.\nSee Also\n‚Äã\nhttps://hpc.am.lilly.com/index.php/JupyterHub\nKore.ai\n‚Äã\nPosition\nEmerging\nTags\nenterprise-automation, ai-platforms, chatbots\nInformation Sensitivity\nRed\nArchitect\nJason Yazell\nContact(s)\nJason Yazell\nCMDB CI\nCI00000053183649\nKore.ai XO Platform is a conversational Ai platform that allows developers build chatbots that use Ai to create intelligent conversations with users and integrate with other systems to perform actions for the user.\nUse Cases\n‚Äã\nIntelligent Virtual Assistants, Human Agent Assistant, Service Desk Automation, Conversational AI, Chatbots\nNotes\n‚Äã\nAble to integrate with other internal services such as Lilly Translate, ServiceNow, etc., allowing automation of user/agent manual tasks.\nSee Also\n‚Äã\nkore.ai\nConversational AI in Enterprise Automation\nLilly Enterprise NLP Solutions (LENS)\n‚Äã\nPosition\nEmerging\nTags\ndata-science-tools\nInformation Sensitivity\nGreen\nArchitect\nRubin Wei\nContact(s)\nTodd Sanger\nUse to ingest, transform and analyze structured, semi structured and unstructured data using NLP ML models\nUse Cases\n‚Äã\nUse this tool to process unstructured data using NLP models ( Entities extraction, Sentiment Analysis, Feedback management etc.)\nNotes\n‚Äã\nThis is Lilly developed NLP framework  using Ontology/NLP based ML models. Capabilities include protocol extraction, ReQuest and Toxicology Report Search. License is not required.\nSee Also\n‚Äã\nhttps://collab.lilly.com/sites/AADSEnterprise/SitePages/NLP.aspx\nLilly Translate\n‚Äã\nPosition\nSpecialized\nTags\ndata-science-tools\nInformation Sensitivity\nRed\nArchitect\nRubin Wei\nContact(s)\nMalika Mahoui\nTranslation unstructured data in human language form from one language to other language\nUse Cases\n‚Äã\nUse this tool if you have a requirements to translate human  content in text format  from one language to another language using low cost APIs\nNotes\n‚Äã\nLicense is not required.\nSee Also\n‚Äã\nhttps://translate.lilly.com\nMicrosoft Copilot Studio\n‚Äã\nPosition\nStandard\nTags\nai-platforms, virtual-agents, copilot-studio\nInformation Sensitivity\nRed\nArchitect\nSteve Scott\nContact(s)\nSteve Scott\nCopilot Studio, a component of the Power Platform, is a low-code platform that enables users to create, manage, and deploy AI agents. It offers intuitive tools and seamless integration with Microsoft 365 to enhance customer interactions and streamline business processes.\nUse Cases\n‚Äã\nCreate custom AI agents grounded in Microsoft 365 data using low code\nCreate custom AI agents that can leverage actions and connectors from the Power Platform\nNotes\n‚Äã\nBe sure to follow all\nguidelines for using AI responsibly at Lilly\nas well as guidance at\nai.lilly.com\n. And always be sure to\nProtect Lilly data\n.\nSee Also\n‚Äã\nhttps://learn.microsoft.com/en-us/microsoft-copilot-studio/\nhttps://learn.microsoft.com/en-us/training/modules/power-virtual-agents-bots/\nTamr\n‚Äã\nPosition\nSpecialized\nTags\ndata-science-tools\nInformation Sensitivity\nRed\nArchitect\nMalika Mahoui\nContact(s)\nAndrea Price\nUse to prepare , cleanse and qualify data to produce consumable data products.  Its unique feature is to map data fields using  inbuild ML models. It works with structured and semi-structed data.\nUse Cases\n‚Äã\nUse to prepare, transform and map data to fields using inbuilt ML algorithms and consume it through various dashboard and analytics tools, this is not suitable if you are already using data transformation capabilities within analytics tools and you don‚Äôt have requirements to share transformed data\nNotes\n‚Äã\nExtract data from EDB data sources (S3, Postgres and Redshift) , cleanse, map to the target field  and then publish cleansed data to the target data sources for consumption. This tool is licensed based specific use case requirements. License is required.\nSee Also\n‚Äã\nhttps://collab.lilly.com/sites/EIP/SitePages/Enterprise-Analytics.aspx\nYseop\n‚Äã\nPosition\nSpecialized\nTags\ndata-science-tools\nInformation Sensitivity\nRed\nArchitect\nRubin Wei\nContact(s)\nNethra Nandakumar\nTemplate-driven Natural Language Generation (NLG) platform positioned for enterprise use in targeted functions. As of December 2022, documents in the Clinical and Finance functions (see use cases section) are available, with additional functions (e.g. Science, Marketing, etc.) to be built into the platform over the next 3 years of collaboration (through 2025). Extension to other areas requires development and prioritization. Yseop is not currently positioned for ML/model-driven NLG scenarios. If you have any NLG request, please email the Architect & Contact. They will help you determine the appropriate technology (template-driven vs ML/model-driven), resources and priority.\nUse Cases\n‚Äã\nClinical (Patient Narratives, Clinical Study Reports, Summary of Clinical Safety) and Finance (Monthly Reports)\nNotes\n‚Äã\nYseop is a no code intelligent report automation platform using NLG. The platform is provided as SaaS hosted in AWS and can be integrated with other solutions via existing API's. A Java-based SDK exists for platform extensibility.\nSee Also\n‚Äã\nhttps://collab.lilly.com/sites/GenerativeAI\nDocument generated from Stack Overflow Enterprise articles (prioritized) and ea_tech.csv (fallback)\nDeclining ‚Üí Retired\n‚Äã\nAzure LUIS\n‚Äã\nPosition\nRetired 2024\nTags\nenterprise-automation, azure\nArchitect\nGreg Graf\nContact(s)\nJason Yazell\nPreferred chatbot framework for inferring intent from user inputs where context could steer user to different answers\nUse Cases\n‚Äã\nAI chatbot service for more complex, intent-driven interactions and solutions\nNotes\n‚Äã\nAdvanced chatbot solution that uses AI to derive user intent and determine actions; uses AI to ensure that the right answer / solution is provided based on intent and context.\nAzure QnA Maker\n‚Äã\nPosition\nRetired 2025\nTags\nenterprise-automation, azure\nArchitect\nGreg Graf\nContact(s)\nJason Yazell\nPreferred for simple chatbots using content from Sharepoint sites, knowledge base articlets, etc.\nUse Cases\n‚Äã\nSimple chatbot used against known article / content. No need for AI to infer user intent.\nNotes\n‚Äã\nChatbots to help users self-serve against a known set of data or article(s). Best used when data or content is changing frequently and to reduce calls / engagements with various business partners.\nWas this helpful?\nEdit this page\nPrevious\nAI Submission Guide Template\nNext\nü§ñ Agentic AI\nAWS Textract\nAzure Bot Composer\nAzure OpenAI Services\nIBM Watson Discovery\nIBM Watson Knowledge Studio\nIBM Watson Language Translator\nIBM Watson NLU\nIBM Watson Speech to Text\nJupyter\nKore.ai\nLilly Enterprise NLP Solutions (LENS)\nLilly Translate\nMicrosoft Copilot Studio\nTamr\nYseop\nDeclining ‚Üí Retired\nAzure LUIS\nAzure QnA Maker\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 5,
    "depth": 3,
    "crawled_at": "2026-02-25T10:07:42.707362"
  },
  "https://techhq.dc.lilly.com/docs/solution/ai/agentic_ai": {
    "url": "https://techhq.dc.lilly.com/docs/solution/ai/agentic_ai",
    "title": "ü§ñ Agentic AI | Tech HQ",
    "description": "Solution Guide: ü§ñ Agentic AI",
    "h1": [
      "Agentic AI"
    ],
    "h2": [],
    "h3": [],
    "text_content": "ü§ñ Agentic AI | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nEcosystem\nPatterns\nPositioning\nExamples\nCoding Tools\nStandards\nAI Submission Guide Template\nüóÑÔ∏è AI & Intelligent Agents BLT\nü§ñ Agentic AI\nüß≠ Conversational AI\nüß≠ Knowledge Bases\nüß≠ Multimodal AI\nüß≠ Translation Services\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nü§ñ AI & Intelligent Agents\nü§ñ Agentic AI\nAgentic AI\nSolution Guide\nLifecycle: Draft\nLast Update: 2026-02-03\nCapability Owner: Brian Lewis\nEBA Lead: Brian Lewis\nContributors & Reviewers: Ali Kharazmi, Archit Kaila, Haitham Maya, Malika Mahoui\nA solution guide for autonomous AI agents and multi-agent orchestration systems at Eli Lilly.\nImportant Strategic Guidance\nMicrosoft Foundry should NOT be used.\nAll agentic orchestration should happen in\nCopilot Studio\n(low-code) or\nCortex Landing Zone\n(pro-code). Agent interoperability happens in Landing Zone with registration of Copilot Studio agents (if desired).\nUse Case/Scenario\nTech Recommendation\nCode Level\nCategory\nPositioning\nNotable Integration/Interop\nOwning Org/Team\nGuidance\nAgentic orchestration & multi-agent systems\nCortex Landing Zone\nPro-Code\nPlatform\nStrategic Core\nAPI endpoints; A2A protocol; Copilot Studio agent registration\nAI & SPE\nConsult AI Architecture\nAI agents & agentic workflows\nMicrosoft Copilot Studio\nLow-Code\nHyperscaler\nStrategic Core\nMicrosoft Teams; SharePoint; APIs; Azure AI Services; Landing Zone interop\nCollaboration Svcs\nUse\nAI chat agents\nChat-In-A-Box\nNo-Code\nPlatform\nStrategic Core\nAI & SPE\nUse\nAI product solutions\nPharm(ai)\nNo-Code\nPlatform\nStrategic Core\nAI & SPE\nUse\nAgent-to-agent interoperability\nCortex Landing Zone\nPro-Code\nPlatform\nStrategic Core\nA2A protocol; Copilot Studio registration; multi-platform agent orchestration\nAI & SPE\nConsult AI Architecture\nAgent orchestration\nMicrosoft Azure AI Agent Service\nLow-Code\nHyperscaler\nControlled\nAzure ecosystem\nCollaboration Svcs\nConsult AI Architecture\nAgent orchestration\nAmazon Bedrock Agents\nPro-Code\nHyperscaler\nControlled\nAWS ecosystem\nCloud Platform\nConsult AI Architecture\nGoogle agent platform\nGoogle Agentspace\nPro-Code\nHyperscaler\nSpecialized\nVertex AI; Gemini models (requires governance approval)\nN/A\nContact Enterprise Architecture\nService management agents\nServiceNow\nLow-Code\nVendor\nContained\nMicrosoft Teams; OpenAI; enterprise integrations\nAutomation\nConsult Automation\nRPA and workflow automation\nAutomation Anywhere\nLow-Code\nVendor\nContained\nBusiness process integrations\nAutomation\nConsult Automation\nDialog flows and decision trees\nKore.AI\nLow-Code\nVendor\nDeclining\nMicrosoft Teams; OpenAI; ServiceNow\nAutomation\nConsult Automation\nWas this helpful?\nTags:\nai\nsolution-guide\nagentic\ntechhq\nEdit this page\nPrevious\nüóÑÔ∏è AI & Intelligent Agents BLT\nNext\nüß≠ Conversational AI\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 8,
    "depth": 3,
    "crawled_at": "2026-02-25T10:07:44.861644"
  },
  "https://techhq.dc.lilly.com/docs/solution/ai/conversational_ai": {
    "url": "https://techhq.dc.lilly.com/docs/solution/ai/conversational_ai",
    "title": "üß≠ Conversational AI | Tech HQ",
    "description": "Stack Overflow Article: üß≠ Conversational AI",
    "h1": [
      "Conversational AI"
    ],
    "h2": [],
    "h3": [],
    "text_content": "üß≠ Conversational AI | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nEcosystem\nPatterns\nPositioning\nExamples\nCoding Tools\nStandards\nAI Submission Guide Template\nüóÑÔ∏è AI & Intelligent Agents BLT\nü§ñ Agentic AI\nüß≠ Conversational AI\nüß≠ Knowledge Bases\nüß≠ Multimodal AI\nüß≠ Translation Services\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nü§ñ AI & Intelligent Agents\nüß≠ Conversational AI\nConversational AI\nSynced daily from Stack Overflow\nSolution Guide\nLifecycle: Draft\nLast Update: 2025-02-14\nCapability Owner: JR Burch\nEBA Lead: Brian Lewis\n-Contributors & Reviewers: J.R. Burch, Marc Jones, Haitham Maya, Andrew Povenilli, Emily Bartman, Jason Yazell, Steve Scott, Brian Lewis, Karl Mayer\nA solution guide for human natural language interaction, including text chat and speech.\nUse Case/Scenario\nTech Recommendation\nPositioning\nComplexity\nNotable Integration/Interop\nOwning Org/Team\nNext Step\nGeneral AI chat & content generation\nMicrosoft Copilot\nStrategic Core\nReady-to-use\nMicrosoft 365\nCollaboration Svcs\nUse\nExpert chat\nChat-in-a-box\nStrategic Core\nReady-to-use\nAI & SPE\nUse\nCustom agentic & RAG (retrieval-augmented generation) chat\nCortex\nStrategic Core\nMedium\nAPI endpoints\nAI & SPE\nConsult AI Architecture\nCustom low-code AI assistants and virtual agents\nCopilot Agents\nEmerging\nLow\nMicrosoft 365\nCollaboration Svcs\nUse\nChat in SharePoint\nCopilot SharePoint Agents\nEmerging\nReady-to-use\nSharePoint\nCollaboration Svcs\nUse\nAgents for specific tasks & workflows\nMicrosoft Copilot Studio\nEmerging\nMedium\nMicrosoft Teams; SharePoint; APIs; Azure AI Services\nCollaboration Svcs\nUse\nDeterministic dialog flow, e.g. decision trees\nKore.AI\nDeclining\nHigh\nAutomation\nConsult Automation\nService desk transactions with human escalation\nKore.AI\nSpecialized\nHigh\nMicrosoft Teams; OpenAI; ServiceNow;\nother prebuilt integrations\nAutomation\nConsult Automation\nScientific chat\nMicrosoft Science Engine\nSpecialized\nHigh\nAI & SPE\nConsult AI Architecture\nWas this helpful?\nTags:\nai\nsolution-guide\ntechhq\nEdit this page\nPrevious\nü§ñ Agentic AI\nNext\nüß≠ Knowledge Bases\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 6,
    "depth": 3,
    "crawled_at": "2026-02-25T10:07:47.700283"
  },
  "https://techhq.dc.lilly.com/docs/solution/ai/knowledge_bases": {
    "url": "https://techhq.dc.lilly.com/docs/solution/ai/knowledge_bases",
    "title": "üß≠ Knowledge Bases | Tech HQ",
    "description": "Stack Overflow Article: üß≠ Knowledge Bases",
    "h1": [
      "Knowledge Bases"
    ],
    "h2": [
      "ServiceNow Knowledge Base‚Äã",
      "Stack Overflow Enterprise‚Äã",
      "Microsoft Teams‚Äã",
      "Microsoft SharePoint‚Äã",
      "Microsoft Stream in SharePoint‚Äã",
      "Atlassian Confluence‚Äã",
      "GitHub‚Äã",
      "Facebook Docusaurus‚Äã"
    ],
    "h3": [],
    "text_content": "üß≠ Knowledge Bases | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nEcosystem\nPatterns\nPositioning\nExamples\nCoding Tools\nStandards\nAI Submission Guide Template\nüóÑÔ∏è AI & Intelligent Agents BLT\nü§ñ Agentic AI\nüß≠ Conversational AI\nüß≠ Knowledge Bases\nüß≠ Multimodal AI\nüß≠ Translation Services\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nü§ñ AI & Intelligent Agents\nüß≠ Knowledge Bases\nOn this page\nKnowledge Bases\nSynced daily from Stack Overflow\nSolution Guide\nLifecycle: Draft\nLast Update: 2025-06-22\nCapability Owner: Karl Mayer\nEBA Lead: Karl Mayer\nContributors & Reviewers:\nA guide for technical knowledge bases and knowledge sharing. This guide aims to provide technical teams with the best practices and\ntools for managing and sharing technical knowledge effectively.\nUse Case/Scenario\nTech Recommendation\nPositioning\nComplexity\nKey Integrations\nOwning Org/Team\nNext Step\nCanonical, trusted information on Tech@Lilly systems for IT Service Desk and end users\nServiceNow Knowledge Base\nStrategic\nLow\nChatNow\nDigital Core Workforce Productivity\nServiceNow Knowledge: How Tos and FAQs\nTech@Lilly-community-driven Q&A and articles for devs, tech leads, & architects\nStack Overflow Enterprise\nStrategic\nLow\nMicrosoft Teams, Slack\nDigital Core Enterprise Business Architecture\nVisit flow.lilly.com\nCollaboration and co-authoring for business and technical teams\nMicrosoft Teams\nStrategic\nLow\nMicrosoft Copilot\nDigital Core Collaboration\nMicrosoft Teams Help & Learning\nStructured, permission-controlled document management, team collaboration, and integration with Microsoft 365 tools\nMicrosoft SharePoint\nStrategic\nLow\nMicrosoft Copilot\nDigital Core Collaboration\nMicrosoft SharePoint Help & Learning\nVideo content, such as screen recordings, training sessions, or meeting recaps\nMicrosoft Stream in SharePoint\nStandard\nLow\nMicrosoft Copilot\nDigital Core Collaboration\nMicrosoft Stream FAQ\nProduct/project team wikis with Jira issue integration\nAtlassian Confluence\nStandard\nLow\nJira, Microsoft Copilot\nSPE EDAT\nConfluence Access\nVersion-controlled files and wikis for dev teams\nGitHub\nStandard\nLow\nMicrosoft Copilot\nSPE EDAT\nGitHub Access\nHighly-customized, knowledge-focused websites, e.g. Cortex, CATS, Cloud, TechHQ\nFacebook Docusaurus\nSpecialized\nHigh\nlocally supported\nConsult your area architect\nServiceNow Knowledge Base\n‚Äã\nServiceNow‚Äôs Knowledge Base enables organizations to capture, structure, and reuse knowledge efficiently. It supports\nthe creation of how-to articles, FAQs, and troubleshooting guides, helping users solve problems independently and\nreducing the burden on IT support teams. The platform promotes continuous improvement through feedback and article\nlinking, making it a dynamic and evolving knowledge\nrepository.\nhttps://www.servicenow.com/now-platform/knowledge-management.html\nhttps://www.servicenow.com/community/knowledge-management-articles/knowledge-management-and-document-management/ta-p/2961096\nStack Overflow Enterprise\n‚Äã\nStack Overflow Enterprise enhances internal collaboration by allowing developers and technical teams to ask questions,\nshare answers, and document solutions in a structured, searchable format. It reduces duplicated efforts, accelerates\nonboarding, and integrates with tools like Slack and Teams to bring knowledge into the flow of\nwork.\nhttps://stackoverflow.com/?products/enterprise\nMicrosoft Teams\n‚Äã\nMicrosoft Teams serves as a central hub for teamwork, combining chat, meetings, file sharing, and app integration. It\nfacilitates real-time collaboration and knowledge sharing across departments, enabling users to co-author documents and\naccess shared knowledge seamlessly. Teams also integrates with Microsoft Search and Copilot to surface relevant\ninformation contextually.\nhttps://techcommunity.microsoft.com/blog/microsoft_365blog/collaboration-communication-and-knowledge-sharing-with-microsoft-teams-sharepoin/1696335\nMicrosoft SharePoint\n‚Äã\nSharePoint provides a structured platform for managing and sharing content, knowledge, and applications. It supports\ndocument libraries, metadata tagging, and version control, making it ideal for building intranet portals and knowledge\nbases. SharePoint‚Äôs integration with Microsoft 365 ensures that knowledge is accessible and governed across the\norganization).\nhttps://learn.microsoft.com/en-us/office365/servicedescriptions/sharepoint-online-service-description/sharepoint-online-service-description\nMicrosoft Stream in SharePoint\n‚Äã\nMicrosoft Stream (on SharePoint) transforms video into a first-class knowledge asset. It allows users to record, upload,\nand share videos like training sessions and meeting recaps, with features like transcripts, chapters, and AI-powered\nsearch. This makes video content as accessible and manageable as documents, enhancing visual knowledge\nsharing.\nhttps://learn.microsoft.com/en-us/stream/streamnew/overview\nAtlassian Confluence\n‚Äã\nConfluence is a collaborative workspace that enables teams to create, organize, and share knowledge through\nopen-by-default pages and spaces. It supports real-time editing, integrates with Jira, and fosters a culture of\ntransparency and continuous improvement. Confluence is especially effective for project documentation and\ncross-functional collaboration.\nhttps://www.atlassian.com/software/confluence/resources/guides/best-practices/knowledge-management\nGitHub\n‚Äã\nGitHub supports knowledge sharing through version-controlled repositories, wikis, and pull requests. It enables\ndevelopers to document code, share best practices, and collaborate on projects transparently. Pull requests facilitate\npeer review and iterative improvement, embedding knowledge sharing directly into the development workflow.\nhttps://github.com/orgs/community/discussions/64860\nFacebook Docusaurus\n‚Äã\nDocusaurus is a static site generator tailored for technical documentation. It allows teams to build fast, customizable,\nand versioned documentation websites using Markdown and React. Ideal for developer portals and product docs, Docusaurus\nsupports multilingual content, search, and integration with CI/CD pipelines, making it a powerful tool for structured\nknowledge dissemination.\nhttps://docusaurus.io/community/resources\nWas this helpful?\nTags:\nai\ntechhq\nsolution-guide\nEdit this page\nPrevious\nüß≠ Conversational AI\nNext\nüß≠ Multimodal AI\nServiceNow Knowledge Base\nStack Overflow Enterprise\nMicrosoft Teams\nMicrosoft SharePoint\nMicrosoft Stream in SharePoint\nAtlassian Confluence\nGitHub\nFacebook Docusaurus\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 5,
    "depth": 3,
    "crawled_at": "2026-02-25T10:07:49.960871"
  },
  "https://techhq.dc.lilly.com/docs/solution/ai/multimodal_ai": {
    "url": "https://techhq.dc.lilly.com/docs/solution/ai/multimodal_ai",
    "title": "üß≠ Multimodal AI | Tech HQ",
    "description": "Stack Overflow Article: üß≠ Multimodal AI",
    "h1": [
      "üß≠ Multimodal AI"
    ],
    "h2": [
      "üé¨ Video Generation‚Äã",
      "üßë‚Äçüé§ AI Avatars (Lip Sync Videos)‚Äã",
      "üó£Ô∏è AI‚ÄëGenerated Speech‚Äã"
    ],
    "h3": [],
    "text_content": "üß≠ Multimodal AI | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nEcosystem\nPatterns\nPositioning\nExamples\nCoding Tools\nStandards\nAI Submission Guide Template\nüóÑÔ∏è AI & Intelligent Agents BLT\nü§ñ Agentic AI\nüß≠ Conversational AI\nüß≠ Knowledge Bases\nüß≠ Multimodal AI\nüß≠ Translation Services\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nü§ñ AI & Intelligent Agents\nüß≠ Multimodal AI\nOn this page\nüß≠ Multimodal AI\nSynced daily from Stack Overflow\nSolution Guide\nLifecycle: Rapidly Evolving\nLast Update: 2025-09-26\nEBA Lead: Doug Gorr\nContributors & Reviewers: Doug Gorr\nA guide for selecting and integrating generative AI technologies to accelerate the creation of marketing visuals, training content, and cinematic-quality media across the enterprise.\nüé¨ Video Generation\n‚Äã\nUse Case/Scenario\nTech Recommendation\nPositioning\nComplexity\nKey Integrations\nOwning Org/Team\nNext Step\nVideo, Images\nAdobe Firefly\nStandard\nLow\nAdobe Creative Cloud\nCTI / BU Global\nEvaluate Firefly via Adobe Creative Cloud, Adobe Express or Photoshop; Get a licence and use it.\nVideo, Images\nSora (OpenAI)\nStandard\nLow to High\nAzure API, MS Copilot\nDigital Core Cloud Team\nGenerate static images via MS Copilot. Video: Test in Azure AI Foundry Video Playground, if adequate, build something and integrate via Azure OpenAI Services.\nVideo with Audio, Images\nVeo3 (Google DeepMind)\nEmerging\nHigh\nGCP API, Google Gemini\nDigital Core Cloud Team\nRequest access via Google Cloud to evaluate for your needs.\nVideo\nNova Reel (AWS)\nStandard\nMedium to High\nAWS Bedrock API\nDigital Core Cloud Team\nTest your prompt in the AWS Bedrock Playground, if adequate, build something and integrate via AWS Bedrock.\nwarning\nGenerative video models are evolving rapidly.\nüßë‚Äçüé§ AI Avatars (Lip Sync Videos)\n‚Äã\nUse Case/Scenario\nTech Recommendation\nPositioning\nComplexity\nKey Integrations\nOwning Org/Team\nNext Step\nInteractive avatars\nAzure AI Avatars\nStandard\nHigh\nAzure Speech Studio, Azure API\nDigital Core Cloud Team\nTest in Azure Speech Studio, if adequate, build and integrate.\nAvatar videos\nSynthesia\nStandard\nHigh, Low\nSynthesia API, SaaS\nCTI / BU Global\nGreen & Yellow data use cases.\nAvatar videos, Interactive\nHeyGen\nEmerging\nHigh, Med, Low\nHeyGen API, Interactive Avatar SDK, SaaS\nCTI / BU Global\nWait! Contracting In Progress.\nwarning\nGenerative video models are evolving rapidly.\nüó£Ô∏è AI‚ÄëGenerated Speech\n‚Äã\nUse Case/Scenario\nTech Recommendation\nPositioning\nComplexity\nKey Integrations\nOwning Org/Team\nNext Step\nReal‚Äëtime voice agents / voice bots / digital people; ultra‚Äëlow latency speech‚Äëto‚Äëspeech\nAzure OpenAI Realtime (Speech‚Äëto‚ÄëSpeech) API\nEmerging\nHigh (Streaming API uses websockets - not REST)\nWebRTC session mgmt via Azure API, Web/Mobile front‚Äëends\nDigital Core Cloud Team\nBuild something and compare to your TTS alternatives. Confirm langugage needs will be met.\nCustomized voice pronunciations TTS for apps, recored voice, video voiceover, avatar voices.\nAzure Speech Studio & API\nStandard\nMed, Low\nAzure API, Azure Speech Studio\nDigital Core Cloud Team\nUse Azure Speech Studio to test voices and scripts. Customize pronunciations if needed, then export the audio file for use or integrate your voice via Azure API.\nSpeech-to-text, Text-to-Speech, Translation\nLilly Translate app\nStandard\nLow\nFull featured Lilly product\nDigital Core\nUse it.\nSpeech‚Äëto‚Äëtext\nAzure OpenAI Whisper (STT)\nStandard\nMed\nAzure API\nDigital Core Cloud Team\nImplement Whisper as STT default for new pipelines.\nWas this helpful?\nTags:\nai\nsolution-guide\ntechhq\nEdit this page\nPrevious\nüß≠ Knowledge Bases\nNext\nüß≠ Translation Services\nüé¨ Video Generation\nüßë‚Äçüé§ AI Avatars (Lip Sync Videos)\nüó£Ô∏è AI‚ÄëGenerated Speech\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 3,
    "depth": 3,
    "crawled_at": "2026-02-25T10:07:52.199049"
  },
  "https://techhq.dc.lilly.com/docs/solution/ai/ecosystem/experience_layer": {
    "url": "https://techhq.dc.lilly.com/docs/solution/ai/ecosystem/experience_layer",
    "title": "Experience Layer | Tech HQ",
    "description": "Last Updated: July 25, 2025",
    "h1": [
      "Experience Layer"
    ],
    "h2": [
      "Definition‚Äã",
      "Components‚Äã",
      "Component Details‚Äã"
    ],
    "h3": [
      "Agent Catalog‚Äã",
      "Tool Catalog‚Äã",
      "Model Catalog‚Äã"
    ],
    "text_content": "Experience Layer | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nEcosystem\nExperience Layer\nAI Layer\nData Layer\nModel Layer\nPatterns\nPositioning\nExamples\nCoding Tools\nStandards\nAI Submission Guide Template\nüóÑÔ∏è AI & Intelligent Agents BLT\nü§ñ Agentic AI\nüß≠ Conversational AI\nüß≠ Knowledge Bases\nüß≠ Multimodal AI\nüß≠ Translation Services\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nü§ñ AI & Intelligent Agents\nEcosystem\nExperience Layer\nOn this page\nExperience Layer\nDocument Information\nLast Updated:\nJuly 25, 2025\nOwner:\nBrian Lewis\nPoint of Contact:\nAli Kharazmi\nContributors and Reviewers:\nArchit Kaila, Haitham Maya, Malika Mahoui\nDefinition\n‚Äã\nConsistent human interfaces for bringing AI-enabled products to meet users and customers where they are. This layer focuses on creating intuitive and accessible user interfaces that deliver AI capabilities to end users in a seamless manner.\nComponents\n‚Äã\nComponents\nAgent Catalog\nCortex Landing Zone\nTool Catalog\nCortex Landing Zone\nModel Catalog\nLLM Gateway\nComponent Details\n‚Äã\nAgent Catalog\n‚Äã\nMarketplace of pre-built task-oriented agents built across Lilly\nPlatform\n:\nCortex Landing Zone\nTool Catalog\n‚Äã\nMarketplace of approved AI-enabled tools, APIs, and re-usable components enabling teams to discover and integrate capabilities to their agentic systems quickly\nPlatform\n:\nCortex Landing Zone\nModel Catalog\n‚Äã\nMarketplace exposing foundation and Lilly fine-tuned models with versioning, metrics, and cost data so engineers can pick the right model\nPlatform\n:\nLLm Gateway\nWas this helpful?\nEdit this page\nPrevious\nEcosystem\nNext\nAI Layer\nDefinition\nComponents\nComponent Details\nAgent Catalog\nTool Catalog\nModel Catalog\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 4,
    "depth": 3,
    "crawled_at": "2026-02-25T10:07:54.461262"
  },
  "https://techhq.dc.lilly.com/docs/solution/ai/ecosystem/ai_layer": {
    "url": "https://techhq.dc.lilly.com/docs/solution/ai/ecosystem/ai_layer",
    "title": "AI Layer | Tech HQ",
    "description": "Last Updated: July 25, 2025",
    "h1": [
      "AI Layer"
    ],
    "h2": [
      "Definition‚Äã",
      "Components‚Äã",
      "Component Details‚Äã"
    ],
    "h3": [
      "GenAI‚Äã",
      "AgenticAI‚Äã"
    ],
    "text_content": "AI Layer | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nEcosystem\nExperience Layer\nAI Layer\nData Layer\nModel Layer\nPatterns\nPositioning\nExamples\nCoding Tools\nStandards\nAI Submission Guide Template\nüóÑÔ∏è AI & Intelligent Agents BLT\nü§ñ Agentic AI\nüß≠ Conversational AI\nüß≠ Knowledge Bases\nüß≠ Multimodal AI\nüß≠ Translation Services\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nü§ñ AI & Intelligent Agents\nEcosystem\nAI Layer\nOn this page\nAI Layer\nDocument Information\nLast Updated:\nJuly 25, 2025\nOwner:\nBrian Lewis\nPoint of Contact:\nAli Kharazmi\nContributors and Reviewers:\nArchit Kaila, Haitham Maya, Malika Mahoui\nDefinition\n‚Äã\nRe-usable capabilities merging deterministic and probabilistic AI approaches; accessible to local engineers. This layer provides modular AI components that combine rule-based (deterministic) and statistical (probabilistic) methods that can be leveraged by developers within the organization.\nComponents\n‚Äã\nComponents\nprimary\nChina\nprimary\nChina\nGenAI\nCortex\nCAFe\nAgentic AI\nCortex\nCAFe\nComponent Details\n‚Äã\nGenAI\n‚Äã\nCortex is our primary and built GenAI Platform that offers a Low-Pro Code environment for software engineers to build secure GenAI solutions with Guardrails. Our secondary platform is Scale AI, and for China we have CAFe.\nPlatforms\n:\nCortex\n,\nCAFe\nAgenticAI\n‚Äã\nCortex is our primary and built GenAI Platform that supports building multi agent, tool-calling systems that can reason over knowledgebases, execute plans, and persist context with memory. Our secondary platform is Scale AI, and for China we have CAFe.\nPlatforms\n:\nCortex\n,\nCAFe\nWas this helpful?\nEdit this page\nPrevious\nExperience Layer\nNext\nData Layer\nDefinition\nComponents\nComponent Details\nGenAI\nAgenticAI\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 3,
    "depth": 3,
    "crawled_at": "2026-02-25T10:07:56.648108"
  },
  "https://techhq.dc.lilly.com/docs/solution/ai/ecosystem/model_layer": {
    "url": "https://techhq.dc.lilly.com/docs/solution/ai/ecosystem/model_layer",
    "title": "Model Layer | Tech HQ",
    "description": "Last Updated: July 25, 2025",
    "h1": [
      "Model Layer"
    ],
    "h2": [
      "Definition‚Äã",
      "Components‚Äã",
      "Model‚Äã",
      "Infrastructure Components‚Äã",
      "Operation Component‚Äã"
    ],
    "h3": [
      "LLM Gateway‚Äã",
      "Model Training‚Äã",
      "Model Gateway‚Äã",
      "Model Lab‚Äã",
      "CATS‚Äã",
      "Kubed‚Äã",
      "MagTrain‚Äã",
      "Model Eval‚Äã",
      "Live Guardrails‚Äã",
      "Red Teaming‚Äã",
      "Detection Guardrails‚Äã"
    ],
    "text_content": "Model Layer | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nEcosystem\nExperience Layer\nAI Layer\nData Layer\nModel Layer\nPatterns\nPositioning\nExamples\nCoding Tools\nStandards\nAI Submission Guide Template\nüóÑÔ∏è AI & Intelligent Agents BLT\nü§ñ Agentic AI\nüß≠ Conversational AI\nüß≠ Knowledge Bases\nüß≠ Multimodal AI\nüß≠ Translation Services\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nü§ñ AI & Intelligent Agents\nEcosystem\nModel Layer\nOn this page\nModel Layer\nDocument Information\nLast Updated:\nJuly 25, 2025\nOwner:\nBrian Lewis\nPoint of Contact:\nAli Kharazmi\nContributors and Reviewers:\nArchit Kaila, Haitham Maya, Malika Mahoui\nDefinition\n‚Äã\nLeading commercial and strategic capabilities for probabilistic approaches, providing common controls, MLOps, and LLMOps features, underpinning AI-enabled products; reserved for domain experts. This layer manages the lifecycle of machine learning and large language models with specialized tools and governance for expert users.\nComponents\n‚Äã\nModel Eval\nAI\nLive Guardrails\nSPE\nRed Teaming\nScale AI\nDetection Guardrails\nScale AI\nCATS\nKubernetes\nKubed\nKubernetes\nMagTrain\nMagTrain\nLLM Gateway\nLiteLLM\nModel Hub\nModel hub\nModel Training\nAWB\nModel Lab\nAWB\nModel Gateway\nModel\nInfrastructure\nOperations\nModel\n‚Äã\nLLM Gateway\nLiteLLM\nModel Hub\nModel hub\nModel Training\nAWB\nModel Lab\nAWB\nModel Gateway\nLLM Gateway\n‚Äã\nUnified SDK raw model access to foundational LLMs with load balancing, guardrail checks, and usage monitoring\nPlatform\n:\nLLM Gateway\nModel Training\n‚Äã\nScalable service leveraging distributed compute & accelerators, with hyperparameter tracking and tuning functionality for training large AI models\nPlatform\n:\nModel Training\nModel Gateway\n‚Äã\nInference serving layer for any Lilly fine-tuned ML model, giving standardized endpoints, authentication, and blue/green deployment support\nPlatform\n:\nModel Gateway\nModel Lab\n‚Äã\nSandbox, Air-Gapped, governed workspace (notebooks + managed compute) where data scientists explore data, fine-tune models, and run experiments\nInfrastructure Components\n‚Äã\nCATS\n‚Äã\nCloud infrastructure to deploy Lilly internal applications\nPlatform\n:\nCATS\nKubed\n‚Äã\nCloud infrastructure to deploy Lilly external applications\nPlatform\n:\nKubed\nMagTrain\n‚Äã\nDedicated GPU/HPC cluster with secure data enclaves and ML-optimized storage, orchestrated for heavy model-training jobs\nOperation Component\n‚Äã\nModel Eval\n‚Äã\nContinuous evaluation and benchmarking of models against curated pharma tasks & public datasets, tracking drift, bias, and performance regressions over versions\nLive Guardrails\n‚Äã\nCortex-Guard: In-line governance hooks that intercept prompts, responses, and data flows to enforce policy checks, PII masking, and security controls before content reaches end users\nRed Teaming\n‚Äã\nStructured adversarial testing that bombards models and agents with malicious or edge-case inputs to surface vulnerabilities and measure robustness\nDetection Guardrails\n‚Äã\nBatch service that scans data for AE/PC, toxicity, bias, or policy violations, issuing explainable reports and remediation guidance\nWas this helpful?\nEdit this page\nPrevious\nData Layer\nNext\nPatterns\nDefinition\nComponents\nModel\nLLM Gateway\nModel Training\nModel Gateway\nModel Lab\nInfrastructure Components\nCATS\nKubed\nMagTrain\nOperation Component\nModel Eval\nLive Guardrails\nRed Teaming\nDetection Guardrails\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 2,
    "depth": 3,
    "crawled_at": "2026-02-25T10:07:58.871850"
  },
  "https://techhq.dc.lilly.com/docs/solution/ai/ecosystem/data_layer": {
    "url": "https://techhq.dc.lilly.com/docs/solution/ai/ecosystem/data_layer",
    "title": "Data Layer | Tech HQ",
    "description": "Last Updated: July 25, 2025",
    "h1": [
      "Data Layer"
    ],
    "h2": [
      "Definition‚Äã",
      "Components‚Äã",
      "Component Details‚Äã"
    ],
    "h3": [
      "Structured Knowledgebases‚Äã",
      "Un-Structured Knowledgebases‚Äã"
    ],
    "text_content": "Data Layer | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nEcosystem\nExperience Layer\nAI Layer\nData Layer\nModel Layer\nPatterns\nPositioning\nExamples\nCoding Tools\nStandards\nAI Submission Guide Template\nüóÑÔ∏è AI & Intelligent Agents BLT\nü§ñ Agentic AI\nüß≠ Conversational AI\nüß≠ Knowledge Bases\nüß≠ Multimodal AI\nüß≠ Translation Services\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nü§ñ AI & Intelligent Agents\nEcosystem\nData Layer\nOn this page\nData Layer\nDocument Information\nLast Updated:\nJuly 25, 2025\nOwner:\nBrian Lewis\nPoint of Contact:\nAli Kharazmi\nContributors and Reviewers:\nArchit Kaila, Haitham Maya, Malika Mahoui\nDefinition\n‚Äã\nAI-enabled and enabling capabilities, such as data products, vector databases, semantic layers, and synthetic data. This layer provides the structured and unstructured data resources necessary for AI systems, including specialized storage and processing technologies optimized for AI workloads.\nComponents\n‚Äã\nComponents\nStructured Knowledgebases\nData Marketplace\nUn-Structured Knowledgebases\nData Marketplace\nComponent Details\n‚Äã\nStructured Knowledgebases\n‚Äã\nCurated data products with schemas, ontologies, and versioning optimized for structured query, reasoning, and compliance traceability\nPlatform\n:\nData Marketplace\nUn-Structured Knowledgebases\n‚Äã\nIndexed documents, images, and media enriched with embeddings and metadata to enable semantic & vector search across Lilly knowledge\nPlatform\n:\nData Marketplace\nWas this helpful?\nEdit this page\nPrevious\nAI Layer\nNext\nModel Layer\nDefinition\nComponents\nComponent Details\nStructured Knowledgebases\nUn-Structured Knowledgebases\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 0,
    "depth": 3,
    "crawled_at": "2026-02-25T10:08:01.114276"
  },
  "https://techhq.dc.lilly.com/docs/solution/ai/patterns/agentic_patterns": {
    "url": "https://techhq.dc.lilly.com/docs/solution/ai/patterns/agentic_patterns",
    "title": "Agentic | Tech HQ",
    "description": "Last Updated: July 25, 2025",
    "h1": [
      "Agentic",
      "Patterns",
      "Memory",
      "Evaluation"
    ],
    "h2": [
      "Agent Pattern Frameworks: Strategic Overview‚Äã",
      "Single Agent‚Äã",
      "Supervisor-Worker Agent‚Äã",
      "ReAct (Reasoning and Action)‚Äã",
      "Plan and Execute‚Äã",
      "Self Critique‚Äã",
      "Peer Network Collaboration‚Äã",
      "Parallel Collaboration‚Äã",
      "Chain of Thought‚Äã",
      "Tree of Thought‚Äã",
      "Graph of Thought‚Äã",
      "System to System‚Äã"
    ],
    "h3": [
      "Foundation Patterns‚Äã",
      "Advanced Cognitive Patterns‚Äã",
      "Collaboration Patterns‚Äã",
      "Memory Architecture‚Äã",
      "Evaluation Framework‚Äã",
      "Pattern Integration‚Äã",
      "Comprehensive Agent Patterns Ecosystem‚Äã"
    ],
    "text_content": "Agentic | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nEcosystem\nPatterns\nQuantum Computing\nAgentic\nWorkflows\nBuilding Agentic Patterns\nContext Engineering\nDeep Agents\nDeep Research\nModel Foundry\nProtocols\nPositioning\nExamples\nCoding Tools\nStandards\nAI Submission Guide Template\nüóÑÔ∏è AI & Intelligent Agents BLT\nü§ñ Agentic AI\nüß≠ Conversational AI\nüß≠ Knowledge Bases\nüß≠ Multimodal AI\nüß≠ Translation Services\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nü§ñ AI & Intelligent Agents\nPatterns\nAgentic\nOn this page\nAgentic\nDocument Information\nLast Updated:\nJuly 25, 2025\nOwner:\nBrian Lewis\nPoint of Contact:\nAli Kharazmi\nContributors and Reviewers:\nArchit Kaila, Haitham Maya, Malika Mahoui\nAgent Pattern Frameworks: Strategic Overview\n‚Äã\nAgent patterns represent architectural approaches to AI problem-solving, each with distinct strengths for specific use\ncases. These patterns form a spectrum from simple single-agent systems to complex multi-agent collaborations, providing\nsolutions for various complexity levels and task types.\nFoundation Patterns\n‚Äã\nSingle Agent, ReAct, and Plan-and-Execute patterns form the foundation, handling tasks ranging from straightforward\nconversational assistance to complex multi-step problems requiring systematic reasoning and structured execution.\nFoundation Patterns\nAdds Observation Feedback\nAdds Upfront Planning\nCore capability: Basic Input ‚Üí Output\nCore capability: Adaptive Feedback\nCore capability: Structured Approach\nSingle Agent Pattern\nReAct Pattern\nPlan & Execute Pattern\nKnowledge + Tool Use\nIterative Improvement\nStrategy Before Action\nAdvanced Cognitive Patterns\n‚Äã\nSelf-Critique, Tree of Thought, and Graph of Thought patterns enhance reasoning quality through metacognitive\nevaluation, path exploration, and complex concept mapping, particularly valuable for quality-critical tasks and complex\nproblem spaces.\nCognitive Patterns\nGenerate Single Path\nGenerate Multiple Paths\nEvaluate Paths\nGenerate Concept Network\nNavigate Connections\nGenerate Draft\nEvaluate & Refine\nInput Problem\nChain of Thought\nTree of Thought\nSelect Best Path\nGraph of Thought\nSynthesize Solution\nSelf-Critique\nImproved Output\nCollaboration Patterns\n‚Äã\nSupervisor-Worker, Peer Network, Parallel Collaboration, and System-to-System patterns enable multi-agent cooperation\nwith different coordination models‚Äîfrom hierarchical oversight to decentralized peer networks and external system\nintegration.\nCollaboration Patterns\nCommunication Patterns\nCoordination Models\nControl Type\nControl Type\nControl Type\nControl Type\nCommunication\nCommunication\nCommunication\nCommunication\nCentralized Communication\nProblem Input\nSupervisor-Worker\nPeer Network\nParallel Collaboration\nSystem-to-System\nHierarchical Control\nSemi-Autonomous Agents\nFully Autonomous Agents\nDirect Peer Communication\nIndependent Processing\nMemory Architecture\n‚Äã\nEffective agents require complementary memory systems: Working Memory for session context, Episodic Memory for\nconversation history, Semantic Memory for persistent facts, Long-Term Memory for archived information, and Procedural\nMemory for learned strategies.\nMemory Systems\nLong-Term\nMedium-Term\nShort-Term\nPurpose\nLifecycle\nPurpose\nPurpose\nLifecycle\nLifecycle\nPurpose\nPurpose\nStorage\nStorage\nFeeds\nArchives to\nConsolidates in\nInforms\nLong-Term Memory\nWorking Memory\nActive Session Context\nResets Between Sessions\nEpisodic Memory\nConversation Continuity\nSemantic Memory\nFactual Knowledge Base\nPersists Across Multiple Sessions\nPersists Across Multiple Users\nHistorical Context Archive\nProcedural Memory\nLearned Strategies\nVector Databases & Structured Tables\nPattern Recognition\nEvaluation Framework\n‚Äã\nComprehensive assessment requires multi-dimensional evaluation across tool usage, planning quality, core skills, memory\neffectiveness, self-reflection capabilities, system performance, safety constraints, and behavioral consistency.\nEvaluation Framework\nInforms\nEnables\nImpacts\nOutcome Evaluation\nAssesses\nAssesses\nAssesses\nPerformance Evaluation\nTask Completion, Efficiency, Resource Usage\nSafety Evaluation\nBoundary Handling, Error Recovery, Security Awareness\nBehavior Evaluation\nConsistency, Adaptability, Initiative\nProcess Evaluation\nAssesses\nAssesses\nAssesses\nTool Use Evaluation\nSelection Accuracy, Parameter Precision, Result Interpretation\nPlanning Evaluation\nStep Decomposition, Contingency Planning, Progress Monitoring\nReasoning Evaluation\nDomain Accuracy, Logical Soundness, Output Quality\nState Management\nAssesses\nAssesses\nMemory Evaluation\nContext Retention, State Consistency, Information Retrieval\nReflection Evaluation\nError Recognition, Self-Correction, Strategy Adaptation\nPattern Integration\n‚Äã\nThe most powerful agent systems often combine multiple patterns, memory types, and evaluation approaches to create\nspecialized solutions for complex tasks. Understanding these building blocks enables architects to design systems that\nleverage the strengths of each pattern while addressing their individual limitations.\nComprehensive Agent Patterns Ecosystem\n‚Äã\nFoundational Patterns\nBuilding blocks for\nagent interactions\nand decision-making\nCognitive Patterns\nAdvanced reasoning,\nself-assessment, and\nexploratory thinking\nCollaborative Patterns\nMulti-agent coordination,\nspecialized roles, and\ncollective intelligence\nSingle Agent\nCore input-output capability\nReAct Pattern\nReasoning + action feedback loop\nPlan & Execute\nStrategic planning before action\nChain of Thought\nStep-by-step reasoning\nSelf-Critique\nOutput quality assessment\nTree of Thought\nMultiple solution path exploration\nGraph of Thought\nNon-linear concept relationships\nSupervisor-Worker\nHierarchical task delegation\nPeer Network\nDirect agent communication\nParallel Collaboration\nIndependent parallel processing\nSystem-to-System\nCross-ecosystem integration\nModel Context Protocol\nTool access & context expansion\nAgent-to-Agent Protocol\nStandardized communication\nPatterns\nSingle Agent\n‚Äã\nRetrieves Information\nExecutes Functions\nGenerates\nUser Query/Task\nAgent/LLM\nKnowledge Base\nTool Set\nResponse to User\nReasoning\nMemory\nTool Selection\nFunction\n: Agent processes user input, retrieves information from knowledge sources when needed, and executes tools\nto perform actions beyond text generation\nWhen to Use\n: Foundational pattern for most agent applications; suitable for general task handling where a single\nagent can process inputs and generate appropriate outputs\nKey Advantage\n: Provides a complete input-to-output workflow with both informational and functional capabilities in\na simple, extensible architecture\nBest For\n: Conversational assistants, task automation, knowledge work, and any application where an intelligent\nagent needs to both access information and take actions\nCommonly known as\n: Basic Agent, Standalone Agent, Monolithic Agent.\nSupervisor-Worker Agent\n‚Äã\nSubmitted to\nDelegates Subtask 1\nDelegates Subtask 2\nDelegates Subtask 3\nReturns Results\nReturns Results\nReturns Results\nSynthesizes\nTask Input\nSupervisor Agent\nWorker Agent 1\nWorker Agent 2\nWorker Agent 3\nFinal Output\nFunction\n: Supervisor breaks down tasks, workers complete specialized subtasks, supervisor synthesizes results\nWhen to Use\n: Complex problems requiring specialized expertise, parallel processing, or multiple perspectives\nKey Advantage\n: Hierarchical oversight enables quality control and resolution of conflicting outputs\nBest For\n: Content creation, research synthesis, strategic analysis, and creative problem solving\nCommonly known as\n: Manager-Worker Pattern, Hierarchical Agent Architecture, Controller-Executor Pattern\nReAct (Reasoning and Action)\n‚Äã\nInitiates\nDetermines\nProduces\nInforms\nIf Goal Achieved\nLoop Until Task Complete\nTask Input\nReasoning Process\nTake Action\nObserve Results\nFinal Output\nFunction\n: Agent thinks, acts, observes results, then thinks again in continuous loop until goal is achieved\nWhen to Use\n: Tasks requiring systematic reasoning, tool use, and ability to adapt based on feedback\nKey Advantage\n: Creates transparent decision trail while enabling course correction from observations\nBest For\n: Information retrieval, multi-step problem solving, data analysis, and systematic troubleshooting\nCommonly known as\n: Think-Act-Observe Loop, Reasoning-Action Cycle, Observe-Think-Act Pattern\nPlan and Execute\n‚Äã\nInitiates\nCreates\nGuides\nImplements\nEncounters Problem\nUpdates\nCompletes All Steps\nTask Input\nPlanning Phase\nDetailed Plan\nExecution Phase\nStep 1, Step 2, Step 3...\nPlan Revision\nFinal Output\nFunction\n: First creates comprehensive plan with steps and contingencies, then systematically executes while\nmonitoring progress\nWhen to Use\n: Complex tasks with clear structure, predictable workflows, or situations where thorough planning\nreduces risks\nKey Advantage\n: Separates strategic thinking from tactical execution, enabling careful consideration before\ncommitment to action\nBest For\n: Software development, project management, complex workflows, document creation, and process automation\nCommonly known as\n:Deliberate-Act\nPattern, Strategic Execution Pattern\nSelf Critique\n‚Äã\nInitiates\nUndergoes\nIdentifies\nInforms\nIf Needed\nWhen Satisfactory\nIteration Loop\nTask Input\nInitial Solution\nCritical Evaluation\nWeaknesses & Issues\nSolution Revision\nFinal Output\nFunction\n: Agent creates solution, critically evaluates it against multiple criteria, identifies weaknesses, and\niteratively refines until quality threshold is met\nWhen to Use\n: Tasks where quality matters more than speed, complex problems with multiple valid approaches, or\nhigh-stakes outputs requiring error minimization\nKey Advantage\n: Improves output quality through metacognitive evaluation, reducing errors and blind spots inherent\nin single-pass generation\nBest For\n: Content creation, decision analysis, code generation, strategy development, and any task requiring\nrigorous quality control\nCommonly known as\n: Deep Research, Reflection Pattern, Self-Evaluation Loop, Metacognitive Pattern\nPeer Network Collaboration\n‚Äã\nInitial Distribution\nRoutes to\nRoutes to\nRoutes to\nCollaborate Directly\nCollaborate Directly\nCollaborate Directly\nContributes\nContributes\nContributes\nTask Input\nRouting Logic\nExpert Agent A\nExpert Agent B\nExpert Agent C\nFinal Output\nFunction\n: Specialized agents communicate directly with each other as peers, dynamically routing tasks and\ninformation based on expertise needs without centralized control\nWhen to Use\n: Complex problems with fuzzy domain boundaries, collaborative tasks requiring rich inter-agent\ncommunication, or situations where flexibility trumps structured hierarchy\nKey Advantage\n: Enables emergent problem solving as agents freely exchange information, negotiate responsibilities,\nand adapt the workflow based on discoveries during the process\nBest For\n: Creative collaboration, complex research, interdisciplinary analysis, dynamic problem spaces, and\nsimulating human team interactions\nCommonly known as\nAgent Mesh, Collaborative Agent Network, Decentralized Agent System, Multi-Agent Cooperative\nSystem\nParallel Collaboration\n‚Äã\nDecomposed by\nSubtask A\nSubtask B\nSubtask C\nResult A\nResult B\nResult C\nCombines\nTask Input\nTask Decomposer\nAgent A\nAgent B\nAgent C\nResult Aggregator\nFinal Output\nFunction\n: Task is decomposed into independent subtasks, multiple agents work simultaneously without direct\ncommunication, results are combined afterward\nWhen to Use\n: Tasks with naturally parallel components, situations requiring maximum throughput, or problems where\nsubtask independence can be maintained\nKey Advantage\n: Maximum efficiency through true parallelism with minimal coordination overhead or communication\nbottlenecks\nBest For\n: Batch processing, independent analyses of different datasets, distributed searching, content generation\nacross distinct domains, and massively parallelizable computations\nCommonly known as\n: Map-Reduce Agent Pattern ,Distributed Task Processing, Divide and Conquer Agents\nChain of Thought\n‚Äã\nInitiates\nLeads to\nLeads to\nLeads to\nLeads to\nProduces\nProblem Input\nReasoning Step 1\nReasoning Step 2\nReasoning Step 3\nReasoning Step 4\nReasoning Step 5\nFinal Solution\nFunction\n: Agent breaks down complex reasoning into explicit sequential steps, articulating each logical connection\nfrom problem statement to conclusion\nWhen to Use\n: Problems requiring transparent reasoning, tasks where step-by-step logic improves accuracy, or\nsituations where the reasoning process itself needs evaluation\nKey Advantage\n: Improves reasoning quality by forcing explicit articulation of intermediate steps, reducing logical\nleaps and enabling targeted refinement\nBest For\n: Mathematical problem solving, logical deductions, complex analyses, and any task where showing the work\nis as important as the final answer\nCommonly known as\n: Step-by-Step Reasoning, Explicit Reasoning Chain, Verbalized Thinking Process\nTree of Thought\n‚Äã\nGenerates\nOption A\nOption B\nOption C\nEvaluate\nEvaluate\nEvaluate\nEvaluate\nPromising\nRejected\nRejected\nRejected\nRejected\nProduces\nProblem Input\nInitial State\nReasoning Branch A\nReasoning Branch B\nReasoning Branch C\nSub-branch A1\nSub-branch A2\nSub-branch B1\nSub-branch B2\nSelected Path\nDiscarded\nDiscarded\nDiscarded\nDiscarded\nFinal Solution\nFunction\n: Agent explores multiple potential reasoning pathways simultaneously, evaluating each branch's promise,\npruning unproductive paths, and ultimately selecting the most viable solution\nWhen to Use\n: Complex problems with multiple potential approaches, situations with high uncertainty, strategic\ndecision-making, or any scenario where exploring alternative hypotheses improves outcomes\nKey Advantage\n: Reduces risk of getting stuck in suboptimal solution paths by systematically exploring and\nevaluating multiple possibilities before committing to one\nBest For\n: Creative problem-solving, strategic planning, game-playing agents, complex decision analysis, and\nsituations where the optimal solution path isn't immediately obvious\nGraph of Thought\n‚Äã\nInitiates\nInitiates\nRelates to\nConnects with\nInfluences\nConnects with\nBuilds on\nCombines with\nRelates to\nContradicts\nNecessitates\nAlternative path\nProduces\nProblem Input\nThought Node 1\nThought Node 2\nThought Node 3\nThought Node 4\nThought Node 5\nThought Node 6\nThought Node 7\nFinal Solution\nFunction\n: Agent builds a complex network of interconnected thoughts with non-hierarchical relationships, allowing\nideas to reference, contradict, support, or build upon each other in multidimensional ways\nWhen to Use\n: Problems requiring complex system thinking, scenarios with multiple interdependent variables, or\nsituations where relationships between concepts are cyclical or non-linear\nKey Advantage\n: Enables representation of rich conceptual relationships that can't be captured in linear chains or\nhierarchical trees, allowing for more nuanced exploration of complex problem spaces\nBest For\n: System design, complex causal analysis, interdisciplinary reasoning, knowledge mapping, multi-constraint\nproblems, and modeling complex relationships between concepts\nSystem to System\n‚Äã\nAPI Call/Response\nAPI Call/Response\nAPI Call/Response\nUser Query/Task\nPrimary Agent\nFinal Response\nExternal Agent A\nExternal Agent B\nExternal Agent C\nFunction\n: Primary agent orchestrates interactions with independent third-party agent systems, translating requests\nand synthesizing responses across different APIs\nWhen to Use\n: Tasks requiring specialized capabilities from multiple external systems or situations needing\ncomposite intelligence from diverse agent ecosystems\nKey Advantage\n: Leverages specialized capabilities from different providers while maintaining a unified experience\nfor the user\nBest For\n: Complex workflows requiring diverse expertise, cross-platform automation, and tasks requiring\ncapabilities beyond a single agent system\nMemory\nActive Session Data\nConversation History\nFacts & Knowledge\nRetrieved Context\nAction Patterns\nArchives to\nConsolidates in\nInforms\nFetches from\nProvides to\nAgent/LLM\nResponse to User\nUser Query\nWorking Memory\nEpisodic Memory\nSemantic Memory\nLong-Term Memory\nProcedural Memory\nRetrieval System\nWorking Memory\n: Temporary session data that resets between interactions\nEpisodic Memory\n: Conversation history across multiple sessions\nSemantic Memory\n: Persistent facts about users and domains\nLong-Term Memory\n: Retrievable historical context in vector stores or databases\nProcedural Memory\n: Learned effective strategies and tool usage patterns\nEvaluation\nFeedback\nTask Input\nReasoning\nPlanning\nAction\nObservation\nResult Synthesis\nOutput\nSkill\nMemory\nBehavior\nPlanning\nSafety\nTool Use\nPerformance\nReflection\nTool Calling & Selection\n: Assesses appropriate tool usage\nPlanning & Strategy\n: Evaluates task decomposition efficiency\nSkill-Based\n: Tests core capability performance\nMemory & State Management\n: Measures context retention effectiveness\nReflection & Self-Evaluation\n: Assesses error recognition capability\nEnd-to-End System\n: Evaluates overall task completion\nSafety & Reliability\n: Tests boundary condition handling\nBehavioral Testing\n: Measures response consistency patterns\nWas this helpful?\nEdit this page\nPrevious\nQuantum Computing\nNext\nWorkflows\nAgent Pattern Frameworks: Strategic Overview\nFoundation Patterns\nAdvanced Cognitive Patterns\nCollaboration Patterns\nMemory Architecture\nEvaluation Framework\nPattern Integration\nComprehensive Agent Patterns Ecosystem\nSingle Agent\nSupervisor-Worker Agent\nReAct (Reasoning and Action)\nPlan and Execute\nSelf Critique\nPeer Network Collaboration\nParallel Collaboration\nChain of Thought\nTree of Thought\nGraph of Thought\nSystem to System\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 10,
    "depth": 3,
    "crawled_at": "2026-02-25T10:08:03.360551"
  },
  "https://techhq.dc.lilly.com/docs/solution/ai/patterns/agents_and_workflows": {
    "url": "https://techhq.dc.lilly.com/docs/solution/ai/patterns/agents_and_workflows",
    "title": "Workflows | Tech HQ",
    "description": "Last Updated: July 25, 2025",
    "h1": [
      "Workflows",
      "What Are Agents and Agentic Systems",
      "Agents and Workflows: Choosing the Right Approach",
      "Workflow Patterns and Future Directions",
      "Conceptual Hierarchy",
      "References"
    ],
    "h2": [
      "Tool Calling‚Äã",
      "From Tools to Agents‚Äã",
      "Agentic AI Systems‚Äã",
      "Building Blocks of an Agentic System‚Äã",
      "When to Use Workflows vs. Agents‚Äã",
      "Understanding Workflow Patterns‚Äã",
      "Workflow: Prompt Chaining‚Äã",
      "Workflow: Routing‚Äã",
      "Workflow: Parallelization‚Äã",
      "Workflow: Orchestrator-Workers‚Äã",
      "Workflow: Evaluator-Optimizer‚Äã",
      "Looking Ahead: Agentic Patterns‚Äã"
    ],
    "h3": [
      "Use Workflows When:‚Äã",
      "Use Agents When:‚Äã",
      "Complexity-Performance Tradeoff‚Äã"
    ],
    "text_content": "Workflows | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nEcosystem\nPatterns\nQuantum Computing\nAgentic\nWorkflows\nBuilding Agentic Patterns\nContext Engineering\nDeep Agents\nDeep Research\nModel Foundry\nProtocols\nPositioning\nExamples\nCoding Tools\nStandards\nAI Submission Guide Template\nüóÑÔ∏è AI & Intelligent Agents BLT\nü§ñ Agentic AI\nüß≠ Conversational AI\nüß≠ Knowledge Bases\nüß≠ Multimodal AI\nüß≠ Translation Services\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nü§ñ AI & Intelligent Agents\nPatterns\nWorkflows\nOn this page\nWorkflows\nDocument Information\nLast Updated:\nJuly 25, 2025\nOwner:\nBrian Lewis\nPoint of Contact:\nAli Kharazmi\nContributors and Reviewers:\nArchit Kaila, Haitham Maya, Malika Mahoui\nIn this article we explore the architectural patterns of AI agents and workflows, examining how LLMs can be orchestrated\nto perform complex tasks through tool calling, reasoning, and various execution strategies.\nTool Calling\n‚Äã\nFunction calling enables LLMs to emit structured JSON \"calls\" to predefined functions (i.e., tools or APIs), rather than\nfree-form text, allowing seamless integration with external systems. A key principle of tool calling is that the model\ndecides when to use a tool based on the input's relevance.\nTool Binding\nWhat is 2 multiplied by 3?\nLLM with Tool Binding\nTool Definition\nPayload:\narguments: {'a':2, 'b':3}\ntool_name: multiply\ndef multiply(a,b):\nreturn a*b\nThis mechanism underpins modern agent frameworks by giving LLMs the ability to decide autonomously which tool to invoke\nand with what parameters, and even to chain multiple calls in service of complex tasks.\nFrom Tools to Agents\n‚Äã\nWhile tool calling provides LLMs with the ability to interact with external systems, agents represent a more\nsophisticated integration where these capabilities are combined with reasoning, planning, and autonomy. Agents leverage\ntool calling as a fundamental mechanism but extend it with additional capabilities.\n1.Natural language\n2.JSON call\n'function_name' + args\n3.Execute tool/api\n4.Return results\n5.Synthesize reply\nUser Prompt\nLLM (with function calling)\nDefined Functions/Tools\nFunction Executor\nFinal Response\nWhat Are Agents and Agentic Systems\nAgentic AI Systems\n‚Äã\nAgentic AI systems can autonomously perform tasks, make decisions and solve complex problems based on context and\nobjectives with minimal human supervision. These systems combine the flexibility of Gen-AI based reasoning (agents) with\nthe precision of traditional software engineering (tools) to plan, adapt and react.\nThe key components of agents are:\nAutonomy\n: Agents act independently within a defined scope, making their own decisions without continuous human\nintervention.\nGoal-Oriented\n: They break down high-level objectives into ordered sub-tasks, ensuring systematic progress toward\nspecific outcomes.\nTool Integration\n: Agents invoke external functions‚ÄîAPIs, databases, custom code‚Äîto execute real-world actions\nrather than just generate text.\nMemory & Context\n: By storing short- and long-term memories, they track past interactions and maintain continuity\nacross complex workflows.\nFeedback Loop\n: Agents evaluate the results of each action, learn from failures, and iteratively refine their\napproach using reinforcement learning or iterative planning.\nDecision-Making Framework\n: They employ reasoning strategies (e.g., chain-of-thought, rule-based logic) to choose\nwhich tools to use and how to sequence tasks for optimal performance.\nAction\nFeedback\nHuman\nLLM Call\nEnvironment\nStop\nBuilding Blocks of an Agentic System\n‚Äã\nAt the core of agentic systems is an LLM enhanced with capabilities like retrieval, tools, and memory. These models can\nactively decide what to search, which tools to use, and what to remember. When implementing, focus on tailoring these\naugmentations to your use case and providing a clear, well-documented interface for your LLM.\nAgents and Workflows: Choosing the Right Approach\nThere is an important architectural distinction between workflows and agents:\nWorkflows\n: Systems where LLMs and tools are orchestrated through predefined code paths. The logic is hard-coded,\nand execution follows a fixed sequence.\nAgents\n: Systems where LLMs dynamically determine their own actions and tool usage. They maintain control over task\nexecution, deciding how to proceed based on goals and context.\nWhen to Use Workflows vs. Agents\n‚Äã\nWhen building LLM-powered applications, it's important to choose the simplest solution that meets your requirements.\nThis decision framework can help determine the appropriate approach:\nUse Workflows When:\n‚Äã\nTasks are well-defined with predictable steps\nConsistency and reliability are critical priorities\nYou need tight control over execution paths\nPerformance and latency requirements are strict\nCost efficiency is a primary concern\nThe problem space is well-understood and stable\nUse Agents When:\n‚Äã\nTasks require dynamic decision-making and adaptation\nProblems are complex with unpredictable paths to solutions\nFlexibility is more important than deterministic behavior\nThe environment or requirements change frequently\nTasks benefit from autonomous exploration of solutions\nMultiple tools need to be orchestrated based on context\nComplexity-Performance Tradeoff\n‚Äã\nAgentic systems often trade latency and cost for better task performance. This tradeoff should be carefully considered:\nStart Simple\n: For many applications, optimizing single LLM calls with retrieval and in-context examples is\nsufficient\nAdd Structure\n: When more control is needed, implement workflows with predefined paths\nIntroduce Agency\n: Only move to fully agentic systems when the benefits of flexibility and autonomous\ndecision-making outweigh the increased complexity, latency, and cost\nThe diagram below illustrates this progression from simple to complex approaches:\nLower complexity\nLower latency\nLower cost\nHigher flexibility\nBetter at complex tasks\nMore autonomous\nSimple LLM Calls\nwith Retrieval\nStructured\nWorkflows\nAutonomous\nAgents\nPerformance\nTradeoffs\nWorkflow Patterns and Future Directions\nUnderstanding Workflow Patterns\n‚Äã\nWorkflow patterns are specific, reusable architectural approaches that solve common orchestration challenges. These\npatterns have emerged from practical experience implementing LLM-based systems and provide proven templates for\ndifferent use cases.\nEach pattern offers distinct advantages for particular scenarios, with tradeoffs in complexity, flexibility, and\nperformance. Understanding these patterns helps developers select the most appropriate architecture for their specific\nrequirements.\nWorkflow: Prompt Chaining\n‚Äã\nWhat it is\n: Breaks a complex task into a linear sequence of LLM calls, where each step depends on the output of\nthe previous one. Optional checks (\"gates\") can be added to validate intermediate results before continuing.\nUse Case\n: Tasks that naturally decompose into smaller, ordered subtasks.\nGoal\n: Improve accuracy by simplifying each step (with increased latency).\nExamples\n:\nWrite marketing copy ‚Üí translate it.\nDraft document outline ‚Üí verify ‚Üí write document.\nOutput 1\nPass\nFail\nOutput 2\nIn\nLLM Call 1\nGate\nLLM Call 2\nExit\nLLM Call 3\nOut\nWorkflow: Routing\n‚Äã\nWhat it is\n: An initial LLM (or classifier) decides what kind of input it's dealing with, then sends it to a\nspecialized follow-up process with tailored prompts, models, or tools.\nUse Case\n: Handling different input types that require different processing logic or prompts.\nGoal\n: Avoid one-size-fits-all prompts and optimize each case for performance.\nExamples\n:\nRoute support tickets by topic (refund, tech, billing).\nSend easy queries to small models, complex ones to powerful models.\nIn\nLLM Call Router\nLLM Call 1\nLLM Call 2\nLLM Call 3\nOut\nWorkflow: Parallelization\n‚Äã\nWhat it is\n: Executes multiple LLM tasks simultaneously. Two major types:\nSectioning\ndivides the task into parts that can be handled independently.\nVoting\nruns the same task multiple times to compare and combine outputs.\nUse Case\n: Tasks where subtasks are independent or benefit from diversity in answers.\nGoal\n: Reduce latency or improve quality through redundancy and diversity.\nExamples\n:\nSectioning\n: One model handles query; another filters for safety.\nVoting\n: Multiple LLMs review code or content and compare flags.\nIn\nLLM Call 2\nLLM Call 1\nLLM Call 3\nAggregator\nOut\nWorkflow: Orchestrator-Workers\n‚Äã\nWhat it is\n: A central LLM (the \"orchestrator\") dynamically analyzes the input, decides what subtasks are needed,\nand delegates each one to a \"worker\" LLM. The orchestrator then combines the results into a final answer.\nUse Case\n: Complex or open-ended tasks where required steps vary case by case.\nGoal\n: Introduce flexibility in how tasks are broken down and executed.\nExamples\n:\nModify code across multiple unpredictable files.\nAggregate and analyze info from multiple sources for research.\nIn\nOrchestrator\nLLM Call 1\nLLM Call 2\nLLM Call 3\nSynthesizer\nOut\nWorkflow: Evaluator-Optimizer\n‚Äã\nWhat it is\n: One LLM generates an initial output, and another LLM (or the same one in a different role) evaluates\nthat output and provides feedback or suggestions. This loop can repeat until the result meets defined quality\nstandards.\nUse Case\n: Tasks that benefit from refinement and where clear quality signals are available.\nGoal\n: Use iteration and self-critique to improve outputs.\nExamples\n:\nRefine literary translation based on nuance critiques.\nPerform multi-round research and validation for deeper coverage.\nSolution\nAccepted\nRejected + Feedback\nIn\nLLM Call Generator\nLLM Call Evaluator\nOut\nLooking Ahead: Agentic Patterns\n‚Äã\nWhile this article has focused primarily on workflow patterns, the agent side of the hierarchy deserves equal attention.\nIn our upcoming article \"Agentic Patterns,\" we will explore in depth the different types of agent architectures:\nSingle Agents\n: Autonomous systems that operate independently, using various reasoning and decision-making\nstrategies to accomplish tasks\nMulti-Agents\n: Collaborative systems where multiple agents work together, often specializing in different aspects\nof a problem\nThese agent patterns represent the next frontier in LLM applications, enabling even more sophisticated reasoning,\nplanning, and problem-solving capabilities. By understanding both workflow patterns and agent patterns, developers can\nmake informed decisions about which architectural approaches best suit their specific use cases.\nConceptual Hierarchy\nThe diagram below illustrates the conceptual hierarchy between tool calling, agents, and workflow patterns:\nTool Calling\nAgents\nWorkflows\nSingle Agents\nMulti Agents\nWorkflow Patterns\nPrompt Chaining\nRouting\nParallelization\nOrchestrator-Workers\nEvaluator-Optimizer\nReferences\nLangChain Tool Calling Concepts\nLangChain Workflow\nLangChain Tool Calling How-To\nOpenAI Function Calling Guide\nPrompting Guide: Function Calling\nAnthropic: Building Effective Agents\nLangGraph Multi-agent systems\nWas this helpful?\nTags:\nagent-workflows\nEdit this page\nPrevious\nAgentic\nNext\nBuilding Agentic Patterns\nTool Calling\nFrom Tools to Agents\nAgentic AI Systems\nBuilding Blocks of an Agentic System\nWhen to Use Workflows vs. Agents\nUse Workflows When:\nUse Agents When:\nComplexity-Performance Tradeoff\nUnderstanding Workflow Patterns\nWorkflow: Prompt Chaining\nWorkflow: Routing\nWorkflow: Parallelization\nWorkflow: Orchestrator-Workers\nWorkflow: Evaluator-Optimizer\nLooking Ahead: Agentic Patterns\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 9,
    "depth": 3,
    "crawled_at": "2026-02-25T10:08:05.617641"
  },
  "https://techhq.dc.lilly.com/docs/solution/ai/patterns/context_engineering": {
    "url": "https://techhq.dc.lilly.com/docs/solution/ai/patterns/context_engineering",
    "title": "Context Engineering | Tech HQ",
    "description": "Last Updated: July 25, 2025",
    "h1": [
      "Context Engineering"
    ],
    "h2": [
      "Context Engineering Overview‚Äã",
      "Types of Context‚Äã",
      "Context Challenges‚Äã",
      "Strategies for Agent Context Engineering‚Äã",
      "Compress‚Äã",
      "Isolate‚Äã",
      "Context Engineering with LangGraph‚Äã",
      "References‚Äã"
    ],
    "h3": [
      "Write‚Äã",
      "Select‚Äã",
      "Write Context‚Äã",
      "Select Context‚Äã",
      "Compress Context‚Äã",
      "Isolate Context‚Äã"
    ],
    "text_content": "Context Engineering | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nEcosystem\nPatterns\nQuantum Computing\nAgentic\nWorkflows\nBuilding Agentic Patterns\nContext Engineering\nDeep Agents\nDeep Research\nModel Foundry\nProtocols\nPositioning\nExamples\nCoding Tools\nStandards\nAI Submission Guide Template\nüóÑÔ∏è AI & Intelligent Agents BLT\nü§ñ Agentic AI\nüß≠ Conversational AI\nüß≠ Knowledge Bases\nüß≠ Multimodal AI\nüß≠ Translation Services\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nü§ñ AI & Intelligent Agents\nPatterns\nContext Engineering\nOn this page\nContext Engineering\nDocument Information\nLast Updated:\nJuly 25, 2025\nOwner:\nBrian Lewis\nPoint of Contact:\nAli Kharazmi\nContributors and Reviewers:\nArchit Kaila, Haitham Maya, Malika Mahoui\nAgents need context to perform tasks. Context engineering is the art and science of filling the context window with just the right information at each step of an agent's trajectory.\nContext Engineering Overview\n‚Äã\nCore\nIsolate\nSplit Across Sub-agents\nSeparation of Concern\nCode Environment\nSeparation\nRuntime State Object\nCompress\nSummarizing\nRecursive/Hierarchical\nSummarization\nAgent to Agent\nSummarization\nTrimming\nRemove Older Messages\nFrom List\nSelect\nRead State Object/Scratchpad\nSelecting Memory\nPull Narrow Sets of File\nKnowledge Graph for\nMemory Indexing\nWrite\nScratchpad\nTool Calling\nWrite to scratchpad file\nRuntime State Object\nMemory\nSelf Generated Memories\nAcross Sessions\nContext Engineering\nWrite\nSelect\nCompress\nIsolate\nTypes of Context\n‚Äã\nContext engineering serves as an umbrella that applies across a few different context types:\nInstructions\n‚Äì prompts, memories, few-shot examples, tool descriptions, etc.\nKnowledge\n‚Äì facts, memories, etc.\nTools\n‚Äì feedback from tool calls\nContext Challenges\n‚Äã\nLong-running tasks and accumulating feedback from tool calls mean that agents often utilize a large number of tokens. This can degrade performance and opens up the door for hallucinations.\nStrategies for Agent Context Engineering\n‚Äã\nThese strategies can be categorized into 4 buckets:\nWrite\n‚Äì Saving context outside the context window to help an agent perform tasks\nSelect\n‚Äì Pulling relevant context into the context window when needed\nCompress\n‚Äì Retaining only the essential tokens required to perform tasks\nIsolate\n‚Äì Splitting context into separate components to optimize agent performance\nWrite\n‚Äã\nJust as humans take notes, agents can use \"scratchpads\" to persist information outside the context window. This approach saves critical information that remains accessible throughout task execution.\nImplementation Methods:\nSingle Session:\nFile-based storage\n‚Äì A tool call that writes information to a persistent file\nRuntime state\n‚Äì A field in a state object that persists during the session\nCross-Session:\n3.\nLong-term memory\n‚Äì Persistent storage that remembers information across multiple sessions (e.g., Windsurf, Cursor, ChatGPT)\nSelect\n‚Äã\nContext selection depends on how the scratchpad is implemented. If it's a tool, agents access it via tool calls. If it's part of the agent's runtime state, developers decide which parts of the state are exposed at each step, allowing more control over what context is visible during reasoning.\nMemory Usage\n‚Äã\nWhen agents have memory, they must also select which memories to use:\nEpisodic memories\nprovide few-shot examples of prior behavior.\nProcedural memories\nstore instructions to guide actions.\nSemantic memories\nstore factual knowledge relevant to tasks.\nSelecting the right memory improves performance and personalization.\nChallenges and Solutions\n‚Äã\nA key challenge is ensuring relevant memories are selected:\nMany code agents use fixed files (e.g.,\nCLAUDE.md\n,\nCursor's rules files\n) for consistent behavior.\nAgents with large memory stores (especially semantic memory) require advanced retrieval strategies.\nEmbeddings and knowledge graphs help with retrieval, but issues like irrelevant memory injection can still happen. For instance,\nChatGPT\nhas mistakenly inserted private user data into responses, raising concerns over context control and trust.\nCompress\n‚Äã\nAgent interactions can span hundreds of turns with token-heavy tool calls, requiring compression strategies to manage context efficiently.\nSummarization:\nUses LLMs to distill essential information from lengthy interactions\nClaude Code's\n\"auto-compact\" feature demonstrates this approach, summarizing full trajectories when context exceeds 95%\nCan employ recursive or hierarchical summarization strategies\nUseful for post-processing token-heavy tool calls (e.g., search results)\nApplied at agent-agent boundaries to reduce tokens during knowledge hand-off\nContext Trimming:\nFilters or \"prunes\" context using hard-coded heuristics rather than LLM processing\nExamples include removing older messages from conversation history\nMore deterministic than summarization but less intelligent about content relevance\nIsolate\n‚Äã\nContext isolation splits information across separate components to optimize agent performance.\nMulti-Agent:\nSplit context across sub-agents with specific tools and instructions\nOpenAI Swarm\nlibrary enables separation of concerns for specialized sub-tasks\nEach agent maintains its own context window\nSandboxed Environments:\nCode execution in isolated environments (e.g.,\nHuggingFace CodeAgent\n)\nSelected context from tool calls passed back to LLM\nPrevents context contamination between execution and reasoning\nState Objects:\nRuntime state schemas with selective field exposure\nContext written to specific fields, only relevant portions exposed to LLM\nEnables controlled context access without full sandboxing\nContext Engineering with LangGraph\n‚Äã\nWrite Context\n‚Äã\nLangGraph\nsupports short-term (thread-scoped) and long-term memory. Short-term memory uses checkpoints as a scratchpad, letting agents write and retrieve state across steps.\nSelect Context\n‚Äã\nAt each step, agents can:\nAccess short-term state\nto retrieve intermediate data\nQuery long-term memory\nusing different retrieval methods\nCompress Context\n‚Äã\nLangGraph\nlets you control context by passing a\nstate\nobject between nodes. A common method is using a message list and periodically\nsummarizing\nor\ntrimming it\nwith built-in tools.\nIsolate Context\n‚Äã\nLangGraph\nuses a structured\nstate\nobject where you define a schema. This lets you store tool outputs or other data in specific fields, keeping them isolated from the LLM until needed.\nReferences\n‚Äã\nContext Engineering for Agents\n- arXiv preprint. Available at:\nhttps://arxiv.org/pdf/2507.13334\nContext Engineering for Agents\n- LangChain Blog. Available at:\nhttps://blog.langchain.com/context-engineering-for-agents/\nWas this helpful?\nEdit this page\nPrevious\nBuilding Agentic Patterns\nNext\nDeep Agents\nContext Engineering Overview\nTypes of Context\nContext Challenges\nStrategies for Agent Context Engineering\nWrite\nSelect\nCompress\nIsolate\nContext Engineering with LangGraph\nWrite Context\nSelect Context\nCompress Context\nIsolate Context\nReferences\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 8,
    "depth": 3,
    "crawled_at": "2026-02-25T10:08:07.921053"
  },
  "https://techhq.dc.lilly.com/docs/solution/ai/patterns/protocols": {
    "url": "https://techhq.dc.lilly.com/docs/solution/ai/patterns/protocols",
    "title": "Protocols | Tech HQ",
    "description": "Last Updated: July 25, 2025",
    "h1": [
      "Protocols",
      "Agent to Tool Communication",
      "Agent to Agent Communication",
      "References"
    ],
    "h2": [
      "Industry Standard Protocols‚Äã",
      "Model Context Protocol (MCP)‚Äã",
      "Agent2Agent (A2A) Protocol‚Äã"
    ],
    "h3": [
      "Emerging Standards‚Äã",
      "Overview and Purpose‚Äã",
      "Architecture and Components‚Äã",
      "Core Primitives‚Äã",
      "Transport Mechanisms‚Äã",
      "Benefits and Ecosystem‚Äã",
      "Protocol Comparison for Agent-Tool Communication‚Äã",
      "MCP Roadmap at Lilly‚Äã",
      "Key Components of an A2A Protocol‚Äã",
      "Key Features and Capabilities‚Äã",
      "A2A and MCP: Complementary Protocols‚Äã",
      "How A2A Works‚Äã",
      "Agent Cards‚Äã",
      "The Challenges of Agent Communication‚Äã",
      "Benefits of A2A‚Äã",
      "A2A Design Principles‚Äã",
      "Agent2Agent Roadmap at Lilly‚Äã",
      "The Future of Agent Interoperability‚Äã",
      "What's Next: Protocol Roadmap‚Äã"
    ],
    "text_content": "Protocols | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nEcosystem\nPatterns\nQuantum Computing\nAgentic\nWorkflows\nBuilding Agentic Patterns\nContext Engineering\nDeep Agents\nDeep Research\nModel Foundry\nProtocols\nPositioning\nExamples\nCoding Tools\nStandards\nAI Submission Guide Template\nüóÑÔ∏è AI & Intelligent Agents BLT\nü§ñ Agentic AI\nüß≠ Conversational AI\nüß≠ Knowledge Bases\nüß≠ Multimodal AI\nüß≠ Translation Services\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nü§ñ AI & Intelligent Agents\nPatterns\nProtocols\nOn this page\nProtocols\nDocument Information\nLast Updated:\nJuly 25, 2025\nOwner:\nBrian Lewis\nPoint of Contact:\nAli Kharazmi\nContributors and Reviewers:\nArchit Kaila, Haitham Maya, Malika Mahoui\nThis section outlines key communication protocols used for agent-to-agent and agent-tool interactions, highlighting\ncurrent practices and emerging trends in AI system design.\nIndustry Standard Protocols\n‚Äã\nIndustry-standard communication protocols for distributed systems include a mix of synchronous APIs and asynchronous\nmessaging:\nREST/HTTP (JSON)\n‚Äì Simple, stateless, and widely supported. REST uses URIs and JSON over HTTP, making it ideal for\ninteroperability but less suited for streaming or large payload efficiency.\ngRPC / Protocol Buffers\n‚Äì A fast, strongly-typed RPC system using HTTP/2 and binary Protobuf. Supports bidirectional\nstreaming and outperforms REST in high-throughput scenarios with auto-generated code across languages.\nGraphQL\n‚Äì A flexible query API that allows clients to request only needed data via a single endpoint. Best for\ncomplex, nested data models and diverse client needs.\nWebSockets / SSE\n‚Äì Real-time communication protocols. WebSockets enable two-way persistent connections, while SSE\nsupports one-way server-to-client streams. Useful for interactive or live-update applications.\nMessage Queues / Event Streams\n‚Äì Enable decoupled, asynchronous communication. MQTT is lightweight for IoT, while\nKafka and RabbitMQ support scalable, reliable pub/sub messaging across services.\nMainCategories\nAgentProtocols\nModel Context Protocol\nAgent-tool standard\nTool discovery\nStandardized interfaces\nAgent2Agent Protocol\nAgent-agent communication\nAgent discovery\nTask coordination\nAsyncProtocols\nWebSockets\nBi-directional streams\nPersistent connection\nReal-time updates\nMessage Queues\nDecoupled processing\nAsynchronous processing\nHigh scalability\nServer-Sent Events\nServer push updates\nSyncProtocols\nREST/HTTP\nSimple, ubiquitous\nStateless\nHuman-readable\ngRPC\nHigh-performance, streaming\nBinary encoding\nCode generation\nStreaming support\nGraphQL\nFlexible queries\nPrecise queries\nSingle endpoint\nCommunication Protocols\nSynchronous Protocols\nAsynchronous Protocols\nAgent-Specific Protocols\nEmerging Standards\n‚Äã\nThe W3C is actively developing two critical community groups focused on agent standardization:\nThe\nAutonomous Agents on the Web (WebAgents) Community Group\nfocuses on hypermedia Multi-Agent Systems that\nleverage Semantic Web standards.\nThe\nAI Agent Protocol Community Group\nis standardizing inter-agent communication protocols with PKI-based identity\nframeworks and cross-origin security mechanisms.\nThe IETF has introduced groundbreaking standards including:\nThe\nagent:// URI Protocol\nprovides a layered architecture supporting minimal to full-featured agent\nimplementations with the format:\nagent:[+<protocol>]://<authority>/<path>[?<query>][#<fragment>]\nThe\nAgent Name Service (ANS)\nspecification introduces DNS-inspired hierarchical agent addressing with PKI-based\ntrust.\nModern AI applications often mix these protocols. For example, many cloud AI tool APIs are offered as HTTPS/JSON\nservices (REST) for ease of integration, while high-performance backends might expose gRPC. In addition, AI-specific\nstandards are emerging: Anthropic's\nModel Context Protocol (MCP)\nand Google's\nAgent2Agent (A2A)\naim to\nstandardize agent‚Äìtool and agent‚Äìagent interfaces.\nAgent to Tool Communication\nModel Context Protocol (MCP)\n‚Äã\nOverview and Purpose\n‚Äã\nThe Model Context Protocol (MCP), developed by Anthropic, is a standardized JSON-based protocol that creates a universal\ninterface between AI models and tools. Acting as a \"USB-C port for AI,\" MCP enables any LLM to connect to any compliant\ntool or data source in a consistent way.\nMCP defines:\nHow tools describe their capabilities (functions, arguments, schemas)\nHow AI agents discover and invoke those functions\nHow responses (synchronous or streaming) are handled and returned\nThis standardization creates true interoperability between tools and models across vendors, allowing AI systems to query\ndatabases, call APIs, manipulate files, and access various services through a unified protocol.\nExternal Tool (MCP Server)\nAI Agent (MCP Client)\nApp or Orchestrator\nExternal Tool (MCP Server)\nAI Agent (MCP Client)\nApp or Orchestrator\nInstruction (e.g., \"Check calendar\")\nJSON Request via MCP\nResult or Event Stream\nFinal Output\nArchitecture and Components\n‚Äã\nMCP follows a client-host-server architecture with clearly defined roles:\nMCP Hosts\n: Applications like Claude Desktop, IDEs, or custom applications that initiate interactions\nMCP Clients\n: Protocol clients (typically AI models) that maintain connections with servers\nMCP Servers\n: Lightweight programs exposing specific capabilities through the standardized protocol\nData Sources\n: Local files/databases or remote services that MCP servers can securely access\nThis separation of concerns creates modular, composable systems where each server can focus on a specific domain (like\nfile access, web search, or database operations).\nInternet\nYour Computer\nMCP Protocol\nMCP Protocol\nMCP Protocol\nWeb APIs\nHost with MCP Client\n(Claude, IDEs, Tools)\nMCP Server A\nMCP Server B\nMCP Server C\nLocal\nData Source A\nLocal\nData Source B\nRemote\nService C\nCore Primitives\n‚Äã\nMCP defines three fundamental primitives that servers can implement:\nTools\n: Model-controlled functions that LLMs can invoke (like API calls, computations)\nExample: A calculator tool that performs mathematical operations\nImplementation: Functions with defined input/output schemas\nResources\n: Application-controlled data that provides context (like file contents, database records)\nExample: A document resource that provides access to text files\nImplementation: URI-addressable data sources with content types\nPrompts\n: User-controlled templates for LLM interactions\nExample: A summarization prompt template with configurable parameters\nImplementation: Parameterized prompt structures with variables\nFor most developers, tools are the most immediately useful primitive, allowing LLMs to perform actions programmatically.\nTransport Mechanisms\n‚Äã\nMCP supports two main transport mechanisms:\nStdio (Standard IO)\n:\nCommunication over standard input/output streams\nBest for local integrations on the same machine\nSimple setup with no network configuration\nIdeal for development and single-application integrations\nSSE (Server-Sent Events)\n:\nHTTP for client-to-server, SSE for server-to-client\nSuitable for remote connections across networks\nEnables distributed architectures\nRequired for remote accessibility\nRemote Deployment\nSSE Transport\nHTTP POST\n(client to server)\nSSE\n(server to client)\nMCP Client\nMCP Server\nLocal Deployment\nStdio Transport\nstdin/stdout\n(bidirectional)\nMCP Client\nMCP Server\nDevelopers familiar with FastAPI will find implementing MCP servers with SSE transport straightforward, as both use HTTP\nendpoints for receiving requests and support streaming responses using Server-Sent Events.\nBenefits and Ecosystem\n‚Äã\nThe true power of MCP lies in standardization rather than introducing new capabilities:\nReusability\n: Build a server once, use it with any MCP-compatible client\nComposability\n: Combine multiple servers to create complex capabilities\nEcosystem growth\n: Leverage servers created by others\nMCP has rapidly become the standard for LLM tool access with 150+ community servers and multi-language SDK support.\nImplementations use JSON-RPC or RESTful payloads with OAuth authentication, enabling diverse tool providers to connect\nto any MCP-compatible agent. For available servers, see the\nMCP GitHub repository\n.\nProtocol Comparison for Agent-Tool Communication\n‚Äã\nDifferent protocols offer different trade-offs:\nProtocol\nPros\nCons\nUse Cases\nWhen to Choose\nREST/HTTP\nSimple, ubiquitous\nStateless, no streaming\nGeneral tool calls\nFor broad compatibility and simple integrations\ngRPC\nBinary, efficient, supports streaming\nRequires stub generation\nHigh-performance backends\nFor performance-critical pipelines\nWebSockets\nBi-directional, persistent connections\nHarder to scale\nLive tool monitoring\nWhen real-time updates are needed\nGraphQL\nFlexible queries, single endpoint\nSchema complexity\nDynamic data needs\nWhen clients need precise control over data fetching\nMCP\nStandardized for agents, tool introspection\nEarly adoption\nAgent‚Äìtool abstraction layers\nFor standardized agent-tool workflows\nWhen comparing these, one key factor is state and streaming. gRPC and WebSockets allow keeping the \"conversation\" alive,\nwhich can reduce overhead for multiple back-and-forth messages. REST and GraphQL are inherently stateless (each call\nstands alone). Another factor is typing and validation: gRPC and GraphQL have strict schemas, whereas REST relies on\nad-hoc schemas that must be documented separately.\nUses\nUses\nUses\nUses\nAgent\nREST/HTTP\nSimple, ubiquitous\ngRPC\nHigh-performance\nWebSocket\nBi-directional\nMCP\nStandardized\nMCP Roadmap at Lilly\n‚Äã\nWith MCP as the foundation connecting development resources to agent tools and external systems for enterprise-wide AI\nintegration, organizations can streamline workflows, accelerate innovation, and ensure scalable, secure deployment\nacross their digital ecosystem.\nGitHub Template\n‚Ä¢ Reference implementation\n‚Ä¢ Protocol adapters\n‚Ä¢ Documentation\nBackstage\n‚Ä¢ Developer portal\n‚Ä¢ API documentation\n‚Ä¢ Service catalog\nTool Deployment\n‚Ä¢ CATS, Kubed, and other\n‚Ä¢ nodx -> dx support\n‚Ä¢ URI-based access\nModel Context Protocol\n‚Ä¢ Industry standards\n‚Ä¢ Built-in Lilly auth\n‚Ä¢ Multi-modal support\nCortex Toolkit\n‚Ä¢ Unified SDK\n‚Ä¢ Multiple protocol support\n‚Ä¢ Developer resources\nCortex Agents\n‚Ä¢ Tool integration\n‚Ä¢ Agent orchestration\n‚Ä¢ Observability and eval\nDiscovery\n‚Ä¢ Tool exploration\n‚Ä¢ Marketplace for discovery\nand configuration\nExternal Agents\n‚Ä¢ Third-party integration\n‚Ä¢ Cross-platform support\n‚Ä¢ Standardized messaging\nSystem to System\nCompatibility\n‚Ä¢ Cross-platform communication\n‚Ä¢ Agent discovery\n‚Ä¢ Capability negotiation\nAgent to Agent Communication\nAgent2Agent (A2A) Protocol\n‚Äã\nThe Agent2Agent (A2A) protocol, developed by Google with over 50 industry partners, addresses a critical challenge in\nthe AI landscape: enabling AI agents built on diverse frameworks by different companies to communicate and collaborate\neffectively. A2A provides a common language for agents, fostering a more interconnected AI ecosystem.\nKey Components of an A2A Protocol\n‚Äã\nMessage Format\n: The structure of requests and responses. For example, each agent might produce or consume JSON\nthat follows a particular schema (such as an \"intents\" field, a \"metadata\" field, etc.).\nTransport Mechanism\n: How messages travel between agents. This could be over standard web protocols (HTTP/HTTPS) or\nspecialized messaging buses (like gRPC or MQTT).\nDiscovery\n: How an agent finds other agents. This might be analogous to DNS in the internet world or a registry\nservice in the microservices world.\nSecurity and Authentication\n: Ensuring that only authorized agents can talk to each other, or that sensitive data\nis encrypted.\nKey Features and Capabilities\n‚Äã\nA2A enables agents to:\nDiscover capabilities\n: Agents can advertise their skills and find other agents with complementary abilities.\nNegotiate interaction modalities\n: Support for text, structured data, and media exchanges.\nCollaborate on tasks\n: Manage stateful, potentially long-running collaborative tasks.\nMaintain opacity\n: Agents can collaborate without exposing their internal state, memory, or proprietary tools.\nA2A is built on established standards:\nJSON-RPC 2.0 over HTTP(S)\nfor standardized communication\nAgent Cards\nfor capability discovery and connection information\nFlexible interaction modes\nincluding synchronous request/response, streaming (SSE), and asynchronous notifications\nA2A and MCP: Complementary Protocols\n‚Äã\nA2A and MCP serve different but complementary roles in the agentic ecosystem:\nMCP (Model Context Protocol)\n: Connects agents to tools, APIs, and resources with structured inputs/outputs. It's\nhow agents access their capabilities.\nA2A (Agent2Agent Protocol)\n: Facilitates dynamic communication between different agents as peers. It's how agents\ncollaborate, delegate, and manage shared tasks.\nA2A Protocol\nMCP Protocol\nMCP Protocol\nUser\nAgent A\nAgent B\nTool 1\nTool 2\nHow A2A Works\n‚Äã\nA2A facilitates communication between a \"client\" agent and a \"remote\" agent through several key mechanisms:\nCapability Discovery\n: Agents advertise their capabilities using \"Agent Cards\" in JSON format, allowing client\nagents to identify the best agent for a task.\nTask Management\n: Communication is oriented around task completion. The task object has a defined lifecycle and\ncan be completed immediately or monitored over time for long-running operations.\nUser Experience Negotiation\n: Messages include \"parts\" with specified content types, allowing agents to negotiate\nthe correct format and explicitly include UI capabilities like iframes, video, or web forms.\nSecure Collaboration\n: A2A is designed with enterprise-grade authentication and authorization, supporting various\nauthentication schemes.\nAgent Cards\n‚Äã\nAgent Cards are JSON documents that serve as digital \"business cards\" for A2A Servers (remote agents). They are crucial\nfor discovery and initiating interaction, containing:\nIdentity\n: Name, description, and provider information\nService Endpoint\n: URL where the A2A service can be reached\nA2A Capabilities\n: Supported protocol features like streaming or push notifications\nAuthentication\n: Required authentication schemes (e.g., \"Bearer\", \"OAuth2\")\nSkills\n: List of specific tasks the agent can perform, including input/output modes and examples\nClient agents parse these cards to determine if a remote agent is suitable for a task, how to structure requests, and\nhow to communicate securely.\nSecurity Considerations\n‚Äã\nAgent Cards may contain sensitive information that should be protected:\nInternal service URLs\nAuthentication details (though storing actual secrets is discouraged)\nDescriptions of sensitive skills\nProtection mechanisms include access control on endpoints, mutual TLS, network restrictions, and authentication\nrequirements. Agent registries can implement selective disclosure, providing different levels of detail based on the\nclient's identity and permissions.\nThe Challenges of Agent Communication\n‚Äã\nWithout standardized protocols like A2A, organizations face several challenges:\nRedundant Work\n: Every time you add a new AI agent, you need to figure out how it speaks to the rest of your\necosystem, leading to redundant or \"copy-paste\" integration code.\nCompatibility Headaches\n: Suppose your NLP model runs on Python, and your knowledge graph is hosted in a Java-based\nmicroservice. Each might pass around data differently, making it hard for them to play nicely together.\nScaling Bottlenecks\n: As you incorporate more specialized agents (e.g., image recognition, forecasting, robotics),\nthe complexity explodes. Non-standardized communication turns into a big tangly mess, harming your ability to innovate\nquickly.\nBenefits of A2A\n‚Äã\nBreaking Down Silos\n: Connect agents across different ecosystems and frameworks.\nComplex Workflows\n: Enable agents to delegate sub-tasks, exchange information, and coordinate actions.\nInteroperability\n: Agents built on different platforms (LangGraph, CrewAI, Semantic Kernel, custom solutions) can\nwork together.\nModality Agnostic\n: Support for various communication modalities beyond text, including audio and video.\nA2A Design Principles\n‚Äã\nA2A follows five key design principles:\nEmbrace agentic capabilities\n: Focus on enabling collaboration between agents without requiring shared memory,\ntools, or context.\nBuild on existing standards\n: Leverage HTTP, SSE, and JSON-RPC for easier integration with existing IT\ninfrastructure.\nSecure by default\n: Support enterprise-grade authentication and authorization.\nSupport long-running tasks\n: Flexible design for quick tasks and deep research that may take hours or days.\nModality agnostic\n: Support various communication modalities beyond text.\nAgent2Agent Roadmap at Lilly\n‚Äã\ncoming soon!\nThe Future of Agent Interoperability\n‚Äã\nA2A has the potential to unlock a new era of agent interoperability, fostering innovation and creating more versatile\nagentic systems. The protocol is being developed collaboratively as open source, with industry partners working toward a\nproduction-ready version later this year.\nWhat's Next: Protocol Roadmap\n‚Äã\nA2A Protocol Roadmap\nAgent Discovery\nAuthorization schemes and\ncredentials in AgentCard\nAgent Collaboration\nQuerySkill method for\ndynamic capability checking\nTask Lifecycle & UX\nDynamic UX negotiation\nwith mid-task media support\nClient Methods & Transport\nClient-initiated methods and\nimproved streaming reliability\nReferences\ngRPC vs. REST | Postman Blog\nEssential Communication Protocols for Modern Software Architecture: REST, GraphQL, SOAP and More | Medium\nA2A and MCP: Start of the AI Agent Protocol Wars? - Koyeb\nModel Context Protocol (MCP) - Anthropic\nMCP connector - Anthropic\nAnnouncing the Agent2Agent Protocol (A2A) - Google Developers Blog\nSmythOS - Agent Communication Protocols: An Overview\nW3C Autonomous Agents on the Web Community Group\nIETF Agent URI Protocol Draft\nNIST Special Publication 800-207: Zero Trust Architecture\nAgent2Agent Protocol Documentation\nWas this helpful?\nEdit this page\nPrevious\nModel Foundry\nNext\nPositioning\nIndustry Standard Protocols\nEmerging Standards\nModel Context Protocol (MCP)\nOverview and Purpose\nArchitecture and Components\nCore Primitives\nTransport Mechanisms\nBenefits and Ecosystem\nProtocol Comparison for Agent-Tool Communication\nMCP Roadmap at Lilly\nAgent2Agent (A2A) Protocol\nKey Components of an A2A Protocol\nKey Features and Capabilities\nA2A and MCP: Complementary Protocols\nHow A2A Works\nAgent Cards\nThe Challenges of Agent Communication\nBenefits of A2A\nA2A Design Principles\nAgent2Agent Roadmap at Lilly\nThe Future of Agent Interoperability\nWhat's Next: Protocol Roadmap\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 6,
    "depth": 3,
    "crawled_at": "2026-02-25T10:08:10.617854"
  },
  "https://techhq.dc.lilly.com/docs/solution/ai/patterns/model_foundry": {
    "url": "https://techhq.dc.lilly.com/docs/solution/ai/patterns/model_foundry",
    "title": "Model Foundry | Tech HQ",
    "description": "Last Updated: July 25, 2025",
    "h1": [
      "Model Foundry"
    ],
    "h2": [
      "Bringing Model Development to Production‚Äã",
      "Current State Initiative‚Äã",
      "Current State SLURM Architecture‚Äã",
      "Current Migration Underway‚Äã",
      "Next Steps‚Äã",
      "MLflow vs Weights & Biases: Feature Comparison‚Äã",
      "comprehenxive list of citations\"‚Äã"
    ],
    "h3": [
      "Key Takeaways‚Äã",
      "üìö Citations‚Äã"
    ],
    "text_content": "Model Foundry | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nEcosystem\nPatterns\nQuantum Computing\nAgentic\nWorkflows\nBuilding Agentic Patterns\nContext Engineering\nDeep Agents\nDeep Research\nModel Foundry\nProtocols\nPositioning\nExamples\nCoding Tools\nStandards\nAI Submission Guide Template\nüóÑÔ∏è AI & Intelligent Agents BLT\nü§ñ Agentic AI\nüß≠ Conversational AI\nüß≠ Knowledge Bases\nüß≠ Multimodal AI\nüß≠ Translation Services\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nü§ñ AI & Intelligent Agents\nPatterns\nModel Foundry\nOn this page\nModel Foundry\nDocument Information\nLast Updated:\nJuly 25, 2025\nOwner:\nBrian Lewis\nPoint of Contact:\nAli Kharazmi\nContributors and Reviewers:\nArchit Kaila, Haitham Maya, Malika Mahoui\nBringing Model Development to Production\n‚Äã\nHypervisor for AI teams to leverage advanced compute for model training, serving, and advanced AI workloads\nAccelerate time-to-value for ML investments by reducing infrastructure and engineering load on ML teams\nEnhanced visibility, collaboration, and reproducibility across model development lifecycle\nIncrease advanced compute utilization rates through optimized resource management\nTrain & Deploy\nConsume Models\nUse Resources\nOrchestrates\nMagtrain Infrastructure\nMagtrain Compute\nTraining & Inference\nObject Storage\nModels & Artifacts\nAI Practitioners\nData Scientists & ML\nEngineers\nApplications\nAI-Powered Services\nModel Foundry\nPlatform\nCurrent State Initiative\n‚Äã\nThe current implementation uses Magtrain with Slurm:\nCentralized job scheduling: Resource allocation, job scheduling, and distribution of workloads\nQueue-based management: Structured queue system that tracks pending, running, and completed tasks\nCommand Line Interface (CLI): Interaction through standardized CLI commands (i.e., sbatch, squeue, scancel) for job submission and monitoring\nStorage access: Direct integration with shared storage systems\nCurrent State SLURM Architecture\n‚Äã\nData Scientists & ML\nEngineers\nJob submission &\nmonitoring\nCommand Line Interface\nsbatch, squeue, scancel\nSLURM Controller\nJob scheduling & resource\nallocation\nJob Queue\nPending, running,\ncompleted jobs\nMagtrain GPU Cluster\nGPU compute nodes\nShared Storage\nData & model artifacts\nCurrent Migration Underway\n‚Äã\nMigration from Slurm to Run.ai, which provides more robust GPU orchestration and resource management capabilities.\nNext Steps\n‚Äã\nEvaluate MLflow or Weights & Biases for model training and serving capabilities, leveraging learnings from LRL's model gateway implementation.\nPlatform Resources:\nMLflow:\nhttps://mlflow.org/\nWeights & Biases:\nhttps://wandb.ai/site\nEvaluation Focus Areas:\nThe assessment will cover ML lifecycle management, experiment tracking, model deployment, and visualization capabilities. Key considerations include:\nComparative analysis of platform capabilities and limitations\nCompatibility and integration potential between the two platforms\nFeature mapping for core ML operations (tracking, registry, deployment)\nData scientist user experience and workflow optimization\nAlignment with existing infrastructure and team requirements\nMLflow vs Weights & Biases: Feature Comparison\n‚Äã\nFeature Category\nMLflow\nWeights & Biases\nOverview\nOpen-source, vendor-neutral platform from Databricks. Emphasizes flexibility and on-premise deployment\nCommercial cloud-based platform. Emphasizes ease-of-use, collaboration, and rich hosted interface\nExperiment Tracking\nSimple API with basic UI. Supports autologging for major frameworks. Requires server setup for teams\nPolished web UI with real-time sync. Plug-and-play setup with rich media support and live updates\nModel Registry & Artifacts\nBuilt-in registry with lifecycle management (staging, production). Works with any storage backend\nIntegrated registry with versioned artifacts. Tightly coupled with tracking and includes collaborative dashboards\nHyperparameter Tuning\nNo built-in HPO. Integrates with external tools (Optuna, Ray Tune, HyperOpt)\nFirst-class Sweeps feature with grid, random, and Bayesian optimization. Automated visualization of results\nVisualization & Reporting\nBasic web UI with simple charts and run comparisons. Limited visualization capabilities\nRich interactive platform with custom charts, embedding projector, and shareable Reports for storytelling\nDeployment & Serving\nBuilt-in model serving as REST APIs. Supports local, cloud, and edge deployments\nNo native serving capabilities. Focuses on handoff to external deployment infrastructure\nPipeline Orchestration\nNot an orchestration framework. Requires external tools (Airflow, Kubeflow) for pipelines\nNot a pipeline engine. Uses external tools for workflow orchestration beyond HPO sweeps\nEcosystem & Integrations\nExtensive framework support (TensorFlow, PyTorch, XGBoost, etc.). Language-agnostic with REST API\nBroad SDK support with official integrations. Includes MLflow importer for migration compatibility\nCollaboration & Team Features\nBasic collaboration via shared server. No built-in user management or access controls\nTeam-centric design with RBAC, SSO, audit logs, and real-time collaboration features\nPricing & Licensing\nCompletely free and open-source. No licensing costs\nTiered SaaS model: Free tier available, Pro (~$50/mo), Enterprise (custom pricing)\nData Scientist Workflow\nDIY approach requiring manual setup. Straightforward but needs infrastructure management\nPlug-and-play experience with instant web interface. More visual and collaborative workflow\nKey Takeaways\n‚Äã\nMLflow\nexcels for organizations wanting vendor-neutral, self-hosted solutions with full control over infrastructure\nWeights & Biases\nexcels for teams prioritizing collaboration, rich visualization, and managed cloud experience\nBoth platforms are\ncompatible\n- W&B can import MLflow experiments, and MLflow can sync with W&B sweeps\nChoice depends on priorities:\nopenness vs convenience\n,\nrequired features\n, and\nbudget constraints\ncomprehenxive list of citations\"\n‚Äã\nüìö Citations\n‚Äã\nMLflow Official Documentation\nCovers all components of MLflow: Tracking, Projects, Models, and Registry.\nMLflow GitHub Repository\nOpen-source repository for the MLflow project including examples and issues.\nW&B Official Website\nOverview of Weights & Biases features, pricing, and platform capabilities.\nW&B Importers Documentation\nInstructions for importing MLflow runs and artifacts into W&B.\nW&B Sweeps Documentation\nGuide to W&B's hyperparameter tuning and optimization tool.\nMLflow Autologging Guide\nDetails supported libraries and how autologging works.\nMLflow Model Registry Guide\nExplanation of versioning, model stages, and approvals.\nWeights & Biases Model Registry\nDescribes model versioning, tracking lineage, and deployment handoff.\nW&B Reports\nHow to build and share collaborative dashboards using W&B Reports.\nW&B Launch\nRun training jobs on cloud infrastructure like SageMaker, GCP, etc.\nW&B SDK Integrations\nCovers TensorFlow, PyTorch, scikit-learn, Hugging Face, and more.\nZenML Comparison: MLflow vs W&B\nSide-by-side breakdown of features and limitations.\nWeights & Biases Pricing\nDetails on Free, Pro, and Enterprise tiers and their capabilities.\nW&B Artifacts Guide\nVersioning and managing datasets, model weights, and results.\nDatabricks MLflow Overview\nManaged MLflow on Databricks including deployment and monitoring features.\nWas this helpful?\nEdit this page\nPrevious\nDeep Research\nNext\nProtocols\nBringing Model Development to Production\nCurrent State Initiative\nCurrent State SLURM Architecture\nCurrent Migration Underway\nNext Steps\nMLflow vs Weights & Biases: Feature Comparison\nKey Takeaways\ncomprehenxive list of citations\"\nüìö Citations\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 5,
    "depth": 3,
    "crawled_at": "2026-02-25T10:08:12.812353"
  },
  "https://techhq.dc.lilly.com/docs/solution/ai/patterns/lilly_cloud_mcp": {
    "url": "https://techhq.dc.lilly.com/docs/solution/ai/patterns/lilly_cloud_mcp",
    "title": "Lilly MCP Servers | Tech HQ",
    "description": "Last Updated: July 25, 2025",
    "h1": [
      "Lilly MCP Servers"
    ],
    "h2": [
      "Overview‚Äã",
      "MCP Servers‚Äã",
      "Ecosystem Diagram‚Äã",
      "Implementation Considerations‚Äã",
      "Next Steps‚Äã"
    ],
    "h3": [
      "Lilly Cloud MCP and Agentic AI Registry‚Äã"
    ],
    "text_content": "Lilly MCP Servers | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nEcosystem\nPatterns\nQuantum Computing\nAgentic\nWorkflows\nBuilding Agentic Patterns\nContext Engineering\nDeep Agents\nDeep Research\nLilly MCP Servers\nModel Foundry\nProtocols\nPositioning\nExamples\nCoding Tools\nStandards\nAI Submission Guide Template\nüóÑÔ∏è AI & Intelligent Agents BLT\nü§ñ Agentic AI\nüß≠ Conversational AI\nüß≠ Knowledge Bases\nüß≠ Multimodal AI\nüß≠ Translation Services\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nUnlisted page\nThis page is unlisted. Search engines will not index it, and only users having a direct link can access it.\nü§ñ AI & Intelligent Agents\nPatterns\nLilly MCP Servers\nOn this page\nLilly MCP Servers\nDocument Information\nLast Updated:\nJuly 25, 2025\nOwner:\nBrian Lewis\nPoint of Contact:\nAli Kharazmi\nContributors and Reviewers:\nArchit Kaila, Haitham Maya, Malika Mahoui\nOverview\n‚Äã\nThis document outlines the Model Context Protocol (MCP) servers planned for Lilly systems. These MCP servers will enable AI agents to access various Lilly tools and resources, with a particular focus on resource provisioning capabilities. The Lilly Cloud MCP server specifically could be used in the Agentic AI registry project to help provision resources.\nMCP Servers\n‚Äã\nThe following MCP servers are planned or under consideration:\nLilly Cloud MCP\n: Resource provisioning for cloud infrastructure\nCortex Docs MCP\n: Access to Cortex documentation\nCATS MCP\n: Internal application deployment (built)\nGitHub MCP\n: Code repository access\nLilly Flow MCP\n: Workflow automation\nLilly Kubed MCP\n: External application deployment (hanger- built)\nLDS MCP\n: Data services access\nDiagram MCP\nSlides MCP\nResearch MCP\nLLM gateway MCP\nArchitecture builder MCP\nModel Gateway MCP\nEcosystem Diagram\n‚Äã\nLilly Cloud MCP\nResource provisioning\nCortex Docs MCP\nDocumentation access\nStack Overflow MCP\nKnowledge base\nCATS MCP\nInternal app deployment\nGitHub MCP\nCode repository access\nLilly Flow MCP\nWorkflow automation\nLilly Kubed MCP\nExternal app deployment\nLDS MCP\nData services\nLilly Cloud MCP and Agentic AI Registry\n‚Äã\nThe Lilly Cloud MCP server could be utilized within the Agentic AI registry project specifically for resource provisioning capabilities.\nImplementation Considerations\n‚Äã\nReusable Components\n: Identify common authentication, logging, and monitoring components that can be shared across MCP servers\nBuild vs. Configure\n: Determine which components need custom development versus configuration of existing tools\nIntegration Points\n: Define how these MCP servers will connect with various Lilly systems\nNext Steps\n‚Äã\nCreate detailed specifications for each MCP server\nPrioritize development based on business impact\nDevelop a phased implementation plan\nEstablish governance and security standards\nWas this helpful?\nEdit this page\nOverview\nMCP Servers\nEcosystem Diagram\nLilly Cloud MCP and Agentic AI Registry\nImplementation Considerations\nNext Steps\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 4,
    "depth": 3,
    "crawled_at": "2026-02-25T10:08:14.996154"
  },
  "https://techhq.dc.lilly.com/docs/solution/ai/positioning/google_positioning": {
    "url": "https://techhq.dc.lilly.com/docs/solution/ai/positioning/google_positioning",
    "title": "Google | Tech HQ",
    "description": "Last Updated: July 25, 2025",
    "h1": [
      "Google"
    ],
    "h2": [
      "Executive Summary‚Äã",
      "Strategic Positioning‚Äã",
      "Key Google Proprietary Services‚Äã",
      "Google Technology Adoption Framework‚Äã",
      "Governance Framework‚Äã"
    ],
    "h3": [
      "AI Platform‚Äã",
      "Dual-Cloud Foundation‚Äã",
      "Vertex AI‚Äã",
      "Gemini Models‚Äã",
      "AgentSpace‚Äã",
      "A2A Protocol Positioning‚Äã",
      "Agent-to-Agent (A2A) Protocol Overview‚Äã",
      "Advanced Cloud and AI Offerings‚Äã",
      "Google Agents and A2A Protocol‚Äã",
      "Containerized Business Services‚Äã",
      "Governance Structure‚Äã"
    ],
    "text_content": "Google | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nEcosystem\nPatterns\nPositioning\nAgentic Enterprise\nGoogle\nPlatforms\nPrompt Management\nExamples\nCoding Tools\nStandards\nAI Submission Guide Template\nüóÑÔ∏è AI & Intelligent Agents BLT\nü§ñ Agentic AI\nüß≠ Conversational AI\nüß≠ Knowledge Bases\nüß≠ Multimodal AI\nüß≠ Translation Services\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nü§ñ AI & Intelligent Agents\nPositioning\nGoogle\nOn this page\nGoogle\nDocument Information\nLast Updated:\nJuly 25, 2025\nOwner:\nBrian Lewis\nPoint of Contact:\nAli Kharazmi\nContributors and Reviewers:\nArchit Kaila, Haitham Maya, Malika Mahoui\nExecutive Summary\n‚Äã\nThis document outlines Eli Lilly's strategic positioning regarding Google technologies, services, and platforms. While\nmaintaining our dual-cloud strategy with Azure and AWS as primary platforms, we recognize Google's specialized\ncapabilities in advanced AI and other unique business solutions. This document provides guidance on when and how Google\ntechnologies should be evaluated, adopted, and governed within our enterprise architecture.\nStrategic Positioning\n‚Äã\nAI Platform\n‚Äã\nWe are taking a cloud-agnostic approach to AI at Lilly, with Cortex as our main platform. For a complete view of our AI\nplatform positioning, refer our AI Positioning document. Approved platforms, such as Cortex, should be explored before\nconsidering Google AI capabilities.\nModel Approval Process\n: All AI models must be approved for specific use cases by the Advanced Intelligence team. Models are not blanket approved but rather evaluated and approved based on their technical suitability for individual business requirements and use cases.\nPlease feel free to reach out to your local area architect, the Enterprise AI Architecture team or the Cortex Product team for consult on best path for your solution.\nRequesting New Model Evaluation\n: If there is a Google model that you would like to explore further that is not available in Cortex or LLM Gateway, please reach out to the Advanced Intelligence team to help you evaluate the model for your specific solution and use case.\nDual-Cloud Foundation\n‚Äã\nLilly maintains a dual-cloud strategy with Azure and AWS as our primary platforms. Google Cloud Platform (GCP) is\npositioned as a specialized solution for specific use cases rather than a third general-purpose hyperscaler.\nPrimary Platforms\n:\nAzure\nand\nAWS\nremain our primary focus for cloud and AI solutions\nEvaluation Process\n:\nAzure\nand\nAWS\nsolutions must be evaluated first before considering Google capabilities\nGoogle's Role\n: Specialized provider for\nunique\nbusiness capabilities.\nKey Google Proprietary Services\n‚Äã\nLilly has identified several proprietary Google services that offer unique capabilities aligned with our strategic\nneeds. These services are evaluated and adopted based on their ability to deliver specialized functionality that\ncomplements our existing technology stack.\ninfo\nFor each of these solutions, the business area team/leader will be responsible for local contracting,\ndirect billing, driving appropriate design approvals (e.g. EBA, Cyber, Legal, Privacy, Quality), and ongoing support\nmodel. Teams should expect slower process for these initiatives given the custom nature of the solution and lack of\nenterprise approved patterns.\nVertex AI\n‚Äã\nVertex AI is Google Cloud's unified machine learning platform that enables the training and deployment of ML models and\nAI applications at scale.\nKey Features\n: AutoML capabilities, custom model training, MLOps tooling, and model monitoring\nStrategic Value\n: Provides advanced ML capabilities with simplified workflows for data scientists and ML engineers\nUse Cases\n: Predictive analytics, computer vision applications, and specialized ML model deployment\nLilly Strategy Today\n: Not approved for separate implementation. Only can be used as part of an Agentspace\ncontainerized solution with governance approval. (See Agentspace below)\nGemini Models\n‚Äã\nGemini represents Google's most advanced multimodal AI models, capable of understanding and generating content across\ntext, code, audio, image, and video.\nKey Features\n: Multimodal capabilities, advanced reasoning, and state-of-the-art performance\nStrategic Value\n: Enables sophisticated AI applications that can process and generate multiple types of content\nUse Cases\n: Complex content generation, multimodal analysis, and advanced reasoning tasks\nLilly Strategy Today\n: Gemini models are available through Cortex with use case limitations. Please contact the\nCortex team to discuss use case specifics. Alternatively, you could leverage the LLM Recommender for available models\nwith strengths comparable with Gemini models:\nhttps://llm-recommender.apps-d.lrl.lilly.com\nAgentSpace\n‚Äã\nAgentSpace is Google's platform for building, deploying, and managing AI agents that can perform complex tasks through\nreasoning and tool use.\nKey Features\n: Agent development framework, built-in reasoning capabilities, and tool integration\nStrategic Value\n: Enables the creation of specialized AI agents tailored to specific business processes\nUse Cases\n: Automated workflows, intelligent assistants, and process automation\nLilly Strategy Today\n: AgentSpace is not established as a foundational enterprise service at this time as feature\nsets are similar (and in some cases less mature) than corresponding features in Azure and AWS. For approved business\nuse cases, a containerized deployment of AgentSpace with custom UI is appropriate to enable usage of the broader\nGoogle AI platform capabilities.\nA2A Protocol Positioning\n‚Äã\nLilly Strategy Today\n: Lilly is committed to advancing industry standard agent communication protocols and to the\npotential for A2A to be a leading standard for our adoption. Since we are still early days with this standard, we should\nbe cautious in assuming it will address full interoperability in the longer term. Our approach at this time is to\nestablish defined standards within Lilly that leverage A2A as input but also keep us independent.\nAgent-to-Agent (A2A) Protocol Overview\n‚Äã\nA2A is an open protocol developed by Google that enables AI agents built on different platforms to communicate and\ncollaborate effectively.\nKey Features\n: Standardized communication format, capability discovery, and task management\nStrategic Value\n: Facilitates interoperability between agents in a heterogeneous AI ecosystem\nUse Cases\n: Complex workflows requiring multiple specialized agents, cross-platform agent collaboration\nAdditional Information\n: Please see our\nprotocols documentation\nfor more detailed\ninformation and connect with the AI Architecture team with any urgent considerations for your projects.\nGoogle Technology Adoption Framework\n‚Äã\nAdvanced Cloud and AI Offerings\n‚Äã\nWe have established a Google Cloud Platform Landing Zone to ensure we have capabilities available to support priority\nuse cases. This is not setup for general use and we are not planning on qualifying any additional Google services until\nwe have determined the value/cost benefits to broadening our Cloud and AI footprint.\nGoogle Agents and A2A Protocol\n‚Äã\nLilly will strategically leverage Google Agents within our environment where they align with business requirements.\nAgent Integration\n: A2A registered agents will be integrated as qualified agents where applicable\nProtocol Standards\n: Lilly is committed to advancing industry standard agent communication protocols\nA2A Approach\n: While recognizing A2A's potential as a leading standard, we maintain a cautious approach\nInternal Standards\n: We will establish defined standards within Lilly that leverage A2A as input while maintaining\nindependence\nContainerized Business Services\n‚Äã\nThe architecture pattern established with tech transfer will serve as our model for other prioritized business value\ninitiatives.\nArchitecture Pattern\n: AgentSpace backend with custom UI, building agents using ADK for Agent Engine\nImplementation Approach\n: Creation of bespoke agents tailored to specific business needs\nGovernance Requirements\n: All new use cases require approval through Google governance managed through Enterprise\nBusiness Architecture. Submit AIR request to initiate process.\nBusiness Responsibility\n: Business area teams are responsible for:\nLocal contracting\nDirect billing\nDriving appropriate design approvals (EBA, Cyber, Legal, Privacy, Quality)\nOngoing support model\nManaging potentially slower processes due to custom solution requirements\nGovernance Framework\n‚Äã\nA centralized governance model will oversee all technology initiatives involving Google services.\nGovernance Structure\n‚Äã\nOwnership\n: Enterprise Business Architecture (EBA) team will own the governance model\nProject Transparency\n: All initiatives will be submitted to the AI registry\nInvestment Decisions\n: Information Officers (IOs) will own decisions to invest in local business solutions\nArchitectural Compliance\n: Local area lead architects are responsible for ensuring solutions adhere to positioning\nguidelines\nAI Capability Decision Authority\n: Enterprise AI Architect and AI Architecture and Operations (AIAO) will own\ndecisions for:\nAddition of new AI services\nA2A framework implementation\nAI Model Decision Authority\n: Advanced Intelligence will own decisions for:\nEvaluation and approval of models not currently available through Cortex\nUse case-specific model approval and technical recommendations\nCloud Capabilities Decision Authority\n: Enterprise Cloud Architect and Cloud Architecture and Strategy Team (CAST)\nwill own decisions for:\nAddition of any new foundational cloud services\nWas this helpful?\nEdit this page\nPrevious\nAgentic Enterprise\nNext\nPlatforms\nExecutive Summary\nStrategic Positioning\nAI Platform\nDual-Cloud Foundation\nKey Google Proprietary Services\nVertex AI\nGemini Models\nAgentSpace\nA2A Protocol Positioning\nAgent-to-Agent (A2A) Protocol Overview\nGoogle Technology Adoption Framework\nAdvanced Cloud and AI Offerings\nGoogle Agents and A2A Protocol\nContainerized Business Services\nGovernance Framework\nGovernance Structure\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 5,
    "depth": 3,
    "crawled_at": "2026-02-25T10:08:17.219314"
  },
  "https://techhq.dc.lilly.com/docs/solution/ai/positioning/prompt_management": {
    "url": "https://techhq.dc.lilly.com/docs/solution/ai/positioning/prompt_management",
    "title": "Prompt Management | Tech HQ",
    "description": "Last Updated: July 25, 2025",
    "h1": [
      "Prompt Management",
      "Proposed platform",
      "Comprehensive guide to prompt management, evaluation, and deployment platforms"
    ],
    "h2": [
      "Prompt management information: Core concepts and infrastructure‚Äã",
      "Prompt evaluation information: Methods and metrics for quality assurance‚Äã",
      "Deployment and UI information: From development to production scale‚Äã",
      "Comprehensive tool analysis‚Äã",
      "Feature comparison chart‚Äã",
      "Key insights for decision-making‚Äã"
    ],
    "h3": [
      "Core concepts of prompt management systems‚Äã",
      "Prompt versioning and lifecycle management‚Äã",
      "Template management and parameterization‚Äã",
      "Collaboration features for teams‚Äã",
      "Different types of prompt evaluation methods‚Äã",
      "Automated evaluation vs human evaluation‚Äã",
      "Metrics and scoring systems‚Äã",
      "A/B testing methodologies for prompts‚Äã",
      "API deployment options‚Äã",
      "Integration patterns with existing systems‚Äã",
      "User interface design for prompt management‚Äã",
      "Monitoring and observability features‚Äã",
      "Microsoft PromptFlow and MLflow‚Äã",
      "LangSmith and Weights & Biases Prompts‚Äã",
      "Humanloop and Promptfoo‚Äã",
      "Open source ecosystem‚Äã"
    ],
    "text_content": "Prompt Management | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nEcosystem\nPatterns\nPositioning\nAgentic Enterprise\nGoogle\nPlatforms\nPrompt Management\nExamples\nCoding Tools\nStandards\nAI Submission Guide Template\nüóÑÔ∏è AI & Intelligent Agents BLT\nü§ñ Agentic AI\nüß≠ Conversational AI\nüß≠ Knowledge Bases\nüß≠ Multimodal AI\nüß≠ Translation Services\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nü§ñ AI & Intelligent Agents\nPositioning\nPrompt Management\nOn this page\nPrompt Management\nDocument Information\nLast Updated:\nJuly 25, 2025\nOwner:\nBrian Lewis\nPoint of Contact:\nAli Kharazmi\nContributors and Reviewers:\nArchit Kaila, Haitham Maya, Malika Mahoui\nProposed platform\nBackend\nFrontend\nAI Layer\nExperience Layer\nPrompt Catalog\nPrompt Evaluation\nPrompt Manager\nPrompt Pipes\nComprehensive guide to prompt management, evaluation, and deployment platforms\nThe rapid evolution of large language models has created an urgent need for sophisticated tools to manage, evaluate, and deploy AI prompts at scale. Organizations deploying LLMs face challenges ranging from version control chaos to unpredictable costs and quality assurance nightmares. This research examines the conceptual foundations and available solutions in the prompt management ecosystem, providing a comprehensive framework for tool selection.\nPrompt management information: Core concepts and infrastructure\n‚Äã\nModern prompt management systems have emerged as critical infrastructure for organizations deploying LLMs at scale. These systems function as\n\"git for prompts\"\nbut with additional features specifically designed for the unique challenges of language model applications. The fundamental problem they solve is prompt proliferation - without systematic management, prompts become scattered across codebases, documentation, and communication channels, creating accountability gaps and production risks.\nCore concepts of prompt management systems\n‚Äã\nA prompt management system provides\ncentralized storage and version control\nfor all prompts used across an organization. The key components include a prompt registry that serves as a single source of truth, a version control system that tracks changes over time, and a template engine supporting dynamic content injection. These systems implement\ndecoupled architecture\npatterns, separating prompt content from application logic to enable non-technical stakeholders to modify prompts without code changes.\nThe architectural foundation typically includes\nclient-side caching\nto eliminate latency impact, with platforms like Langfuse implementing asynchronous cache refreshing for optimal performance. This design pattern allows teams to iterate rapidly on prompts while maintaining production stability. The separation of concerns also enables environment-specific configurations, allowing different prompt versions for development, staging, and production environments.\nPrompt versioning and lifecycle management\n‚Äã\nEffective versioning strategies follow\nsemantic versioning principles\n(X.Y.Z format), where major versions indicate breaking changes, minor versions represent new features maintaining compatibility, and patches address bug fixes. Modern systems implement branching strategies similar to software development, with environment-based branches for different deployment stages and feature branches for experimental variations.\nThe prompt lifecycle encompasses six distinct phases:\nplanning and design\n, where teams define success criteria and target audiences;\ndevelopment\n, involving initial creation and iteration;\ntesting and validation\nthrough rigorous evaluation;\noptimization\nbased on performance metrics;\nrelease and deployment\nwith comprehensive documentation; and\nmaintenance\nthrough continuous monitoring and adaptation. Each phase requires specific tooling and processes to ensure quality and reliability.\nDeployment strategies have evolved to include\nlabel-based deployment\nmechanisms, where prompts are tagged as production, staging, or latest. Advanced platforms support gradual rollout mechanisms including blue-green deployments for instant switching between versions, canary releases for subset testing, and feature flags for user segment targeting.\nTemplate management and parameterization\n‚Äã\nTemplate systems form the backbone of dynamic prompt generation, with various syntax options including\nF-string format\nfor Python-style variable injection,\nJinja2 templates\nfor complex logic,\nMustache templates\nfor cross-language compatibility, and proprietary formats like Anthropic's double bracket notation. These systems support static variables for consistent elements, function-based variables for runtime content generation, and contextual variables that adapt based on user attributes or business rules.\nAdvanced template features include\nconditional logic\nfor dynamic content paths,\nnested templates\nenabling composition and inheritance, and\nmodular architectures\nthat reduce duplication. Security considerations are paramount, requiring input sanitization to prevent template injection attacks and secure handling of sensitive data within templates.\nCollaboration features for teams\n‚Äã\nModern prompt management systems recognize that effective prompt engineering requires\ncross-functional collaboration\n. Role-based access control (RBAC) systems define clear permissions for prompt engineers who create and optimize prompts, domain experts who provide business context, developers who handle technical integration, and product managers who coordinate requirements.\nMulti-stage approval workflows\nensure quality control through draft, review, approval, and deployment stages. Each stage involves different stakeholders, from informal team reviews during drafting to formal compliance checks before production deployment. Real-time collaboration features include simultaneous editing capabilities, comment systems for asynchronous feedback, and integration with communication platforms like Slack for notifications.\nPrompt evaluation information: Methods and metrics for quality assurance\n‚Äã\nThe evaluation of LLM outputs represents one of the most challenging aspects of AI deployment. Unlike traditional software where outputs are deterministic, LLMs produce varied responses that require sophisticated evaluation frameworks combining automated metrics, human judgment, and statistical analysis.\nDifferent types of prompt evaluation methods\n‚Äã\nTraditional NLP metrics\nlike BLEU and ROUGE provide baseline measurements through n-gram overlap analysis. BLEU measures precision-based similarity using a brevity penalty to prevent gaming through short outputs, while ROUGE focuses on recall, making it particularly effective for summarization tasks. However, these metrics show\npoor correlation with human judgment\nfor creative or conversational tasks.\nModern evaluation approaches leverage\nmodel-based metrics\nsuch as BERTScore, which uses contextual embeddings to capture semantic relationships beyond surface-level text matching. Task-specific metrics have emerged to address particular concerns: faithfulness metrics ensure outputs are grounded in provided context, relevance scores measure alignment with input queries, and specialized detectors identify toxicity, bias, and safety violations.\nLLM-as-a-judge\napproaches represent a paradigm shift in evaluation methodology. These systems use one language model to evaluate outputs from another, implementing various patterns including single output scoring (with or without references) and pairwise comparison. Research indicates these approaches achieve approximately\n80% agreement with human evaluators\n, with GPT-4 showing the highest correlation (Spearman's œÅ = 0.67 for correctness assessments).\nAutomated evaluation vs human evaluation\n‚Äã\nThe choice between automated and human evaluation involves fundamental tradeoffs.\nAutomated evaluation\noffers unlimited scalability, processing thousands of examples at minimal cost while maintaining perfect consistency. However, it struggles with nuanced understanding, creative assessment, and cultural context. These systems excel at high-volume production monitoring, regression testing, and initial quality screening for objective tasks.\nHuman evaluation\nprovides irreplaceable value through nuanced understanding of context, creativity assessment, and ethical considerations. Human evaluators can identify subtle quality issues that automated systems miss, understand cultural sensitivities, and evaluate complex multi-step reasoning. The primary limitations are scalability constraints, inter-annotator disagreement reaching 20-30% for subjective tasks, and the significant time and cost requirements.\nIndustry best practices increasingly favor\nhybrid approaches\nthat leverage the strengths of both methods. A typical implementation uses automated screening for 95% of outputs, escalating edge cases and high-importance scenarios to human review. This tiered system begins with basic automated checks, progresses to LLM-as-judge evaluation for sophisticated assessment, and reserves human verification for critical decisions.\nMetrics and scoring systems\n‚Äã\nEvaluation frameworks must consider multiple dimensions of quality.\nTraditional metrics\nremain relevant for specific use cases - BLEU scores work well for translation tasks, while perplexity measurements help assess language modeling capability. However, their limitations have driven adoption of more sophisticated approaches.\nCustom metrics development\nhas become essential for domain-specific applications. Organizations develop metrics for cultural sensitivity in global applications, technical accuracy in specialized domains like medicine or law, and brand voice alignment for customer-facing content. The implementation framework involves defining specific success criteria, creating representative evaluation datasets, developing clear scoring rubrics, validating metrics against human judgment, and iterating based on production performance.\nThe emergence of\nmulti-criteria evaluation\nrecognizes that prompt quality cannot be captured by single metrics. Modern platforms enable combining multiple evaluation methods, weighting different criteria based on use case requirements, and creating composite scores that reflect overall quality.\nA/B testing methodologies for prompts\n‚Äã\nRigorous A/B testing requires\nstatistical foundations\nincluding appropriate sample size calculations, effect size determination, and statistical power considerations. The standard approach targets 95% confidence levels (p < 0.05) with 80% statistical power to detect meaningful differences. Sample size calculations must account for expected conversion rates and minimum detectable effects.\nTest design best practices\nemphasize randomization to eliminate selection bias, stratification to account for user segments, and temporal control spanning complete business cycles. Tests should run for minimum seven days to capture weekly patterns, with longer durations for B2B applications that follow monthly cycles. Concurrent testing of variants controls for external factors that might influence results.\nThe implementation framework progresses through\nhypothesis formation\n, where teams define null and alternative hypotheses with specific success criteria;\ntest execution\nwith random assignment and continuous monitoring; and\nanalysis and interpretation\nconsidering both statistical and practical significance. Organizations must analyze results across user segments and document findings for future reference.\nDeployment and UI information: From development to production scale\n‚Äã\nThe deployment of prompt management systems requires sophisticated infrastructure supporting everything from local development to global production scale. Modern architectures emphasize API-first design, comprehensive monitoring, and user interfaces that accommodate both technical and non-technical stakeholders.\nAPI deployment options\n‚Äã\nREST API patterns\ndominate the landscape with standard endpoints for CRUD operations on prompts, streaming endpoints using Server-Sent Events for real-time responses, and batch processing capabilities for high-volume operations. Best practices include semantic versioning in URL paths, backward compatibility through additive changes, and comprehensive pagination for large prompt libraries.\nGraphQL implementations\noffer advantages through single endpoints serving all operations, flexible data fetching that reduces over-fetching, and real-time subscriptions for prompt updates. The introspection capabilities enable dynamic schema discovery, supporting more flexible client implementations. However, the added complexity may not be justified for simpler use cases.\nStreaming architectures\nhave become essential for real-time applications. HTTP/2 Server Push enables efficient real-time delivery, WebSocket connections support bidirectional communication for interactive applications, and event-driven architectures using message queues handle high-throughput scenarios. Implementation requires careful attention to backpressure handling, connection pooling, and automatic reconnection mechanisms.\nVersioning strategies\nmust balance stability with evolution. URI path versioning (/api/v1/ vs /api/v2/) offers clarity but requires careful URL management. Header-based versioning provides cleaner URLs while maintaining version control. Migration best practices include maintaining 2-3 major versions concurrently, implementing feature flags for gradual rollouts, and providing 6-12 month deprecation notices.\nIntegration patterns with existing systems\n‚Äã\nSDK design patterns\nemphasize consistency across programming languages while providing language-specific optimizations. Modern SDKs implement automatic retry logic with exponential backoff, built-in instrumentation for observability, and async/await support for non-blocking operations. The architecture should maintain consistent API surfaces while leveraging language-specific features for optimal developer experience.\nMiddleware patterns\nenable seamless integration with existing applications. Request/response middleware handles cross-cutting concerns including authentication, rate limiting, caching, and logging. These patterns support gradual adoption, allowing teams to integrate prompt management without wholesale application rewrites.\nOrchestration patterns\nbecome critical for complex workflows. Platforms like Apache Airflow and Kubeflow provide DAG-based workflow management, while newer solutions support agent-based orchestration for dynamic routing. The Model Context Protocol (MCP) enables standardized tool-use coordination across different agent frameworks.\nUser interface design for prompt management\n‚Äã\nEffective UI design must balance power with usability.\nCore patterns\ninclude Monaco Editor integration for syntax-highlighted prompt editing, variable injection with auto-completion, real-time template preview, and side-by-side version comparison. These features must be accessible to non-technical users while providing advanced capabilities for power users.\nThe\nprompt development lifecycle\nfollows a clear workflow from creation through testing, versioning, deployment, monitoring, and iteration. User experience patterns emphasize progressive disclosure, starting with simple interfaces and revealing advanced features as needed. Collaborative editing with real-time updates and conflict resolution enables team-based development.\nEvaluation and testing interfaces\nrequire special attention. Batch testing capabilities allow uploading datasets for comprehensive evaluation, while interactive playgrounds enable real-time experimentation. Comparison views showing multiple models or prompts side-by-side facilitate decision-making, supported by metrics dashboards visualizing performance trends.\nMonitoring and observability features\n‚Äã\nStructured logging\nforms the foundation of effective monitoring. Logs must capture timestamp, trace and span IDs for correlation, prompt and execution identifiers, detailed metrics including token usage and latency, and evaluation scores. This structured approach enables sophisticated analysis while maintaining human readability.\nDistributed tracing\nprovides end-to-end visibility across complex LLM pipelines. OpenTelemetry integration has emerged as the standard, enabling vendor-neutral instrumentation. Tracing must capture prompt execution flows, identify performance bottlenecks, track error propagation, and attribute costs per request and user.\nMetrics and KPIs\nspan technical and business dimensions. Technical metrics include latency percentiles (P50, P95, P99), token usage and associated costs, error rates by type, and throughput measurements. Quality metrics track accuracy scores, user feedback ratings, hallucination detection rates, and safety compliance. Business metrics focus on user engagement, cost efficiency, and performance trends over time.\nAlerting and incident response\nsystems categorize issues by severity. Performance alerts trigger on latency or error rate thresholds, cost alerts prevent budget overruns, quality alerts detect accuracy degradation, and infrastructure alerts ensure system availability. Platform implementations like Datadog LLM Observability provide end-to-end tracing with built-in security scanners, while open-source alternatives like Langfuse offer similar capabilities without vendor lock-in.\nComprehensive tool analysis\n‚Äã\nThe prompt management ecosystem has evolved rapidly, with tools ranging from open-source projects to enterprise platforms. Each tool addresses specific aspects of the prompt lifecycle, from development and testing to deployment and monitoring.\nMicrosoft PromptFlow and MLflow\n‚Äã\nMicrosoft PromptFlow\nprovides a visual, flow-based approach to LLM application development. The platform offers a DAG-based visual interface where users can create standard flows for general applications, chat flows for conversational apps, and evaluation flows for assessment scenarios. While the\ncore platform is open source\n(MIT license), costs arise from underlying Azure services, typically running $50+ per day for moderate usage.\nThe platform excels in enterprise scenarios with full\nAzure RBAC integration\nand built-in roles for different user types. However, collaboration features remain limited, requiring a \"clone\" workflow for multi-user editing. The visual development paradigm makes it accessible to non-technical users while maintaining the power needed for complex applications.\nMLflow\nhas evolved into a comprehensive platform for both traditional ML and LLM applications. The\n3.0 release\nintroduced significant GenAI capabilities including a git-inspired prompt registry with commit-based versioning, enhanced LLM tracking, and sophisticated evaluation frameworks. The platform remains\ncompletely free\nas open source, though Databricks offers managed enterprise features.\nMLflow's strength lies in its\nmature ecosystem\nand provider-agnostic approach. The LLM-as-a-judge evaluation framework provides built-in metrics for toxicity, correctness, and relevance, while supporting custom evaluators. The unified interface works across OpenAI, Anthropic, and other providers, making it ideal for organizations using multiple LLM services.\nLangSmith and Weights & Biases Prompts\n‚Äã\nLangSmith\nrepresents a purpose-built solution for LLM observability and prompt engineering. The platform offers comprehensive prompt versioning with git-like controls, an interactive prompt canvas for optimization, and advanced tracing capabilities. Pricing starts at\n$39/user/month\nfor teams, with a generous free tier for individual developers.\nThe platform's\ntight integration\nwith the LangChain ecosystem provides seamless setup for LangChain applications while supporting other frameworks through OpenTelemetry. The evaluation framework combines LLM-as-judge capabilities with human feedback collection, enabling sophisticated quality assurance workflows. Zero-latency observability ensures production applications aren't impacted by monitoring overhead.\nWeights & Biases Prompts/Weave\nextends the established W&B MLOps platform into LLM territory. The unified platform approach allows teams to manage traditional ML and LLM experiments in one place. However, the\npricing model\nhas drawn criticism, with enterprise costs reported at $200-400/user/month and a \"tracked hours\" model that can become expensive for parallel workloads.\nThe platform excels in\nvisualization and reporting\n, leveraging W&B's mature dashboarding capabilities. Integration with the broader W&B ecosystem provides advantages for teams already using the platform for traditional ML workflows, though the complexity may overwhelm teams focused solely on LLM applications.\nHumanloop and Promptfoo\n‚Äã\nHumanloop\npositions itself as an enterprise-grade AI evaluation platform with strong collaborative features. The platform emphasizes\nhuman-in-the-loop\nworkflows, enabling domain experts to participate in prompt engineering without technical expertise. Pricing starts at $100/month for small teams, scaling to custom enterprise agreements.\nThe evaluation framework supports\nmulti-modal approaches\ncombining AI-based, code-based, and human evaluations. Sophisticated RBAC controls with organization and project-level permissions enable enterprise deployment. The platform is\nSOC 2 Type II compliant\nwith GDPR and HIPAA compliance options, making it suitable for regulated industries.\nPromptfoo\ntakes a different approach as an\nopen-source, CLI-first\ntool focused on systematic testing. The MIT-licensed community edition runs entirely locally with no cloud dependencies, addressing privacy concerns. The tool has achieved significant adoption with\n100,000+ users\nand offers enterprise features for larger organizations.\nThe platform excels in\nsecurity testing\nwith advanced red teaming capabilities, vulnerability scanning, and jailbreak detection. Integration with CI/CD pipelines enables automated testing in development workflows. While the CLI-first approach may deter non-technical users, it provides unmatched flexibility for developer teams.\nOpen source ecosystem\n‚Äã\nLangfuse\nhas emerged as the\nmost popular open-source\nLLM observability platform. The Apache 2.0 licensed tool provides comprehensive tracing, centralized prompt management, and evaluation capabilities. With self-hosted and cloud options, it offers flexibility while maintaining a generous free tier (100k events/month for $59).\nOpenLLMetry\ntakes a\nvendor-neutral approach\nusing OpenTelemetry standards. This enables integration with existing observability infrastructure, supporting 15+ backends including Datadog, Honeycomb, and New Relic. The approach prevents vendor lock-in while leveraging established monitoring tools.\nAgenta\nprovides an open-source LLMOps platform emphasizing\ncollaborative workflows\n. The interactive playground supports 50+ models while enabling non-technical stakeholders to participate in prompt engineering. The framework-agnostic approach works with any LLM architecture including RAG systems and multi-agent workflows.\nAdditional notable tools include\nPezzo\n(cloud-native with cost optimization focus),\nLiteral AI\n(multimodal support from Chainlit creators), and\nPortkey\n(AI gateway supporting 200+ LLMs with built-in guardrails).\nFeature comparison chart\n‚Äã\nThe following comprehensive comparison evaluates tools across critical dimensions for prompt management, evaluation, and deployment:\nTool\nCost Structure\nRBAC\nEvaluation Features\nDeployment Options\nVisualization\nIntegration\nCollaboration\nVersion Control\nScalability\nDocumentation\nEase of Use\nCustomization\nMonitoring\nData Management\nPromptFlow\nFree OSS; Azure service costs ($50+/day typical)\nFull Azure RBAC integration\nBuilt-in evaluation flows, batch testing\nAzure, local, Docker, K8s, hybrid\nInteractive DAG, debugging tools\nAzure native, Git, CI/CD\nLimited (clone workflow)\nGit integration, flow versioning\nEnterprise Azure scale\nComprehensive Microsoft docs\nModerate (DAG concepts)\nVisual flow customization\nAzure App Insights\nAzure storage, artifacts\nMLflow\nFree OSS; Databricks managed option\nBasic OSS; enterprise with Databricks\nLLM-as-judge, custom metrics, built-in scorers\nLocal, Docker, K8s, cloud platforms\nWeb UI, experiment comparison\nProvider agnostic, AI Gateway\nProject-based\nGit-inspired prompt registry\nDistributed execution support\nExtensive, can be overwhelming\nComplex for non-technical\nPlugin-style evaluators\nBuilt-in metrics, traces\nUnity Catalog (Databricks)\nLangSmith\nFree tier; $39/user/month team\nOrganization workspaces, project permissions\nLLM-as-judge, human feedback, datasets\nCloud (US/EU), self-hosted enterprise\nTrace visualization, dashboards\nNative LangChain, OpenTelemetry\nTeam sharing, comments\nGit-like versioning, tags\n99.9% uptime, enterprise scale\nExcellent, framework-specific\nVery intuitive\nCustom evaluators, code-based\nZero-latency observability\nDataset versioning, 400-day retention\nW&B Prompts\nFree tier; $35-400/user/month\nProject-based roles\nBuilt-in metrics, comparisons\nCloud, self-hosted\nAdvanced dashboards, reports\nExtensive (PyTorch, TF, HF)\nCross-functional teams\nArtifact management\nEnterprise-grade\nGood but complex navigation\nSteep learning curve\nExtensive customization\nComprehensive ML/LLM\nArtifact storage, lineage\nHumanloop\n$100/month starter; enterprise custom\nGranular org/project roles\nMulti-modal (AI, code, human)\nCloud, VPC, on-premises\nPerformance dashboards\nCI/CD, GitHub, Slack, webhooks\nDomain expert friendly\nComplete version history\nEnterprise scale\nComprehensive guides\nUser-friendly\nCustom evaluators\nReal-time observability\nSOC2, GDPR, HIPAA compliant\nPromptfoo\nFree OSS; enterprise pricing available\nEnterprise edition only\nMatrix testing, security scanning\n100% local; no cloud needed\nCLI + web viewer\nExtensive CI/CD support\nLimited (technical focus)\nConfig file based\nHandles large test suites\nExcellent OSS docs\nCLI-first (technical)\nHighly extensible\nLocal performance metrics\nLocal storage, no cloud\nLangfuse\nFree OSS; $59/month for 100k events\nBasic OSS; cloud has team features\nLLM-as-judge, user feedback\nSelf-hosted, cloud\nIntuitive dashboards\n50+ providers, OpenTelemetry\nPlayground, shared views\nPrompt versioning\nBattle-tested scale\nExtensive with demos\nVery intuitive\nFramework agnostic\nComprehensive tracing\nGenerous retention, export\nOpenLLMetry\nFree tier 10k traces; hosted backend\nDepends on backend choice\nVia integrated platforms\nExisting observability stack\nUses backend tools\n15+ observability backends\nVia integrated tools\nThrough backends\nEnterprise scale\nGood OpenTelemetry focus\nRequires OTel knowledge\nVendor neutral\nFull OTel capabilities\nBackend dependent\nAgenta\nFree OSS; cloud available\nRole-based access\nHuman + automated feedback\nSelf-hosted, cloud, on-prem\nWeb interface\nFramework agnostic\nNon-technical friendly\nVersion control\nEnterprise workflows\nGood, limited advanced\nModerate complexity\nAny LLM architecture\nIntegrated monitoring\nFlexible storage\nPortkey\nFree tier 10k logs; paid plans\nEnterprise features\nVia gateway features\nSelf-hosted gateway, cloud\nComprehensive dashboard\n200+ LLM providers\nTeam features\nTemplate versioning\n99.99% uptime SLA\nExtensive deployment guides\nGateway: easy; platform: moderate\nGuardrails, routing\nReal-time monitoring\nEdge architecture\nKey insights for decision-making\n‚Äã\nThe prompt management landscape reveals\nthree distinct categories\nof solutions. Comprehensive platforms like Langfuse and Agenta provide full-stack capabilities suitable for organizations needing integrated prompt management, evaluation, and observability. Specialized tools like Promptfoo and OpenLLMetry excel in specific areas - security testing and observability respectively. Enterprise platforms like Humanloop and LangSmith offer compliance, collaboration, and scale for large organizations.\nCost considerations\nvary dramatically across solutions. Open-source tools provide the lowest total cost of ownership for teams with technical expertise, while managed platforms trade higher costs for reduced operational overhead. The W&B \"tracked hours\" model and Azure service dependencies can create unexpectedly high costs at scale.\nTechnical architecture\nchoices have long-term implications. Vendor-neutral approaches using OpenTelemetry provide flexibility and prevent lock-in, while integrated platforms offer faster time-to-value. Organizations must balance immediate productivity gains against future flexibility needs.\nThe\ncollaboration dimension\nincreasingly determines platform success. Tools enabling non-technical stakeholder participation see higher adoption rates and better prompt quality through domain expert involvement. However, technical teams may prefer CLI-first tools that integrate seamlessly with existing development workflows.\nSelection criteria\nshould prioritize your organization's specific context. Start-ups benefit from free tiers and open-source solutions, while enterprises require compliance certifications and dedicated support. Consider your team's technical sophistication, existing tool investments, and scaling requirements when making decisions.\nThe prompt management ecosystem continues evolving rapidly, with new tools emerging monthly and existing platforms adding capabilities. Organizations should design their architecture to accommodate change, using abstraction layers and standard protocols where possible. Success requires not just tool selection but also process design, team training, and continuous optimization based on production metrics.\nWas this helpful?\nEdit this page\nPrevious\nPlatforms\nNext\nExamples\nPrompt management information: Core concepts and infrastructure\nCore concepts of prompt management systems\nPrompt versioning and lifecycle management\nTemplate management and parameterization\nCollaboration features for teams\nPrompt evaluation information: Methods and metrics for quality assurance\nDifferent types of prompt evaluation methods\nAutomated evaluation vs human evaluation\nMetrics and scoring systems\nA/B testing methodologies for prompts\nDeployment and UI information: From development to production scale\nAPI deployment options\nIntegration patterns with existing systems\nUser interface design for prompt management\nMonitoring and observability features\nComprehensive tool analysis\nMicrosoft PromptFlow and MLflow\nLangSmith and Weights & Biases Prompts\nHumanloop and Promptfoo\nOpen source ecosystem\nFeature comparison chart\nKey insights for decision-making\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 3,
    "depth": 3,
    "crawled_at": "2026-02-25T10:08:19.899303"
  },
  "https://techhq.dc.lilly.com/docs/solution/ai/examples/enterprise/PRD": {
    "url": "https://techhq.dc.lilly.com/docs/solution/ai/examples/enterprise/PRD",
    "title": "Press Release Drafter (PRD) | Tech HQ",
    "description": "Last Updated: July 25, 2025",
    "h1": [
      "Press Release Drafter (PRD)",
      "An Agentic AI System for Press Release Generation"
    ],
    "h2": [
      "Product Overview‚Äã",
      "Problem Statement‚Äã",
      "Target Users‚Äã",
      "Product Goals‚Äã",
      "Key Features and Requirements‚Äã",
      "User Stories‚Äã",
      "Technical Requirements‚Äã",
      "Architecture Diagrams‚Äã",
      "Demo Video‚Äã",
      "Success Metrics‚Äã",
      "Implementation Considerations‚Äã",
      "Risks and Mitigations‚Äã",
      "Conclusion‚Äã"
    ],
    "h3": [
      "1. Multi-Agent Reasoning System‚Äã",
      "2. Advanced Insights Extraction‚Äã",
      "3. MLR Guardrails for Compliance‚Äã",
      "4. Dynamic Content Adjustment‚Äã",
      "5. Rich Authoring Experience‚Äã",
      "Integration Requirements‚Äã",
      "Security and Compliance‚Äã",
      "Performance Requirements‚Äã",
      "High-Level Architecture‚Äã",
      "C2 Level Architecture‚Äã",
      "AI Ecosystem Integration‚Äã"
    ],
    "text_content": "Press Release Drafter (PRD) | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nEcosystem\nPatterns\nPositioning\nExamples\nEnterprise\nPress Release Drafter (PRD)\nPress Release Miner (PRM)\nProduct Exemplars\nAMIGO\nChatNow\nChat in a box\nClinical Programming Assistant\nHR Talent Acquisition Agents\nLillyNow\nPrescriberPoint Kisunla AI Pilot\nPromo Maker\nRevenue Defender\nScientific E-Author\nCoding Tools\nStandards\nAI Submission Guide Template\nüóÑÔ∏è AI & Intelligent Agents BLT\nü§ñ Agentic AI\nüß≠ Conversational AI\nüß≠ Knowledge Bases\nüß≠ Multimodal AI\nüß≠ Translation Services\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nü§ñ AI & Intelligent Agents\nExamples\nEnterprise\nPress Release Drafter (PRD)\nOn this page\nPress Release Drafter (PRD)\nDocument Information\nLast Updated:\nJuly 25, 2025\nOwner:\nBrian Lewis\nPoint of Contact:\nAli Kharazmi\nContributors and Reviewers:\nArchit Kaila, Haitham Maya, Malika Mahoui\nAn Agentic AI System for Press Release Generation\nProduct Overview\n‚Äã\nThe Press Release Drafter (PRD) is an agentic AI product designed for Lilly's corporate communications team to automate\nthe generation of preliminary press releases for clinical trial results. This advanced system leverages multi-agent\nreasoning and natural language processing to create accurate, compliant, and professionally styled press releases that\nalign with Lilly's tone and communication standards.\nProblem Statement\n‚Äã\nCreating press releases for clinical trial results is a time-consuming process that requires:\nExtracting and synthesizing complex clinical data from various sources\nEnsuring medical and regulatory compliance\nMaintaining consistent messaging and brand voice\nCoordinating input from multiple stakeholders\nMeeting tight deadlines for market-sensitive information\nThis process currently demands significant time from skilled communications professionals, medical reviewers, and\nregulatory experts, often leading to bottlenecks in the release pipeline.\nTarget Users\n‚Äã\nCorporate Communications team members\nMedical Affairs professionals\nRegulatory Affairs specialists\nClinical Trial teams\nPublic Relations managers\nProduct Goals\n‚Äã\nReduce the time required to create initial press release drafts by 70%\nEnsure 100% compliance with regulatory requirements and Lilly's communication guidelines\nImprove consistency in messaging across all clinical trial press releases\nEnable communications team to focus on strategic messaging rather than initial drafting\nAccelerate the review cycle by providing higher quality first drafts\nKey Features and Requirements\n‚Äã\n1. Multi-Agent Reasoning System\n‚Äã\nSpecialized agents for data extraction, medical accuracy, regulatory compliance, and content generation\n(\nSupervisor-Worker Agent Pattern\n)\nCollaborative reasoning between agents to resolve conflicts and ensure coherence\n(\nPeer Network Collaboration Pattern\n)\nTransparent decision-making process with justifications for content choices\n2. Advanced Insights Extraction\n‚Äã\nSupport for multiple data source formats:\nText documents (TXT, PDF)\nPresentations (PPTX)\nSpreadsheets (XLSX)\nImages and charts\nWord documents (DOCX)\nAutomated extraction of key clinical trial data points\n(\nReAct Pattern\n):\nPrimary and secondary endpoints\nSafety profiles\nStatistical significance\nPatient demographics\nStudy design details\n3. MLR Guardrails for Compliance\n‚Äã\nBuilt-in Medical, Legal, and Regulatory (MLR) checks\n(\nSelf Critique Pattern\n)\nAutomatic flagging of potential compliance issues\nCitation and reference management\nVerification of claims against source data\nAdverse event reporting compliance\n4. Dynamic Content Adjustment\n‚Äã\nConfigurable tone settings (formal, conversational, technical)\nAdjustable draft length (brief announcement, detailed release, comprehensive background)\nAudience-specific language calibration\nCustomizable emphasis on different aspects of trial results\n5. Rich Authoring Experience\n‚Äã\nInteractive editing interface\nReal-time suggestions and alternatives\nCollaborative review capabilities\nVersion control and change tracking\nComment and feedback integration\nUser Stories\n‚Äã\nCommunications Manager\n\"As a communications manager, I want to quickly generate a first draft press release from clinical trial results so\nthat I can focus on refining messaging strategy rather than creating content from scratch.\"\nMedical Reviewer\n\"As a medical reviewer, I want the AI to accurately represent clinical data and flag potential medical claims that\nrequire additional verification so that I can ensure scientific accuracy.\"\nRegulatory Specialist\n\"As a regulatory specialist, I want the system to automatically check content against current regulatory guidelines\nso that compliance issues are identified early in the drafting process.\"\nPR Director\n\"As a PR director, I want to be able to adjust the tone and length of press releases based on the significance of\nthe results so that our communications are appropriately calibrated to the news.\"\nTechnical Requirements\n‚Äã\nIntegration Requirements\n‚Äã\nSecure connection to clinical trial databases\nIntegration with document management systems\nAPI access to regulatory guidelines database\nConnection to Lilly's brand voice guidelines\nSecurity and Compliance\n‚Äã\nEnd-to-end encryption for all data\nRole-based access controls\nAudit trail for all content generation and edits\nCompliance with data privacy regulations\nSecure handling of pre-public material information\nPerformance Requirements\n‚Äã\nInitial draft generation in under 5 minutes\nSupport for concurrent users across global teams\n99.9% uptime during business hours\nArchitecture Diagrams\n‚Äã\nHigh-Level Architecture\n‚Äã\nThe following diagram illustrates the high-level architecture of the Press Release Drafter (PRD), showing the agents and\ntools involved in the process:\nC2 Level Architecture\n‚Äã\nThe following diagram provides a more detailed C2 (Component and Connector) level view of the system architecture:\nDemo Video\n‚Äã\nA demonstration video of the Press Release Drafter (PRD) in action is available at the following link:\nPress Release Drafter Demo Video\nThis video showcases the key features and workflow of the system, demonstrating how it processes clinical trial data and\ngenerates compliant press releases.\nSuccess Metrics\n‚Äã\nEfficiency Metrics\nTime saved in draft creation process\nReduction in review cycles\nDecrease in time-to-publish\nQuality Metrics\nAccuracy of extracted clinical data\nCompliance success rate\nConsistency with brand voice\nUser satisfaction ratings\nAdoption Metrics\nNumber of active users\nFrequency of use\nFeature utilization rates\nUser retention\nImplementation Considerations\n‚Äã\nAI Ecosystem Integration\n‚Äã\nLeverage Lilly's existing AI ecosystem components:\nGenAI Cortex for core language generation\nAgentic AI framework for multi-agent orchestration\nStructured and Unstructured Knowledgebases for information retrieval\nMLR guardrails from the Responsible AI Layer\nRisks and Mitigations\n‚Äã\nRisk\nImpact\nLikelihood\nMitigation\nInaccurate data extraction\nHigh\nMedium\nImplement confidence scoring and human verification for critical data points\nRegulatory non-compliance\nHigh\nLow\nRegular updates to compliance rules and mandatory human review\nUser resistance\nMedium\nMedium\nPhased rollout with extensive training and clear demonstration of value\nSystem performance issues\nMedium\nLow\nRobust testing and scalable infrastructure design\nData security breach\nHigh\nLow\nEnd-to-end encryption and strict access controls\nConclusion\n‚Äã\nThe Press Release Drafter (PRD) represents a significant advancement in how Lilly approaches corporate communications\nfor clinical trial results. By automating the initial drafting process while ensuring compliance and quality, this tool\nwill enable the communications team to focus on strategic messaging and stakeholder engagement, ultimately improving the\nefficiency and effectiveness of Lilly's external communications.\nWas this helpful?\nEdit this page\nPrevious\nExamples\nNext\nPress Release Miner (PRM)\nProduct Overview\nProblem Statement\nTarget Users\nProduct Goals\nKey Features and Requirements\n1. Multi-Agent Reasoning System\n2. Advanced Insights Extraction\n3. MLR Guardrails for Compliance\n4. Dynamic Content Adjustment\n5. Rich Authoring Experience\nUser Stories\nTechnical Requirements\nIntegration Requirements\nSecurity and Compliance\nPerformance Requirements\nArchitecture Diagrams\nHigh-Level Architecture\nC2 Level Architecture\nDemo Video\nSuccess Metrics\nImplementation Considerations\nAI Ecosystem Integration\nRisks and Mitigations\nConclusion\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 12,
    "depth": 3,
    "crawled_at": "2026-02-25T10:08:22.108487"
  },
  "https://techhq.dc.lilly.com/docs/solution/ai/examples/enterprise/PRM": {
    "url": "https://techhq.dc.lilly.com/docs/solution/ai/examples/enterprise/PRM",
    "title": "Press Release Miner (PRM) | Tech HQ",
    "description": "Last Updated: July 25, 2025",
    "h1": [
      "Press Release Miner (PRM)",
      "An Agentic AI System for Research and Content Generation"
    ],
    "h2": [
      "Introduction‚Äã",
      "System Architecture Overview‚Äã",
      "The PRM Process‚Äã",
      "System Architecture‚Äã",
      "Agentic Patterns in PRM‚Äã",
      "Technical Implementation‚Äã",
      "Demo Video‚Äã",
      "Conclusion‚Äã"
    ],
    "h3": [
      "UI Components‚Äã",
      "Agents‚Äã",
      "Tools‚Äã",
      "Data Sources‚Äã",
      "Multi-Agent Coordination‚Äã",
      "Tool Usage Patterns‚Äã",
      "Parallel Processing‚Äã",
      "Agent Specialization and Collaboration‚Äã",
      "LLM Integration‚Äã",
      "Technologies Used‚Äã",
      "Prompt Engineering‚Äã",
      "Quality Control Mechanisms‚Äã"
    ],
    "text_content": "Press Release Miner (PRM) | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nEcosystem\nPatterns\nPositioning\nExamples\nEnterprise\nPress Release Drafter (PRD)\nPress Release Miner (PRM)\nProduct Exemplars\nAMIGO\nChatNow\nChat in a box\nClinical Programming Assistant\nHR Talent Acquisition Agents\nLillyNow\nPrescriberPoint Kisunla AI Pilot\nPromo Maker\nRevenue Defender\nScientific E-Author\nCoding Tools\nStandards\nAI Submission Guide Template\nüóÑÔ∏è AI & Intelligent Agents BLT\nü§ñ Agentic AI\nüß≠ Conversational AI\nüß≠ Knowledge Bases\nüß≠ Multimodal AI\nüß≠ Translation Services\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nü§ñ AI & Intelligent Agents\nExamples\nEnterprise\nPress Release Miner (PRM)\nOn this page\nPress Release Miner (PRM)\nDocument Information\nLast Updated:\nJuly 25, 2025\nOwner:\nBrian Lewis\nPoint of Contact:\nAli Kharazmi\nContributors and Reviewers:\nArchit Kaila, Haitham Maya, Malika Mahoui\nAn Agentic AI System for Research and Content Generation\nIntroduction\n‚Äã\nPress Release Miner (PRM) is an advanced LLM-powered multi-agent system designed to extract insights from scientific source documents and news journals. Its primary purpose is to answer questions and generate briefing documents for key stakeholders using standard Eli Lilly language and style. The system employs a sophisticated agentic architecture that coordinates multiple specialized AI agents working together to conduct comprehensive research and produce high-quality content.\nPRM addresses a critical need in corporate communications: conducting thorough research and generating media briefing documents for executives that ensure completeness, accuracy, and consistent messaging. By automating this process through an agentic approach, PRM significantly reduces the time and effort required while maintaining high standards of quality and consistency.\nSystem Architecture Overview\n‚Äã\nTools\nAgents\nUI Components\nExternal Data Sources\nNews Journals\\n(NYT, WP, WSJ, etc.)\nWeb Search\\n(DDG, Bing)\nYouTube Transcriptions\nWikipedia\nInternal Data Sources\nLilly Press Release\nEarnings Call Transcripts\nPrior Briefing Documents\nResearch Topic\nResearch Factors\nHuman Interaction\nBriefing Document\nResearch Planner\\nContextualization Agent\nResearcher\\nResearch Agent (CoT / ToT)\nSynthesizer\\nInterview / Q&A Agent\nCopyeditor\\nFormatting Agent\nRAG\nWeb Search\nYouTube\nWikipedia\nThe diagram above illustrates the architecture of the Press Release Miner system, showing how UI components, specialized agents, tools, and data sources interact to create a comprehensive research and content generation pipeline.\nThe PRM Process\n‚Äã\nThe Press Release Miner follows a structured six-step process to generate comprehensive briefing documents:\nDevelop a research plan\n: The system begins by analyzing the research topic and developing a strategic plan for gathering information. This includes identifying key factors and questions that need to be addressed.\nSearch internal press releases\n: PRM leverages Retrieval Augmented Generation (RAG) to search through Eli Lilly's internal press releases, extracting relevant information and insights.\nSearch web for news sources\n: The system extends its research to external sources, searching trusted news outlets and websites for additional context and information.\nConduct follow-up research\n: Based on initial findings, PRM performs deeper, targeted research to fill knowledge gaps and answer specific questions that arise.\nSynthesize research results\n: The system integrates information from multiple sources, removing duplicates and organizing content into a coherent structure with proper citations.\nGenerate briefing document\n: Finally, PRM produces a well-formatted briefing document that adheres to Eli Lilly's language and style guidelines.\nThis entire process is orchestrated through approximately 50 LLM calls, distributed across various agents and tools within the system.\nSystem Architecture\n‚Äã\nPRM's architecture consists of four main components: UI interfaces, specialized agents, tools, and data sources. These components work together to create a seamless research and content generation pipeline.\nUI Components\n‚Äã\nResearch Topic\n: Interface for users to input the main subject for research\nResearch Factors\n: Display and selection of key aspects to investigate\nHuman Interaction\n: Interface for user feedback and guidance during the research process\nBriefing Document\n: Final output display with the generated content\nAgents\n‚Äã\nPRM employs four specialized agents, each with distinct responsibilities:\nResearch Planner (Contextualization Agent)\n: Responsible for understanding the research topic, breaking it down into manageable components, and developing a comprehensive research strategy. This agent provides the overall direction for the research process.\nResearcher (Research Agent/CoT/ToT)\n: The primary information gathering agent that executes the research plan. It utilizes Chain-of-Thought (CoT) and Tree-of-Thought (ToT) reasoning to conduct thorough investigations across multiple sources.\nSynthesizer (Interview/Q&A Agent)\n: Processes and integrates the collected information, identifying patterns, resolving contradictions, and organizing content into a coherent narrative. This agent can also generate follow-up questions to fill information gaps.\nCopyeditor (Formatting Agent)\n: Ensures the final document adheres to Eli Lilly's style guidelines, maintains consistent formatting, and includes proper citations for all information sources.\nTools\n‚Äã\nThe agents leverage several tools to perform their tasks:\nRAG (Retrieval Augmented Generation)\n: Enables semantic search across internal documents, retrieving relevant information based on context rather than just keywords.\nWeb Search\n: Interfaces with search engines (primarily Bing) to gather information from trusted external sources.\nYouTube\n: Extracts information from video content, particularly from news channels and official company presentations.\nWikipedia\n: Provides background information and context for topics and entities mentioned in the research.\nData Sources\n‚Äã\nPRM accesses both internal and external data sources:\nInternal Data Sources\n:\nLilly Press Releases\nEarnings Call Transcripts\nPrior Briefing Documents\nExternal Data Sources\n:\nNews Journals (NYT, WP, WSJ, etc.)\nWeb Search Results (DDG, Bing)\nYouTube Transcriptions\nWikipedia\nAgentic Patterns in PRM\n‚Äã\nThe Press Release Miner implements several sophisticated agentic patterns that enable effective coordination, parallel processing, and specialized task execution. These patterns represent the cutting edge of multi-agent system design and contribute significantly to PRM's effectiveness.\nMulti-Agent Coordination\n‚Äã\nPRM's architecture demonstrates how multiple specialized agents can work together to accomplish complex tasks. The system implements a coordination mechanism that allows agents to:\nShare context\n: Agents pass information and context to each other, ensuring continuity throughout the research process.\nSequential processing\n: The Research Planner sets the direction, followed by the Researcher gathering information, the Synthesizer integrating it, and finally the Copyeditor formatting the output.\nFeedback loops\n: Agents can request additional information or clarification from other agents when needed, creating a dynamic research process.\nThis coordination pattern enables each agent to focus on its specialized task while contributing to the overall goal of producing a comprehensive briefing document.\nTool Usage Patterns\n‚Äã\nPRM implements three distinct tool usage patterns across its agents:\nTool Calling Pattern\n: The system uses a structured approach for agents to invoke tools, with clear input/output specifications. This pattern is implemented in\ntool_calling_tool.py\n, which creates a runnable agent that can use multiple tools based on a system template and human instructions.\nfunction tool_calling_runnable(tool_runtime):\nInitialize system template from tool_runtime\nCopy and initialize tools from tool_runtime\nFor each tool:\nAdd tool name and description to system template\nCreate prompt template with system template, human template, and agent scratchpad\nCreate tool-calling agent with LLM, tools, and prompt\nReturn agent executor with configured agent and tools\nReAct Pattern (Reasoning and Acting)\n: PRM implements the ReAct pattern in\nreact_tool.py\n, allowing agents to alternate between reasoning about their next steps and taking actions. This pattern enhances the agent's ability to plan and execute complex research tasks.\nfunction react_agent_runnable(tool_runtime):\nCopy and initialize tools from tool_runtime\nCreate prompt from react template\nCreate ReAct agent with LLM, tools, and prompt\nReturn agent executor with configured agent and tools\nSelf-Ask Pattern\n: Implemented in\nself_ask_tool.py\n, this pattern enables agents to break down complex questions into simpler sub-questions, answer them sequentially, and then synthesize a final answer. This is particularly useful for complex research topics that require multi-step reasoning.\nfunction self_ask_runnable(tool_runtime):\nCopy tools from tool_runtime\nSelect first tool and rename it to \"Intermediate Answer\"\nCreate prompt from self-ask template\nCreate self-ask agent with LLM, tool, and prompt\nReturn agent executor with configured agent and tool\nParallel Processing\n‚Äã\nOne of PRM's most powerful features is its ability to conduct research on multiple topics or factors simultaneously. This is implemented through the\nparallelization_tool.py\n, which enables:\nConcurrent tool execution\n: Multiple tools can be executed in parallel across different research topics.\nAsynchronous processing\n: The system uses Python's asyncio to manage concurrent tasks efficiently.\nResult aggregation\n: Results from parallel processes are collected and integrated into a coherent output.\nasync function async_parallel_tool_call(topics, tool_runtime, id_task, target):\nCreate queue for communication\nCreate empty tasks list\nFor each topic:\nCreate async task for processing topic and add to tasks list\nWhile true:\nGet item from queue\nYield item to caller\nMark task as done\nIf all tasks are done and queue is empty:\nBreak loop\nAwait all tasks to complete\nThis parallel processing capability significantly reduces the time required to conduct comprehensive research across multiple topics or factors.\nAgent Specialization and Collaboration\n‚Äã\nPRM demonstrates how specialized agents can collaborate effectively to produce high-quality outputs. Each agent has a specific role:\nResearch Pipeline Agent\n: Orchestrates the overall research process, managing the flow of information between other agents and tools.\nclass ResearchPipeline:\nfunction __init__(llm, bing_subscription_key, retriever, templates, etc):\nStore all parameters as instance variables\nInitialize tools array with web search, RAG, and other tools\nCreate dictionary of agent runnables (simple, react, self-ask)\nInitialize tool runtime configuration\nasync function research_question(target, agent_type):\nSelect appropriate agent type\nFor each event from parallel tool execution:\nYield event to caller\nasync function research_question_with_factors(target, topics, format_section_prompt, remove_duplicates_prompt, agent_type):\nSelect appropriate agent type\nProcess research results in parallel\nFormat and deduplicate results\nYield final output\nFormatting Agent\n: Specializes in ensuring consistent formatting and removing duplicates across research outputs.\nclass FormatResearchAgent:\nfunction __init__(llm):\nStore LLM as instance variable\nasync function async_format_section(research_output, format_section_prompt):\nFind reference text with maximum length\nCreate formatting instruction with reference and candidate texts\nCreate formatting chain with LLM\nProcess each research output for uniform formatting\nReturn formatted outputs\nasync function async_remove_duplicates(research_output, remove_duplicates_prompt):\nCreate deduplication instruction template\nCreate deduplication chain with LLM\nCompare each pair of research outputs\nRemove duplicate information\nReturn deduplicated outputs\nBrainstorming Agent\n: Generates research factors and questions to guide the research process.\nfunction brainstorming_agent(brainstorming_prompt_template, llm):\nCreate prompt template with target variable and max 5 items limit\nCreate LLM chain with prompt and LLM\nReturn the chain for generating research factors\nThis specialization allows each agent to focus on what it does best while contributing to the overall system goal.\nTechnical Implementation\n‚Äã\nLLM Integration\n‚Äã\nPRM integrates with Azure OpenAI's GPT-4o model to power its agents. The system makes approximately 50 LLM calls throughout the research and content generation process, distributed across various agents and tasks. The LLM integration is managed through a custom model class that handles authentication and API interactions with Azure OpenAI services and AWS resources.\nTechnologies Used\n‚Äã\nPRM leverages several key technologies:\nLangChain\n: Used extensively for agent creation, tool integration, and prompt management. LangChain provides the foundation for the agentic patterns implemented in PRM.\nFastAPI\n: Powers the backend API, providing WebSocket endpoints for real-time communication during the research process.\nFAISS\n: Implements the vector store for efficient semantic search across internal documents.\nAzure OpenAI\n: Provides the LLM capabilities that power the agents' reasoning and content generation.\nBing Search API\n: Enables web search functionality for gathering information from external sources.\nPrompt Engineering\n‚Äã\nPrompt engineering is a critical aspect of PRM's implementation. The system uses carefully crafted prompts to guide each agent's behavior and ensure high-quality outputs. For example:\nResearch Agent Prompt\n: Guides the research process with specific instructions for gathering comprehensive information:\n# The research agent prompt instructs the agent to gather detailed information with proper citations, format responses with chronological ordering and bulleted lists, and ensure all facts are properly attributed to their sources.\nFormatting Agent Prompt\n: Ensures consistent formatting and structure across research outputs:\n# The formatting agent prompt guides the agent to combine research outputs from multiple sources into a cohesive document, maintain proper citations for all facts and figures, and structure the content with appropriate headings and subheadings.\nDe-duplication Prompt\n: Removes redundant information across research outputs:\n# The de-duplication prompt instructs the agent to compare reference and candidate texts, remove duplicate facts, quotes, and figures from the candidate text, and ensure the final output has no redundant information while maintaining coherent flow.\nThese prompts are critical for guiding the agents' behavior and ensuring they produce outputs that meet Eli Lilly's standards for quality and consistency.\nQuality Control Mechanisms\n‚Äã\nPRM implements several quality control mechanisms to ensure the accuracy and consistency of its outputs:\nUniform Formatting\n: The FormatResearchAgent ensures consistent formatting across all research outputs:\nasync function async_format_section(research_output, format_section_prompt):\nCreate async queue for communication\nFind reference topic with longest text\nCreate formatting instruction template\nCreate formatting chain with LLM\nFor each topic and text:\nIf not reference topic:\nCreate task to format candidate text\nElse:\nAdd to output without formatting\nProcess queue until all tasks complete\nReturn formatted research outputs\nDe-duplication\n: Removes redundant information across research outputs:\nasync function async_remove_duplicates(research_output, remove_duplicates_prompt):\nCreate async queue for communication\nCreate deduplication instruction template\nCreate deduplication chain with LLM\nTrack processed pairs to avoid duplicate work\nFor each reference topic/text pair:\nFor each candidate topic/text pair:\nIf not same topic and not already processed:\nCreate task to deduplicate candidate text\nMark pair as processed\nElse:\nAdd to output without deduplication\nProcess queue until all tasks complete\nReturn deduplicated research outputs\nCitation Requirements\n: Ensures all information is properly cited with source details:\nThe system enforces strict citation requirements, ensuring that every fact, figure, date, and quote is properly attributed to its source with a standardized citation format that includes the source name, publication date, headline, and full web address.\nThese quality control mechanisms ensure that PRM produces outputs that are accurate, well-structured, and properly cited.\nDemo Video\n‚Äã\nA demonstration video of the Press Release Miner (PRM) in action is available at the following link:\nPress Release Miner Demo Video\nConclusion\n‚Äã\nPress Release Miner represents a significant advancement in agentic AI systems for research and content generation. By implementing sophisticated agentic patterns such as multi-agent coordination, specialized tool usage, and parallel processing, PRM demonstrates how AI can effectively automate complex knowledge work while maintaining high standards of quality.\nThe system's architecture‚Äîwith its specialized agents, diverse tools, and comprehensive data sources‚Äîprovides a blueprint for designing effective multi-agent systems for other knowledge work applications. The implementation details, particularly around prompt engineering and quality control, offer valuable insights into ensuring AI-generated content meets professional standards.\nAs agentic AI systems continue to evolve, the patterns and approaches demonstrated in PRM will likely influence the design of future systems across various domains, from research and content generation to data analysis and decision support.\nWas this helpful?\nEdit this page\nPrevious\nPress Release Drafter (PRD)\nNext\nAMIGO\nIntroduction\nSystem Architecture Overview\nThe PRM Process\nSystem Architecture\nUI Components\nAgents\nTools\nData Sources\nAgentic Patterns in PRM\nMulti-Agent Coordination\nTool Usage Patterns\nParallel Processing\nAgent Specialization and Collaboration\nTechnical Implementation\nLLM Integration\nTechnologies Used\nPrompt Engineering\nQuality Control Mechanisms\nDemo Video\nConclusion\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 11,
    "depth": 3,
    "crawled_at": "2026-02-25T10:08:24.843446"
  },
  "https://techhq.dc.lilly.com/docs/tags/ai": {
    "url": "https://techhq.dc.lilly.com/docs/tags/ai",
    "title": "9 docs tagged with \"ai\" | Tech HQ",
    "description": "",
    "h1": [
      "9 docs tagged with \"ai\""
    ],
    "h2": [
      "ü§ñ Agentic AI",
      "ü§ñ AI & Intelligent Agents",
      "üß≠ Conversational AI",
      "üß≠ Knowledge Bases",
      "üß≠ Multimodal AI",
      "üß≠ Translation Services",
      "AI Coding Tools",
      "Knowledge Base Standards",
      "Lilly Code"
    ],
    "h3": [],
    "text_content": "9 docs tagged with \"ai\" | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\n9 docs tagged with \"ai\"\nView all tags\nü§ñ Agentic AI\nSolution Guide: ü§ñ Agentic AI\nü§ñ AI & Intelligent Agents\n_The AI Section of TechHQ is the primary tool for making AI technology decisions at Lilly. It defines the layers (the\nüß≠ Conversational AI\nStack Overflow Article: üß≠ Conversational AI\nüß≠ Knowledge Bases\nStack Overflow Article: üß≠ Knowledge Bases\nüß≠ Multimodal AI\nStack Overflow Article: üß≠ Multimodal AI\nüß≠ Translation Services\nStack Overflow Article: üß≠ Translation Services\nAI Coding Tools\nLast Updated: November 7, 2025\nKnowledge Base Standards\nLast Updated: December 2, 2025\nLilly Code\nLast Updated: December 2, 2025\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 2,
    "depth": 3,
    "crawled_at": "2026-02-25T10:08:27.079093"
  },
  "https://techhq.dc.lilly.com/docs/solution/business-enablement/business-enablement-blt": {
    "url": "https://techhq.dc.lilly.com/docs/solution/business-enablement/business-enablement-blt",
    "title": "üóÑÔ∏è Business Enablement BLT | Tech HQ",
    "description": "The Business Enablement Big List of Technology (BLT) is maintained using data from the Enterprise Architecture Tech Positioning Database and LillyFlow, with AI-assisted updates. While we aim for accuracy, please verify critical information and contact the EBA team with questions or corrections.",
    "h1": [
      "üóÑÔ∏è Business Enablement BLT"
    ],
    "h2": [
      "Adobe Experience Manager Assets‚Äã",
      "Adobe Experience Manager Sites‚Äã",
      "Adobe Sign‚Äã",
      "Avoka Transact‚Äã",
      "Business Process Workflow Automation‚Äã",
      "Decipher (a.k.a. Beacon)‚Äã",
      "Docusign‚Äã",
      "IBM Blueworks‚Äã",
      "Declining ‚Üí Retired‚Äã"
    ],
    "h3": [
      "Automation Anywhere‚Äã",
      "Business Objects‚Äã",
      "Celonis‚Äã"
    ],
    "text_content": "üóÑÔ∏è Business Enablement BLT | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nüè¢ Business Enablement\nüóÑÔ∏è Business Enablement BLT\nüß≠ AE/PC Detection\nüß≠ Customer Engagement Reference Architecture\nüß≠ PI & Sensitive Data Detection\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nüè¢ Business Enablement\nüóÑÔ∏è Business Enablement BLT\nOn this page\nüóÑÔ∏è Business Enablement BLT\nThe Business Enablement Big List of Technology (BLT) is maintained using data from the Enterprise Architecture Tech Positioning Database and LillyFlow, with AI-assisted updates. While we aim for accuracy, please verify critical information and contact the EBA team with questions or corrections.\nCan't find a technology?\nUse TechHQ's search feature to look across all technology documentation. For questions or to suggest additions to this list, contact the\nEnterprise Business Architecture team\n.\nAdobe Experience Manager Assets\n‚Äã\nPosition\nSpecialized\nTags\ncontent-management, adobe, adobe-aem\nInformation Sensitivity\nOrange\nArchitect\nChad Stout\nContact(s)\nJared Vanderford\nSpecialized for managing digital assets and content fragments that will be incorporated into experiences created using AEM Sites. Scope BU IDS Only\nUse Cases\n‚Äã\nStoring digital assets, videos, re-usable content modules.\nNotes\n‚Äã\nVeeva Vault PromoMats is still the source of truth for what is released to the FDA as output of internal promo materials reviews.\nSee Also\n‚Äã\nhttps://www.content.lilly.com/\nAdobe Experience Manager Sites\n‚Äã\nPosition\nSpecialized\nTags\ncontent-management, adobe, adobe-aem\nInformation Sensitivity\nOrange\nArchitect\nChad Stout\nContact(s)\nJared Vanderford\nSpecialized for omni-channel modular content authoring and assembly. Scope BU IDS Only\nUse Cases\n‚Äã\nAuthoring and Publishing digital content such as Email, Banner, Interactive Visual Aids and distribution to 3rd party clinical education platforms\nNotes\n‚Äã\n2021 focus will be on omni-channel content authoring for HQ Email, Field Channels, and 3rd Party Sources of Authoriting.  Pilots for the web will begin in 2022.\nSee Also\n‚Äã\nhttps://www.content.lilly.com/\nAdobe Sign\n‚Äã\nPosition\nStandard\nTags\ncontent-management\nInformation Sensitivity\nRed\nArchitect\nRob Lehman\nContact(s)\nRon Blankenship\nCMDB CI\nCI00000000426918\nPreferred for e-signature\nUse Cases\n‚Äã\nPreferred for all e-signature use cases: Under certain cases Docusign may be a better alteernative\nNotes\n‚Äã\nLocal business area accountable for decision to migrate from Docusign to Adobe Sign\nAvoka Transact\n‚Äã\nPosition\nSpecialized\nTags\nlow-code-platforms\nInformation Sensitivity\nRed\nArchitect\nBryan Marks\nContact(s)\nBryan Marks\nMobile forms for field personnel\nUse Cases\n‚Äã\nBasic and Advanced formsOffline usageDevelop forms for mobile devicesExternal use cases\nBusiness Process Workflow Automation\n‚Äã\nPosition\nStandard\nTags\nenterprise-automation, baw, bpm\nInformation Sensitivity\nRed\nArchitect\nSumit Bhardwaj\nContact(s)\nSumit Bhardwaj, Alex Wasik\nCMDB CI\nCI00000000209419\nBusiness Process Workflow Automation unites information, processes, and users to help you automate digital workflows on-premises or on a cloud. Recommended tool for Automating end to end Business Processes/Workflows that increase productivity, improve collaboration between teams, and gain new insight to resolve cases and drive better business outcomes.\nUse Cases\n‚Äã\nNon-streamlined Manual Business Process with bottlenecks and In-efficiencies:\nScenarios involving automation of end-to-end business processes where multiple people or departments are working together to achieve a goal. Example: Screen fail process in clinical trials, Material tracking in manufacturing and quality departments etc.\nProcesses involving reviews and approvals by multiple people\nbefore finalizing a result. Example: A process where documents are produced by one set of people and reviewed and enhanced by another set of people i.e. Translation and Transcription of files.\nUse case has a well-defined start, end and in-between steps\nto be performed to achieve a specific result.\nUse cases which are identified as business processes, and the automation\ngo-to-market time is of utmost importance.\nExample: Product launch processes\nNotifications and Task routing required\nto achieve\nSLA goals.\nExample: Complaints and customer claims settlement, Government regulations implementation tracking etc.\nNotes\n‚Äã\nWhile business process automations can be done using multiple tools, platforms and even by developing custom applications, here are some of the scenarios where IBM BAW can be the best choice:\nWorkflows with potential to grow medium to large in size with high level of complexity.\nHybrid types of integration needs, integration heavy projects.\nLarge user base to be served by the solution\nTask management features required (Flexible Task Routing, Task Segregation, Task focused custom interfaces, Granular Team Filtering & Assignment)...\nRead full article\nDecipher (a.k.a. Beacon)\n‚Äã\nPosition\nSpecialized\nTags\nlow-code-platforms\nInformation Sensitivity\nGreen\nArchitect\nChad Stout\nContact(s)\nCortney Klimkowski\nSpecialized for use-cases requesting non-personally identifiable information from EXTERNAL audiences, e.g. Market Research surveys.\nUse Cases\n‚Äã\nSurveys to external respondents that are classified as touchpoint / satisfaction surveys or formal external Market Research conducted via Lilly Market Research departments.\nNotes\n‚Äã\nIt is prohibited to collect any type of personal identifiable information within Decipher (Name, Email, Address, etc.) from external respondents.\nSee Also\n‚Äã\nhttps://collab.lilly.com/sites/Decipher/SitePages/Welcome-To-The-Decipher-Landing-Page.aspx\nDocusign\n‚Äã\nPosition\nSpecialized\nTags\ncontent-management\nWhen Adobe sign cannot be used (see use cases)\nUse Cases\n‚Äã\nApplication has an integration with DocuSign and not Adobe Sign. There is a need to build a native app in iOS or Android, DocuSign has mobile SDKs. Documents over 500 pages.\nNotes\n‚Äã\nNo central support. Local business area is accountable for licensing, compliance, and governance.\nIBM Blueworks\n‚Äã\nPosition\nStandard\nTags\nenterprise-automation\nInformation Sensitivity\nOrange\nArchitect\nGreg Graf\nPreferred for Business Process Modelling\nUse Cases\n‚Äã\nAnalyze and improve business operations, Capture process and decision knowledge, Collaborating / communicating on business process model development across teams\nNotes\n‚Äã\nIBM Software as a service (SaaS) cloud-based process with a decision modeling environment, Used by the business - everything stored in a repository, Provides capabilities to capture business processes and decisions, User Experience similar to Visio\nSee Also\n‚Äã\nhttps://collab.lilly.com/sites/AutomationLilly2/SitePages/Blueworks-Live.aspx\nDeclining ‚Üí Retired\n‚Äã\nAutomation Anywhere\n‚Äã\nPosition\nDeclining 2023-06-01\nTags\nenterprise-automation\nInformation Sensitivity\nRed\nArchitect\nGreg Graf\nContact(s)\nKevin Gingerich\nPreferred for Robotic Process Automation (RPA) when RPA is the only way to accomplish the automation need. However, RPA should not be used for general workflow/orchestration, decision automation, document processing, etc.. Other automation technologies are available for non-RPA automation needs.\nUse Cases\n‚Äã\nAutomation of common tasks in graphical user interfaces in line of business systems. Deterministic, rules-based automation (current)\nNotes\n‚Äã\nBOT development and support by RPA COE (with chargebacks) or preferred vendors\nBusiness Objects\n‚Äã\nPosition\nDeclining 2023-05-15\nTags\nbusiness-intelligence\nInformation Sensitivity\nRed\nArchitect\nMalika Mahoui\nContact(s)\nSrihari S\nUse for adhoc report creation. Tight integration with SAP data sources. Not recommended for new use cases\nUse Cases\n‚Äã\nCreate adhoc tabular column report which span across multiple pages with different charts on the side, Create a highly formatted reports with customheader/footer. Example financial summary reports, Use this tool if you have a requirement to create reports by connecting to SAP HANA database, This tool is not suitable to create visualizations and story telling\nNotes\n‚Äã\nThis tool is available only for existing BO users. New users should consider using Power BI.As Microsoft is incorporating lot of SSRS reporting capabilities into Power BI, Power BI should be considered for any new  reporting requirements\nSee Also\n‚Äã\nhttps://collab.lilly.com/sites/EIP/SitePages/Enterprise-Analytics.aspx\nCelonis\n‚Äã\nPosition\nRetired 2024\nTags\nenterprise-automation\nInformation Sensitivity\nOrange\nArchitect\nGreg Graf\nContact(s)\nJon Thomas\nPreferred for process mining\nUse Cases\n‚Äã\nAnalyze processes to identify optimization opportunities, improve process performance, reduce process variability, improve process cycle times\nNotes\n‚Äã\n3 years agreement begun on 12/2020. Business area will be charged after year 1\nWas this helpful?\nEdit this page\nPrevious\nüè¢ Business Enablement\nNext\nüß≠ AE/PC Detection\nAdobe Experience Manager Assets\nAdobe Experience Manager Sites\nAdobe Sign\nAvoka Transact\nBusiness Process Workflow Automation\nDecipher (a.k.a. Beacon)\nDocusign\nIBM Blueworks\nDeclining ‚Üí Retired\nAutomation Anywhere\nBusiness Objects\nCelonis\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 4,
    "depth": 3,
    "crawled_at": "2026-02-25T10:08:29.295038"
  },
  "https://techhq.dc.lilly.com/docs/solution/business-enablement/aepc_detection": {
    "url": "https://techhq.dc.lilly.com/docs/solution/business-enablement/aepc_detection",
    "title": "üß≠ AE/PC Detection | Tech HQ",
    "description": "Stack Overflow Article: üß≠ AE/PC Detection",
    "h1": [
      "üß≠ Adverse Event and Product Complaint Detection"
    ],
    "h2": [
      "AE/PC Routing‚Äã",
      "Additional Considerations‚Äã"
    ],
    "h3": [],
    "text_content": "üß≠ AE/PC Detection | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nüè¢ Business Enablement\nüóÑÔ∏è Business Enablement BLT\nüß≠ AE/PC Detection\nüß≠ Customer Engagement Reference Architecture\nüß≠ PI & Sensitive Data Detection\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nüè¢ Business Enablement\nüß≠ AE/PC Detection\nOn this page\nüß≠ Adverse Event and Product Complaint Detection\nSynced daily from Stack Overflow\nSolution Guide\nLifecycle: Draft\nLast Update: 2025-10-05\nCapability Owner: Eric A. Wolf\nEBA Lead: Eric A. Wolf\nContributors & Reviewers: n/a\nSummary:\nAccurate and timely reporting of safety events disclosed in digital consumer channels is critical for patient safety and to comply with global regulations.  While there are no generally-available enterprise solutions to automate adverse event/product complaint detection or routing yet, significant progress is underway.\nDigital consumer channels which require AE/PC detection include incoming email, chatbots, voicebots, survey free-text, and web/mobile application free-text.  Projects needing this capability should connect with the individual(s) mentioned above and teams referenced below.\nUse Case/Scenario\nTech Recommendation\nCurrent Positiong\nFuture Direction\nAdverse event/product complaint detection in US drug product chatbots and TLAC voicebot conversations\nUse Authenticx 3rd Party AE/PC Detection Service\nSpecialized for US product chatbots US TLAC customer voicebot channels only\nWill migrate to using the emerging AI-based detection service capability in 2026\nAdverse event/product complaint detection in all other chatbot/voicebot conversations or survey/application free-text\nn/a ‚Äì Use 100% human analysis of all consumer communications to identify safety events\nDeclining\nPlan to use emerging AI-based detection service capability planned for 1H 2026\nSafety event routing for Authenticx AE/PC service to Lilly safety case management systems (AE and PC)\nLeverage DHISP MuleSoft workflow to route to GCC for agent processing and downstream transmission to the Lilly Safety System and Veeva QMS.\nSpecialized for Authenticx and legacy applications\nWill migrate to the forthcoming LRL ClearPath safety event routing service in 2026\nSafety event routing to Lilly safety case management systems (AE and PC)\nPer-product direct system integration with the Lilly Safety System (adverse events) and/or Veeva QMS (product complaints) platforms, as needed.\nDeclining\nUse the forthcoming LRL ClearPath safety event routing service as the single enterprise endpoint for AE/PC submission coming 2H 2026\nUse Cases ‚Äì Adverse Event/Product Complaint Detection\nDetection requires the analysis of consumer communications (e.g. chatbot text, voicebot transcript, etc.) to identify potential safety events.¬† Currently, that requires human agents to manually scan all communications and report events.¬† This is labor intensive, not a good use of human talent, and not easily scalable.¬† This problem requires automation but goes well beyond just simple keyword scanning.\nThe Lilly Advanced Intelligence (AI) group is currently developing artificial intelligence models to detect these events.¬† The Customer Engagement group in Software Product Engineering (SPE) will be building a service layer to invoke these models and develop the infrastructure necessary to provide logging, monitoring, and risk-based human review.\nThis advanced detection enterprise service offering is expected 1H 2026.\nCurrent Approach\nOwning Org/Team\nManual human review of all patient communications and manual identification of potential safety events using standard Lilly and industry criteria.\nn/a\nEmerging Approach\nOwning Org/Team\nA new enterprise Lilly AE/PC detection service (API) which leverages AI models to detect safety events in the provided communication stream.  The solution will also employ named entity recognition to use AI to also detect and extract key elements of the safety event, itself (e.g. reporter contact information, suspect product name, reported batch #, etc.).\nLilly Advanced Intelligence (AI), Software Product Engineering (SPE)\nAE/PC Routing\n‚Äã\nUse Case ‚Äì Adverse Event/Product Complaint Routing\nOnce a potential adverse event and/or product complaint has been identified, it must be routed to the appropriate Lilly safety case management platform for processing and reporting.¬† Currently, the Lilly Safety System (LSS) is the platform for adverse events and Veeva QMS is the platform for product complaints.¬† Today, this requires separate integrations between different products and these platforms.\nThis creates multiple, redundant interfaces; risk in processing events differently across these interfaces; and it creates unnecessary burden to maintain and support multiple integrations providing the same function.\nThe LRL Software Engineering Hub is working to develop an single, enterprise routing service to replace all of these separate integrations.\nThis enterprise service offering is expected for general use starting 2H 2026.\nCurrent Approach\nSeparate, per-system integration to Mosaic/Lilly Safety System (adverse events) and to Veeva QMS (product complaints).\nn/a ‚Äì each team develops their own\nEmerging Approach\nSingle, enterprise AE/PC ingest API endpoint which manages appropriate routing to downstream case management platforms.\nLRL Software Engineering Hub (SEH)\nAdditional Considerations\n‚Äã\nRationale\nJust as Lilly requires its employees to report cases of adverse events (AEs), product complaints (PCs), suspect products, and tampering(1), Lilly also requires that any mention of the same in a Lilly-provided digital channel also be reported.¬† This means that any externally-facing digital product which enables communications from consumers (e.g. Lilly chatbots, voicebots, online surveys with free-text fields, web/mobile applications with free-text fields, etc) needs to also identify and route potential safety events.\n(1) Lilly's Ethics and Compliance Procedure (US & PR), Section 4.3.\nhttps://now.lilly.com/page/global-ethics-and-compliance-requirements\nhttps://collab.lilly.com/sites/GSLD_TrainingMaterialsActive/EC/Policies_Procedures/Procedure_Training_Resource/Batch_1_2024/Reporting_AE_and_PC/EN_PCoE_PTR_Reporting_AE_PC.pdf\nhttps://lilly.service-now.com/ec?id=kb_article_view&sys_kb_id=f69fac9a873eea149c2e0d4c8bbb3559\n‚ÄúReal-Time‚Äù vs. ‚ÄúBest Quality‚Äù Detection\nThe best AE/PC detection approach likely requires analyzing the entire communication dialog, potentially leveraging multiple AI models, and might even require escalation to a human expert.¬† This can take significantly longer time but provides fewer false negatives.¬† This is preferred from a patient safety and compliance perspective.\nHowever, some situations are different and may require a different approach.¬† If one wants to detect safety events in the midst of a live, two-way conversation (for example, to redirect the user to a more appropriate tool), it warrants more rapid analysis but runs the risk of missing things (higher false negatives).¬† In fact, an overly aggressive detection approach in live conversations could be perceived as ‚Äúnagging‚Äù and could impact the end user experience.\nA combination of approaches may be warranted to provide the best user experience and achieve the best detection for patient safety.\nWas this helpful?\nTags:\nsolution-guide\nbusiness-enablement\ntechhq\nEdit this page\nPrevious\nüóÑÔ∏è Business Enablement BLT\nNext\nüß≠ Customer Engagement Reference Architecture\nAE/PC Routing\nAdditional Considerations\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 6,
    "depth": 3,
    "crawled_at": "2026-02-25T10:08:33.642424"
  },
  "https://techhq.dc.lilly.com/docs/solution/business-enablement/customer_engagement_reference_architecture": {
    "url": "https://techhq.dc.lilly.com/docs/solution/business-enablement/customer_engagement_reference_architecture",
    "title": "üß≠ Customer Engagement Reference Architecture | Tech HQ",
    "description": "Stack Overflow Article: üß≠ Customer Engagement Reference Architecture",
    "h1": [
      "üß≠ Customer Engagement Reference Architecture",
      "Customer Engagement Capability Reference Model"
    ],
    "h2": [],
    "h3": [
      "Deliver & Activate‚Äã",
      "Measure & Analyze‚Äã",
      "Creative & Production‚Äã",
      "Review & Approval‚Äã",
      "Data Foundation‚Äã"
    ],
    "text_content": "üß≠ Customer Engagement Reference Architecture | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nüè¢ Business Enablement\nüóÑÔ∏è Business Enablement BLT\nüß≠ AE/PC Detection\nüß≠ Customer Engagement Reference Architecture\nüß≠ PI & Sensitive Data Detection\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nüè¢ Business Enablement\nüß≠ Customer Engagement Reference Architecture\nOn this page\nüß≠ Customer Engagement Reference Architecture\nSynced daily from Stack Overflow\nSolution Guide\nLifecycle: Draft\nLast Update: 2025-10-14\nCapability Owner: Chad E. Stout\nEBA Lead: Chad E. Stout\nContributors & Reviewers: Allan Kirui, Annapurna Calyam, Chad E. Stout, Cherish Joseph, Eric Wolf, Jeff Tibbitts, Mike Madison\nA guide for designing and assembling marketing technology (MarTech) capabilities to deliver best-in-class customer experiences across digital and personal channels, while enabling measurement, personalization, and compliance.\nCustomer Engagement Capability Reference Model\nThe visual below will be used as a reference model that will describe the business and technology capabilities in Lilly's customer engagement architecture models.\nDeliver & Activate\n‚Äã\nThe technologies under deliver & activate are used to orchestrate the delivery of the best in any class experiences to our customers.\nDigital Channels\n‚Äã\nCategory\nCapability\nEmerging/Standard Vendor\nDeclining/Exiting Vendor\nPositioning\nDescription\nDeliver & Activate\nTransactional Email\nAWS Simple Email Service\nn/a\nStandard\nA Transactional Email is an automated message sent to an individual based on specific actions or interactions they have with a business or platform. These emails are triggered by user behavior and are intended to provide essential information that the recipient needs or expects.\nDeliver & Activate\nCommercial Email\nAdobe Journey Optimizer\nSalesforce Marketing Cloud\nSpecialized - Commercial Use-Cases\nA Commercial Email is a message sent electronically to promote a product, service, or brand, typically for marketing or business purposes. It often includes offers or calls to action intended to drive sales or customer engagement. In many countries, these emails must have an opt-out link.\nDeliver & Activate\nTransactional SMS / MMS\nInfoBip\nSalesforce Marketing Cloud\nSpecialized - Commercial Use-Cases\nA Transactional SMS/MMS is a text or multimedia message sent to convey essential information related to a transaction or service, such as order confirmations, delivery updates, account alerts, or appointment reminders. It is not promotional in nature and is typically triggered by a user action or business process.\nDeliver & Activate\nCommercial SMS / MMS\nInfoBip\nSalesforce Marketing Cloud\nSpecialized ‚Äì Commercial Use-Cases\nA Commercial SMS / MMS is a text or multimedia message sent to promote a product, service, brand, or event, typically for marketing purposes. It often includes offers or calls to action intended to drive customer engagement or sales. In many countries the recipient needs to have an explicit opt-in and can opt-out through STOP commands.\nDeliver & Activate\nOmnichannel Journey Orchestration\nAdobe Journey Optimizer\nSalesforce Marketing Cloud Journey Builder\nZAIDYN Orchestration Engine\nSpecialized - Commercial Use-Cases\nAn Omnichannel Journey Orchestration is a tool or platform designed to enhance customer engagement by managing and personalizing interactions across multiple channels.\nDeliver & Activate\nCustomer Data Platform\nAdobe Real-Time CDP\nSalesforce Data Cloud ‚Äì Specialized for Health Cloud Data Integration\nTealium\nSpecialized ‚Äì Commercial Use-Cases\nA Customer Data Platform is a technology solution that centralizes and unifies customer data from various sources to create a single, comprehensive customer profile to support personalization and journey orchestration.\nDeliver & Activate\nAnonymous Web Personalization\nAdobe Target ‚Äì Uses Cookies\nAdobe Edge Delivery Services Experimentation ‚Äì Cookie-less\nn/a\nSpecialized ‚Äì Commercial Use-Cases\nAnonymous Web Personalization enables businesses to tailor website content and experiences for visitors who have not yet provided identifiable information. By leveraging real-time data on user behavior and preferences, businesses can deliver relevant content, product recommendations, and personalized messaging.\nDeliver & Activate\nKnown / Profile Based Web Personalization\nAdobe Journey Optimizer\nn/a\nSpecialized ‚Äì Commercial Use-Cases\nKnown/Profile-Based Web Personalization leverages detailed customer profiles to deliver tailored web experiences. By using data such as past interactions, preferences, and demographics, businesses can provide highly relevant content, product recommendations, and personalized messaging.\nDeliver & Activate\nWebsite Tag Management\nAdobe Data Collection Tags (Formerly known as Adobe Launch)\nGoogle Tag Manager\nSpecialized ‚Äì Commercial Use-Cases\nWebsite Tag Management involves organizing and managing tags‚Äîsnippets of code embedded in a website‚Äîto collect data and facilitate third-party services. This capability ensures that tags fire correctly and collect accurate data, supporting analytics, advertising, and personalization efforts.\nDeliver & Activate\nWebsite Tag Monitoring\nObservePoint\nn/a\nStandard\nWebsite Tag Monitoring ensures third-party tags are working correctly, collecting accurate data, and identifying unauthorized tags or cookies deployed on websites.\nDeliver & Activate\nVideo Streaming Platform\nMicrosoft Stream ‚Äì Intended audience is internal employees / contractors / agents of Lilly\nAdobe Dynamic Media - Intended audience is customers (Consumers, Patients, Healthcare Professionals,)\nKaltura Video Experience Cloud\nStandard\nA Video Streaming Platform enables users to watch video content over the internet in real-time or on demand, without requiring downloads.\nDeliver & Activate\nWeb Content Management\nAdobe Experience Manager ‚Äì Specialized for Commercial\nContentful ‚Äì Standard\nSharePoint - Standard\nn/a\nStandard - Contentful - Custom developed websites\nStandard - SharePoint ‚Äì No-code websites\nSpecialized ‚Äì Commercial Use-Cases - Adobe Experience Manager\nA Web Content Management system allows users to create, manage, and publish digital content on websites without needing to write code. It provides tools for editing, organizing, and maintaining web pages through a user-friendly interface.\nDelivery & Activate\nURL Shortener\nBl.ink\nBitly\nStandard\nA URL shortener is a tool that converts long web addresses into shorter, more manageable links. It helps improve link sharing, track click data, and enhance user experience‚Äîespecially on platforms with character limits or for marketing campaigns.\nDeliver & Activate\nSearch Engine Marketing\nGoogle Search Ads 360\nSkai\nn/a\nStandard\nSearch Engine Marketing is a form of digital marketing that involves promoting websites by increasing their visibility in search engine results pages (SERPs), primarily through paid advertising.\nDeliver & Activate\nSearch Engine Advertising Editor\nGoogle Ads, Microsoft Advertising, Google Display Network, Google Ads Editor, Microsoft Ads Editor\nn/a\nStandard\nA Search Engine Advertising Editor allows marketers to efficiently build and modify search ad campaigns, including ad copy, keywords, bids, and targeting settings‚Äîoften in bulk and offline‚Äîbefore uploading changes to the ad platform.\nDeliver & Activate\nDigital Brand Compliance & Protection\nBrand Verity ‚Äì Search Ads Monitoring\nZeroFox ‚Äì Website and Social Media Monitoring\nNot Appliable\nStandard\nDigital Brand Compliance & Protection involves monitoring paid search and affiliate channels to ensure brand integrity, enforce trademark policies, and prevent unauthorized or misleading advertising.\nDeliver & Activate\nSearch Engine Optimization\nLumar, Semrush, Screaming Frog, Conductor\nn/a\nStandard\nSearch Engine Optimization (SEO) is the practice of improving a website‚Äôs visibility in search engine results pages (SERPs) to attract more organic (non-paid) traffic. It involves optimizing content, technical elements, and user experience to align with search engine algorithms and user intent.\nDeliver & Activate\nLarge Language Optimization\nAuthoritas ‚Äì Standard\nAdobe LLM Optimizer - Emerging\nn/a\nSpecialized ‚Äì Commercial Use-Cases\nLarge Language Model Optimization is the practice of improving the content on a website to make it machine-readable, semantically rich, and contextually clear so large language models can understand and use it effectively.\nDeliver & Activate\nSocial Media Engagement\nSprinklr\nKhoros\nStandard\nSocial Media Engagement platform centralizes the publishing and monitoring of social media content across the various social networks.\nPersonal Channels\n‚Äã\nCategory\nCapability\nEmerging/Standard Vendor\nDeclining/Exiting Vendor\nPositioning\nDescription\nDeliver & Activate\nField Customer Relationship Management\nVeeva Multi-Channel CRM\nZAIDYN Field Insights App\nn/a\nSpecialized ‚Äì Commercial Use-Cases\nA Field Customer Relationship Management (CRM) platform enables field personnel to manage customer interactions, track interactions, and streamline workflows. Key capabilities include contact and lead management and automation of routine tasks such as follow-ups emails.\nDeliver & Activate\nCustomer Relationship Management\nSalesforce Platform ‚Äì Standard\nSalesforce Health Cloud - Emerging\nn/a\nSpecialized ‚Äì Commercial Use-Cases\nCustomer Relationship Management is a capability that helps customer support teams deliver personalized support through a unified view of each customer‚Äôs journey‚Äîcustomer data, communication history, and support interactions‚Äîto improve engagement, streamline case management, and enhance health outcomes.\nDeliver & Activate\nCustomer Meeting Management\nVeeva Events\nMercury - Lilly Developed Solution\nSpecialized ‚Äì Commercial Use-Cases\nA Customer Meeting Management solution is designed to plan, manage, and execute all types of customer-facing events‚Äîsuch as speaker programs, advisory boards, and virtual meetings.\nMeasure & Analyze\n‚Äã\nThe technologies under measure & analyze are used to uncover patterns, trends, and insights that can be used to improve the experiences of our customers.\nCategory\nCapability\nEmerging/Standard Vendor\nDeclining/Exiting Vendor\nPositioning\nDescription\nMeasure & Analyze\nWeb Analytics\nGoogle Analytics\nn/a\nStandard\nA Web Analytics tool will collect and analyze data on website traffic and user behavior, providing insights into how users interact with a site.\nMeasure & Analyze\nCustomer Journey Analytics\nAdobe Customer Journey Analytics\nn/a\nSpecialized ‚Äì Commercial Use-Cases\nA Customer Journey Analytics is a solution that collects, integrates, and analyzes data from multiple customer touchpoints to visualize and understand the complete customer experience. It helps businesses identify patterns, optimize interactions, and improve outcomes across the customer lifecycle.\nMeasure & Analyze\nWeb Browsing Heatmap\nMedallia Digital Experience Analytics\nn/a\nSpecialized ‚Äì Commercial Use-Cases\nA Web Browsing Heatmap capability is a tool that visually represents user interactions on a webpage‚Äîsuch as clicks, scrolls, or mouse movements‚Äîusing color-coded overlays. It helps identify which areas attract the most attention, enabling optimization of layout, content, and user experience.\nMeasure & Analyze\nCustomer Satisfaction Survey\nMedallia Surveys\nn/a\nSpecialized ‚Äì Commercial Use-Cases\nA Customer Satisfaction Survey is an online tool used to collect opinions, ratings, or suggestions from customers about their experiences with a product, service, or brand. It helps businesses understand customer satisfaction and identify areas for improvement.\nMeasure & Analyze\nSocial Listening\nTubular ‚Äì Video\nMeltwater ‚Äì IBU Social Networks\nQuid ‚Äì US Social Networks\nn/a\nStandard\nSocial Listening is the process of monitoring and analyzing online conversations across social media and digital platforms to understand public sentiment, trends, and brand perception. It helps organizations gain insights into customer opinions, emerging issues, and competitive positioning.\nMeasure & Analyze\nReporting and Dashboards\nPower BI, QuickSight, Tableau\nTableau\nStandard\nDashboarding and reporting is method of visualizing KPIs(Key performance indicators) to monitor performance and business health. Dashboards provide high level interactive snapshots for rapid decision making.\nMeasure & Analyze\nInsights Discovery\nAWB, Amazon Sagemaker studio, Jupyter Lab, RStudio, Python\nn/a\nStandard\nInsights discovery is a process of exploring and analyzing data from various sources to uncover patterns, trends, and actionable insights, often using interactive and visual tools.\nCreative & Production\n‚Äã\nThe technologies under creative & production are used to ideate, design, and develop the breakthrough creative that powers our best in any class experiences.\nCategory\nCapability\nEmerging/Standard Vendor\nDeclining/Exiting Vendor\nPositioning\nDescription\nCreative & Production\nCreative Production Tools\nAdobe Creative Cloud\nn/a\nStandard\nCreative Production Tools is a collection of tools for graphic design, video editing, photography, animation, 3D modeling, etc. that are used for creating Lilly owned content.\nCreative & Production\nModular Content Authoring\nAdobe Experience Manager\nDocuvera (Medical)\nJigsaw & Jarvis\nSpecialized ‚Äì Commercial Use-Cases\nModular Content Authoring is a structured approach to creating reusable, adaptable content components‚Äîsuch as headlines, body copy, images, and calls-to-action‚Äîthat can be assembled dynamically across channels and formats.\nCreative & Production\nCommercial Content Authoring Assistants\nAI Promomaker (Promotional)\nMediAuthor (Medical)\nn/a\nSpecialized ‚Äì Commercial Use-Cases\nCommercial Content Authoring Assistant is a category of AI enabled tools that leverage information about Lilly‚Äôs products and/or medical information to generate drafts of omnichannel content.\nCreative & Production\nCommercial Tactic Generation Tools\nAdobe GenStudio for Performance Marketing\nSlide Genie\nJigsaw & Jarvis\nSpecialized ‚Äì Commercial Use-Cases\nCommercial Tactic Generation Tools is a category of AI enabled tools that leverage previously generated content, previously approved content modules, or drafts generated from Content Authoring Assistants to generate final commercial tactics (email, social posts, banners, etc).\nCreative & Production\nGenAI Asset Production Tools\nAdobe Firefly (Image & Video)\nSynthesia.io (Avatars)\nn/a\nEmerging\nGenAI Asset Production Tools are a category of AI agents that use prompts to generate multi-media assets (images, videos, avatars).\nCreative & Production\nDigital Asset Management\nAdobe Experience Manager\nn/a\nStandard\nDigital Asset Management is the practice of organizing, storing, and managing digital content - including source, b-roll, raw files - in a centralized system.\nReview & Approval\n‚Äã\nThe technologies under review & approval are used to collect review, approve, track, and withdraw content created by Lilly.\nCategory\nCapability\nEmerging/Standard Vendor\nDeclining/Exiting Vendor\nPositioning\nDescription\nReview & Approval\nPromotional Material Review and Approval\nVeeva Vault PromoMats\nn/a\nStandard\nPromotional Material Review and Approval process governs the end-to-end management of promotional materials‚Äîfrom initial creation and cross-functional review through approval, distribution, ongoing monitoring, and eventual withdrawal and archival.\nReview & Approval\nMedical Affairs Content Review & Approval\nVeeva Vault MedComms\nn/a\nStandard\nMedical Affairs Content Review & Approval process governs the end-to-end management of medical information through approval, distribution, ongoing monitoring and eventual withdraw and archival.\nReview & Approval\nLilly Policy and Procedure Review & Approval\nVeeva Vault QualityDocs\nn/a\nStandard\nLilly Policy and Procedure Review & Approval process pertains to any Lilly document management process that requires formal electronic approval, versioning, and document life-cycle management.\nReview & Approval\nPortfolio & Project Management\nJira (Agile Teams)\nJira Align (Agile Portfolio)\nPlanisware (Enterprise Portfolio)\nSmartsheets (Low complexity)\nWorkfont (Commercial Use-Cases)\nWrike\nStandard ‚Äì Jira, JiraAlign, Planisware\nSpecialized ‚Äì Smartsheets, Workfront\nDeclining - Wrike\nPortfolio & Project Management is the centralized management of processes, methods, and technologies used to analyze and collectively manage current or proposed projects. It ensures that projects are prioritized, resourced, and executed in alignment with strategic objectives, while providing visibility into performance, risks, and value across the portfolio.\nData Foundation\n‚Äã\nThe technologies within our Data Foundation serve as the master data systems that fuel our entire commercial eco-system.\nCategory\nCapability\nEmerging/Standard Vendor\nDeclining/Exiting Vendor\nPositioning\nDescription\nData Foundation\nMaster Data Management\nReltio (HCP, Consumer, Material)\nn/a\nStandard\nMaster Data Management is the discipline and technology-driven process of creating and maintaining a single, consistent, and authoritative source of critical business data‚Äîsuch as customer, product information‚Äîacross an organization.\nData Foundation\nConsent & Preference Management\nSyrenis Cassie\nn/a\nStandard\nConsent & Preference Management is a capability designed to help global enterprises like Lilly manage customer data in compliance with privacy regulations such as GDPR, CCPA, and HIPAA.\nData Foundation\nIdentity Management\nAuth0 ‚Äì Specialized Commercial\nAzure ‚Äì Lilly Enterprise\nOkta\nStandard\nIdentity management platforms are systems that help organizations securely manage digital identities and control access to resources.\nWas this helpful?\nTags:\nsolution-guide\nbusiness-enablement\ntechhq\nEdit this page\nPrevious\nüß≠ AE/PC Detection\nNext\nüß≠ PI & Sensitive Data Detection\nDeliver & Activate\nMeasure & Analyze\nCreative & Production\nReview & Approval\nData Foundation\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 5,
    "depth": 3,
    "crawled_at": "2026-02-25T10:08:35.878220"
  },
  "https://techhq.dc.lilly.com/docs/solution/business-enablement/pi_sensitive_data_detection": {
    "url": "https://techhq.dc.lilly.com/docs/solution/business-enablement/pi_sensitive_data_detection",
    "title": "üß≠ PI & Sensitive Data Detection | Tech HQ",
    "description": "Stack Overflow Article: üß≠ PI & Sensitive Data Detection",
    "h1": [
      "üß≠ PI and Sensitive Data Detection Using AWS Services"
    ],
    "h2": [
      "AWS Macie‚Äã",
      "AWS Glue Data Quality‚Äã",
      "AWS Kinesis + Lambda‚Äã",
      "AWS Comprehend‚Äã",
      "List of 18 PHI identifiers‚Äã",
      "Cost comparison Enterprise Data Guidance page‚Äã"
    ],
    "h3": [
      ""
    ],
    "text_content": "üß≠ PI & Sensitive Data Detection | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nüè¢ Business Enablement\nüóÑÔ∏è Business Enablement BLT\nüß≠ AE/PC Detection\nüß≠ Customer Engagement Reference Architecture\nüß≠ PI & Sensitive Data Detection\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nüè¢ Business Enablement\nüß≠ PI & Sensitive Data Detection\nOn this page\nüß≠ PI and Sensitive Data Detection Using AWS Services\nSynced daily from Stack Overflow\nSolution Guide\nLifecycle: Draft\nLast Update: 2025-10-10\nCapability Owner: Annapurna Calyam\nEBA Lead: Annapurna Calyam\nContributors & Reviewers: Annapurna Calyam, EDB - Confluence contributors\nA guide for detecting, classifying, and protecting personally identifiable information (PII) and sensitive data using AWS-native services such as Macie, Glue, and Comprehend.\nUse Case(s)\nTech Recommendation\nPositioning\nComplexity\nKey Integrations\nOwning Org/Team\nNext Step\nAutomated PI data detection in S3 buckets\nAWS Macie\nStrategic Core\nLow\nS3, CloudTrail, IAM\nCloud/Data Engineering\nEnable Macie in AWS Console\nData masking during ETL workflows\nAWS Glue Data Quality\nStandard\nMedium\nS3, Lambda, ETL pipeline RDS, Redshift\nData Engineering\nBuild Glue workflows with masking\nReal-time sensitive data classification\nAWS Kinesis + Lambda\nSpecialized\nHigh\nKinesis Firehose, Lambda, Macie\nData engineering\nCreate Lambda triggers for alerts\ncontext-aware text analysis\nAmazon Comprehend\nStandard\nMedium\nETL pipeline, S3, lambda, RDS Redshift\nData Engineering\nBuild Glue workflows with context aware analysis and masking\nwarning\nAvoid storing unencrypted sensitive data in S3 buckets or transmitting PI without TLS encryption. Always apply IAM policies and KMS encryption.\nAWS Macie\n‚Äã\nMacie is a fully managed data security and privacy service that uses machine learning to discover and protect sensitive data in AWS. It automatically scans S3 buckets for PI keywords such as names, addresses, credit card numbers, and social security numbers. Macie integrates with CloudTrail for audit logging and IAM for access control.\nUse Cases\n: S3 bucket scanning, compliance audits, alerting on sensitive data exposure.\nKey Features\n: ML-based classification, customizable detection rules, automated remediation workflows. Allows to detect a large and growing list of sensitive data types for many countries and Regions, including multiple types of financial data, personal health information (PHI), and personally identifiable information (PII), as well as custom types.\nAWS Glue Data Quality\n‚Äã\nGlue DQ\ncan measure and monitor the data quality of Amazon S3 based data lakes, data warehouses, and other data repositories. It automatically computes statistics, recommends quality rules, and can monitor and alert you when it detects PI key words, missing, stale, or bad data. You can access it in the AWS Glue Data Catalog and in AWS Glue Data Catalog ETL jobs.\nUse Cases\n: ETL pipelines with embedded PI detection, data masking before analytics.\nKey Features\n: Detect PII transform, regex-based masking, column-level sensitivity scanning.\nAWS Kinesis + Lambda\n‚Äã\nFor real-time data streams, AWS Kinesis can ingest data while Lambda functions inspect and classify sensitive content on the fly. This is ideal for use cases like chatbots, IoT telemetry, or transactional logs.\nUse Cases\n: Streaming PII detection, real-time alerts, dynamic masking.\nKey Features\n: Event-driven architecture, scalable processing, integration with Macie.\nAWS Comprehend\n‚Äã\nAWS Comprehend is service that can be applied to various use cases, including PI data detection, customer feedback analysis, content categorization, and sentiment analysis. By leveraging this service, businesses can gain deeper insights into their data, identify and dedact PI data, improve decision-making, and enhance customer experience.\nAWS Comprehend is typically integrated into ETL pipelines to\nenrich data\nbefore loading it into analytics platforms or data lakes.\nUse Cases\n: Natural language processing (NLP) based PI detection, real-time and batch data processing\nKey Features\n: country specific PI entity types, Redact PI entities, Locate PI entities\nList of 18 PHI identifiers\n‚Äã\nNames\nGeographic data\nAll elements of dates directly related to an Individual, including birth date, admission date, discharge date, date of death, all ages overs 89 and dates indicative of such age and ages aggregated into single category of age 90 or older.\nTelephone numbers\nFax Numbers\nElectronic Mail addresses\nsocial security numbers\nmedical record numbers\nHealth plan beneficiary numbers\nAccount numbers\ncertificate/license numbers\nvehicle identifiers and serial numbers, including license plate numbers\ndevice identifiers and serial numbers\nweb universal resource locators (Web URLs)\nInternet Protocol (IP) address numbers\nBiometric identifiers, including finger and voice prints\nFull face photographic images and any comparable images\nany other unique identifying number, characteristic,\nGuidance on Methods for de-identification of PHI ->\nhttps://www.hhs.gov/hipaa/for-professionals/special-topics/de-identification/index.html\nCost comparison Enterprise Data Guidance page\n‚Äã\nhttps://lilly-confluence.atlassian.net/wiki/spaces/EDB/pages/1744175392/AWS+PII+Detection+Capabilities\nWas this helpful?\nTags:\nsolution-guide\nbusiness-enablement\ntechhq\nEdit this page\nPrevious\nüß≠ Customer Engagement Reference Architecture\nNext\n‚òÅÔ∏è Cloud & Infrastructure\nAWS Macie\nAWS Glue Data Quality\nAWS Kinesis + Lambda\nAWS Comprehend\nList of 18 PHI identifiers\nCost comparison Enterprise Data Guidance page\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 3,
    "depth": 3,
    "crawled_at": "2026-02-25T10:08:38.195177"
  },
  "https://techhq.dc.lilly.com/docs/solution/cloud/cloud-blt": {
    "url": "https://techhq.dc.lilly.com/docs/solution/cloud/cloud-blt",
    "title": "üóÑÔ∏è Cloud & Infrastructure BLT | Tech HQ",
    "description": "The Cloud & Infrastructure Big List of Technology (BLT) is maintained using data from the Enterprise Architecture Tech Positioning Database and LillyFlow, with AI-assisted updates. While we aim for accuracy, please verify critical information and contact the EBA team with questions or corrections.",
    "h1": [
      "üóÑÔ∏è Cloud & Infrastructure BLT"
    ],
    "h2": [
      "Amazon API Gateway‚Äã",
      "Amazon CloudWatch RUM‚Äã",
      "Amazon Detective‚Äã",
      "Amazon Elastic File System (EFS)‚Äã",
      "Amazon Guard Duty‚Äã",
      "Amazon Location Services (ALS)‚Äã",
      "Amazon MQ (Message Queue)‚Äã",
      "Amazon OpenSearch‚Äã",
      "Amazon Route 53‚Äã",
      "Amazon Simple Notification Service (SNS)‚Äã",
      "Amazon Simple Queue Service (SQS)‚Äã",
      "Amazon Simple Storage Service (S3)‚Äã",
      "Amazon Translate‚Äã",
      "Ansible Automation Platform (AAP)‚Äã",
      "Apache Airflow‚Äã",
      "App Connect Enterprise (ACE)‚Äã",
      "Aspera‚Äã",
      "AWS AMI provisioning via Lambda‚Äã",
      "AWS App Runner‚Äã",
      "AWS AppFlow‚Äã",
      "AWS Application Migration Services‚Äã",
      "AWS AppSync‚Äã",
      "AWS Athena‚Äã",
      "AWS Aurora PostgreSQL‚Äã",
      "AWS Auto Scaling‚Äã",
      "AWS Backup Elastic Block Store (EBS)‚Äã",
      "AWS Batch‚Äã",
      "AWS Budgets‚Äã",
      "AWS Certificate Manager‚Äã",
      "AWS Cloud Directory‚Äã",
      "AWS Cloud Map‚Äã",
      "AWS Cloud WAN‚Äã",
      "AWS CloudFormation‚Äã",
      "AWS CloudHSM‚Äã",
      "AWS CloudShell‚Äã",
      "AWS CloudTrail‚Äã",
      "AWS CloudWatch‚Äã",
      "AWS CodeArtifact‚Äã",
      "AWS CodeBuild‚Äã",
      "AWS Cognito‚Äã",
      "AWS Comprehend‚Äã",
      "AWS Comprehend Medical‚Äã",
      "AWS Config‚Äã",
      "AWS Cost Explorer‚Äã",
      "AWS Data Exchange‚Äã",
      "AWS Data Pipeline‚Äã",
      "AWS Data Transfer‚Äã",
      "AWS Database Migration Service (DMS)‚Äã",
      "AWS DataSync‚Äã",
      "AWS Direct Connect‚Äã",
      "AWS Directory Service‚Äã",
      "AWS DocumentDB (with MongoDB compatibility)‚Äã",
      "AWS DynamoDB‚Äã",
      "AWS EBS w/EC2‚Äã",
      "AWS EC2‚Äã",
      "AWS EC2 Amazon Linux‚Äã",
      "AWS EC2 Linux DMZ AMI‚Äã",
      "AWS EC2 RHEL‚Äã",
      "AWS EC2 Ubuntu DMZ AMI‚Äã",
      "AWS EC2 Ubuntu Linux‚Äã",
      "AWS EC2 Ubuntu Version 18.04 AMI‚Äã",
      "AWS EC2 Windows‚Äã",
      "AWS ECR (Container Registry)‚Äã",
      "AWS ECS / Fargate‚Äã",
      "AWS EKS / Fargate‚Äã",
      "AWS Elastic Disaster Recovery‚Äã",
      "AWS ElastiCache Redis‚Äã",
      "AWS ELB‚Äã",
      "AWS EMR‚Äã",
      "AWS EventBridge‚Äã",
      "AWS Firewall Manager‚Äã",
      "AWS FSx‚Äã",
      "AWS GameLift‚Äã",
      "AWS Glacier‚Äã",
      "AWS Global Accelerator‚Äã",
      "AWS Glue (Catalog)‚Äã",
      "AWS Glue Crawler‚Äã",
      "AWS Glue DataBrew‚Äã",
      "AWS Glue ETL‚Äã",
      "AWS Glue Studio‚Äã",
      "AWS IaaS - DLAB‚Äã",
      "AWS IAM‚Äã",
      "AWS Inspector‚Äã",
      "AWS IoT‚Äã",
      "AWS IoT Core‚Äã",
      "AWS IoT Device Manager‚Äã",
      "AWS IoT Events‚Äã",
      "AWS IoT Greengrass‚Äã",
      "AWS IoT SiteWise‚Äã",
      "AWS IoT TwinMaker‚Äã",
      "AWS Kendra‚Äã",
      "AWS Key Management Service (KMS)‚Äã",
      "AWS Kinesis Data Streaming‚Äã",
      "AWS Kinesis Firehose‚Äã",
      "AWS Kinesis Video Streams‚Äã",
      "AWS Lake Formation‚Äã",
      "AWS Lambda‚Äã",
      "AWS License Manager‚Äã",
      "AWS Lightsail‚Äã",
      "AWS Lookout for Metrics‚Äã",
      "AWS Managed Streaming for Apache Kafka‚Äã",
      "AWS Mechanical Turk‚Äã",
      "AWS Migration Hub Refactor Spaces‚Äã",
      "AWS Neptune‚Äã",
      "AWS Polly‚Äã",
      "AWS Private Marketplace‚Äã",
      "AWS RoboMaker‚Äã",
      "AWS S3 & CloudFront for SPAs‚Äã",
      "AWS SageMaker‚Äã",
      "AWS Secrets Manager‚Äã",
      "AWS Security Hub‚Äã",
      "AWS Service Catalog‚Äã",
      "AWS SES (Simple Email Service)‚Äã",
      "AWS Shield‚Äã",
      "AWS SimbleDB‚Äã",
      "AWS Simple Workflow Service‚Äã",
      "AWS Snowball‚Äã",
      "AWS Step Functions‚Äã",
      "AWS Storage Gateway‚Äã",
      "AWS Systems Manager‚Äã",
      "AWS Transfer for FTPS‚Äã",
      "AWS Transfer for SFTP‚Äã",
      "AWS VPC‚Äã",
      "AWS WAF‚Äã",
      "AWS X-Ray‚Äã",
      "Axway - eCommerce‚Äã",
      "Azure AI Services‚Äã",
      "Azure AKS‚Äã",
      "Azure API Management‚Äã",
      "Azure App Service‚Äã",
      "Azure AVS‚Äã",
      "Azure Bastion‚Äã",
      "Azure Batch‚Äã",
      "Azure Blob / CDN‚Äã",
      "Azure Blog Storage‚Äã",
      "Azure Bot Services‚Äã",
      "Azure Cognitive Search‚Äã",
      "Azure Container Apps‚Äã",
      "Azure Container Registry‚Äã",
      "Azure Containers‚Äã",
      "Azure Digital Twin‚Äã",
      "Azure File Storage‚Äã",
      "Azure Functions‚Äã",
      "Azure Key Vault‚Äã",
      "Azure Logic Apps‚Äã",
      "Azure Machine Learning‚Äã",
      "Azure Machine Learning Studio‚Äã",
      "Azure Maps‚Äã",
      "Azure Monitor‚Äã",
      "Azure NetApp File Storage‚Äã",
      "Azure Purview‚Äã",
      "Azure Queue Storage‚Äã",
      "Azure Redis Cache‚Äã",
      "Azure Service Fabric‚Äã",
      "Azure Virtual Machines‚Äã",
      "Contentful‚Äã",
      "Dell/EMC Isilon Smartlock (eArchive Service)‚Äã",
      "Heroku‚Äã",
      "Heroku - NewRelic‚Äã",
      "Heroku - PaperTrail‚Äã",
      "Informatica Axon / EDC‚Äã",
      "LAN Drives - Application Storage & File-based System Integration‚Äã",
      "Oauth (AzureAD)‚Äã",
      "OpenIDConnect (OIDC) (AzureAD)‚Äã",
      "Openshift (onPrem)‚Äã",
      "Salesforce‚Äã",
      "Declining ‚Üí Retired‚Äã"
    ],
    "h3": [
      ".NET Classic & WHO IIS Hosting‚Äã",
      "AWS CodeCommit‚Äã",
      "AWS CodePipeline‚Äã",
      "AWS IaaS - Anzo V4‚Äã",
      "AWS Pipeline v2 (Code Pipeline/Build/Deploy)‚Äã",
      "AWS Squid‚Äã",
      "Azure Pipelines‚Äã",
      "CAS (Centera/Disk Extender)‚Äã",
      "Crownpeak‚Äã",
      "Fuse‚Äã",
      "Informatica Intelligent Cloud Services‚Äã",
      "Informatica MDM / BDM‚Äã",
      "Informatica PowerCenter‚Äã",
      "JBoss‚Äã",
      "Layer7 API (SOA) Gateway‚Äã",
      "Openshift on AWS‚Äã",
      "Tomcat‚Äã",
      "VMware Servers (OnPrem)‚Äã",
      "WebSphere MQ‚Äã"
    ],
    "text_content": "üóÑÔ∏è Cloud & Infrastructure BLT | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüóÑÔ∏è Cloud & Infrastructure BLT\nüß≠ Cloud Architecture\nüß≠ Cloud FinOps\nüß≠ Container Services\nüß≠ Networking Services\nüß≠ Workflow & Orchestration\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\n‚òÅÔ∏è Cloud & Infrastructure\nüóÑÔ∏è Cloud & Infrastructure BLT\nOn this page\nüóÑÔ∏è Cloud & Infrastructure BLT\nThe Cloud & Infrastructure Big List of Technology (BLT) is maintained using data from the Enterprise Architecture Tech Positioning Database and LillyFlow, with AI-assisted updates. While we aim for accuracy, please verify critical information and contact the EBA team with questions or corrections.\nCan't find a technology?\nUse TechHQ's search feature to look across all technology documentation. For questions or to suggest additions to this list, contact the\nEnterprise Business Architecture team\n.\nAmazon API Gateway\n‚Äã\nPosition\nStandard\nTags\naws, integration-platforms\nInformation Sensitivity\nRed\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nSecurity enforcement and Life Cycle Management for serverless (AWS lambda) APIs, within the constraints of\nLCS-517 Appropriate Use of Lilly Public Cloud Environments\n.  Specifically: APIs whose backend lambdas run within a DX VPC must be\nprivate\n.\nUse Cases\n‚Äã\nAPIs built on serverless (lambda) technology with a single consistent authentication pattern.  Requires AWS technical and lambda coding expertise.\nProject teams are responsible for all support; no enterprise service provided.\nTeams can ask questions on Lilly Flow.\nNotes\n‚Äã\nAuthentication must be enforced, even for private APIs.  The strength of the authentication should be commiserate with the sensitivity of the data/functionality provided by the API.  When in doubt, consult with your\nBusiness Information Security Officer (BISO)\nSee Also\n‚Äã\nAmazon CloudWatch RUM\n‚Äã\nPosition\nEmerging\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nCurrently that service has not been Enterprise Reviewed and is only approved for Local Use Only.  The MD IDS Architect associated with the project which brought this request forward is Andrew Povinellli.\nSee Also\n‚Äã\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-RUM.html\nAmazon Detective\n‚Äã\nPosition\nEmerging\nTags\naws, cloud-computing\nInformation Sensitivity\nRed\nArchitect\nCiaran Carragher\nContact(s)\nAsh Edmondson\nSecurity\nUse Cases\n‚Äã\nSecurity\nNotes\n‚Äã\nSecurity\nSee Also\n‚Äã\nhttps://aws.amazon.com/detective/faqs/\nAmazon Elastic File System (EFS)\n‚Äã\nPosition\nStandard\nTags\naws, cloud-computing\nArchitect\nJarret DiPace\nContact(s)\nSriojeet Mukherjee\nInformation Sensitivity\nArchitect\nJarret DiPace\nContact(s)\nSriojeet Mukherjee\nAmazon Guard Duty\n‚Äã\nPosition\nEmerging\nTags\naws, cloud-computing\nArchitect\nAsh Edmonson\nContact(s)\nCiaran Carragher\nInformation Sensitivity\nArchitect\nAsh Edmonson\nContact(s)\nCiaran Carragher\nAmazon Location Services (ALS)\n‚Äã\nPosition\nSpecialized\nTags\naws\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nInformation Sensitivity\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nUse Cases\n‚Äã\nCAST Review for LillyAccess Use Case\n(contact Yun Heai Mun for details)\nAmazon MQ (Message Queue)\n‚Äã\nPosition\nEmerging\nTags\naws, cloud-computing\nInformation Sensitivity\nUnder Review\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nInformation Sensitivity\nUnder Review\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nSee Also\n‚Äã\nhttps://aws.amazon.com/amazon-mq/faqs/\nAmazon OpenSearch\n‚Äã\nPosition\nStandard\nTags\naws, search-platforms\nInformation Sensitivity\nRed\nArchitect\nMalika Mahoui\nContact(s)\nSomya Gupta\nCMDB CI\nCI00000000428364\nAWS Paas solution to index and search large amount of data. Use for pinpoint search cases rather than data exploration and analytics.\nUse Cases\n‚Äã\nThis is used for log management; kibana dashboard is integrated with this service used for monitoring and visualization. keyword-based search with synonyms/hierarchy for multiple data sources (SP, Regulus, DCTM) with custom UI or Kibanakeyword-based search on a custom-built Heroku app with a Contentful backendSearch Business Specific Content without giving access to the data (Cue)General purpose searchUnstructured search\nNotes\n‚Äã\nElasticSearch is the core search technology underpinning Anzo and Watson explorer.  When there is no need for semantics or analytics stand-alone ElasticSearch is a more managable and small footprint approach.Available in multiple platforms and implementation patterns Very customizable search experience based on the app contentBased on an open API - consumable from any API-centric data sourceCaution: Can require some complex configuration. √Ç¬†We do not have dedicated vendor for integrations\nSee Also\n‚Äã\nhttps://collab.lilly.com/sites/EIP/SitePages/Enterprise-Analytics.aspx\nAmazon Route 53\n‚Äã\nPosition\nStandard\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nInformation Sensitivity\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAmazon Simple Notification Service (SNS)\n‚Äã\nPosition\nStandard\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nInformation Sensitivity\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAmazon Simple Queue Service (SQS)\n‚Äã\nPosition\nStandard\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nInformation Sensitivity\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAmazon Simple Storage Service (S3)\n‚Äã\nPosition\nStandard\nTags\naws, aws-s3, cloud-computing\nInformation Sensitivity\nRed\nArchitect\nJarret DiPace\nContact(s)\nSriojeet Mukherjee\nPreferred for storage of documents, files, static assets for an applications\nUse Cases\n‚Äã\nManage (external facing) websites, Manage content in a structured way (including documents and files), Consume/deliver static and dynamic content, Manage mobile application content, Share content across different areas, Consume/deliver Rich Media/VideoStore documents and files supporting an applications (Raw Storage)\nAmazon Translate\n‚Äã\nPosition\nEmerging\nTags\naws, cloud-computing\nArchitect\nMalika Mahoui\nContact(s)\nMalika Mahoui\nInformation Sensitivity\nArchitect\nMalika Mahoui\nContact(s)\nMalika Mahoui\nAnsible Automation Platform (AAP)\n‚Äã\nTags\nautomation, ansible, aap\nPosition\nStandard\nInformation Sensitivity\nYellow\nArchitect\nJarret DiPace\nContact(s)\nJarret DiPace, Adam Esslinger\nCMDB CI\nCI00000084632853\nInfrastructure Automation, Configuration-As-Code, and Full-stack application automation\nNotes\n‚Äã\nAnsible Automation Platform should not be used in-place of cloud native IAC solutions such as Cloud-Formation, Bicep, or Terraform.\nSee Also\n‚Äã\nAAP Foundational Strategy\nAAP Shared Responsibility Model\nApache Airflow\n‚Äã\nPosition\nEmerging\nTags\nintegration-platforms\nInformation Sensitivity\nRed\nArchitect\nLarry Zetzl\nContact(s)\nAndy Mulder\nUse for orchestrating complex workflows and data processing piplines in the cloud\nUse Cases\n‚Äã\nCreate Directed Acyclic Graphs (DAGs) python code to configure complex workflows and data pipelines in the cloud\nNotes\n‚Äã\nApache Airflow is open-source. Caution use with handling Red CI\nApp Connect Enterprise (ACE)\n‚Äã\nPosition\nStandard\nTags\nenterprise-automation\nArchitect\nAbhijeet Sutrave\nContact(s)\nAbhijeet Sutrave\nApp Connect Enterprise (ACE) is a powerful integration solution designed to facilitate the seamless connection of applications and data across various environments, enabling organizations to automate workflows and streamline business processes. ACE supports deployment both on-premises and in the cloud, offering robust capabilities for integration and API management.\nUse Cases\n‚Äã\nHere are some common use cases for App Connect Enterprise:\nEnterprise Integration:\nAutomating end-to-end business processes involving multiple systems and data sources, such as ERP integration, CRM synchronization, and supply chain automation.\nAPI Management:\nCreating, publishing, and managing APIs to enable seamless interaction between applications and services.\nEvent-Driven Architecture:\nImplementing real-time data processing and event-driven workflows for instant response and action.\nHybrid Cloud Integration:\nFacilitating integration between on-premises systems and cloud applications to leverage the benefits of hybrid cloud environments.\nData Transformation:\nConverting data formats and protocols to ensure compatibility and interoperability across different systems.\nNotes\n‚Äã\nWhile integration solutions can vary, here are scenarios where IBM App Connect Enterprise stands out:\nComplex Integration Needs:\nProjects requiring integration across diverse systems with high complexity and scalability requirements.\nHybrid Integration:\nManaging integrations that span across on-premises and cloud environments, supporting hybrid cloud strategies.\nLarge User Base:\nServing a significant number of users with robust task management and workflow automation capabilities.\nEnterprise Licensing Model:\nOffering flexible licensing options suitable for enterprise deployment and scalability.\nDevelopment Environment:\nProviding a familiar development environment with support for JavaScript-like scripting for customization a...\nRead full article\nAspera\n‚Äã\nPosition\nSpecialized\nTags\nintegration-platforms\nInformation Sensitivity\nRed\nArchitect\nLarry Zetzl\nContact(s)\nAndrew Mulder\nUse for big file movmenet solution for transfers between Lilly and external partners. Enterprise environment is restricted use and requires management approval.\nUse Cases\n‚Äã\nTransfer large amounts of data between Lilly and external partner, Transfer data regularly between Lilly and external partner, Allow a non-onboarded individual manually transfer data with Lilly\nAWS AMI provisioning via Lambda\n‚Äã\nPosition\nEmerging\nTags\naws, cloud-computing\nArchitect\nJarret DiPace\nContact(s)\nShakya RadhaKrishna\nInformation Sensitivity\nArchitect\nJarret DiPace\nContact(s)\nShakya RadhaKrishna\nAWS App Runner\n‚Äã\nPosition\nSpecialized\nTags\naws, cloud-computing\nInformation Sensitivity\nRed\nArchitect\nTodd Walters\nContact(s)\nTodd Walters\nBased on inability to put effective compliance controls around the service via Cloud Custodian or AWS Config, this service is being fully blocked in DX based AWS accounts, and in order to use this service in a NoDX account the AWS account needs to be added to the service allow list.  In order to be added to the Lilly AWS Organization Service Allow List a CAST review and approval is required.\nUse Cases\n‚Äã\nDeploying from source code or a container image directly to a scalable and secure web application in the AWS Cloud.\nAWS AppFlow\n‚Äã\nPosition\nEmerging\nTags\naws, integration-platforms\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nFully managed integration service to securely transfer data between Software-as-a-Service (SaaS) applications like Salesforce and AWS S3 or Redshift, using configuration instead of code.\nAWS Application Migration Services\n‚Äã\nPosition\nSpecialized\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nLucinda Brewer\nCurrently being evaluated by Digital Core\nUse Cases\n‚Äã\nMigration of existing on-premises applications into AWS\nAWS AppSync\n‚Äã\nPosition\nSpecialized\nTags\naws, cloud-computing\nInformation Sensitivity\nUnder Review\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nInformation Sensitivity\nUnder Review\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAWS Athena\n‚Äã\nPosition\nStandard\nTags\naws, cloud-computing\nArchitect\nMalika Mahoui\nContact(s)\nMumtaz\nInformation Sensitivity\nArchitect\nMalika Mahoui\nContact(s)\nMumtaz\nAWS Aurora PostgreSQL\n‚Äã\nPosition\nStandard\nTags\naws, database-platforms\nInformation Sensitivity\nRed\nArchitect\nLarry Zetzl\nContact(s)\nCraig Welch\nPrimary preferred relational database if availability and performance requirements are critical. Compliant with the Enterprise Data Backbone architecture.\nUse Cases\n‚Äã\nRelational OLTP workloads\nReferential integrity\nACID transactions\nNotes\n‚Äã\nBest AWS alternative for Oracle RDBMS\nFully managed database\nDistributed, fault-tolerant, and self-healing\nSee Also\n‚Äã\nhttps://lilly.service-now.com/kb_view.do?sys_kb_id=1028758f1333d7880961bbc76144b0c8\nAWS Auto Scaling\n‚Äã\nPosition\nStandard\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nInformation Sensitivity\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAWS Backup Elastic Block Store (EBS)\n‚Äã\nPosition\nStandard\nTags\naws, cloud-computing\nArchitect\nJarret DiPace\nContact(s)\nJovin Jose\nInformation Sensitivity\nArchitect\nJarret DiPace\nContact(s)\nJovin Jose\nAWS Batch\n‚Äã\nPosition\nEmerging\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nInformation Sensitivity\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nUse Cases\n‚Äã\nAWS Batch helps you to run batch computing workloads on the AWS Cloud. Batch computing is a common way for developers, scientists, and engineers to access large amounts of compute resources. AWS Batch removes the undifferentiated heavy lifting of configuring and managing the required infrastructure, similar to traditional batch computing software. This service can efficiently provision resources in response to jobs submitted in order to eliminate capacity constraints, reduce compute costs, and deliver results quickly.\nAWS Budgets\n‚Äã\nPosition\nStandard\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nAnjali Oleksy\nInformation Sensitivity\nArchitect\nTodd Walters\nContact(s)\nAnjali Oleksy\nAWS Certificate Manager\n‚Äã\nPosition\nStandard\nTags\naws, cloud-computing\nInformation Sensitivity\nUnder Review\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nInformation Sensitivity\nUnder Review\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAWS Cloud Directory\n‚Äã\nPosition\nProhibited\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nInformation Sensitivity\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAWS Cloud Map\n‚Äã\nPosition\nSpecialized\nTags\naws, cloud-computing\nInformation Sensitivity\nUnder Review\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAWS Cloud WAN\n‚Äã\nPosition\nSpecialized\nTags\naws, cloud-computing\nInformation Sensitivity\nUnder Review\nArchitect\nTodd Walters\nContact(s)\nDoug Aleksa\nAWS CloudFormation\n‚Äã\nPosition\nStandard\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nInformation Sensitivity\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAWS CloudHSM\n‚Äã\nPosition\nProhibited\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nInformation Sensitivity\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAWS CloudShell\n‚Äã\nPosition\nStandard\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nInformation Sensitivity\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAWS CloudTrail\n‚Äã\nPosition\nStandard\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nInformation Sensitivity\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAWS CloudWatch\n‚Äã\nPosition\nStandard\nTags\naws, dev-tools\nInformation Sensitivity\nRed\nArchitect\nTodd Walters\nContact(s)\nTodd Walters\nPreferred Monitoring Tool for Applications using AWS.\nUse Cases\n‚Äã\nIn your application you would use CloudWatch to write logs to monitor your application.\nNotes\n‚Äã\nGreat tool when interrgrating with other public cloud services like AWS SNS\nAWS CodeArtifact\n‚Äã\nPosition\nSpecialized\nTags\naws, dev-tools\nInformation Sensitivity\nRed\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nSome teams are using it within AWS to store & retrieve artefacts\nUse Cases\n‚Äã\nAWS Binaries needing to be pulled into apps during build time.\nNotes\n‚Äã\nUpand coming. Not many teams using it now, but some teams are as it keeps everything within the Public Cloud space.\nAWS CodeBuild\n‚Äã\nPosition\nStandard\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nInformation Sensitivity\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAWS Cognito\n‚Äã\nPosition\nSpecialized\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nInformation Sensitivity\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAWS Comprehend\n‚Äã\nPosition\nEmerging\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nInformation Sensitivity\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAWS Comprehend Medical\n‚Äã\nPosition\nEmerging\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nInformation Sensitivity\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAWS Config\n‚Äã\nPosition\nStandard\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nInformation Sensitivity\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAWS Cost Explorer\n‚Äã\nPosition\nSpecialized\nTags\naws, cloud-computing\nInformation Sensitivity\nRed\nArchitect\nTodd Walters\nContact(s)\nAnjali Oleksy\nInformation Sensitivity\nRed\nArchitect\nTodd Walters\nContact(s)\nAnjali Oleksy\nNotes\n‚Äã\nAccess granted to financial analysts and Lilly employees via self-service through\nhttps://cloud.lilly.com\nAWS Data Exchange\n‚Äã\nPosition\nSpecialized\nTags\naws, cloud-computing\nInformation Sensitivity\nRed\nArchitect\nLarry Zetzl\nContact(s)\nAndy Mulder\nAWS Data Pipeline\n‚Äã\nPosition\nSpecialized\nTags\naws, cloud-computing\nInformation Sensitivity\nRed\nArchitect\nLarry Zetzl\nContact(s)\nAndy Mulder\nAWS Data Transfer\n‚Äã\nPosition\nEmerging\nTags\naws, cloud-computing\nInformation Sensitivity\nUnder Review\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nInformation Sensitivity\nUnder Review\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAWS Database Migration Service (DMS)\n‚Äã\nPosition\nStandard\nTags\naws, cloud-computing\nArchitect\nManuela Elena Garcia\nContact(s)\nAvishek Mukherjee\nInformation Sensitivity\nArchitect\nManuela Elena Garcia\nContact(s)\nAvishek Mukherjee\nAWS DataSync\n‚Äã\nPosition\nStandard\nTags\naws, integration-platforms\nInformation Sensitivity\nUnder Review\nArchitect\nLarry Zetzl\nContact(s)\nAndy Mulder\nUse for large bulk file transfers between on-premise network storage and AWS\nUse Cases\n‚Äã\nQuickly migrate large amounts of data from on-premise into AWS S3\nAWS Direct Connect\n‚Äã\nPosition\nStandard\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nDoug Aleksa\nInformation Sensitivity\nArchitect\nTodd Walters\nContact(s)\nDoug Aleksa\nAWS Directory Service\n‚Äã\nPosition\nSpecialized\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nInformation Sensitivity\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAWS DocumentDB (with MongoDB compatibility)\n‚Äã\nPosition\nEmerging\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nInformation Sensitivity\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAWS DynamoDB\n‚Äã\nPosition\nStandard\nTags\naws, database-platforms\nInformation Sensitivity\nRed\nArchitect\nLarry Zetzl\nContact(s)\nManuela Elena Garcia\nPreferred solution for Key-value / Document / NoSQL database use cases\nUse Cases\n‚Äã\nNon-relational OLTP workloads; High throughput-low latency\nShopping Cart\nProduct Catalog\nMobile\nNotes\n‚Äã\nKey-value and Document typeEndless scaleServerless\nSee Also\n‚Äã\nhttps://lillydev.com/backing%20services/aws%20dynamodb\nAWS EBS w/EC2\n‚Äã\nPosition\nStandard\nTags\naws, cloud-computing\nArchitect\nSriojeet Mukherjee\nContact(s)\nJovin Jose\nInformation Sensitivity\nArchitect\nSriojeet Mukherjee\nContact(s)\nJovin Jose\nAWS EC2\n‚Äã\nPosition\nStandard\nTags\naws, cloud-computing\nInformation Sensitivity\nRed\nArchitect\nJarret DiPace\nContact(s)\nJovin Jose\nPreferred for COTS installations on Servers\nUse Cases\n‚Äã\nHosting COTS\nNotes\n‚Äã\nFull scale availability for RHEL and Amazon Linux. Small scale for Windows and UbuntuNOTE: Up the stack options should be preferred to Server based solution\nAWS EC2 Amazon Linux\n‚Äã\nPosition\nStandard\nTags\naws, cloud-computing\nArchitect\nJarret DiPace\nContact(s)\nShakya RadhaKrishna\nInformation Sensitivity\nArchitect\nJarret DiPace\nContact(s)\nShakya RadhaKrishna\nAWS EC2 Linux DMZ AMI\n‚Äã\nPosition\nStandard\nTags\naws, cloud-computing\nArchitect\nJarret DiPace\nContact(s)\nShakya RadhaKrishna\nInformation Sensitivity\nArchitect\nJarret DiPace\nContact(s)\nShakya RadhaKrishna\nAWS EC2 RHEL\n‚Äã\nPosition\nStandard\nTags\naws, cloud-computing\nArchitect\nJarret DiPace\nContact(s)\nShakya RadhaKrishna\nInformation Sensitivity\nArchitect\nJarret DiPace\nContact(s)\nShakya RadhaKrishna\nAWS EC2 Ubuntu DMZ AMI\n‚Äã\nPosition\nStandard\nTags\naws, cloud-computing\nArchitect\nJarret DiPace\nContact(s)\nShakya RadhaKrishna\nInformation Sensitivity\nArchitect\nJarret DiPace\nContact(s)\nShakya RadhaKrishna\nAWS EC2 Ubuntu Linux\n‚Äã\nPosition\nStandard\nTags\naws, cloud-computing\nArchitect\nJarret DiPace\nContact(s)\nShakya RadhaKrishna\nInformation Sensitivity\nArchitect\nJarret DiPace\nContact(s)\nShakya RadhaKrishna\nAWS EC2 Ubuntu Version 18.04 AMI\n‚Äã\nPosition\nStandard\nTags\naws, cloud-computing\nArchitect\nJarret DiPace\nContact(s)\nShakya RadhaKrishna\nInformation Sensitivity\nArchitect\nJarret DiPace\nContact(s)\nShakya RadhaKrishna\nAWS EC2 Windows\n‚Äã\nPosition\nStandard\nTags\naws, cloud-computing\nArchitect\nJarret DiPace\nContact(s)\nJovin Jose\nInformation Sensitivity\nArchitect\nJarret DiPace\nContact(s)\nJovin Jose\nAWS ECR (Container Registry)\n‚Äã\nPosition\nStandard\nTags\naws, dev-tools\nInformation Sensitivity\nRed\nArchitect\nTodd Walters\nContact(s)\nTodd Walters\nPreferred Tool for applications needing to storing containers within AWS; to be used across the broader AWS ecosystem\nUse Cases\n‚Äã\nStoring containers within AWS. Teams are able to get Scanning of containers easily with Amazon ECR.\nNotes\n‚Äã\nBecoming on the most popular container registry tools at Lilly. Quite a bit behind Artifactory but up and coming.\nAWS ECS / Fargate\n‚Äã\nPosition\nStandard\nTags\naws, cloud-computing\nInformation Sensitivity\nRed\nArchitect\nTodd Walters\nContact(s)\nJosh Bloxsome\nAWS EKS / Fargate\n‚Äã\nPosition\nStandard\nTags\naws, cloud-computing\nInformation Sensitivity\nRed\nArchitect\nTodd Walters\nContact(s)\nJosh Bloxsome\nAWS Elastic Disaster Recovery\n‚Äã\nPosition\nSpecialized\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nApproved for Specialized, Local Use Only use case, specifically for the SAP team to be used in the creation of an AWS DR capability for their on-premises SAP systems.  Additionally, the SAP team will use this service in the future for Cloud DR of their AWS deployment in us-east-2 into us-east-1.\nLead Architect from Lilly SAP team is Brett Saksa.\nSee Also\n‚Äã\nhttps://aws.amazon.com/disaster-recovery/\nAWS ElastiCache Redis\n‚Äã\nPosition\nStandard\nTags\naws, database-platforms\nInformation Sensitivity\nRed\nArchitect\nLarry Zetzl\nContact(s)\nCraig Welch\nCMDB CI\nCI00000000420438\nPreferred solution for in-memory key value NoSQL workloads\nUse Cases\n‚Äã\nHigh performance/low latency cache\nReal-time transactions & leaderboards\nInternet Applications & Gaming, Ad-Tech\nIoT Applications\nGeospatial\nMachine Learning\nchat\nNotes\n‚Äã\nFully Managed\nServerless\nScale out through shard replicas\nSee Also\n‚Äã\nhttps://lillydev.com/aws/aws%20services%23aws-services-database-services-amazon-elasticache\nAWS ELB\n‚Äã\nPosition\nStandard\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nInformation Sensitivity\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAWS EMR\n‚Äã\nPosition\nStandard\nTags\naws, cloud-computing\nArchitect\nLarry Zetzl\nContact(s)\nDoug Marshall\nInformation Sensitivity\nArchitect\nLarry Zetzl\nContact(s)\nDoug Marshall\nAWS EventBridge\n‚Äã\nPosition\nStandard\nTags\naws, integration-platforms\nArchitect\nLarry Zetzl\nContact(s)\nAndrew Mulder\nMechanism for event-driven application\nUse Cases\n‚Äã\nServerless, scalable ability to create, ingest, filter, transform and deliver events without custom coding.  Supports events for 60+ AWS services and other SaaS applications.  Permits up to 5 targets per rule.\nCompare this capability with AWS SNS to select the best option.\nEventBridge is approved for use, but teams must consult with their InfoSec and Quality reps for approved use\nNotes\n‚Äã\nConsulting-only service available from EIP. Security-reviewed CloudFormation scripts available for use.\nAWS Firewall Manager\n‚Äã\nPosition\nSpecialized\nTags\naws, cloud-computing\nArchitect\nAsh Edmonson\nContact(s)\nCiaran Carragher\nInformation Sensitivity\nArchitect\nAsh Edmonson\nContact(s)\nCiaran Carragher\nAWS FSx\n‚Äã\nPosition\nStandard\nTags\naws, cloud-computing\nArchitect\nSriojeet Mukherjee\nContact(s)\nSubhankar Biswal, Sriojeet Mukherjee\nInformation Sensitivity\nArchitect\nSriojeet Mukherjee\nContact(s)\nSubhankar Biswal, Sriojeet Mukherjee\nUse Cases\n‚Äã\nAWS FSx for NetApp ONTAP is a fully managed service that provides highly reliable, scalable, high-performing, and feature-rich file storage built on NetApp's ONTAP file system.\nFSx for ONTAP also provides highly available and durable storage with fully managed backups and support for cross-Region disaster recovery. FSx for ONTAP is an ideal solution to migrate, back up, or burst your file-based applications from on-premises to AWS without the need to change your application code or how you manage your data.\nIt supports SMB based access, NFS access as well as dual-protocol access.\nNotes\n‚Äã\nThis is a Lilly HCS supported service and is fully managed by the HCS Storage team.\nSee Also\n‚Äã\nhttps://docs.aws.amazon.com/fsx/latest/ONTAPGuide/what-is-fsx-ontap.html\nhttps://aws.amazon.com/fsx/netapp-ontap/\nAWS GameLift\n‚Äã\nPosition\nProhibited\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nInformation Sensitivity\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAWS Glacier\n‚Äã\nPosition\nStandard\nTags\naws, content-management\nInformation Sensitivity\nOrange\nArchitect\nJarret DiPace\nContact(s)\nSriojeet Mukherjee\nStandard for low cost storage of active systems. Must not be used for long term archival of records subject to regulatory retention or legal hold for retired systems.\nUse Cases\n‚Äã\nlow cost storage of active systems\nNotes\n‚Äã\nGlacier is not an e-archive solution. For long term archival of business records subject to regulatory and/or legal hold use the eArchive service at\nhttps://collab.lilly.com/sites/earchiveservice566\nSee Also\n‚Äã\nhttps://lillydev.com/developer%20guidance/security%20principles%23security-principles-cloud-service-hardening-aws-glacier\nAWS Global Accelerator\n‚Äã\nPosition\nSpecialized\nTags\naws, cloud-computing\nInformation Sensitivity\nUnder Review\nArchitect\nTodd Walters\nContact(s)\nDoug Aleksa\nAWS Glue (Catalog)\n‚Äã\nPosition\nStandard\nTags\naws, cloud-computing\nArchitect\nLarry Zetzl\nContact(s)\nAndy Mulder\nInformation Sensitivity\nArchitect\nLarry Zetzl\nContact(s)\nAndy Mulder\nAWS Glue Crawler\n‚Äã\nPosition\nStandard\nTags\naws, integration-platforms\nInformation Sensitivity\nRed\nArchitect\nLarry Zetzl\nContact(s)\nAndy Mulder\nIndex data in AWS for additional analytics\nUse Cases\n‚Äã\nScan data in AWS and store metadata in Glue Data Catalog for querying, data transformation and analytics\nNotes\n‚Äã\nConsulting-only service. Security-reviewed CloudFormation scripts available for use.\nAWS Glue DataBrew\n‚Äã\nPosition\nEmerging\nTags\naws, data-science-tools\nInformation Sensitivity\nRed\nArchitect\nMalika Mahoui\nContact(s)\nAndrea Price\nProvides visual, code free transformation and profiling of data for power users, and APIs for developers to integrate with other AWS services for more complex data pipelines. AWS ‚Äì native data preparation  option for the Enterprise Data Backbone (EDB)‚Äôs Data Wrangling capability.\nUse Cases\n‚Äã\nData exploration; creation of cleansed, transformed and normalized data assets and data products for analytics and machine learning; and creation of data quality applications\nNotes\n‚Äã\nNo license required; usage charges based on AWS utilization.\nEIP -provided support for EDB users.\nConsulting-only or EDB-like support services available for other AWS accounts\nAWS Glue ETL\n‚Äã\nPosition\nStandard\nTags\naws, integration-platforms\nInformation Sensitivity\nGreen\nArchitect\nLarry Zetzl\nContact(s)\nAndy Mulder\nUse for ETL integrations  inside AWS\nUse Cases\n‚Äã\nConfigure Glue job to perform data movement / tranformation / integration within AWS\nNotes\n‚Äã\nConsulting-only service. Security-reviewed CloudFormation scripts available for use.\nAWS Glue Studio\n‚Äã\nPosition\nStandard\nTags\naws, integration-platforms\nArchitect\nLarry Zetzl\nContact(s)\nAndy Mulder\nVisual interface for AWS Glue that makes it easy for extract-transform-and-load developers to author, run, and monitor AWS Glue ETL jobs.\nAWS IaaS - DLAB\n‚Äã\nPosition\nEmerging\nTags\naws, cloud-computing\nArchitect\nJames Rimell\nContact(s)\nJames Rimell\nInformation Sensitivity\nArchitect\nJames Rimell\nContact(s)\nJames Rimell\nAWS IAM\n‚Äã\nPosition\nStandard\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nLuis Espinoza\nInformation Sensitivity\nArchitect\nTodd Walters\nContact(s)\nLuis Espinoza\nAWS Inspector\n‚Äã\nPosition\nEmerging\nTags\naws, cloud-computing\nArchitect\nAsh Edmonson\nContact(s)\nCiaran Carragher\nInformation Sensitivity\nArchitect\nAsh Edmonson\nContact(s)\nCiaran Carragher\nAWS IoT\n‚Äã\nPosition\nSpecialized\nTags\naws, cloud-computing\nInformation Sensitivity\nUnder Review\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAWS IoT Core\n‚Äã\nPosition\nSpecialized\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAWS IoT Device Manager\n‚Äã\nPosition\nSpecialized\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAWS IoT Events\n‚Äã\nPosition\nSpecialized\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAWS IoT Greengrass\n‚Äã\nPosition\nSpecialized\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAWS IoT SiteWise\n‚Äã\nPosition\nSpecialized\nTags\naws, cloud-computing\nInformation Sensitivity\nUnder Review\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAWS IoT TwinMaker\n‚Äã\nPosition\nSpecialized\nTags\naws, cloud-computing\nInformation Sensitivity\nUnder Review\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAWS Kendra\n‚Äã\nPosition\nEmerging\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nInformation Sensitivity\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAWS Key Management Service (KMS)\n‚Äã\nPosition\nStandard\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nInformation Sensitivity\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAWS Kinesis Data Streaming\n‚Äã\nPosition\nEmerging\nTags\naws, integration-platforms\nInformation Sensitivity\nGreen\nArchitect\nLarry Zetzl\nContact(s)\nAndy Mulder\nUse for high-throughput data streaming to AWS services\nUse Cases\n‚Äã\nCapture, transform and load high-throughput raw streaming data into AWS target services in near real time\nNotes\n‚Äã\nConsulting-only service. Security-reviewed CloudFormation scripts available for use.\nAWS Kinesis Firehose\n‚Äã\nPosition\nEmerging\nTags\naws, integration-platforms\nInformation Sensitivity\nGreen\nArchitect\nLarry Zetzl\nContact(s)\nAndy Mulder\nUse for data streaming to AWS services\nUse Cases\n‚Äã\nCapture, transform and load raw streaming data into AWS target services in near real time\nNotes\n‚Äã\nConsulting-only service. Security-reviewed CloudFormation scripts available for use.\nAWS Kinesis Video Streams\n‚Äã\nPosition\nEmerging\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nInformation Sensitivity\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAWS Lake Formation\n‚Äã\nPosition\nSpecialized\nTags\naws, cloud-computing\nInformation Sensitivity\nUnder Review\nArchitect\nLarry Zetzl\nContact(s)\nAndy Mulder\nAWS Lambda\n‚Äã\nPosition\nStandard\nTags\naws, cloud-computing\nInformation Sensitivity\nRed\nArchitect\nTodd Walters\nContact(s)\nTodd Walters\nPreferred for Event Driven (FaaS, Serverless) use cases\nUse Cases\n‚Äã\nHosting Serverless / FaaS\nNotes\n‚Äã\nNot enterprise service support provided\nAWS License Manager\n‚Äã\nPosition\nEmerging\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nInformation Sensitivity\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAWS Lightsail\n‚Äã\nPosition\nEmerging\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nInformation Sensitivity\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAWS Lookout for Metrics\n‚Äã\nPosition\nEmerging\nTags\naws, observability\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nInformation Sensitivity\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nNotes\n‚Äã\nCAST review on 02-May-2024. Jarrett Meyer was approved to move forward with a request to Enterprise Data for a POC of this with one of his applications.  This service is not centrally managed or supported.  Teams wishing to use the service within their AWS account should ensure that they do not grant access to lookout objects those who they wouldn't grant access to the underlying sources being analyzed.  Additionally, if you wish to export the lookout objects anywhere engage your BISO so that a Cyber Cloud Security Architecture and Engineering assessment can done prior to enabling the export.\nSee Also\n‚Äã\nhttps://aws.amazon.com/lookout-for-metrics/faqs/\nAWS Managed Streaming for Apache Kafka\n‚Äã\nPosition\nEmerging\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nInformation Sensitivity\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAWS Mechanical Turk\n‚Äã\nPosition\nProhibited\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nInformation Sensitivity\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAWS Migration Hub Refactor Spaces\n‚Äã\nPosition\nSpecialized\nTags\naws, cloud-computing\nInformation Sensitivity\nUnder Review\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAWS Neptune\n‚Äã\nPosition\nEmerging\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nInformation Sensitivity\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAWS Polly\n‚Äã\nPosition\nEmerging\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nInformation Sensitivity\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAWS Private Marketplace\n‚Äã\nPosition\nSpecialized\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nInformation Sensitivity\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAWS RoboMaker\n‚Äã\nPosition\nEmerging\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nInformation Sensitivity\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAWS S3 & CloudFront for SPAs\n‚Äã\nPosition\nStandard\nTags\naws, content-management, cloud-computing\nInformation Sensitivity\nRed\nArchitect\nTodd Walters\nContact(s)\nTodd Walters\nPreferred for single page applications or progressive web apps on AWS\nNotes\n‚Äã\nNo enterprise service support provided\nSee Also\n‚Äã\nNo additional guidance has been published\nAWS SageMaker\n‚Äã\nPosition\nStandard\nTags\naws, data-science-tools\nInformation Sensitivity\nRed\nArchitect\nRubin Wei\nContact(s)\nSomya Gupta\nCMDB CI\nCI00000000409558\nUse to prepare, build , train and deploy machine learning models on AWS\nUse Cases\n‚Äã\nDeveloping and hosting a custom ML model\nExploring foundation models provided through\nJumpStart\nRunning managed training jobs\nAccess to a varitey of compute options (GPU, large memory, etc)\nNotes\n‚Äã\nPay for what you use, no license cost.  Two common access patterns available:\nAccess SageMaker AI through the\nAnalytics Workbench\n(AWB), an integral part of the Enterprise Data Program. Self-service access to SageMaker AI services, within minutes on\ndata.lilly.com\n. Runs in a production account that allows for experimentation with all data classifications.\nRequest help\nsetting up a qualified deployment in your own AWS account. Enterprise Data (formerly EIP) offers SageMaker AI as a qualified platorm, where core resources (domains, keys, etc.) can be deployed and managed by them into your own environment.\nSee Also\n‚Äã\nhttps://aws.amazon.com/sagemaker-ai/\nAWS Secrets Manager\n‚Äã\nPosition\nStandard\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nInformation Sensitivity\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAWS Security Hub\n‚Äã\nPosition\nStandard\nTags\naws, cybersecurity\nInformation Sensitivity\nOrange\nArchitect\nAsh Edmonson\nContact(s)\nCiaran Carragher\nUtilized for retrieving details about security events and findings within AWS.\nUse Cases\n‚Äã\nAggregate repository to review findings, alerts, and non-compliant configurations.\nAWS Service Catalog\n‚Äã\nPosition\nEmerging\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nInformation Sensitivity\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAWS SES (Simple Email Service)\n‚Äã\nPosition\nStandard\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nInformation Sensitivity\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nNotes\n‚Äã\nThere is an Amazon SES Enterprise offering now available for general use:\nhttps://collab.lilly.com/sites/GlobalMessaging/SitePages/Amazon-SES.aspx\n. This offering is a managed service offering by the Global Messaging team. Uses cases that are not compliant with the pattern in this article might be candidates for the Enterprise solution.\nAWS Shield\n‚Äã\nPosition\nEmerging\nTags\naws, cloud-computing\nArchitect\nAsh Edmonson\nContact(s)\nCiaran Carragher\nInformation Sensitivity\nArchitect\nAsh Edmonson\nContact(s)\nCiaran Carragher\nAWS SimbleDB\n‚Äã\nPosition\nProhibited\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nInformation Sensitivity\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAWS Simple Workflow Service\n‚Äã\nPosition\nProhibited\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nInformation Sensitivity\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAWS Snowball\n‚Äã\nPosition\nStandard\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nInformation Sensitivity\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAWS Step Functions\n‚Äã\nPosition\nStandard\nTags\naws, integration-platforms\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nServerless orchestrator to sequence Lambda and other multiple AWS services.¬† You can create event-driven workflows that maintain the application state and handling failures and retries\nAWS Storage Gateway\n‚Äã\nPosition\nEmerging\nTags\naws, integration-platforms\nInformation Sensitivity\nUnder Review\nArchitect\nLarry Zetzl\nContact(s)\nAndy Mulder\nUse for real-time replication between on-premise network storage and AWS\nUse Cases\n‚Äã\nCloud-enable an on-premise application by replicating storage from on-premise into AWS S3\nNotes\n‚Äã\nIn Pilot - not currently an enterprise service.\nAWS Systems Manager\n‚Äã\nPosition\nStandard\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nInformation Sensitivity\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAWS Transfer for FTPS\n‚Äã\nPosition\nEmerging\nTags\naws, integration-platforms\nArchitect\nLarry Zetzl\nContact(s)\nAndy Mulder\nUse to transfer files into and out of AWS S3 buckets via FTPS\nUse Cases\n‚Äã\nOn-prem project teams or external third party partners upload and download files into AWS S3 buckets via FTPS\nNotes\n‚Äã\nIn Pilot - not currently an enterprise service.\nAWS Transfer for SFTP\n‚Äã\nPosition\nStandard\nTags\naws, integration-platforms\nInformation Sensitivity\nOrange\nArchitect\nLarry Zetzl\nContact(s)\nAndy Mulder\nUse to transfer files into and out of AWS S3 buckets via SFTP\nUse Cases\n‚Äã\nOn-prem project tams or external third party partners upload and download files into AWS S3 buckets\nNotes\n‚Äã\nNot suiteable for Red CI at this time - single factor authetication only supported. Investigating  IP Address restrictions to AWS SFTP enviornment for future fulfillment of MFA.\nAWS VPC\n‚Äã\nPosition\nStandard\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nInformation Sensitivity\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAWS WAF\n‚Äã\nPosition\nStandard\nTags\naws, cloud-computing\nArchitect\nAsh Edmonson\nContact(s)\nCiaran Carragher\nInformation Sensitivity\nArchitect\nAsh Edmonson\nContact(s)\nCiaran Carragher\nAWS X-Ray\n‚Äã\nPosition\nEmerging\nTags\naws, cloud-computing\nInformation Sensitivity\nUnder Review\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nInformation Sensitivity\nUnder Review\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAxway - eCommerce\n‚Äã\nPosition\nStandard\nTags\nintegration-platforms\nInformation Sensitivity\nRed\nArchitect\nLarry Zetzl\nContact(s)\nAndrew Mulder\nUse for automated file movement between Lilly and external partners using a veriety of file transfer protocols.\nUse Cases\n‚Äã\nTransfer data regularly between Lilly and external partner, Allow a non-onboarded individual manually transfer data with Lilly. Can be used to transfer data from Lilly on prem to Lilly cloud\nAzure AI Services\n‚Äã\nPosition\nEmerging\nTags\ncloud-computing, azure-ai, ai-platforms\nInformation Sensitivity\nUnder Review\nArchitect\nTodd Walters\nContact(s)\nRichard Schmidt\nInformation Sensitivity\nUnder Review\nArchitect\nTodd Walters\nContact(s)\nRichard Schmidt\nAzure AKS\n‚Äã\nPosition\nEmerging\nTags\nazure, cloud-computing\nInformation Sensitivity\nUnder Review\nArchitect\nTodd Walters\nContact(s)\nRichard Schmidt\nAzure API Management\n‚Äã\nPosition\nStandard\nTags\nazure, integration-platforms\nInformation Sensitivity\nRed\nArchitect\nMatthew Van Auwelaer\nContact(s)\nMatthew Van Auwelaer, Abhishek Bose\nCMDB CI\nCI00000036223661\nStandard service offering from\nEnterprise Data\nUse Cases\n‚Äã\nAPI Security (Authentication, Authorization, Threat Protection) for Lilly private and cloud-hosted APIs\nSee Also\n‚Äã\nhttps://collab.lilly.com/sites/ApiManagement\nAzure App Service\n‚Äã\nPosition\nEmerging\nTags\nazure, cloud-computing\nInformation Sensitivity\nRed\nArchitect\nTodd Walters\nContact(s)\nRichard Schmidt\nInformation Sensitivity\nRed\nArchitect\nTodd Walters\nContact(s)\nRichard Schmidt\nAzure AVS\n‚Äã\nPosition\nSpecialized\nTags\nazure, cloud-computing\nInformation Sensitivity\nRed\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAzure Bastion\n‚Äã\nPosition\nEmerging\nTags\nazure, cloud-computing\nInformation Sensitivity\nUnder Review\nArchitect\nTodd Walters\nContact(s)\nRichard Schmidt\nInformation Sensitivity\nUnder Review\nArchitect\nTodd Walters\nContact(s)\nRichard Schmidt\nAzure Batch\n‚Äã\nPosition\nEmerging\nTags\nazure, cloud-computing\nInformation Sensitivity\nRed\nArchitect\nTodd Walters\nContact(s)\nRichard Schmidt\nInformation Sensitivity\nRed\nArchitect\nTodd Walters\nContact(s)\nRichard Schmidt\nAzure Blob / CDN\n‚Äã\nPosition\nEmerging\nTags\nazure, content-management, cloud-computing\nInformation Sensitivity\nUnder Review\nArchitect\nTodd Walters\nContact(s)\nRichard Schmidt\nAzure Blog Storage\n‚Äã\nPosition\nEmerging\nTags\nazure, cloud-computing\nInformation Sensitivity\nRed\nArchitect\nJarret DiPace\nContact(s)\nSriojeet Mukherjee\nInformation Sensitivity\nRed\nArchitect\nJarret DiPace\nContact(s)\nSriojeet Mukherjee\nAzure Bot Services\n‚Äã\nPosition\nEmerging\nTags\nazure, cloud-computing\nInformation Sensitivity\nUnder Review\nArchitect\nGreg Graf\nContact(s)\nJason Yazell\nInformation Sensitivity\nUnder Review\nArchitect\nGreg Graf\nContact(s)\nJason Yazell\nAzure Cognitive Search\n‚Äã\nPosition\nEmerging\nTags\nazure, cloud-computing\nInformation Sensitivity\nUnder Review\nArchitect\nTodd Walters\nContact(s)\nRichard Schmidt\nInformation Sensitivity\nUnder Review\nArchitect\nTodd Walters\nContact(s)\nRichard Schmidt\nAzure Container Apps\n‚Äã\nPosition\nEmerging\nTags\nazure, cloud-computing\nInformation Sensitivity\nRed\nArchitect\nTodd Walters\nContact(s)\nRichard Schmidt\nInformation Sensitivity\nRed\nArchitect\nTodd Walters\nContact(s)\nRichard Schmidt\nAzure Container Registry\n‚Äã\nPosition\nEmerging\nTags\nazure, cloud-computing\nInformation Sensitivity\nRed\nArchitect\nTodd Walters\nContact(s)\nRichard Schmidt\nInformation Sensitivity\nRed\nArchitect\nTodd Walters\nContact(s)\nRichard Schmidt\nAzure Containers\n‚Äã\nPosition\nEmerging\nTags\nazure, cloud-computing\nInformation Sensitivity\nUnder Review\nArchitect\nTodd Walters\nContact(s)\nRichard Schmidt\nAzure Digital Twin\n‚Äã\nPosition\nEmerging\nTags\nazure, cloud-computing\nInformation Sensitivity\nUnder Review\nArchitect\nTodd Walters\nContact(s)\nRichard Schmidt\nInformation Sensitivity\nUnder Review\nArchitect\nTodd Walters\nContact(s)\nRichard Schmidt\nAzure File Storage\n‚Äã\nPosition\nEmerging\nTags\nazure, cloud-computing\nInformation Sensitivity\nRed\nArchitect\nJarret DiPace\nContact(s)\nSriojeet Mukherjee\nInformation Sensitivity\nRed\nArchitect\nJarret DiPace\nContact(s)\nSriojeet Mukherjee\nAzure Functions\n‚Äã\nPosition\nEmerging\nTags\nazure, cloud-computing\nInformation Sensitivity\nUnder Review\nArchitect\nTodd Walters\nContact(s)\nRichard Schmidt\nAzure Key Vault\n‚Äã\nPosition\nEmerging\nTags\nazure, cloud-computing\nInformation Sensitivity\nUnder Review\nArchitect\nTodd Walters\nContact(s)\nRichard Schmidt\nInformation Sensitivity\nUnder Review\nArchitect\nTodd Walters\nContact(s)\nRichard Schmidt\nAzure Logic Apps\n‚Äã\nPosition\nEmerging\nTags\nazure, cloud-computing\nInformation Sensitivity\nUnder Review\nArchitect\nTodd Walters\nContact(s)\nRichard Schmidt\nInformation Sensitivity\nUnder Review\nArchitect\nTodd Walters\nContact(s)\nRichard Schmidt\nAzure Machine Learning\n‚Äã\nPosition\nEmerging\nTags\nazure, cloud-computing\nInformation Sensitivity\nUnder Review\nArchitect\nMalika Mahoui\nContact(s)\nSurya Prakash Rawat\nInformation Sensitivity\nUnder Review\nArchitect\nMalika Mahoui\nContact(s)\nSurya Prakash Rawat\nAzure Machine Learning Studio\n‚Äã\nPosition\nEmerging\nTags\nazure, data-science-tools\nArchitect\nMalika Mahoui\nContact(s)\nSurya Prakash Rawat\nWeb portal in Azure Machine learning used to create end-to-end machine learning projects. It supports low-code and code-first experiences\nNotes\n‚Äã\nAzure ML workspace needs to be created for launching the Studio.\nAzure Maps\n‚Äã\nPosition\nEmerging\nTags\nazure, cloud-computing\nInformation Sensitivity\nUnder Review\nArchitect\nTodd Walters\nContact(s)\nRichard Schmidt\nInformation Sensitivity\nUnder Review\nArchitect\nTodd Walters\nContact(s)\nRichard Schmidt\nAzure Monitor\n‚Äã\nPosition\nEmerging\nTags\nazure, dev-tools\nInformation Sensitivity\nRed\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nEmerging within Azure Public Cloud for Monitoring Azure Resources.\nUse Cases\n‚Äã\nYou would use Azure Monitor to print logs and create monitor metrics of your application.\nAzure NetApp File Storage\n‚Äã\nPosition\nEmerging\nTags\nazure, cloud-computing\nInformation Sensitivity\nUnder Review\nArchitect\nJarret DiPace\nContact(s)\nSriojeet Mukherjee\nInformation Sensitivity\nUnder Review\nArchitect\nJarret DiPace\nContact(s)\nSriojeet Mukherjee\nAzure Purview\n‚Äã\nPosition\nEmerging\nTags\nazure, cloud-computing\nInformation Sensitivity\nUnder Review\nArchitect\nTodd Walters\nContact(s)\nRichard Schmidt\nInformation Sensitivity\nUnder Review\nArchitect\nTodd Walters\nContact(s)\nRichard Schmidt\nAzure Queue Storage\n‚Äã\nPosition\nEmerging\nTags\nazure, cloud-computing\nInformation Sensitivity\nUnder Review\nArchitect\nJarret DiPace\nContact(s)\nSriojeet Mukherjee\nInformation Sensitivity\nUnder Review\nArchitect\nJarret DiPace\nContact(s)\nSriojeet Mukherjee\nAzure Redis Cache\n‚Äã\nPosition\nStandard\nTags\nazure, database-platforms\nArchitect\nMalika Mahoui\nContact(s)\nManuela Elena Garcia\nPreferred solution for in-memory key value NoSQL workloads\nUse Cases\n‚Äã\nHigh performance low latency cache\nReal-time transactions\nAzure Service Fabric\n‚Äã\nPosition\nEmerging\nTags\nazure, cloud-computing\nInformation Sensitivity\nRed\nArchitect\nTodd Walters\nContact(s)\nRichard Schmidt\nInformation Sensitivity\nRed\nArchitect\nTodd Walters\nContact(s)\nRichard Schmidt\nAzure Virtual Machines\n‚Äã\nPosition\nEmerging\nTags\nazure, cloud-computing\nInformation Sensitivity\nUnder Review\nArchitect\nTodd Walters\nContact(s)\nRichard Schmidt\nContentful\n‚Äã\nPosition\nStandard\nTags\ncontent-management\nInformation Sensitivity\nOrange\nArchitect\nJosh Bloxsome\nContact(s)\nMartin Clarke\nPreferred for dynamic content delivery\nUse Cases\n‚Äã\nManage content in a structured way (including documents and files), WYSIWYG Authoring, Consume/deliver static content, Consume/deliver dynamic content, Manage mobile application content, Share content across different areas, Store documents and files supporting an applications (Raw Storage)\nSee Also\n‚Äã\nhttps://lillydev.com/backing%20services/contentful\nDell/EMC Isilon Smartlock (eArchive Service)\n‚Äã\nPosition\nStandard\nTags\ncontent-management\nInformation Sensitivity\nRed\nArchitect\nScott Kuckuck\nContact(s)\nScott Kuckuck\nIsilon with smartlock is the core solution for long-term immutable archiving, replacing Centera/Disk Extender CAS\nUse Cases\n‚Äã\nContent archival (immutable), Archiving of unstructured data only (structured data must be converted to file-based formats - .csv, .xml, etc.)\nSee Also\n‚Äã\nhttps://collab.lilly.com/sites/earchiveservice566\nHeroku\n‚Äã\nPosition\nSpecialized\nTags\ncloud-computing\nInformation Sensitivity\nRed\nArchitect\nJosh Bloxsome\nContact(s)\nPeter Burke\nTargeted for applications that require¬†server-side compute with light weight¬†integrations (DB, Caching, etc.) or a¬†Salesforce connection requirement using the API.\nUse Cases\n‚Äã\nEWI sites, applications that require¬†server-side compute with light weight¬†integrations (DB, Caching, etc.), a¬†Salesforce connection requirement using the Salesforce API.\nHeroku - NewRelic\n‚Äã\nPosition\nSpecialized\nTags\ndev-tools\nInformation Sensitivity\nYellow\nArchitect\nJosh Bloxsome\nContact(s)\nPeter Burke\nMonitoring add-on with Heroku. Helps teams monitor performance of an application within Heroku.\nUse Cases\n‚Äã\nOnly allowed to be used for applications within the Heroku Ecosystem.\nHeroku - PaperTrail\n‚Äã\nPosition\nSpecialized\nTags\ndev-tools\nInformation Sensitivity\nYellow\nArchitect\nJosh Bloxsome\nContact(s)\nPeter Burke\nMonitoring add-on with Heroku. Helps teams see logs of what an application is doing.\nUse Cases\n‚Äã\nOnly allowed to be used for applications within the Heroku Ecosystem.\nInformatica Axon / EDC\n‚Äã\nPosition\nSpecialized\nTags\nintegration-platforms\nInformation Sensitivity\nRed\nArchitect\nLarry Zetzl\nContact(s)\nVishwesh Deshpande\nCMDB CI\nCI00000000422336\nUse for data governance and metadata management for extracting, indexing and cataloging\nUse Cases\n‚Äã\nUnderstand data dictionary, impact analysis on potential change, data lineage and discovery, Map out business definitions and ingest application's metadata repository to integrate to the Enterprise Catalog\nLAN Drives - Application Storage & File-based System Integration\n‚Äã\nPosition\nSpecialized\nTags\nintegration-platforms\nInformation Sensitivity\nRed\nArchitect\nJarret Dipace\nContact(s)\nJarret Dipace\nRefer to LillyNow guidance:\nhttps://now.lilly.com/landingoverview/global-approved-tools-and-services-option-v2/global-approved-tools-and-services-store\nEvaluate the applicability of potentially better options, such as Cloud Object Storage (AWS S3 & Azure Blob Storage), Cloud Network-attached Storage (AWS EFS), Managed File Transfer Services, and eArchive\nUse Cases\n‚Äã\nLAN/SAN storage used directly by applications, instruments, databases, etc. 2) File transfer interfaces supporting the movement of data between systems 3) System-based publication/consumption of files to/from users\nNotes\n‚Äã\nAdditional cloud network-attached storage options are available, including Azure Files and Azure NetApp Files\nSee Also\n‚Äã\nhttps://now.lilly.com/landingoverview/global-approved-tools-and-services-option-v2/global-approved-tools-and-services-store\nOauth (AzureAD)\n‚Äã\nPosition\nStandard\nTags\nazure, cybersecurity\nInformation Sensitivity\nRed\nArchitect\nBrian Wacker\nContact(s)\nJohn Dahler\nAn alternative option to OIDC for federated identity.\nUse Cases\n‚Äã\nPerform authentication with an application\nOpenIDConnect (OIDC) (AzureAD)\n‚Äã\nPosition\nStandard\nTags\nazure, cybersecurity\nInformation Sensitivity\nRed\nArchitect\nRyan Elkins\nContact(s)\nRyan Elkins\nUse as the preferred option for all authentication\nUse Cases\n‚Äã\nPerform authentication with an application\nOpenshift (onPrem)\n‚Äã\nPosition\nSpecialized\nTags\ncloud-computing\nInformation Sensitivity\nRed\nArchitect\nJosh Bloxsome\nContact(s)\nChris Tornatta\nPreferred for OnPrem Kubernetes and Container use cases and for use cases where Low Latency/High Volume on Prem Data is required\nUse Cases\n‚Äã\nCOTS applications that have OpenShift (Kubernetes) as a requirement\nIBM Products (if VMs aren't an option or High Availability/Fault Tolerance is required)\nLevel 3 manufacturing workloads\nZone-3 China (DMZ and Internal)\nRed Hat JBoss workloads\nUse only if Low Latency/High Volume on Prem Data is required\nNotes\n‚Äã\nContainer Strategy:\nhttps://collab.lilly.com/sites/ContainerStrategy\nSee Also\n‚Äã\nhttps://lillydev.com/platforms/openshift/\nSalesforce\n‚Äã\nPosition\nSpecialized\nTags\ncloud-computing\nInformation Sensitivity\nRed\nArchitect\nSivaraman K\nContact(s)\nSivaraman K\nTargeted for applications that require¬†CRM capabilities without targeting the whole enterprise.\nUse Cases\n‚Äã\nVeeva, Other CRM solutions\nNotes\n‚Äã\nContact EDAT if your application have a high consumption of SF licenses\nDeclining ‚Üí Retired\n‚Äã\n.NET Classic & WHO IIS Hosting\n‚Äã\nPosition\nDeclining 2019-12-31\nTags\ncloud-computing\nInformation Sensitivity\nRed\nArchitect\nKarl Mayer\nContact(s)\nMartin Clarke\nAll .NET and Java apps on Web Hosting Operations (WHO) need retirement or migration plans. WHO service retirement has been extended from 31-DEC-2022 to 01-OCT-2024. Exceptions (including business justification and remediation plan) are recommended for apps that need to exist beyond 31-Dec-2022.\nNotes\n‚Äã\nSee your Lead Architect for questions, or to initiate an exception\nAWS CodeCommit\n‚Äã\nPosition\nDeclining\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nDiscuss with Todd Walters before using within the Lilly AWS Organization\nNotes\n‚Äã\nGitHub should be used rather than AWS Code Commit\nAWS CodePipeline\n‚Äã\nPosition\nDeclining\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nInformation Sensitivity\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAWS IaaS - Anzo V4\n‚Äã\nPosition\nDeclining\nTags\naws, cloud-computing\nPosition\nDeclining\nInformation Sensitivity\nArchitect\nContact(s)\nAWS Pipeline v2 (Code Pipeline/Build/Deploy)\n‚Äã\nPosition\nRetired 2023-04-30\nTags\naws, dev-tools\nInformation Sensitivity\nRed\nArchitect\nJosh Bloxsome\nContact(s)\nChris Johnson\nLegacy Tool for Deploying Cloud Native Resources into AWS. Any Continuous Delivery (CD) should occur via AWS Code Pipeline/Build/Deploy.\nNotes\n‚Äã\nPositioned for CD and less CI as apart of CI/CD.\nSee Also\n‚Äã\nhttps://developer.lilly.com/aws/ci%20cd\nAWS Squid\n‚Äã\nPosition\nDeclining\nTags\naws, cloud-computing\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nInformation Sensitivity\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nAzure Pipelines\n‚Äã\nPosition\nDeclining\nTags\nazure, dev-tools\nArchitect\nTodd Walters\nContact(s)\nAmrit Anand\nEmerging Tool within the Azure Cloud Service that teams will be using to deploy Azure resources.\nUse Cases\n‚Äã\nAzure Custom Developed Applications.\nNotes\n‚Äã\nEmerging, right now no offical offering, expect more in the coming months.\nCAS (Centera/Disk Extender)\n‚Äã\nPosition\nRetiring\nTags\ncontent-management\nInformation Sensitivity\nRed\nArchitect\nScott Kuckuck\nContact(s)\nScott Kuckuck\nRetired. Use Isilon with smartlock.\nUse Cases\n‚Äã\nContent archival (immutable)\nNotes\n‚Äã\nUse for content archival. Centera will go out of support  in March 2023 and Disk Extender in December 2020. A replacement project is underway with Isilon Smartlock as the core alternative solution. Archive owners will be contacted to schedule their migration. The target retirement date for CAS (Centera/Disk Extender) is currently June 2021\nCrownpeak\n‚Äã\nPosition\nRetired 2020-12-31\nTags\ncontent-management\nInformation Sensitivity\nOrange\nFuse\n‚Äã\nPosition\nDeclining 2021-04-01\nTags\nintegration-platforms\nInformation Sensitivity\nRed\nArchitect\nLarry Zetzl\nContact(s)\nAndrew Mulder\nFor MDU's TRANCIT project exclusively. A model that uses multiple technologies to construct and control communication between disparate systems.\nUse Cases\n‚Äã\nTransfer large amounts of data between databases / data warehousesTransfer files between internal Lilly systemsTransfer messages between internal Lilly systemsCreate an Enterprise Service BusControl subscriptions to data using API's\nNotes\n‚Äã\nNo support is provided by GIS/EIP\nInformatica Intelligent Cloud Services\n‚Äã\nPosition\nDeclining 2023-05-15\nTags\nintegration-platforms\nInformation Sensitivity\nRed\nArchitect\nLarry Zetzl\nContact(s)\nVishwesh Deshpande\nPreferred solution for GUI-based ETL integrations solutions with data transformation requirements\nUse Cases\n‚Äã\nSetup an ETL Interface, Transform the data as it's transferred, Use ETL to create a DataMart from a data warehouse, Integrate cloud (Salesforce or Veeva) data with an on-premise Lilly system or another cloud environment, Transfer data between databases and validate that source and target are identical\nInformatica MDM / BDM\n‚Äã\nPosition\nRetiring\nTags\nintegration-platforms\nInformation Sensitivity\nRed\nArchitect\nLarry Zetzl\nContact(s)\nVishwesh Deshpande\nUse for Master Data Management and Big Data Management - ETL integrations with data transformation requirements\nUse Cases\n‚Äã\nonPrem management of Master Data (reference data commonly used across the enterprise) and Big Data ingestion, integration, aggregation and transformation by leveraging Hadoop cluster\nInformatica PowerCenter\n‚Äã\nPosition\nRetiring\nTags\nintegration-platforms\nInformation Sensitivity\nRed\nArchitect\nLarry Zetzl\nContact(s)\nVishwesh Deshpande\nCMDB CI\nCI00000000048775\nPreferred legacy solution for ETL integrations with data transformation requirements on-premise\nUse Cases\n‚Äã\nSetup an ETL InterfaceTransform the data as it's transferred, Use ETL to create a DataMart from a data warehouse, Integrate cloud (Salesforce or Veeva) data with an on-premise Lilly system or another cloud environment, Transfer data between databases and validate that source and target are identical\nNotes\n‚Äã\nThe vendor is focusing R&D effort on IICS and not on power center\nJBoss\n‚Äã\nPosition\nDeclining 2019-12-31\nTags\ncloud-computing\nInformation Sensitivity\nRed\nArchitect\nJon Thomas\nContact(s)\nMartin Clarke\nAll JBoss apps should look to be migrated or retired by 31-Dec-2022. Web Hosting support will end on that date. Exceptions (including business justification and remediation plan) are required for apps that need to exist beyond 31-Dec-2022\nNotes\n‚Äã\nSee your Lead Architect for questions, or to initiate an exception\nLayer7 API (SOA) Gateway\n‚Äã\nPosition\nRetiring\nTags\nintegration-platforms\nInformation Sensitivity\nRed\nArchitect\nLarry Zetzl\nContact(s)\nMatt Van Auwelaer\nCMDB CI\nCI00000000046371\nDeprecated platform retiring 1-Dec-2025. Replaced by\nAzure API Management\nUse Cases\n‚Äã\nDo not use\nSee Also\n‚Äã\nOpenshift on AWS\n‚Äã\nPosition\nRetired 2020-05-08\nTags\naws, cloud-computing\nInformation Sensitivity\nRed\nArchitect\nTodd Walters\nPosition\nExited 2020-05-08\nInformation Sensitivity\nRed\nArchitect\nTodd Walters\nContact(s)\nTomcat\n‚Äã\nPosition\nDeclining 2019-12-31\nTags\ncloud-computing\nInformation Sensitivity\nRed\nArchitect\nJon Thomas\nContact(s)\nMartin Clarke\nAll Tomact apps should look to be migrated or retired by 31-Dec-2022. Web Hosting support will end on that date. Exceptions (including business justification and remediation plan) are required for apps that need to exist beyond 31-Dec-2022\nNotes\n‚Äã\nSee your Lead Architect for questions, or to initiate an exception\nVMware Servers (OnPrem)\n‚Äã\nPosition\nDeclining 2021-03-11\nTags\ncloud-computing\nInformation Sensitivity\nRed\nArchitect\nJarret Dipace\nContact(s)\nAdam Esslinger\nUse only for approved exceptions to the Cloud based servers\nUse Cases\n‚Äã\nApproved exceptions: Manufacturing, DMZ, MetroHA, Zone 1 Oracle VMWare clusters\nNotes\n‚Äã\nOnPrem Virtual Server built on VMWare, formerly known as GCRS. Access via the Lilly Cloud Portal. For RED CCI use cases follow up with the security liaison for your business\nSee Also\n‚Äã\nhttps://lillycloud.global.lilly.com/gcrs/home.php\nWebSphere MQ\n‚Äã\nPosition\nDeclining 2023-05-01\nTags\nintegration-platforms\nInformation Sensitivity\nRed\nArchitect\nLarry Zetzl\nContact(s)\nMurali Pitti\nCMDB CI\nCI00000000036866\nUse for file or message-based transfers between on-prem Lilly servers when API not available.\nUse Cases\n‚Äã\nTransfer large amounts of data between databases / data warehouses, Transfer files between internal on-prem Lilly systems, Transfer messages between internal Lilly systems, Transfer files internally adhering to First in First out (FIFO)\nDocument generated from Stack Overflow Enterprise articles (prioritized) and ea_tech.csv (fallback)\nWas this helpful?\nEdit this page\nPrevious\n‚òÅÔ∏è Cloud & Infrastructure\nNext\nüß≠ Cloud Architecture\nAmazon API Gateway\nAmazon CloudWatch RUM\nAmazon Detective\nAmazon Elastic File System (EFS)\nAmazon Guard Duty\nAmazon Location Services (ALS)\nAmazon MQ (Message Queue)\nAmazon OpenSearch\nAmazon Route 53\nAmazon Simple Notification Service (SNS)\nAmazon Simple Queue Service (SQS)\nAmazon Simple Storage Service (S3)\nAmazon Translate\nAnsible Automation Platform (AAP)\nApache Airflow\nApp Connect Enterprise (ACE)\nAspera\nAWS AMI provisioning via Lambda\nAWS App Runner\nAWS AppFlow\nAWS Application Migration Services\nAWS AppSync\nAWS Athena\nAWS Aurora PostgreSQL\nAWS Auto Scaling\nAWS Backup Elastic Block Store (EBS)\nAWS Batch\nAWS Budgets\nAWS Certificate Manager\nAWS Cloud Directory\nAWS Cloud Map\nAWS Cloud WAN\nAWS CloudFormation\nAWS CloudHSM\nAWS CloudShell\nAWS CloudTrail\nAWS CloudWatch\nAWS CodeArtifact\nAWS CodeBuild\nAWS Cognito\nAWS Comprehend\nAWS Comprehend Medical\nAWS Config\nAWS Cost Explorer\nAWS Data Exchange\nAWS Data Pipeline\nAWS Data Transfer\nAWS Database Migration Service (DMS)\nAWS DataSync\nAWS Direct Connect\nAWS Directory Service\nAWS DocumentDB (with MongoDB compatibility)\nAWS DynamoDB\nAWS EBS w/EC2\nAWS EC2\nAWS EC2 Amazon Linux\nAWS EC2 Linux DMZ AMI\nAWS EC2 RHEL\nAWS EC2 Ubuntu DMZ AMI\nAWS EC2 Ubuntu Linux\nAWS EC2 Ubuntu Version 18.04 AMI\nAWS EC2 Windows\nAWS ECR (Container Registry)\nAWS ECS / Fargate\nAWS EKS / Fargate\nAWS Elastic Disaster Recovery\nAWS ElastiCache Redis\nAWS ELB\nAWS EMR\nAWS EventBridge\nAWS Firewall Manager\nAWS FSx\nAWS GameLift\nAWS Glacier\nAWS Global Accelerator\nAWS Glue (Catalog)\nAWS Glue Crawler\nAWS Glue DataBrew\nAWS Glue ETL\nAWS Glue Studio\nAWS IaaS - DLAB\nAWS IAM\nAWS Inspector\nAWS IoT\nAWS IoT Core\nAWS IoT Device Manager\nAWS IoT Events\nAWS IoT Greengrass\nAWS IoT SiteWise\nAWS IoT TwinMaker\nAWS Kendra\nAWS Key Management Service (KMS)\nAWS Kinesis Data Streaming\nAWS Kinesis Firehose\nAWS Kinesis Video Streams\nAWS Lake Formation\nAWS Lambda\nAWS License Manager\nAWS Lightsail\nAWS Lookout for Metrics\nAWS Managed Streaming for Apache Kafka\nAWS Mechanical Turk\nAWS Migration Hub Refactor Spaces\nAWS Neptune\nAWS Polly\nAWS Private Marketplace\nAWS RoboMaker\nAWS S3 & CloudFront for SPAs\nAWS SageMaker\nAWS Secrets Manager\nAWS Security Hub\nAWS Service Catalog\nAWS SES (Simple Email Service)\nAWS Shield\nAWS SimbleDB\nAWS Simple Workflow Service\nAWS Snowball\nAWS Step Functions\nAWS Storage Gateway\nAWS Systems Manager\nAWS Transfer for FTPS\nAWS Transfer for SFTP\nAWS VPC\nAWS WAF\nAWS X-Ray\nAxway - eCommerce\nAzure AI Services\nAzure AKS\nAzure API Management\nAzure App Service\nAzure AVS\nAzure Bastion\nAzure Batch\nAzure Blob / CDN\nAzure Blog Storage\nAzure Bot Services\nAzure Cognitive Search\nAzure Container Apps\nAzure Container Registry\nAzure Containers\nAzure Digital Twin\nAzure File Storage\nAzure Functions\nAzure Key Vault\nAzure Logic Apps\nAzure Machine Learning\nAzure Machine Learning Studio\nAzure Maps\nAzure Monitor\nAzure NetApp File Storage\nAzure Purview\nAzure Queue Storage\nAzure Redis Cache\nAzure Service Fabric\nAzure Virtual Machines\nContentful\nDell/EMC Isilon Smartlock (eArchive Service)\nHeroku\nHeroku - NewRelic\nHeroku - PaperTrail\nInformatica Axon / EDC\nLAN Drives - Application Storage & File-based System Integration\nOauth (AzureAD)\nOpenIDConnect (OIDC) (AzureAD)\nOpenshift (onPrem)\nSalesforce\nDeclining ‚Üí Retired\n.NET Classic & WHO IIS Hosting\nAWS CodeCommit\nAWS CodePipeline\nAWS IaaS - Anzo V4\nAWS Pipeline v2 (Code Pipeline/Build/Deploy)\nAWS Squid\nAzure Pipelines\nCAS (Centera/Disk Extender)\nCrownpeak\nFuse\nInformatica Intelligent Cloud Services\nInformatica MDM / BDM\nInformatica PowerCenter\nJBoss\nLayer7 API (SOA) Gateway\nOpenshift on AWS\nTomcat\nVMware Servers (OnPrem)\nWebSphere MQ\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 6,
    "depth": 3,
    "crawled_at": "2026-02-25T10:08:40.504746"
  },
  "https://techhq.dc.lilly.com/docs/solution/cloud/cloud_architecture": {
    "url": "https://techhq.dc.lilly.com/docs/solution/cloud/cloud_architecture",
    "title": "üß≠ Cloud Architecture | Tech HQ",
    "description": "Stack Overflow Article: üß≠ Cloud Architecture",
    "h1": [
      "Cloud Architecture"
    ],
    "h2": [
      "Terminology in Lilly Cloud Architecture‚Äã",
      "AWS‚Äã",
      "Azure‚Äã",
      "Google Cloud Platform (GCP)‚Äã",
      "Governance and Decision Frameworks‚Äã"
    ],
    "h3": [
      "Landscape‚Äã",
      "Important Constraints‚Äã",
      "Deployment Patterns‚Äã",
      "Landscape‚Äã",
      "Important Constraints‚Äã",
      "Deployment Patterns‚Äã",
      "Important Constraints‚Äã"
    ],
    "text_content": "üß≠ Cloud Architecture | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüóÑÔ∏è Cloud & Infrastructure BLT\nüß≠ Cloud Architecture\nüß≠ Cloud FinOps\nüß≠ Container Services\nüß≠ Networking Services\nüß≠ Workflow & Orchestration\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\n‚òÅÔ∏è Cloud & Infrastructure\nüß≠ Cloud Architecture\nOn this page\nCloud Architecture\nSynced daily from Stack Overflow\nSolution Guide\nLifecycle: Draft\nLast Update: 2025-09-30\nCapability Owner: Todd Walters\nEBA Lead: Todd Walters\nContributors & Reviewers: Amrit Anand, Richard Schmidt, Abhimanyu Rawat, Todd Walters, Karl Mayer\nTechHQ cloud articles provide summary-level solution guides to help teams quickly understand architectural patterns, governance models, and deployment strategies.\nCloud Architecture & Engineering Deeper Dives...\nFor deeper technical documentation, platform-specific guidance, and access to tooling resources, Lilly‚Äôs centralized\nCloud Library\nat\nhttps://cloud.lilly.com\nis the authoritative source. It includes detailed information on AWS, Azure, and GCP platforms, FinOps practices, developer tooling, AI offerings, and enterprise cloud strategy. Use it as your starting point for cloud enablement across the organization.\nTerminology in Lilly Cloud Architecture\n‚Äã\nDX\n‚Äì Data center connected environments without Internet-facing endpoints\nNoDX\n‚Äì Internet-facing environments without data center connectivity\nPrivate Cloud\n‚Äì Lilly data center workloads\nfor a more complete explaination of DX vs NoDX Terminology reference the following Lilly Flow Article:\nAWS Security Guardrails and Restrictions\nAWS\n‚Äã\nLandscape\n‚Äã\nAvailable Regions\nPrimary region\n:\nus-east-2\n‚Äî closest to Indianapolis\nOther approved regions\n:\nus-east-1\n(N. Virginia),\nus-west-1\n(N. California),\nus-west-2\n(Oregon),\nap-northeast-1\n(Tokyo),\neu-west-2\n(London)\nImportant DX accounts\n:\nEDB\n‚Äì Enterprise Data Backbone for development, QA, and production\nSAP\n- Hosts critical SAP ERP infrastructure.\nInfoHub\n‚Äì Hosts critical infrastructure like regmol-redqa-db and livedesignwk\nHanger (CATS)\n‚Äì Modular content and API gateway deployments\nBioinformatics DX\n‚Äì Genomics workflows with S3, Batch, Parallel Cluster, and ECR\nImportant NoDX accounts\n:\nGS\n‚Äì LillyNow\nGS\n- iTravel\nDigital Core\n- TPET\nMQ\n- One QMS\nImportant Constraints\n‚Äã\nService Control & Boundary Policies\nDX accounts governed by stricter boundary policies\nNoDX accounts require additional governance for public internet exposure\nFully Denied Services\nApp Runner\n(apprunner:*)\nAmazon Chime\n(chime:*)\nAmazon Cloud Directory\n(clouddirectory:*)\nAWS CloudHSM\n(cloudhsm:*)\nAmazon GameLift\n(gamelift:*)\nAWS IQ\n(iq:*)\nAmazon Mechanical Turk\n(mechanicalturk:*)\nAWS Outposts\n(outposts:*)\nAmazon SimpleDB\n(sdb:*)\nAmazon Simple Workflow Service (SWF)\n(swf:*)\nAmazon WorkDocs\n(workdocs:*)\nAWS WAM (Workspaces Application Manager)\n(wam:*)\nAmazon WorkMail\n(workmail:*)\nAmazon Workspaces\nworkspaces:*)\nPartially Denied Services\nLambda\nCreate and Update FunctionUrlConfig API calls are blocked\nRolesAnywhere\nCreate, Update and Delete of Trust Anchor API calls are blocked\nKendra\nCreate Experience API call is blocked\nAWS Public Marketplace\nAccept, Cancel, Complete, Create, Put, Reject, Start, Subscribe, Unsubscribe, Update API calls are blocked\nEC2\nAll Network/VPC related create, update, delete API calls are blocked\nUse of Public AMI's are denied\nDeployment of non-encrypted EC2 instance volumes are denied\nBedrock\nAPI only allowed in AI Use Case Approved AWS Accounts\nAPI calls allowed to only approved foundational models\nS3\nDeny S3 Bucket Policy Create, Update, Delete API calls are blocked\nIAM\nCreation of IAM Users is denied\nCreation of SAML and OpenID Connect Providers is denied\nPassRole to Federated Roles is denied\nDeployment Patterns\n‚Äã\nHanger/CATS\n‚Äì Orchestrates DX deployments\nCloud Central Vending Machine\n‚Äì Provisions S3 buckets, databases\nInfrastructure as Code\n‚Äì GitHub Actions + CloudFormation\nAzure\n‚Äã\nLandscape\n‚Äã\nAzure follows the\nCloud Adaption Framework\n(CAF) of Microsoft and applies the Enterprise Scale\nLanding Zone\narchitecture.\nAvailable Regions\nPrimary region\n:\nCentral US\n‚Äî closest to Indianapolis\nOther approved regions\n:\nNorth Central US\nUK South\n/\nUK West\nJP East\n/\nJP West\nFor Special Use-cases\nEast US\n/\nEast US 2\n/\nWest US\nImportant DX subscriptions\n:\nAzure VMWare\nCitrix\nData Protect\nCloud DR\nAPIM\nConnectivity\nIAM\nMQ Science Engine\nMQ GMDF\nMQ IoT\nMQ Labvantage\nMQ Qrius\nMQ Mulesoft\nLillyNowKB\nED SQL DB\nED SSIS\nSPE Cortex\nImportant NoDX subscriptions\n:\n(To be defined)\nChatNow\nImportant Constraints\n‚Äã\nDX subscriptions bypass Bastion host requirements\nNoDX subscriptions require Bastion hosts and DNS policy exemptions\nBoth Environments are governed through\nEnterprise Policy as Code\n(EPAC)\nAccess to Subscriptions is granted through membership of explicit Access Packages via\nMicrosoft My Access\nAll deployments need to happen through a pipeline executing Infrastructure as Code, leverage linting and coding best practices.\n! MESSAGING - \"To be discussed\" since Email services are supposed to be used either by AWS Service or M365 Graph access\nDeployment Patterns\n‚Äã\nCloud Central Vending\nProvision individual subscriptions (SBX / DEV / QA / PROD)\nAzure One-Click with DEV/QA/PROD Subscriptions tied to a single GitHub Repo with OIDC connected Service Principal for CI/CD.\nProvision Databases & Analytics Services\nProvision NetApp Storage\nProvision Compute - Windows & soon RHEL\nHangar -\nContainerized Solutions in Azure RedHat OpenShift\nInfrastructure as Code\nGitHub Actions\nBicep -> Curated\nBicep Module Repository\nbased on\nAzure-Verified-Modules (AVM)\nGoogle Cloud Platform (GCP)\n‚Äã\nImportant Constraints\n‚Äã\nSee the Google Article on TechHQ for GCP Positioning:\nhttps://techhq.dc.lilly.com/docs/solution/ai/positioning/google_positioning\nGovernance and Decision Frameworks\n‚Äã\nCAST\n‚Äì Cloud Architecture Standards Team\nSAE\n‚Äì Secure Architecture & Engineering\nWas this helpful?\nTags:\ncloud\nsolution-guide\ntechhq\nEdit this page\nPrevious\nüóÑÔ∏è Cloud & Infrastructure BLT\nNext\nüß≠ Cloud FinOps\nTerminology in Lilly Cloud Architecture\nAWS\nLandscape\nImportant Constraints\nDeployment Patterns\nAzure\nLandscape\nImportant Constraints\nDeployment Patterns\nGoogle Cloud Platform (GCP)\nImportant Constraints\nGovernance and Decision Frameworks\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 8,
    "depth": 3,
    "crawled_at": "2026-02-25T10:08:44.441616"
  },
  "https://techhq.dc.lilly.com/docs/solution/cloud/cloud_finops": {
    "url": "https://techhq.dc.lilly.com/docs/solution/cloud/cloud_finops",
    "title": "üß≠ Cloud FinOps | Tech HQ",
    "description": "Stack Overflow Article: üß≠ Cloud FinOps",
    "h1": [
      "üß≠ Cloud FinOps"
    ],
    "h2": [
      "AWS‚Äã",
      "Microsoft Azure‚Äã",
      "Google Cloud‚Äã"
    ],
    "h3": [],
    "text_content": "üß≠ Cloud FinOps | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüóÑÔ∏è Cloud & Infrastructure BLT\nüß≠ Cloud Architecture\nüß≠ Cloud FinOps\nüß≠ Container Services\nüß≠ Networking Services\nüß≠ Workflow & Orchestration\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\n‚òÅÔ∏è Cloud & Infrastructure\nüß≠ Cloud FinOps\nOn this page\nüß≠ Cloud FinOps\nSynced daily from Stack Overflow\nSolution Guide\nLifecycle: Draft\nLast Update: 2025-09-25\nCapability Owner: Anjali Oleksy\nEBA Lead: Todd Walters\nContributors & Reviewers: Anjali Oleksy\nA guide for implementing cost transparency and optimization dashboards across AWS and Azure using enterprise-grade tools aligned to cloud Finance and DevOps practices.\nNote: Refer to\nthe landing page for access to all dashboard links\nUse Case\nTech Recommendation\nPositioning\nComplexity\nData Ingestion\nOwning Org/Team\nNext Step\nRole-Based Cloud Spend Visibility in Native Consoles\nAWS & Azure Cost Management\nStrategic\nLow\nIAM roles, Cost Explorer (AWS), Azure Cost Analysis, APIs, Power BI/Fabric, Power BI Workspace access\nCloud\nGranular AWS Cost Analysis by Service, Account, Cost Center\nAWS QuickSight\nSpecialized\nMedium\nAWS Cost Explorer, Athena, Glue, FOCUS 1.0, Cloud Intelligence Dashboard, Fabric Data Gen2 Flow, Pipeline, Notebook\nCloud\n1) Connect QuickSight to AWS CUR via Glue/Athena 2) Connect Power BI to Transformed AWS Data Product in EDB from Redshift Database 3)Schedule refreshes\nAzure High-Level Spend Visibility for Cost Center Owners\nPower BI\nStandard\nLow\nAzure Cost Management exports, FinOps Toolkit, Power BI Service, Fabric Lakehouse\nCloud\nUse FinOps Toolkit templates in Power BI to construct the Azure detailed dashboards\nAWS Unit Economics Dashboard\nPower BI\nStrategic\nHigh\nCUR + business unit tagging\nCloud\nRefine tagging strategy and validate cost drivers - use the\nPower BI Dashboard\nAWS Anomaly Dashboard\nAWS QuickSight\nSpecialized\nHigh\nAnamoly API export from AWS + anomaly detection logic\nCloud\nSet Threshold, alerting rules and alerts - use the\nAWS Dashboard\nExecutive view of AWS Savings Plan\nPower BI\nStrategic\nMedium\nCUR, Savings Plans usage data, Savings plan coverage data, Unit Cost data\nCloud\nReview SP coverage and optimize purchase strategy - use the\nPower BI Dashboard\nS3 Storage Lens Dashboard\nPower BI\nSpecialized\nMedium\nS3 Storage Lens metrics API\nCloud\nReview bucket-level insights and apply lifecycle policies - use the\nPower BI Dashboard\nAzure Department-Level Cost Accountability\nPower BI\nStrategic\nMedium\nCloud\nAutomate dashboard completely by refining allocation logic - use the\nPower BI Dashboard\nAzure Unit Economics Dashboard\nPower BI\nStrategic\nHigh\nAzure cost management exports\nCloud\nRefine tagging strategy and validate cost drivers - use the\nPower BI Dashboard\nAWS\n‚Äã\nSee the LillyFlow article\nAWS Cost & Usage Visibility\nMicrosoft Azure\n‚Äã\n\"\nI want to get granular and scoped access (all the subscriptions, resources groups, etc. I can Read) AND I am able to use the Azure portal to access Azure Cost Management at the scope of my desired cost visibility.\"\nLogin to\nAzure porta\nl.\nNavigate to the Cost Analysis service and filter/slice as desired.\n\"I want cost at a chargeback, cost center level granularity.\"\nAz\nure PBI Chargeback Dashboard\n- This dashboard gives Cost Center level granularity.\nGoogle Cloud\n‚Äã\nReach out to\nCloud_FinOps_Help@lists.lilly.com\nto get cost visibility.\nFor more information about cost data visibility options in progress or submit a feature request or problem statement for the Cloud FinOps team, contact\nCloud_FinOps_Help@lists.lilly.com\n.\nWas this helpful?\nTags:\ncloud\nsolution-guide\ntechhq\nEdit this page\nPrevious\nüß≠ Cloud Architecture\nNext\nüß≠ Container Services\nAWS\nMicrosoft Azure\nGoogle Cloud\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 7,
    "depth": 3,
    "crawled_at": "2026-02-25T10:08:46.637928"
  },
  "https://techhq.dc.lilly.com/docs/solution/cloud/container_services": {
    "url": "https://techhq.dc.lilly.com/docs/solution/cloud/container_services",
    "title": "üß≠ Container Services | Tech HQ",
    "description": "Stack Overflow Article: üß≠ Container Services",
    "h1": [
      "Container Services"
    ],
    "h2": [
      "Footnotes‚Äã"
    ],
    "h3": [],
    "text_content": "üß≠ Container Services | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüóÑÔ∏è Cloud & Infrastructure BLT\nüß≠ Cloud Architecture\nüß≠ Cloud FinOps\nüß≠ Container Services\nüß≠ Networking Services\nüß≠ Workflow & Orchestration\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\n‚òÅÔ∏è Cloud & Infrastructure\nüß≠ Container Services\nContainer Services\nSynced daily from Stack Overflow\nSolution Guide\nLifecycle: Draft\nLast Update: 2024-12-16\nCapability Owner: Josh Bloxsome\nEBA Lead: Karl Mayer\nContributors & Reviewers: Josh Bloxsome, Chris Tornatta, Ross Grinvalds, Todd Walters, Sonia Gurdian, Karl Mayer\nA guide for common containerization and container orchestration use cases.\nLocal Containerization Options\nFor local workstation and container development options, see\nWhat is our position on using Docker Desktop?\nUse Case(s)\nTech Recommendation\nPositioning\nComplexity\nKey Integrations\nOwning Org/Team\nNext Step\nInternal apps & APIs; containerized 'high compute' workloads\nCATS\n(backed by\nAWS EKS\n)\nStrategic Core\nLow\nAWS\n1\n; cloud resources; HTTPS connectivity to on-prem resources\nSPE\nCATS\nself-service guide;\nfor tech questions\nPublic-facing apps/APIs; multi-region deployment\nKubed\n(backed by\nAWS ROSA\n)\nStrategic Core\nLow\nAWS\nSPE\nKubed\nself-service guide;\nfor tech questions\nApp/API/workloads needing full on-prem connectivity\nOpenShift\nStandard\nMedium\non-prem & cloud resources\nSPE\nfully-managed container orchestration with serverless execution\nAWS ECS Fargate + AWS ECR\nSpecialized\nHigh\nAWS\nCloud\nCAST architecture review\nYour own Kubernetes cluster with serverless execution\nAWS EKS Fargate + AWS ECR\nSpecialized\nHigh\nAWS\nCloud\nCAST architecture review\nLambda functions packaged as container images (serverless workloads with custom runtimes)\nAWS Lambda + AWS SAM\nSpecialized\nMedium\nAWS\nCloud\nOpenShift container workloads in Azure\nAzure Red Hat OpenShift\nEmerging\nHigh\nAzure\n2\nTBD\nSPE\nOther containerization in Azure\nAzure Container Services\nEmerging\nHigh\nAzure TBD\nSPE & Cloud\nwarning\nUnless vendor-specified, container workloads on self-managed, VM-based Docker or Kubernetes is not recommended.\nFootnotes\n‚Äã\nManage access to AWS resources with\nAWS IAM\n.\n‚Ü©\nManage access to Azure resources with\nAzure Entra ID\n.\n‚Ü©\nWas this helpful?\nTags:\ncloud\nsolution-guide\ntechhq\nEdit this page\nPrevious\nüß≠ Cloud FinOps\nNext\nüß≠ Networking Services\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 6,
    "depth": 3,
    "crawled_at": "2026-02-25T10:08:49.026331"
  },
  "https://techhq.dc.lilly.com/docs/solution/cloud/networking_services": {
    "url": "https://techhq.dc.lilly.com/docs/solution/cloud/networking_services",
    "title": "üß≠ Networking Services | Tech HQ",
    "description": "Stack Overflow Article: üß≠ Networking Services",
    "h1": [
      "üß≠ Networking Services"
    ],
    "h2": [
      "Services Provided‚Äã",
      "Knowledge Articles‚Äã"
    ],
    "h3": [],
    "text_content": "üß≠ Networking Services | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüóÑÔ∏è Cloud & Infrastructure BLT\nüß≠ Cloud Architecture\nüß≠ Cloud FinOps\nüß≠ Container Services\nüß≠ Networking Services\nüß≠ Workflow & Orchestration\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\n‚òÅÔ∏è Cloud & Infrastructure\nüß≠ Networking Services\nOn this page\nüß≠ Networking Services\nSynced daily from Stack Overflow\nSolution Guide\nLifecycle: Draft\nLast Update: 2025-10-02\nCapability Owner: Ryan Castor\nEBA Lead: Todd Walters\nContributors & Reviewers:\nEnterprise Connectivity delivers secure, reliable, and innovative network solutions that empower our global workforce and enable seamless connectivity. Enterprise Connectivity (EC) is member of the Digital Core family in the delivery of architectural and infrastructure services that enable information technology solutions across the enterprise.\nVisit the\nEnterprise Connectivity (EC) Landing Page\nfor Service Requests, Change Requests, Incidents, and Knowledge Resources.\nServices Provided\n‚Äã\nEnterprise Connectivity provides the following services across all Lilly geographies:\nWide Area network (WAN)\nLocal Area Network (LAN)\nWireless (WLAN)\nDomain Name System (DNS)\nDynamic Host Configuration Protocol (DHCP)\nInternet Protocol address (IP)s\nNetwork Security Policy Management (Firemon)\nNetwork Visibility in terms of SPAN/TAP traffic (Gigamon)\nNetwork Access Control (Cisco ISE)\nApplication Micro-segmentation (VMware NSX)\nLoad Balancing (F5)\nNext Generation Firewalls (Palo Alto)\nLAN Segmentation (ISE)\nAudio Hardware (Audicodes)\nKnowledge Articles\n‚Äã\nRefer to the\nNetwork Services Knowledge Articles\nlist for guidance on the following and more:\nGuidance on firewall requirements and access lists for CHG submissions\nStandardized templates required for submitting catalog items related to DHCP scopes, subnets, IP addresses, aliases, and miscellaneous records\nInstructions for requesting network port activation or a new network port; enabling an existing port or applying a static IP address\nReference article for submitting the\nIndianapolis Telecom Room Card Access Request\ncatalog item\nInformation on\nAppropriate Network Access (ANA)\n, which restricts wired network access to trusted devices\nSupport for ANA checks to ensure correct Windows Group Policy settings are applied to end-user machines\nTroubleshooting guidance for ANA-related issues such as ‚ÄúUnidentified Network / No Network Access‚Äù when connecting non-Lilly devices\nEC-only instructions for\nLiveAction Inventory Maintenance\nand\nNetwork Utilization Reports\nCriteria to determine whether a project qualifies as a Large or Small Project, and whether it requires Project Management or can be tracked solely within RITM\nReference information for\nSite ID\nassociated with Eli Lilly locations and Global Network Services-managed equipment\nIf you need a specific procedure, escalation path, or architecture guidance for any of the services above, open a Service Request from the EC Landing Page or contact the Enterprise Connectivity team via the internal support channels.\nWas this helpful?\nTags:\ncloud\nsolution-guide\ntechhq\nEdit this page\nPrevious\nüß≠ Container Services\nNext\nüß≠ Workflow & Orchestration\nServices Provided\nKnowledge Articles\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 5,
    "depth": 3,
    "crawled_at": "2026-02-25T10:08:51.231362"
  },
  "https://techhq.dc.lilly.com/docs/solution/cloud/workflow_orchestration": {
    "url": "https://techhq.dc.lilly.com/docs/solution/cloud/workflow_orchestration",
    "title": "üß≠ Workflow & Orchestration | Tech HQ",
    "description": "Stack Overflow Article: üß≠ Workflow & Orchestration",
    "h1": [
      "Workflow and Orchestration"
    ],
    "h2": [
      "Key Areas‚Äã"
    ],
    "h3": [],
    "text_content": "üß≠ Workflow & Orchestration | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüóÑÔ∏è Cloud & Infrastructure BLT\nüß≠ Cloud Architecture\nüß≠ Cloud FinOps\nüß≠ Container Services\nüß≠ Networking Services\nüß≠ Workflow & Orchestration\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\n‚òÅÔ∏è Cloud & Infrastructure\nüß≠ Workflow & Orchestration\nOn this page\nWorkflow and Orchestration\nSynced daily from Stack Overflow\nSolution Guide\nLifecycle: Living Document\nLast Update: 2025-09-08\nCapability Owner: Greg Graf\nEBA Lead: Sumit Bhardwaj\nContributors & Reviewers: Sumit Bhardwaj, Alex Wasik\nA guide with common automation use cases for Workflow and Orchestration, using services that can handle up to Red Lilly information. Workflow and orchestration technologies enable automation of business processes, system integrations, and task management across the enterprise. Our approach aligns technologies to specific use cases based on complexity, scale, and requirements.\nKey Areas\n‚Äã\nTask Automation: Focuses on automating discrete, repeatable tasks to improve efficiency and reduce manual effort\nBusiness Process Automation (BPA): Coordinates multiple tasks into complete business processes with defined workflows\nSystem Orchestration: Manages interactions between multiple systems and services in a cohesive flow\nUse Case/Scenario\nTech Recommendation\nPositioning\nComplexity\nKey Integrations\nOwning Org/Team\nNext Step\nTask Based / Orchestration / Business Process Automation, Low Complexity\nPowerAutomate\nStrategic Core\nLow\nPre-made built-in connectors\nEnterprise Automation\nSubmit Request\nOrchestration/Choreography, Low Complexity\nAutomation Anywhere\nStandard\nMedium\nGeneric platform-provided connectors\nEnterprise Automation\nSubmit Request\nRPA, Business Process Automation, Any Complexity\nAWS Lambda & StepFunctions\nSpecialized\nMedium\nSelf-provided connectors\nEnterprise Automation\nSubmit Request\nPro Code Business Process Orchestration, High Complexity\nTemporal\nSpecialized\nHigh\nGeneric platform-provided connectors\nEnterprise Automation\nSubmit Request\nTask Based / Orchestration / Business Process Automation, High Complexity\nIBM BAW\nStandard\nHigh\nSelf-provided connectors; legacy system integrations available\nEnterprise Automation\nSubmit Request\nLow Complexity Automation\nFor simple task automation and low-code/no-code solutions, PowerAutomate provides the fastest time-to-value with built-in connectors for most common enterprise systems.\nWas this helpful?\nTags:\ncloud\nsolution-guide\ntechhq\nEdit this page\nPrevious\nüß≠ Networking Services\nNext\nüîê Cybersecurity\nKey Areas\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 3,
    "depth": 3,
    "crawled_at": "2026-02-25T10:08:53.438897"
  },
  "https://techhq.dc.lilly.com/docs/solution/cybersecurity/cybersecurity-blt": {
    "url": "https://techhq.dc.lilly.com/docs/solution/cybersecurity/cybersecurity-blt",
    "title": "üóÑÔ∏è Cybersecurity BLT | Tech HQ",
    "description": "The Cybersecurity Big List of Technology (BLT) is maintained using data from the Enterprise Architecture Tech Positioning Database and LillyFlow, with AI-assisted updates. While we aim for accuracy, please verify critical information and contact the EBA team with questions or corrections.",
    "h1": [
      "üóÑÔ∏è Cybersecurity BLT"
    ],
    "h2": [
      "Checkmarx‚Äã",
      "Cloud Custodian‚Äã",
      "Kerberos (via AD)‚Äã",
      "Microsoft 365 Defender‚Äã",
      "Palo Alto Prisma‚Äã",
      "Prisma Cloud (Palo Alto)‚Äã",
      "Restricted VPN Thick Client (Cisco VPN)‚Äã",
      "SAML 2.0‚Äã",
      "Secure LDAP: Active Directory‚Äã",
      "Secure LDAP: Virtual Directory (RadiantLogic)‚Äã",
      "SSL VPN (LillyConnect - QuickConnect F5)‚Äã",
      "VPN Tunnel (Cisco VPN)‚Äã",
      "Web Application Proxy‚Äã",
      "Declining ‚Üí Retired‚Äã"
    ],
    "h3": [
      "B2B VPN Connections‚Äã",
      "EDS‚Äã",
      "LSSO‚Äã",
      "NTLM‚Äã",
      "Unencrypted LDAP (AD)‚Äã",
      "VPN on Demand (MobileIron)‚Äã"
    ],
    "text_content": "üóÑÔ∏è Cybersecurity BLT | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüóÑÔ∏è Cybersecurity BLT\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nüîê Cybersecurity\nüóÑÔ∏è Cybersecurity BLT\nOn this page\nüóÑÔ∏è Cybersecurity BLT\nThe Cybersecurity Big List of Technology (BLT) is maintained using data from the Enterprise Architecture Tech Positioning Database and LillyFlow, with AI-assisted updates. While we aim for accuracy, please verify critical information and contact the EBA team with questions or corrections.\nCan't find a technology?\nUse TechHQ's search feature to look across all technology documentation. For questions or to suggest additions to this list, contact the\nEnterprise Business Architecture team\n.\nCheckmarx\n‚Äã\nPosition\nSpecialized\nTags\ndev-tools\nInformation Sensitivity\nUnder Review\nArchitect\nJoshua Day\nContact(s)\nJoshua Day\nSpecialized  Static Application Security Testing (SAST) Tool for Mobile & R Software Applications.\nUse Cases\n‚Äã\nMobile + R SAS Scans. E.G any language that GHAS doesn't support\nNotes\n‚Äã\nCurrently going through contract renegotiations\nSee Also\n‚Äã\nhttps://developer.lilly.com/tools/checkmarx%23checkmarx-about-checkmarx\nCloud Custodian\n‚Äã\nPosition\nStandard\nTags\ncybersecurity\nInformation Sensitivity\nOrange\nArchitect\nRyan Elkins\nContact(s)\nPhil Bristow\nKerberos (via AD)\n‚Äã\nPosition\nSpecialized\nTags\ncybersecurity\nInformation Sensitivity\nOrange\nArchitect\nRyan Elkins\nContact(s)\nRyan Elkins\nMicrosoft 365 Defender\n‚Äã\nPosition\nStandard\nTags\ncybersecurity\nInformation Sensitivity\nOrange\nArchitect\nRyan Elkins\nContact(s)\nJD Guckenberger\nPalo Alto Prisma\n‚Äã\nPosition\nEmerging\nTags\ncybersecurity\nInformation Sensitivity\nOrange\nArchitect\nJosh Bloxsome\nContact(s)\nJoshua R Day\nPalo Alto Prisma is the Container Security tool provided by Lilly. This tool is currently being stood up in partnership between Enterprise DevOps, Automation and Tools and Information Security. It will allow for runtime integration, image scanning and integration into the CI/CD pipelines.\nUse Cases\n‚Äã\nImage Scanning\nRuntime Integration\nCI/CD Integration\nPrisma Cloud (Palo Alto)\n‚Äã\nPosition\nSpecialized\nTags\ncybersecurity\nInformation Sensitivity\nRed\nArchitect\nDoug Gorr\nContact(s)\nDoug Gorr\nUsed in Digital Health - Contact Doug Gorr. Will likely expand coverage across enterprise.\nUse Cases\n‚Äã\nProvide container security, governance, and runtime analysis.\nRestricted VPN Thick Client (Cisco VPN)\n‚Äã\nPosition\nSpecialized\nTags\ncybersecurity\nInformation Sensitivity\nRed\nArchitect\nRyan Elkins\nContact(s)\nRyan Elkins\nThis method should only be used if a non-Lilly managed endpoint requires access to a system on the internal Lilly network and cannot leverage another method.\nUse Cases\n‚Äã\nNon-Lilly managed endpoint\nSAML 2.0\n‚Äã\nPosition\nStandard\nTags\ncybersecurity\nInformation Sensitivity\nRed\nArchitect\nRyan Elkins\nContact(s)\nRyan Elkins\nUse as a secondary preferred option for application authentication.\nUse Cases\n‚Äã\nPerform authentication with an application\nSecure LDAP: Active Directory\n‚Äã\nPosition\nSpecialized\nTags\ncybersecurity\nInformation Sensitivity\nRed\nArchitect\nRyan Elkins\nContact(s)\nRyan Elkins\nSecure LDAP: Virtual Directory (RadiantLogic)\n‚Äã\nPosition\nStandard\nTags\ncybersecurity\nInformation Sensitivity\nOrange\nArchitect\nRyan Elkins\nContact(s)\nRyan Elkins\nVirtual directory services through RadiantLogic is provided for when secure LDAP is needed. DMZ usage will require risk analysis and acceptance\nUse Cases\n‚Äã\nPerform authentication with an application\nSee Also\n‚Äã\nhttps://collab.lilly.com/sites/IdM/Web%20Part%20Pages/VDS.aspx\nSSL VPN (LillyConnect - QuickConnect F5)\n‚Äã\nPosition\nSpecialized\nTags\ncybersecurity\nVPN Tunnel (Cisco VPN)\n‚Äã\nPosition\nStandard\nTags\ncybersecurity\nInformation Sensitivity\nRed\nArchitect\nRyan Elkins\nContact(s)\nRyan Elkins\nUse by Lilly Employees and External Workers that are on boarded with Lilly credentials when using a Lilly managed business computer\nUse Cases\n‚Äã\nLilly Business ComputerOnBoarded users with Lilly Credentials\nWeb Application Proxy\n‚Äã\nPosition\nStandard\nTags\ncybersecurity\nInformation Sensitivity\nRed\nArchitect\nRyan Elkins\nContact(s)\nRyan Elkins\nUse for remote access to virtual applications and desktops. Access to a Virtual Desktop should only be used when a user has a business justification to participate on the network as if they were onsite with a Lilly managed endpoint.\nUse Cases\n‚Äã\nVirtual desktop and applications accessible from Lilly and non-Lilly managed computers.\nNotes\n‚Äã\nCitrix/XenApp\nSee Also\n‚Äã\nhttps://virtual.lilly.com/\nDeclining ‚Üí Retired\n‚Äã\nB2B VPN Connections\n‚Äã\nPosition\nRetired 2021-02-22\nTags\ncybersecurity\nArchitect\nRyan Elkins\nContact(s)\nRyan Elkins\nThis service is exiting (dates TBD). Existing uses of the service should remediate to an alternative solution.\nNotes\n‚Äã\nCertain cases of System to System access may require a B2B VPN with inbound access to the Lilly network. This service is exiting and existing uses of the service should remediate to an alternative solution.\nEDS\n‚Äã\nPosition\nRetired 2019-03-31\nTags\ncybersecurity\nArchitect\nRyan Elkins\nContact(s)\nRyan Elkins\nLSSO\n‚Äã\nPosition\nRetired 2018-12-31\nTags\ncybersecurity\nArchitect\nRyan Elkins\nContact(s)\nRyan Elkins\nNTLM\n‚Äã\nPosition\nDeclining 2019-01-01\nTags\ncybersecurity\nArchitect\nRyan Elkins\nContact(s)\nRyan Elkins\nUnencrypted LDAP (AD)\n‚Äã\nPosition\nRetiring\nTags\ncybersecurity\nArchitect\nRyan Elkins\nContact(s)\nRyan Elkins\nVPN on Demand (MobileIron)\n‚Äã\nPosition\nRetired 2020-12-31\nTags\ncybersecurity\nInformation Sensitivity\nRed\nArchitect\nRyan Elkins\nContact(s)\nRyan Elkins\nLimited Availability service that should only be used if the application is not able to surface data through the API gateway\nNotes\n‚Äã\nVPN on Demand is a service for Lilly managed mobile devices that allows for the launch of a restricted VPN tunnel (Production Profile) when the user is accessing specified internal web applications such as LillyNet.  This VPN tunnel will trigger 'On Demand' by automatically launching when a user attempts to browse to one of the specified URLs. VPN on Demand is a Limited Availability service that should only be used if the application is not able to surface data through the API gateway.\nDocument generated from Stack Overflow Enterprise articles (prioritized) and ea_tech.csv (fallback)\nWas this helpful?\nEdit this page\nPrevious\nüîê Cybersecurity\nNext\nüìä Data & Analytics\nCheckmarx\nCloud Custodian\nKerberos (via AD)\nMicrosoft 365 Defender\nPalo Alto Prisma\nPrisma Cloud (Palo Alto)\nRestricted VPN Thick Client (Cisco VPN)\nSAML 2.0\nSecure LDAP: Active Directory\nSecure LDAP: Virtual Directory (RadiantLogic)\nSSL VPN (LillyConnect - QuickConnect F5)\nVPN Tunnel (Cisco VPN)\nWeb Application Proxy\nDeclining ‚Üí Retired\nB2B VPN Connections\nEDS\nLSSO\nNTLM\nUnencrypted LDAP (AD)\nVPN on Demand (MobileIron)\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 0,
    "depth": 3,
    "crawled_at": "2026-02-25T10:08:55.628678"
  },
  "https://techhq.dc.lilly.com/docs/solution/data/data-blt": {
    "url": "https://techhq.dc.lilly.com/docs/solution/data/data-blt",
    "title": "üóÑÔ∏è Data & Analytics BLT | Tech HQ",
    "description": "The Data & Analytics Big List of Technology (BLT) is maintained using data from the Enterprise Architecture Tech Positioning Database and LillyFlow, with AI-assisted updates. While we aim for accuracy, please verify critical information and contact the EBA team with questions or corrections.",
    "h1": [
      "üóÑÔ∏è Data & Analytics BLT"
    ],
    "h2": [
      "AWS QuickSight‚Äã",
      "AWS RDS Oracle‚Äã",
      "AWS RDS PostgreSQL‚Äã",
      "AWS RDS SQL Server‚Äã",
      "AWS Redshift‚Äã",
      "AWS Redshift Spectrum‚Äã",
      "Azure CosmosDB‚Äã",
      "Azure Data Explorer‚Äã",
      "Azure Data Factory‚Äã",
      "Azure Data Lake Storage‚Äã",
      "Azure Data Share‚Äã",
      "Azure Database Migration Service‚Äã",
      "Azure DataBricks‚Äã",
      "Azure PostgreSQL‚Äã",
      "Azure SQL‚Äã",
      "Azure SQL Managed Instance‚Äã",
      "Azure Stream Analytics‚Äã",
      "Azure Synapse Analytics‚Äã",
      "Azure Table Storage‚Äã",
      "Denodo‚Äã",
      "Heroku Databases‚Äã",
      "IBM Watson Explorer‚Äã",
      "InVision (Kaltura)‚Äã",
      "JMP‚Äã",
      "Knime‚Äã",
      "Microsoft Search‚Äã",
      "Microsoft SQL Server on-prem‚Äã",
      "Microsoft Stream‚Äã",
      "OneDrive for Business‚Äã",
      "OpenShift Databases‚Äã",
      "OpenText‚Äã",
      "Posit (formerly RStudio) Workbench‚Äã",
      "R‚Äã",
      "Reltio‚Äã",
      "Salesforce Marketing Cloud - Messaging (Email & SMS)‚Äã",
      "Salesforce Marketing Cloud ‚Äì Landing Pages‚Äã",
      "SAS‚Äã",
      "SharePoint Search‚Äã",
      "Spotfire‚Äã",
      "Tableau AWS‚Äã",
      "Trifacta‚Äã",
      "Veeva Quality Docs‚Äã",
      "Veeva Vault‚Äã",
      "Declining ‚Üí Retired‚Äã"
    ],
    "h3": [
      "Alteryx‚Äã",
      "Anzo‚Äã",
      "HAWQ‚Äã",
      "Hortonworks DPP/DLM‚Äã",
      "Hortonworks Hadoop‚Äã",
      "IBM BigSQL‚Äã",
      "IBM DB2‚Äã",
      "Microsoft SQL Server on AVS‚Äã",
      "Modern Search (O365/Delve)‚Äã",
      "Netezza PDA‚Äã",
      "Oracle‚Äã",
      "Regulus‚Äã",
      "Salesforce Communities‚Äã",
      "SSAS‚Äã",
      "SSIS‚Äã",
      "SSRS‚Äã",
      "Tableau (On-prem)‚Äã",
      "Top Braid‚Äã"
    ],
    "text_content": "üóÑÔ∏è Data & Analytics BLT | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüóÑÔ∏è Data & Analytics BLT\nüß≠ Data Catalogs and Access\nüß≠ Data Visualization\nüß≠ Integration Middleware\nüß≠ Specialized Databases\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nüìä Data & Analytics\nüóÑÔ∏è Data & Analytics BLT\nOn this page\nüóÑÔ∏è Data & Analytics BLT\nThe Data & Analytics Big List of Technology (BLT) is maintained using data from the Enterprise Architecture Tech Positioning Database and LillyFlow, with AI-assisted updates. While we aim for accuracy, please verify critical information and contact the EBA team with questions or corrections.\nCan't find a technology?\nUse TechHQ's search feature to look across all technology documentation. For questions or to suggest additions to this list, contact the\nEnterprise Business Architecture team\n.\nAWS QuickSight\n‚Äã\nPosition\nStandard\nTags\nbusiness-intelligence, aws\nInformation Sensitivity\nRed\nArchitect\nMalika Mahoui\nContact(s)\nMumtaz.\nUse if cost, data on AWS, simple visualizations and tight integration with AWS services are main considerations.\nUse Cases\n‚Äã\nCreate visualization dashboards  using different chart options, Compelling story telling using data from different AWS data stores, Use this tool if simple visualization is main objective, This tool is not suitable if you have requirements to do complex calculations by merging data from different data sources and data modeling, This tool is not suitable if you have a requirements to collaborate using Microsoft Teams\nNotes\n‚Äã\nCost effective tool to create visualizations if you have data on AWS.  If complex calculations and more visualizations features required, consider Power BI or Tableau. This is a great  tool to visualize large dataset in AWS.\nSee Also\n‚Äã\nhttps://collab.lilly.com/sites/EIP/SitePages/Enterprise-Analytics.aspx\nAWS RDS Oracle\n‚Äã\nPosition\nSpecialized\nTags\naws, database-platforms\nArchitect\nLarry Zetzl\nContact(s)\nCraig Welch\nUsed when application is only supporting Oracle\nUse Cases\n‚Äã\nHIPAA applications / data\nNotes\n‚Äã\nOnly for:\nStandard Edition 2 (SE2)\nLicense Included\nAWS RDS PostgreSQL\n‚Äã\nPosition\nStandard\nTags\naws, database-platforms\nInformation Sensitivity\nRed\nArchitect\nLarry Zetzl\nContact(s)\nDayanand Sagar\nSeconday preferred RDS choice for OLTP workloads, rapid provisioning and high scalability use cases\nUse Cases\n‚Äã\nTransactional OLTP workloads\nStoring small to medium volume of structured data that can be consumed by standards SQL tools/APIs\nHIPAA applications / data\nGxP Applications\nConnectivity between on-prem apps and AWS RDS Postgres instances\nNotes\n‚Äã\nThe most Oracle-compatible open-source database. Use of SQL significantly reduces learning curve. Secure, compliant, controlled platform. Encryption at rest & in transit. Service primary available in Ohio(US) Region. Read-Replica as an HA solution is available.\nhttps;//\nwww.lillydev.com/patterns/database%20patterns\nSee Also\n‚Äã\nhttps://lilly.service-now.com/$knowledge.do?query=AWS%20RDS&sysparm_order=view_count\nAWS RDS SQL Server\n‚Äã\nPosition\nStandard\nTags\naws, database-platforms\nArchitect\nLarry Zetzl\nContact(s)\nManuela Elena Garcia\nUsed when application is not supporting open source DBMS in AWS\nUse Cases\n‚Äã\nRelational OLTP workloads\nReferential integrity\nACID transactions\nLift & shift\nAWS Redshift\n‚Äã\nPosition\nStandard\nTags\naws, database-platforms\nInformation Sensitivity\nRed\nArchitect\nLarry Zetzl\nContact(s)\nPrem Kishore\nPreferred analytical database for OLAP workloads and data warehouse. Compliant with the Enterprise Data Backbone architecture.\nUse Cases\n‚Äã\nData warehouse\nAnalytical workloads\nCloud Alternative to Netezza/PDA\nNotes\n‚Äã\nExplore S3 data through Spectrum connector Compute scales to handle high concurrency Not for OLTP workloads. Cost of this service is a consideration.\nSee Also\n‚Äã\nhttps://lilly.service-now.com/$knowledge.do?query=AWS%20Redshift&sysparm_order=view_count\nAWS Redshift Spectrum\n‚Äã\nPosition\nStandard\nTags\naws, cloud-computing\nArchitect\nManuela Elena Garcia\nContact(s)\nArvind Sharma\nInformation Sensitivity\nArchitect\nManuela Elena Garcia\nContact(s)\nArvind Sharma\nAzure CosmosDB\n‚Äã\nPosition\nStandard\nTags\nazure, database-platforms\nInformation Sensitivity\nRed\nArchitect\nMalika Mahoui\nContact(s)\nManuela Elena Garcia\nAzure preferred solution for Key-value / Document / column family / Graph database use cases\nUse Cases\n‚Äã\nHigh throughput-low latency,IoT data, multi-model, multi- API database service\nNotes\n‚Äã\nAzure Cosmos DB natively supports multiple data models. The core type system of Azure Cosmos DB‚Äôs database engine is atom-record-sequence (ARS) based.\nAzure Data Explorer\n‚Äã\nPosition\nEmerging\nTags\nazure, cloud-computing\nInformation Sensitivity\nUnder Review\nArchitect\nTodd Walters\nContact(s)\nRichard Schmidt\nInformation Sensitivity\nUnder Review\nArchitect\nTodd Walters\nContact(s)\nRichard Schmidt\nAzure Data Factory\n‚Äã\nPosition\nStandard\nTags\nazure, integration-platforms\nInformation Sensitivity\nRed\nArchitect\nLarry Zetzl\nContact(s)\nPrem Kishore Dandamudi\nCloud-based data integration service used to create data-driven workflows in the cloud for orchestrating and automating data movement and data transformation.\nAzure Data Lake Storage\n‚Äã\nPosition\nEmerging\nTags\nazure, cloud-computing\nInformation Sensitivity\nUnder Review\nArchitect\nJarret DiPace\nContact(s)\nSriojeet Mukherjee\nInformation Sensitivity\nUnder Review\nArchitect\nJarret DiPace\nContact(s)\nSriojeet Mukherjee\nAzure Data Share\n‚Äã\nPosition\nEmerging\nTags\nazure, cloud-computing\nInformation Sensitivity\nUnder Review\nArchitect\nJarret DiPace\nContact(s)\nSriojeet Mukherjee\nInformation Sensitivity\nUnder Review\nArchitect\nJarret DiPace\nContact(s)\nSriojeet Mukherjee\nAzure Database Migration Service\n‚Äã\nPosition\nEmerging\nTags\nazure, cloud-computing\nInformation Sensitivity\nUnder Review\nArchitect\nMalika Mahoui\nContact(s)\nManuela Elena Garcia\nInformation Sensitivity\nUnder Review\nArchitect\nMalika Mahoui\nContact(s)\nManuela Elena Garcia\nAzure DataBricks\n‚Äã\nPosition\nEmerging\nTags\nazure, cloud-computing\nInformation Sensitivity\nUnder Review\nArchitect\nTodd Walters\nContact(s)\nRichard Schmidt\nInformation Sensitivity\nUnder Review\nArchitect\nTodd Walters\nContact(s)\nRichard Schmidt\nAzure PostgreSQL\n‚Äã\nPosition\nStandard\nTags\nazure, database-platforms\nArchitect\nMalika Mahoui\nContact(s)\nManuela Elena Garcia\nPreferred relational database in Azure environment\nUse Cases\n‚Äã\nRelational Transactional OLTP workloads; Referential integrity; ACID transactions; HIPAA applications / data; GxP Applications; Lift & shift\nAzure SQL\n‚Äã\nPosition\nStandard\nTags\nazure, database-platforms\nArchitect\nMalika Mahoui\nContact(s)\nManuela Elena Garcia\nAccepted relational database for applications requiring Microsoft SQL\nUse Cases\n‚Äã\nRelational Transactional OLTP workloads; Referential integrity; ACID transactions; HIPAA applications / data; GxP Applications; Lift & shift\nAzure SQL Managed Instance\n‚Äã\nPosition\nEmerging\nTags\nazure, database-platforms\nInformation Sensitivity\nUnder Review\nArchitect\nMalika Mahoui\nContact(s)\nManuela Elena Garcia\nInformation Sensitivity\nUnder Review\nArchitect\nMalika Mahoui\nContact(s)\nManuela Elena Garcia\nAzure Stream Analytics\n‚Äã\nPosition\nEmerging\nTags\nazure, data-science-tools\nArchitect\nMalika Mahoui\nContact(s)\nSrihari S.\nReal-time analytics service that provides a simple way to stream data, designed to analyze and process high volumes of fast streaming data from multiple input sources simultaneously\nUse Cases\n‚Äã\nUse stream analytics if want to filter and analyze real-time telemetry streams from multiple sources like IoT Hub, Event hubs, ADLS gen2, Azure blob storage and store the data in your output storage like ADLS gen 2.\nAzure Synapse Analytics\n‚Äã\nPosition\nEmerging\nTags\nazure, data-science-tools\nArchitect\nMalika Mahoui\nContact(s)\nSrihari S.\nMainly used for data integration, data warehousing, and big data analytics\nUse Cases\n‚Äã\nAzure Synapse Analytics is a limitless analytics service that brings together data integration, enterprise data warehousing and big data analytics. It gives you the freedom to query data on your terms, using either serverless or dedicated options‚Äîat scale. Azure Synapse brings these worlds together with a unified experience to ingest, explore, prepare, transform, manage and serve data for immediate BI and machine learning needs.\nNotes\n‚Äã\nAzure Synapse¬†is an enterprise analytics service that accelerates time to insight across data warehouses and big data systems. Azure Synapse brings together the best of¬†SQL¬†technologies used in enterprise data warehousing,¬†Spark¬†technologies used for big data,¬†Data Explorer¬†for log and time series analytics,¬†Pipelines¬†for data integration and ETL/ELT, and deep integration with other Azure services such as¬†Power BI,¬†CosmosDB, and¬†AzureML.\nAzure Table Storage\n‚Äã\nPosition\nEmerging\nTags\nazure, cloud-computing\nInformation Sensitivity\nUnder Review\nArchitect\nJarret DiPace\nContact(s)\nSriojeet Mukherjee\nInformation Sensitivity\nUnder Review\nArchitect\nJarret DiPace\nContact(s)\nSriojeet Mukherjee\nDenodo\n‚Äã\nPosition\nStandard\nTags\nintegration-platforms\nInformation Sensitivity\nRed\nArchitect\nLarry Zetzl\nContact(s)\nTorsten Eid\nCMDB CI\nCI00000000047612\nUse for real-time or cached access to heterogeneous and distributed data sources without moving the data (data virtualization)\nUse Cases\n‚Äã\nSetup an ETL Interface, Setup an API Transform the data as it's transferred, Create an API that references data on multiple repositories, Access distributed data in multiple formats real-time or cached, Integrate cloud (Salesforce or Veeva) data with an on-premise Lilly system or another cloud environment, Facilitate brokered authentication between one cloud environment and another\nHeroku Databases\n‚Äã\nPosition\nSpecialized\nTags\ndatabase-platforms\nArchitect\nRob Blacketter\nContact(s)\nRob Blackketter\nIBM Watson Explorer\n‚Äã\nPosition\nStandard\nTags\nsearch-platforms\nInformation Sensitivity\nRed\nArchitect\nMalika Mahoui\nContact(s)\nSurya Prakash Rawat, Rubin Wei\nUse for searching multiple content repositories/databases (taxonomy). Use for search engines uses cases ( Google like search ) and content analytics to identify various insights from unstructured data.\nUse Cases\n‚Äã\nGeneral purpose searchStructured and unstructured searchKeyword-based search (with synonyms/hierarchy for multiple data sources with OOB AppBuilder UI or custom UI)Ability to leverage Cognitive Cloud APIs in the search pipeline (e.g. Natural Language Understanding, Knowledge Studio, Discovery Service, Text Speech)Search Business Specific Content with security control on the data (Scout). Use this tool if you want to have 360 degree search application by indexing data from various structured and unstructured data sources, Use this tool if you want to extract entities in structured format to consume it using  data analytics tools such as power bi and tableau.\nNotes\n‚Äã\nWatson Cognitive cloud services not included, but can add more capabilitiesDevelopment skillset requiredTerm hirarchy and synonym driven indexing over data and contentGoogle like search box or user centric UI development environmentIntegration potential with some IBM Watson offeringsDesigned as an enterprise search tool for knowledge discoveryWEX has standard connectors for websites.  Ability to identity KOL (SEID)\nConnect to different unstructured data sources ( web sites, pdf files, document repository etc.) to search for key contents. You can create custom ML models using Watson Knowledge studio and deploy it in Watson Explorer. License is required.\nSee Also\n‚Äã\nhttps://collab.lilly.com/sites/EIP/SitePages/Enterprise-Analytics.aspx\nInVision (Kaltura)\n‚Äã\nPosition\nStandard\nTags\ncontent-management\nInformation Sensitivity\nOrange\nArchitect\nMary Codrington\nContact(s)\nShannon Britt\nSecure enterprisewide video platform for Lilly internal videos.\nUse Cases\n‚Äã\nBroad video sharing and distribution for internal videos.\nNotes\n‚Äã\nCan be used for hosting videos intended for external audience; the video owner needs to take the video through local content review and approval process. Specific video links can be shared externally, but external users are not able to access the full inVision site.\nJMP\n‚Äã\nPosition\nStandard\nTags\ndata-science-tools\nInformation Sensitivity\nRed\nArchitect\nRubin Wei\nContact(s)\nSteven D Randolph\nPrimarily for users who prefer point and click expereince to perform Statistical analysis and visualization\nUse Cases\n‚Äã\nUse to perform desktop statistical analysis and create visualizations\nNotes\n‚Äã\nJMP is a desktop application with a wizard-based user interface for statistical analysis, visaulziaton, machine learning, six sigma, quality control, design of experiments and R&D. It is developed by the JMP business unit of SAS Institute. JMP, JMP Pro, JMP Clinical and JMP Genomics are available at Lilly. You can install JMP from Workspace ONE or request JMP Pro, JMP Genomics and JMP Clinical through EIP team.\nSee Also\n‚Äã\nhttps://collab.lilly.com/sites/EIP/SitePages/Enterprise-Analytics.aspx\nKnime\n‚Äã\nPosition\nSpecialized\nTags\ndata-science-tools\nArchitect\nRubin Wei\nContact(s)\nNeeraj Wadhwa or\nknime_admin@lists.lilly.com\nCMDB CI\nCI00000000003244\nKNIME is a user-friendly graphical workbench for the entire analysis process: data access, data transformation, exploratory data analysis/visualization, machine learning, predictive analytics, and reporting. It‚Äôs primarily designed for data analyst and citizen data scientists who lacks programming experiences or prefers workflow/pipeline/node-based analytics platform for analytics. Python and R programmers might find it helpful to facilitate their daily work.\nYou are free to use KNIME Desktop (KNIME Analytics Platform) for your Lilly project. If you have a workflow that you want to publish to KNIME server (to be replaced by KNIME Hub in 2025), so it can run automatically on a schedule or someone can collaborate on the project, please note the following. As of February 2025, the KNIME server is funded and supported by LRL Tech at Lilly. Use cases within LRL will be fully supported. Any use cases outside of LRL will be supported on a best effort basis. If you have any question regarding the positioning and you are part of LRL, please reach out to the support team (\nknime_admin@lists.lilly.com\n). If you have any question regarding the positioning and you are part of BU/IBU, please reach out to Koen Petit (for IBU) or Annapurna Calyam (for BU). Otherwise feel free to reach out to Rubin Wei. As of February 2025, there is no plan to exit KNIME for LRL users.\nUse Cases\n‚Äã\nWorkflow-based data analytics\nNotes\n‚Äã\nKNIME Desktop (KNIME Analytics Platform) can be installed following the instruction: 1) Request ‚ÄúAll KNIME users group‚Äù entitlement in MyAccess (\nhttps://myaccess.lilly.com/\n)  2) Navigate to any of the following shares in order to get the latest KNIME desktop available (currently 4.5 as of August 2023): US (Z1) share: \\styx.am.lilly.com\\KNIME\\KNIME-INSTALLER-45-PRD   UK (Z2) share: \\yo2cifs01.ema.lilly.com\\KNIME\\KNIME-INSTALLER-45-PRD ES (Z2) share: \\ys2discsvr01.ema.lilly.com\\KNIME\\KNIME-INSTALLER-45-PRD (access to those shares is governe...\nRead full article\nMicrosoft Search\n‚Äã\nPosition\nEmerging\nTags\nsearch-platforms\nInformation Sensitivity\nRed\nArchitect\nSteve Scott\nContact(s)\nAditya Jichakar\nModern search capabilty under evaluation\nUse Cases\n‚Äã\nGraph and Azure Services (AI and Cognitive Services) powered, personalized search for O365 and LillyNow contentPeople search defaults to profile card resultPromoted results (best bets) become Bookmarks, with additional functionality\nNotes\n‚Äã\nMicrosoft vision for all search experiences.  Microsoft Search is the next generation of Modern Search. Result verticals include conversation search (Teams and Yammer). Exists currently only in Bing for Business in Public Preview status, and search box display exists in O365 homepage. No ability to customize UI or relevancy yet. Bing for Business includes federated results list from Bing content (separated from tenant results)\nMicrosoft SQL Server on-prem\n‚Äã\nPosition\nStandard\nTags\ndatabase-platforms\nInformation Sensitivity\nRed\nArchitect\nLarry Zetzl\nContact(s)\nManuela Elena Garcia\nUse for on-prem RDBMS use cases that cannot move to the cloud\nUse Cases\n‚Äã\nRDBMS supporting Transactional/OLTP\nOLAP workloads with Concurrent Users\nStoring small to Large volume of structured data that can be consumed by standard SQL tools  & API\nEase for project/application team to use and develop with easiest integration with MS Stack/ application and toolset\nCan hosts GxP Applications Data\nService available in On-premise environment across Lilly.\nNotes\n‚Äã\nLower cost than alternatives; free Express edition.Widely used in Lilly for hosting application database. Rapidly Provisioned using Chef and automated patching using DSSP for faster delivery/patching. Integrated with Analytical Services ( SSAS) & Reporting Services ( SSRS). Always on Solution/setup for Database layer HA\nSee Also\n‚Äã\nhttps://lilly.service-now.com/nav_to.do?uri=%2Fkb_view.do%3Fsys_kb_id%3Dcb0f706b0f4fb90028ed6509b1050ed6\nMicrosoft Stream\n‚Äã\nPosition\nStandard\nTags\ncontent-management\nInformation Sensitivity\nRed\nArchitect\nSteve Scott\nContact(s)\nShannon Britt\nCMDB CI\nCI00000042101769\nMicrosoft Stream is only to be used in support of Yammer and Teams Live events or Teams meeting recordings.\nUse Cases\n‚Äã\nCapture and re-watch of meeting / event recordings produced as a result of using Teams or Yammer for a recorded meeting or live event.\nNotes\n‚Äã\nThe inVision platform should continue to be used for all other video needs.\nSee Also\n‚Äã\nhttps://www.microsoft365.com/launch/stream?auth=2\nOneDrive for Business\n‚Äã\nPosition\nStandard\nTags\ncontent-management\nInformation Sensitivity\nRed\nArchitect\nSteve Scott\nContact(s)\nScott Gondeck\nCMDB CI\nCI00000000396148\nPreferred solution for individual content storage for anyone with Lilly credentials\nUse Cases\n‚Äã\nStore my own document & files\nShare links to my documents with on-boarded workforce OR with external parties (email verification step required, no download)\nCreate Request File links for external party document upload into specific OneDrive folders.\nSee Also\n‚Äã\nhttps://collab.lilly.com/sites/onedriveforbusiness\nOpenShift Databases\n‚Äã\nPosition\nSpecialized\nTags\ndatabase-platforms\nArchitect\nChris Tornatta\nContact(s)\nChris Tornatta\nOpenShift Databases: popular relational and NoSQL database technologies. brAvailable for applications hosted inside of Openshift only.\nUse Cases\n‚Äã\nGxP Applications\nNotes\n‚Äã\nOpenShift database services: Redis, MariaDB, MongoDB, MySQL, PostgreSQLAvailable through the OpenShift platform. Not EIP Supported Services\nNo support offered by OpenShift team. No backup and restore services available. Bring your own backup solution.\nOpenText\n‚Äã\nPosition\nSpecialized\nTags\ncontent-management\nInformation Sensitivity\nRed\nArchitect\nScott Kuckuck\nContact(s)\nScott Kuckuck\nUse for e-mail or hardcopy business records retention\nUse Cases\n‚Äã\nLilly Email Management system (e-mail record retention), Lilly Records Management system (hardcopy)\nNotes\n‚Äã\nCore for the management of hardcopy record inventories (Lilly Records Management system) and for the management of emails that have been declared as business records (Lilly Email Management system)\nSee Also\n‚Äã\nhttps://now.lilly.com/landingoverview/global-global-records-and-information-management/overview\nPosit (formerly RStudio) Workbench\n‚Äã\nPosition\nStandard\nTags\ndata-science-tools\nArchitect\nRubin Wei\nContact(s)\nMargaret Xue or Analytics_OPS for EIP support installing R; Jeff Kriske for RStudio IDE on HPC or Posit Workbench\nRecommended IDE for R users.\nNotes\n‚Äã\nPosit (formerly RStudio) Workbench is the most popular Integrated Development Environment (IDE) for R users (\nhttps://www.jetbrains.com/lp/devecosystem-2021/r/\n) with interoperability for Python, SQL, D3 and Stan. RStudio IDE is a regular desktop application available for Windows, macOS, and Linux. Posit Workbench (formerly RStudio Workbench and RStudio Server Pro) runs on a remote (Linux) server and allows accessing RStudio using a web browser. RStudio IDE can be installed from Workspace One on Windows or Mac. If you use Lilly High Performance Computing (\nhttps://hpc.am.lilly.com/index.php/Main_Page\n), can you use either RStudio IDE via remote desktop (\nhttp://hpc-analytics.am.lilly.com/r_info/rstudio_remote_desktop/\n) or Posit Workbench via browser (\nhttp://hpc-analytics.am.lilly.com/r_info/rstudio_workbench/\n).\nSee Also\n‚Äã\nhttp://hpc-analytics.am.lilly.com/r_info/rstudio_workbench/\nR\n‚Äã\nPosition\nStandard\nTags\ndata-science-tools\nInformation Sensitivity\nRed\nArchitect\nRubin Wei\nContact(s)\nSteven D Randolph or Analytics_OPS for EIP support installing R; Jeff Kriske for R on HPC\nPreferred tool to perform Statistical Analysis, simulation and visualization when SAS is not the required tool\nUse Cases\n‚Äã\nR is one of the most commonly used programming language for statistical computing, machine learning and visaulziaton. It's an open-source free software. At Lilly, you can install R on your Windows computer through Workspace One. You can also access R (\nhttp://hpc-analytics.am.lilly.com/r_info/r-on-command-line/\n) from Lilly High Performance Computing environment (\nhttps://hpc.am.lilly.com/index.php/Main_Page\n). EIP Team also supports R server to run R scripts invoked from Tableau Server.\nNotes\n‚Äã\nEIPTeam supports R server to run R scripts invoked from Tableau Server.\nSee Also\n‚Äã\nhttps://collab.lilly.com/sites/EIP/SitePages/Enterprise-Analytics.aspx\nReltio\n‚Äã\nPosition\nStandard\nTags\ndatabase-platforms\nInformation Sensitivity\nRed\nArchitect\nJon Grocock\nContact(s)\nAntonia Costa\nUsed to enable the following capabilities:\nMaster Data Management: capabilities to cleanse, match, merge / unmerge master data of any type and domain; e.g. organizations, customers, suppliers,  locations and products. Source Crosswalks to manage identifiers from a variety of contributing data sources, for ongoing updates and downstream data distribution.\nReference Data Management: Enables standardisation of reference data such as lookups and codes from multiple source systems.\nUse Cases\n‚Äã\nCurrently used to master customer (EPH), material (APIMS) and patient (PPH) domains\nNotes\n‚Äã\nCloud subscription based and charges based on number of users, consolidated profiles, API calls and support levels. Single standard tenant (hosts customer and material domains) and HIPPA tenant for patient domain.\nSalesforce Marketing Cloud - Messaging (Email & SMS)\n‚Äã\nPosition\nSpecialized\nTags\ncontent-management\nInformation Sensitivity\nOrange\nArchitect\nChad Stout\nContact(s)\nSierra Doucette\nSpecialized for external campaigns to customers (HCPS, Consumers, Payers, etc)\nUse Cases\n‚Äã\nProduct Updates, Disease State Awareness Campaigns, Meeting Confirmations, etc\nNotes\n‚Äã\nUsage for internal communication purposes can be evaluated on an exception basis\nSee Also\n‚Äã\nhttps://teamgco.lilly.com/platform/email\nSalesforce Marketing Cloud ‚Äì Landing Pages\n‚Äã\nPosition\nSpecialized\nTags\ncontent-management\nInformation Sensitivity\nOrange\nArchitect\nChad Stout\nContact(s)\nSierra Doucette\nUse to host web page for SalesForce marketing compaigns\nUse Cases\n‚Äã\nConsume/deliver static content\nSee Also\n‚Äã\nhttps://teamgco.lilly.com/platform/email\nSAS\n‚Äã\nPosition\nStandard\nTags\ndata-science-tools\nInformation Sensitivity\nRed\nArchitect\nRubin Wei\nContact(s)\nSteven D Randolph\nUse for Statistical Analysis\nUse Cases\n‚Äã\nTo Perform statistical analysis\nNotes\n‚Äã\nSAS offers GUI and array of statistical functions using drag and drop. Technical support is available from EIP. Coding support is available from the vendor- SAS TECH SUPPORT\nSee Also\n‚Äã\nhttps://collab.lilly.com/sites/EIP/SitePages/Enterprise-Analytics.aspx\nSharePoint Search\n‚Äã\nPosition\nStandard\nTags\nsearch-platforms\nInformation Sensitivity\nRed\nArchitect\nSteve Scott\nContact(s)\nAditya Jichakar\nPreferred for SharePoint content & People search\nUse Cases\n‚Äã\nKeyword based search for SharePoint and LillyNow (Contentful) contentPeople/profile search with nickname matchingPromoted results (best bets) for specific top level content results\nNotes\n‚Äã\nClassic Search. Use this as default for SharePoint content and internal People contentSharePoint search is often consumed via LillyNow search boxCustomization available for results display, relevancy and rankingNo results personalization (Graph input) or enhancement (AI, Cognitive services)\nSpotfire\n‚Äã\nPosition\nSpecialized\nTags\nbusiness-intelligence\nInformation Sensitivity\nRed\nArchitect\nMalika Mahoui\nContact(s)\nMargaret Xue\nUse to create data visualizations and dashboards and wanted to have chemical structures in the graphics.\nUse Cases\n‚Äã\nUse this tool if you need to analyze chemical structures using graphics and create visualizations\nNotes\n‚Äã\nCreate visualizations, analyze chemical structures. If you don‚Äôt have specific requirements such using chemical structures, considering using Power BI or QuickSight. License is required to use this tol\nSee Also\n‚Äã\nhttps://collab.lilly.com/sites/EIP/SitePages/Enterprise-Analytics.aspx\nTableau AWS\n‚Äã\nPosition\nStandard\nTags\nbusiness-intelligence, aws\nInformation Sensitivity\nRed\nArchitect\nMalika Mahoui\nContact(s)\nMumtaz.\nUse if dashboards data, Visualization, Visual Analytics & Exploration, Augmented Analytics, Natural Language Querying, Statistical Clustering, Trendline, AI/ML Forecasting (basic), other statistical models and summaries, Integration with R, Python, Matlab  are the main considerations and wanted to make use of  wide range of ways to manipulate and visualize data. Supports more volume of data.\nUse Cases\n‚Äã\nCreate visualization dashboards using different chart options, Compelling story-telling using data from different data stores, Use this tool if data visualization, visual data exploratory analytics, Natural Language Querying, no-code AI/ML, no code statistical modeling is the main objective.This tool is not suitable if you have requirements to collaborate using Microsoft Teams.\nNotes\n‚Äã\nRole Based license rquirement - Viewer (for end users) & Creator (for Dekstop Users). This tool is expensive than Power BI and QuickSight. Recommendation is to start with QuickSight if you have data on AWS and need to create simple dashboards/Visualizations. All other use cases,  consider Power BI. If both Power BI and QuickSight cannot meet the requirements, Tableau can be considered.\nSee Also\n‚Äã\nhttps://collab.lilly.com/sites/EIP/SitePages/Enterprise-Analytics.aspx\nTrifacta\n‚Äã\nPosition\nStandard\nTags\ndata-science-tools\nInformation Sensitivity\nRed\nArchitect\nMalika Mahoui\nContact(s)\nAndrea Price\nUse to prepare , cleanse and qualify data to produce consumable data products.  It works with structured and semi-structed data.\nUse Cases\n‚Äã\nUse to prepare and transform data  from EDB and consume it through various dashboard and analytics tools,  this is not suitable if you are already  using data transformation capabilities within analytics tool and you don‚Äôt have requirements to share transformed data\nNotes\n‚Äã\nExtract data from EDB data sources (S3, Postgres and Redshift) , cleanse and then publish cleansed data to the target data sources for consumption. License is required.\nSee Also\n‚Äã\nhttps://collab.lilly.com/sites/EIP/SitePages/Enterprise-Analytics.aspx\nVeeva Quality Docs\n‚Äã\nPosition\nStandard\nTags\ncontent-management\nInformation Sensitivity\nRed\nArchitect\nTed Carpenter\nContact(s)\nCristina Olza\nPreferred for regulated and controlled content\nUse Cases\n‚Äã\nLong-term in-use and complex document management, Controlled and/or regulated documents, Training content and material, Manage content that supports a local business process (controlled and regulated), Manage enterprise content, Batch Printing Records (Controlled printing of Manufacturing tickets). Controlled copies and printing\nNotes\n‚Äã\nPreferred for regulated and controlled content. Approval is required by the Veeva Quality Docs Steering Team (Contact Cristina Olza) for non-regulated content use cases that require complex version control/workflow\nSee Also\n‚Äã\nhttps://collab.lilly.com/sites/QDocsProgram\nVeeva Vault\n‚Äã\nPosition\nSpecialized\nTags\ncontent-management\nInformation Sensitivity\nRed\nArchitect\nTim LaCreta\nContact(s)\nTim LaCreta\nSpecialized for controlled content in a variety of use cases\nUse Cases\n‚Äã\nPre-Built Frameworks for content solutions in Clinical-Trial, Quality-Mfg, Regulatory, Marketing, and Medical as well as a Platform for custom solutions with synergy capabilities between them\nDeclining ‚Üí Retired\n‚Äã\nAlteryx\n‚Äã\nPosition\nRetired 2021-03-01\nTags\nbusiness-intelligence\nArchitect\nScott Albright\nContact(s)\nScott Albright\nAnzo\n‚Äã\nPosition\nDeclining 2022-01-02\nTags\nsearch-platforms\nInformation Sensitivity\nRed\nArchitect\nScott Albright\nContact(s)\nScott Albright\nUse for managing multiple content repositories/databases (ontology) and business terms - (GraphDB, Triple Store)\nUse Cases\n‚Äã\nSearch and visualize relationships among business concepts (e.g. compound, study, patient)Search across multiple semantically integrated data sourcesSearch business Specific Content without giving access to the data (Cue)Search business Specific Content with security control on the data (Scout)General purpose SearchStructured and Unstructured SearchTool can store original Content\nNotes\n‚Äã\nExample: Project needs to manage relationships among business concepts (e.g. compound, study, patient) to be visualized via a search experience.   Anzo shines in patterns where search across multiple semantically integrated data sources. Out of the box or custom UI for exploration and dashboards/visualization. Agile data/content integration. MS Excel add-in to ingest spreadsheets. Designed as a semantic platform. Potential for business power users to create value without additional help. Caution: Emerging technology and expertise. Cost of entry\nHAWQ\n‚Äã\nPosition\nRetired 2019-12-31\nTags\ndatabase-platforms\nArchitect\nLarry Zetzl\nContact(s)\nDouglas Marshall\nHAWQ is a high speed sql-on-hadoop RDBMS. Exit by Q4-2019\nUse Cases\n‚Äã\nANSI SQL compliant relational dbms\nNotes\n‚Äã\nHAWQ is off support and exitingAlternatives: IBM BigSQL is the preferred high speed sql-on-hadoop solution\nHortonworks DPP/DLM\n‚Äã\nPosition\nRetired 2022-12-31\nTags\ndatabase-platforms\nInformation Sensitivity\nRed\nArchitect\nManuela Elena Garcia\nContact(s)\nPrem Kishore Dandamudi\nOpen source data transfer applicance that is integrated with Datalake. Exit along with Hadoop\nUse Cases\n‚Äã\nData transfer from Hadoop to any cloud storage\nNotes\n‚Äã\nDeclining as in general Lilly is looking at more flexible, elastic, and cost-effective data management solutions such as AWS datasync,SFTP\nSee Also\n‚Äã\nhttps://collab.lilly.com/sites/Hadoop/SitePages/Home.aspx\nHortonworks Hadoop\n‚Äã\nPosition\nRetired 2023-08-31\nTags\ndatabase-platforms\nInformation Sensitivity\nRed\nArchitect\nManuela Elena Garcia\nContact(s)\nPrem Kishore Dandamudi\nOpen source datalake appliance with Data storage, processing and analytical capabilities. Exit by Q3 -2023\nUse Cases\n‚Äã\nManage wide variety ( Structure, Semistructure, unstructure) and huge volume of data for distributed storage and distributed processing\nNotes\n‚Äã\nDeclining as in general Lilly is looking at more flexible, elastic, and cost-effective data management solutions such as AWS EMR, S3, Athena.\nSee Also\n‚Äã\nhttps://collab.lilly.com/sites/Hadoop/SitePages/Home.aspx\nIBM BigSQL\n‚Äã\nPosition\nRetired 2020-07-31\nTags\ndatabase-platforms\nArchitect\nLarry Zetzl\nContact(s)\nDouglas Marshall\nRetired due to lack of business demand\nUse Cases\n‚Äã\nLow latency with high performance\nSweet spot when  100 TB\nOLAP and some OLTP workloads\nNotes\n‚Äã\nMassively Parallel Processing (MPP)\nANSI SQL Compliant\nSee Also\n‚Äã\nhttps://lilly.service-now.com/kb_view.do?sysparm_article=KB0093301&sysparm_rank=2&sysparm_tsqueryId=ba575dc21bf2bf049d52b9dcdd4bcbfc\nIBM DB2\n‚Äã\nPosition\nDeclining 2020-07-01\nTags\ndatabase-platforms\nArchitect\nVijay Govindan\nContact(s)\nMarc Krieg\nSpecialized for only SAP. Exit by Q1-2025\nNotes\n‚Äã\nOnly available for SAP.  Not for Enterprise consumption. SAP S4 go live with be Jan 2024 however the legacy SAP production system will retained in 2024 in order to provide a reference to the business.\nMicrosoft SQL Server on AVS\n‚Äã\nPosition\nDeclining 2023-08-22\nTags\ndatabase-platforms\nInformation Sensitivity\nRed\nArchitect\nManuela Elena Garcia\nContact(s)\nManuela Elena Garcia, Manikandan M\nReplace SQL Server running in AVS server with Azure Native services PaaS, Azure SQL DB or Azure SQL Manage Instance. When will be not possible to go to PaaS then replace by Azure VM server (IaaS).\nUse Cases\n‚Äã\nWas created as part of lift and shift project.\nSee Also\n‚Äã\nhttps://collab.lilly.com/sites/SQLServer/SitePages/SQL-Server-positioning.aspx\nModern Search (O365/Delve)\n‚Äã\nPosition\nRetired 2021-12-31\nTags\nsearch-platforms\nInformation Sensitivity\nRed\nArchitect\nSteve Scott\nContact(s)\nAditya Jichakar\nUse to search across the Microsoft O365 suite\nUse Cases\n‚Äã\nGraph powered, personalized search for O365 and LillyNow content.  No AIPeople search defaults to profile card result  Promoted results (best bets) for specific top level content results.\nNotes\n‚Äã\nRemoved with the implementation of Microsoft Search\nNetezza PDA\n‚Äã\nPosition\nRetiring\nTags\ndatabase-platforms\nInformation Sensitivity\nRed\nArchitect\nManuela Elena Garcia\nContact(s)\nCraig Welch\nPurpose built data warehouse / data analytics appliance for OLAP workloads. Exit by Q4-2023\nUse Cases\n‚Äã\nOLAP; not for OLTP workloads\nConsider Cloud First alternatives (e.g. Redshift)\nNotes\n‚Äã\nDeclining as in general Lilly is looking at more flexible, elastic, and cost-effective analytical solutions such as AWS Redshift\nSee Also\n‚Äã\nhttps://lilly.service-now.com/$knowledge.do?query=Netezza&sysparm_type_filter=all&sysparm_order=view_count\nOracle\n‚Äã\nPosition\nRetiring\nTags\ndatabase-platforms\nInformation Sensitivity\nRed\nArchitect\nManuela Elena Garcia\nContact(s)\nCraig Welch\nCMDB CI\nCI00000000019230\nNo new Oracle databases will be created without an EA exception.\nUse Cases\n‚Äã\nCOTS requirement\nOLTP workloads\nNotes\n‚Äã\nWhile the exit date is TBD, business area should seek opportunities to retire their Oracle databases.\nSee Also\n‚Äã\nhttps://collab.lilly.com/sites/OracleServiceSite/\nRegulus\n‚Äã\nPosition\nRetiring\nTags\ncontent-management\nInformation Sensitivity\nRed\nArchitect\nJoaquin Rincon\nContact(s)\nJoaquin Rincon\nLegacy solution for controlled documents in M&Q and centralized business functions. Retire by mid 2022\nUse Cases\n‚Äã\nLong-term in-use and complex document managementControlled and/or regulated documentsDrug Dev & Mkting content, including regulatory contentTraining content and materialCollaborate on team/project contentManage content that supports a local business processManage enterprise contentBatch Printing Records (Controlled printing of Manufacturing tickets\nNotes\n‚Äã\nLegacy solution for controlled documents for M&Q components and centralized business functions. No new repositories. Retire by mid 2022. The Regulus team will contact business areas to discuss the individual migration plans. All repositories will be migrated to Veeva Quality Docs\nSalesforce Communities\n‚Äã\nPosition\nDeclining 2021-04-15\nTags\ncontent-management\nInformation Sensitivity\nOrange\nArchitect\nRob Blackketter\nContact(s)\nRob Blackketter\nSSAS\n‚Äã\nPosition\nRetiring\nTags\nbusiness-intelligence\nArchitect\nMalika Mahoui\nContact(s)\nMargaret Xue\nSQL Server Analysis Services is an online analytical processing (OLAP) and data mining tool.\nUse Cases\n‚Äã\nThis tool is used for Bio-Meds area for data warehouse query to create cubes using data from data marts / data warehouse for deeper and faster data analysis\nNotes\n‚Äã\nThis tool is available only for existing SSAS users. Plan to use Azure Analysis Services to replace the tool. We will have POC testing soon in Azure.\nSee Also\n‚Äã\nhttps://collab.lilly.com/sites/EIP/SitePages/Enterprise-Analytics.aspx\nSSIS\n‚Äã\nPosition\nRetiring\nTags\nbusiness-intelligence\nInformation Sensitivity\nRed\nArchitect\nMalika Mahoui\nContact(s)\nMargaret Xue\nSQL Server Integration Services is a platform for data integration and workflow applications, used for data extraction, transformation, and loading (ETL).\nUse Cases\n‚Äã\nUpload data to another database or transfer data and ETL;Use for SAS licensing review and true up, to read data from spread sheet and calculate number of licenses automatically\nNotes\n‚Äã\nThis tool is available only for existing SSIS users. New users should plan to use Azure Data Factory to replace it.\nSee Also\n‚Äã\nhttps://collab.lilly.com/sites/EIP/SitePages/Enterprise-Analytics.aspx\nSSRS\n‚Äã\nPosition\nRetiring\nTags\nbusiness-intelligence\nInformation Sensitivity\nRed\nArchitect\nMalika Mahoui\nContact(s)\nMargaret Xue\nSQL Server Reporting Services is a server-based report generating software system. SQL reporting services can automate SSRS reports, and even drive tasks based on events.\nUse Cases\n‚Äã\nCreate adhoc tabular column report which span across multiple pages with different charts, create formal static reports, like budget or auditing reports.\nNotes\n‚Äã\nThis tool is available only for existing SSRS users. New users should consider using Power BI. Free tool  as license is part of Microsoft SQL server enterprise edition\nSee Also\n‚Äã\nhttps://collab.lilly.com/sites/EIP/SitePages/Enterprise-Analytics.aspx\nTableau (On-prem)\n‚Äã\nPosition\nRetiring\nTags\nbusiness-intelligence\nInformation Sensitivity\nRed\nArchitect\nMalika Mahoui\nContact(s)\nMumtaz.\nTableau On-Premises servers are gradually  being reduced and will retire by end of 2023. User Tableau AWS instead.\nUse if dashboards data, Visualization, Visual Analytics & Exploration, Augmented Analytics, Natural Language Querying, Statistical Clustering, Trendline, AI/ML Forecasting (basic), other statistical models and summaries, Integration with R, Python, Matlab  are the main considerations and wanted to make use of  wide range of ways to manipulate and visualize data. Supports more volume of data.\nUse Cases\n‚Äã\nCreate visualization dashboards using different chart options, Compelling story telling using data from different data stores, Use this tool if data visualization, visual data exploratory analytics, Natural Language Querying, no-code AI/ML, no code statistical modeling is the main objective.This tool is not suitable if you have requirements to collaborate using Microsoft Teams.\nNotes\n‚Äã\nTableau On-Premises servers are gradually  being reduced and will retire by end of 2023. User Tableau AWS instead.\nTableau On-Premises  Japan server is an exception as it will be retired by 2024.\nSee Also\n‚Äã\nhttps://collab.lilly.com/sites/EIP/SitePages/Enterprise-Analytics.aspx\nTop Braid\n‚Äã\nPosition\nRetired 2018-01-09\nTags\ndatabase-platforms\nArchitect\nLarry Zetzl\nContact(s)\nDouglas Marshall\nDocument generated from Stack Overflow Enterprise articles (prioritized) and ea_tech.csv (fallback)\nWas this helpful?\nEdit this page\nPrevious\nüìä Data & Analytics\nNext\nüß≠ Data Catalogs and Access\nAWS QuickSight\nAWS RDS Oracle\nAWS RDS PostgreSQL\nAWS RDS SQL Server\nAWS Redshift\nAWS Redshift Spectrum\nAzure CosmosDB\nAzure Data Explorer\nAzure Data Factory\nAzure Data Lake Storage\nAzure Data Share\nAzure Database Migration Service\nAzure DataBricks\nAzure PostgreSQL\nAzure SQL\nAzure SQL Managed Instance\nAzure Stream Analytics\nAzure Synapse Analytics\nAzure Table Storage\nDenodo\nHeroku Databases\nIBM Watson Explorer\nInVision (Kaltura)\nJMP\nKnime\nMicrosoft Search\nMicrosoft SQL Server on-prem\nMicrosoft Stream\nOneDrive for Business\nOpenShift Databases\nOpenText\nPosit (formerly RStudio) Workbench\nR\nReltio\nSalesforce Marketing Cloud - Messaging (Email & SMS)\nSalesforce Marketing Cloud ‚Äì Landing Pages\nSAS\nSharePoint Search\nSpotfire\nTableau AWS\nTrifacta\nVeeva Quality Docs\nVeeva Vault\nDeclining ‚Üí Retired\nAlteryx\nAnzo\nHAWQ\nHortonworks DPP/DLM\nHortonworks Hadoop\nIBM BigSQL\nIBM DB2\nMicrosoft SQL Server on AVS\nModern Search (O365/Delve)\nNetezza PDA\nOracle\nRegulus\nSalesforce Communities\nSSAS\nSSIS\nSSRS\nTableau (On-prem)\nTop Braid\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 5,
    "depth": 3,
    "crawled_at": "2026-02-25T10:08:58.041316"
  },
  "https://techhq.dc.lilly.com/docs/solution/data/data_catalogs_and_access": {
    "url": "https://techhq.dc.lilly.com/docs/solution/data/data_catalogs_and_access",
    "title": "üß≠ Data Catalogs and Access | Tech HQ",
    "description": "Stack Overflow Article: üß≠ Data Catalogs and Access",
    "h1": [
      "Data Catalogs and Access"
    ],
    "h2": [
      "Metadata Manager‚Äã",
      "Marketplace‚Äã"
    ],
    "h3": [],
    "text_content": "üß≠ Data Catalogs and Access | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüóÑÔ∏è Data & Analytics BLT\nüß≠ Data Catalogs and Access\nüß≠ Data Visualization\nüß≠ Integration Middleware\nüß≠ Specialized Databases\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nüìä Data & Analytics\nüß≠ Data Catalogs and Access\nOn this page\nData Catalogs and Access\nSynced daily from Stack Overflow\nSolution Guide\nLifecycle: Draft\nLast Update: 2025-10-02\nCapability Owner: Will Sherman\nEBA Lead: Christopher Blessing\nContributors & Reviewers: Will Sherman\nA data catalog acts like a library index for all enterprise data assets‚Äîdatasets, tables, files, and databases‚Äîacross cloud, on-prem, and hybrid environments. It enables users to:\nQuickly locate and understand data through metadata, lineage, and quality scores.\nAvoid duplication and reduce time spent searching for data.\nSupport self-service analytics by empowering non-technical users to find and use data confidently.\nLilly creates two things: Medicine and Data. A LOT of data. Data catalogs help Lilly manage that data to help us ensure we know where the data is, who owns it, how sensitive it is, and describes that data. Quality descriptions of data help teams identify if the data is good for their use case, enables AI to use the data effectively, and define what the data can, or cannot, be used for.\nLilly is a global company and so has challenges due to the different types of data collected, the technologies it is stored in, and the many different global governance rules that dictate what the data can be used for. Enterprise Data is helping Lilly meet those challenges by building a robust catalog solution that serves as single source of truth for metadata describing our data, access to that data, and a doorway to view observability facets of that data. A single solution for Lilly to describe and manage Lilly data.\nThe solution is provided through two applications found at\ndata.lilly.com\nMetadata Manager to manage catalog records\nMarketplace to manage access to data\nMetadata Manager\n‚Äã\nMetadata Manager\nenables consistent metadata collection, classification, and governance across enterprise inventories. It supports:\nMetadata harvesting from structured and unstructured sources\nIntegration with external data catalogs and APIs\nTagging, lineage tracking, policy management, API Usage Reporting, CI/PI/SPI classifications\nLilly governance built into the user experience, helping team members not miss crucial Lilly governance requirements\nFull integration to the ServiceNow Privacy Module, automating the privacy assessment process (Q1 2026)\nMarketplace\n‚Äã\nMarketplace\nautomates access provisioning through a self-service workflows and policy-driven approvals. It supports:\nComplete Automation for access to low sensitivity data for AD Groups, AWS Database Groups, ABAC user attributes and groups, IAM Roles (Q4 2025), and Entra Id (Q4 2025)\nAutomate provisioning after approval to AD Groups, AWS Database Groups, ABAC user attributes and groups, IAM Roles (Q4 2025), and Entra Id (Q4 2025)\nCentralized Access Management governance for policies, approvers, and provisioning for both the producers and the consumers of data\nRoster Management assistance for a single solution to view all roster details for data (Q1 2026)\nAudit logging and compliance tracking meeting GMP requirements for access management\nWas this helpful?\nTags:\ndata\nsolution-guide\ntechhq\nEdit this page\nPrevious\nüóÑÔ∏è Data & Analytics BLT\nNext\nüß≠ Data Visualization\nMetadata Manager\nMarketplace\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 7,
    "depth": 3,
    "crawled_at": "2026-02-25T10:09:00.239270"
  },
  "https://techhq.dc.lilly.com/docs/solution/data/data_visualization": {
    "url": "https://techhq.dc.lilly.com/docs/solution/data/data_visualization",
    "title": "üß≠ Data Visualization | Tech HQ",
    "description": "Stack Overflow Article: üß≠ Data Visualization",
    "h1": [
      "üß≠ Data Visualization"
    ],
    "h2": [
      "Power BI (PBI)‚Äã",
      "QuickSight‚Äã",
      "Tableau‚Äã",
      "Spotfire‚Äã",
      "Footnotes‚Äã"
    ],
    "h3": [
      "Footnotes‚Äã"
    ],
    "text_content": "üß≠ Data Visualization | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüóÑÔ∏è Data & Analytics BLT\nüß≠ Data Catalogs and Access\nüß≠ Data Visualization\nüß≠ Integration Middleware\nüß≠ Specialized Databases\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nüìä Data & Analytics\nüß≠ Data Visualization\nOn this page\nüß≠ Data Visualization\nSynced daily from Stack Overflow\nSolution Guide\nLifecycle: Draft\nLast Update: 2025-04-28\nCapability Owner: Ryan W Miller\nEBA Lead: Chris Blessing\nContributors & Reviewers: TBD\nThis is a solution guide for Visualization Tools. Visualization tools are powerful software applications that transform\ncomplex data into easy-to-understand visuals, allowing businesses to track relevant key metrics from one centralized,\nintuitive interface.\nThis guide is focused on recommending the\nbest\ntool for each scenario. The features of the most common tech tools\noverlap, but we are providing recommendations for best practices at Lilly. These scenarios are targeted at business\nanalyst type users, looking for a UI-based option to create visuals. Additional tech recommendations for code based\nvisuals (D3, Plotly, etc.) at Lilly will exist in a separate solution guide.\nUse Case/Scenario\nComplexity\n1\nTech Recommendation\n2\nIntegrations & Interop\nOwning Org/Team\nNext Step\nQuick report or visual creation\n0\nPower BI\nS3, Databases, APIs\nEnterprise Data\nUse, or request license if needed\nCreating dashboards with Copilot or Q&A features\n1\nPower BI\nS3, Databases, APIs\nEnterprise Data\nRequest PBI Copilot Access\nCollaboration, sharing, and embedding in Office apps\n1\nPower BI\nO365, SharePoint (both as source and publish)\nEnterprise Data\nUse, or request license if needed\nDashboards based on S3 Files\n3\n1\nQuickSight\nS3, AWS Databases, APIs\nEnterprise Data\nRequest Access\nExecutive Overview\n1\nPower BI Narrative Visuals\nS3, Databases, APIs\nEnterprise Data\nRequest PBI Copilot Access\nPaginated reporting\n2\nPower BI\nS3, Databases, APIs\nEnterprise Data\nUse, or request license if needed\nComplex data prep computations for visuals\n2\nPower BI\nS3, Databases, APIs\nEnterprise Data\nUse, or request license if needed\nCreating dashboards primarily for mobile devices\n2-3\nTableau\nDatabases, APIs\nEnterprise Data\nRequest License\nVisualizing chemical structures\n2\nSpotfire\nDatabases, and various third party connectors\nEnterprise Data\nRequest Access\nReal-time Data Monitoring\n3\nPower BI w/ Eventhouse\nS3, Databases, APIs\nEnterprise Data\nUse, or request license if needed\nIntegrating custom ML models with dashboards\n4\nQuickSight + SageMaker AI\nS3, Databases, APIs\nEnterprise Data\nUse AWB and review documentation\nSharing dashboards with external parties\n4\nPower BI\nS3, Databases, APIs\nEnterprise Data\nReview documentation and submit TPET\nPower BI (PBI)\n‚Äã\nPower BI is a commonly used tool at Lilly that integrates well with O365. Many users at Lilly have the access / license\nthey need by default as part of their standard Lilly account. Those needing to request access or a license can do so on\ndata.lilly.com\nCost: $\nPower BI Service Site\nQuickSight\n‚Äã\nQuickSight is AWS's offering for creating dashboards and visuals. It does not offer as many features as more mature tool\nlike PowerBI, but it has strong integrations with other AWS services.\nCost: $\nQuickSight Service Site\nTableau\n‚Äã\nAt the time of this writing, Tableau at Lilly lacks several of the AI related features offered in other tech options.\nDue to that, Power BI is recommended over Tableau in cases where both tech options meet the use case need.\nCost: $$$\nTableau Service Site\nSpotfire\n‚Äã\nSpotfire is a specialized tool, suggested specifically for visualization needs related to chemical structures.\nCost: $$\nSpotfire Service Site\nFootnotes\n‚Äã\nFootnotes\n‚Äã\n(0) Ready-to-use, (1) Configurable, (2) Customizable, (3) Requires integration, (4) High-complexity\n‚Ü©\nOnly the most significant integrations & interoperability are listed.\n‚Ü©\nOther tools can use S3, but QuickSight offers the simplest integration.\n‚Ü©\nWas this helpful?\nTags:\ndata\nsolution-guide\ntechhq\nEdit this page\nPrevious\nüß≠ Data Catalogs and Access\nNext\nüß≠ Integration Middleware\nPower BI (PBI)\nQuickSight\nTableau\nSpotfire\nFootnotes\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 6,
    "depth": 3,
    "crawled_at": "2026-02-25T10:09:02.440315"
  },
  "https://techhq.dc.lilly.com/docs/solution/data/integration_middleware": {
    "url": "https://techhq.dc.lilly.com/docs/solution/data/integration_middleware",
    "title": "üß≠ Integration Middleware | Tech HQ",
    "description": "Stack Overflow Article: üß≠ Integration Middleware",
    "h1": [
      "üß≠ Integration Middleware"
    ],
    "h2": [
      "AWS Glue‚Äã",
      "Azure Data Factory‚Äã",
      "Mulesoft‚Äã",
      "Apache Airflow‚Äã",
      "Informatica‚Äã",
      "Splunk‚Äã"
    ],
    "h3": [],
    "text_content": "üß≠ Integration Middleware | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüóÑÔ∏è Data & Analytics BLT\nüß≠ Data Catalogs and Access\nüß≠ Data Visualization\nüß≠ Integration Middleware\nüß≠ Specialized Databases\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nüìä Data & Analytics\nüß≠ Integration Middleware\nOn this page\nüß≠ Integration Middleware\nSynced daily from Stack Overflow\nSolution Guide\nLifecycle: Published\nLast Update: 2025-10-03\nCapability Owner: Chris Blessing\nEBA Lead: Larry Zetzl\nContributors & Reviewers: Larry Zetzl, Andy Mulder, Matt Van Auwelaer, Jacob Bishop\nA guide for designing secure, performant, and scalable integration architectures across AWS, Azure, and on-prem systems for real-time analytics and notifications.\nUse Case/Scenario\nPositioning\nComplexity\nTech Recommendation\nIntegrations & Interoperability\nOwning Org/Team\nNext Step\nInformation exchange between applications, microservices, and analytics solutions\nStandard\nMedium\nFind APIs in Backstage - use FastAPI accelerator to create new APIs and/or use code generation to invoke APIs\nEasiest method to securely connect among cloud, on-prem and 3rd parties\nEnterprise Data\nView the\nBackstage Entity Catalog\nUse\nBackstage API Code Generator\nData ingest into EDB ecosystem\nStandard\nLow\nRe-use one of the 16 configurable methods that are approved for all data classifications\nIncludes API, Database, Streaming, and File exchange templates\nEnterprise Data\nReview the ingest reusable components in this\nLillyFlow article\nTransform cloud data for consumption\nStandard\nMedium\nUse pre-built data pipelines, Configure reusable code\nFor more complex scenarios write Python or Spark code\nData transformation is executed in a compute platform (AWS, Azure, Databricks)\nEnterprise Data\nCheck if any of these configurable\nreusable code would work\nWrite Cloud Native ETL (AWS Glue, Azure ADF)\nTransform on-prem data for consumption\nSpecialized\nMedium\nUse existing ETL on-prem tooling (Informatica and Mulesoft) running on containers\nconnects systems at S95 L3 network\nEnterprise Data\nLimited to existing M&Q integrations\nOrchestrate multiple services for data pipeline processing\nStandard\nMedium\nUse Airflow\nConnects both cloud and on-prem services\nEnterprise Data\nReview Airflow Standards and Best Practice\nGuide\nNote:\nAll direct database connections are required to have an abstration layer and appropriate network controls.  See guidance\nhere\nAWS Glue\n‚Äã\nAWS Glue is a serverless data integration service ideal for ETL workloads. It supports real-time and batch processing, integrates natively with AWS services, and can be orchestrated via Airflow or Lambda. It‚Äôs well-suited for preparing data for analytics and machine learning.  Glue, Glue Studio and Glue Catalogs are used extensively in Enterprise Data.\nAzure Data Factory\n‚Äã\nADF is Azure‚Äôs ETL and data movement tool. It supports hybrid data integration and can connect to AWS S3, on-prem databases, and third-party services. It is used for orchestrating data flows across environments with built-in monitoring and retry capabilities.\nMulesoft\n‚Äã\nMulesoft provides a robust integration platform for connecting cloud and on-prem systems. It supports API-led connectivity, reusable connectors, and policy enforcement. It‚Äôs particularly valuable for legacy system integration and enterprise service bus (ESB) patterns. At Lilly, Mulesoft is classified as specialized and used only in Digital Health and Manufacturing.  Open Source stacks like Backstage and FastAPI are cheaper options and avoid vendor lock-in.\nApache Airflow\n‚Äã\nAirflow is a workflow orchestration tool used to schedule and monitor complex data pipelines. It integrates with AWS Glue, Azure ADF, and other analytics platforms. It‚Äôs useful for managing dependencies and ensuring reliable execution of analytics workflows.  Airflow is used extensively in Enterprise Data.\nInformatica\n‚Äã\nInformatica offers enterprise-grade data integration, governance, and quality tools. It supports hybrid cloud deployments and is often used for regulated environments requiring lineage and auditability. It integrates with AWS, Azure, and Splunk for end-to-end data management.  At Lilly, Informatica is classified as specialized and is used for on-prem Manufacturing worksloads, particularly with systems residing behind S95 Level-3 network frewalls.\nSplunk\n‚Äã\nSplunk is a real-time analytics and observability platform. It ingests logs, metrics, and events from cloud and on-prem systems. It‚Äôs used for security monitoring, alerting, and dashboarding. Integration with API Gateway and Glue enables real-time notifications and insights.\nWas this helpful?\nTags:\ndata\nsolution-guide\ntechhq\nEdit this page\nPrevious\nüß≠ Data Visualization\nNext\nüß≠ Specialized Databases\nAWS Glue\nAzure Data Factory\nMulesoft\nApache Airflow\nInformatica\nSplunk\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 5,
    "depth": 3,
    "crawled_at": "2026-02-25T10:09:04.736697"
  },
  "https://techhq.dc.lilly.com/docs/solution/data/specialized_databases": {
    "url": "https://techhq.dc.lilly.com/docs/solution/data/specialized_databases",
    "title": "üß≠ Specialized Databases | Tech HQ",
    "description": "Stack Overflow Article: üß≠ Specialized Databases",
    "h1": [
      "üß≠ Specialized Databases"
    ],
    "h2": [
      "AWS DynamoDB‚Äã",
      "Amazon Redshift‚Äã",
      "Azure Cosmos DB (SQL and Gremlin)‚Äã",
      "AWS Neptune‚Äã",
      "AWS ElastiCache for Redis‚Äã",
      "Azure Cache for Redis‚Äã"
    ],
    "h3": [],
    "text_content": "üß≠ Specialized Databases | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüóÑÔ∏è Data & Analytics BLT\nüß≠ Data Catalogs and Access\nüß≠ Data Visualization\nüß≠ Integration Middleware\nüß≠ Specialized Databases\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nüìä Data & Analytics\nüß≠ Specialized Databases\nOn this page\nüß≠ Specialized Databases\nSynced daily from Stack Overflow\nSolution Guide\nLifecycle: Draft\nLast Update: 2025-10-02\nCapability Owner: Manuela Elena Garc√≠a Conde\nEBA Lead: Chris Blessing\nContributors & Reviewers: TBD\nA guide for selecting and applying specialized NoSQL, graph, in-memory, search, and analytical databases optimized for high-performance, low-latency, and scalable workloads across AWS and Azure platforms.\nAvoid using these technologies for traditional OLTP workloads requiring strict ACID compliance across multiple tables.\nUse Case/Scenario\nTech Recommendation\nPositioning\nComplexity\nKey Integrations\nOwning Org/Team\nNext Step\nKey-value and document store for serverless apps\nAWS DynamoDB\nStandard\nMedium\nAWS Lambda, API Gateway, IAM, CloudWatch, DAX\nEnterprise Data\nhttps://collab.lilly.com/sites/CloudPaaSDatabases\nCloud data warehouse for analytics\nAmazon Redshift\nStandard\nHigh\nAWS Glue, QuickSight, S3, Redshift Spectrum, IAM\nEnterprise Data\nhttps://collab.lilly.com/sites/CloudPaaSDatabases\nGlobally distributed multi-model database\nAzure Cosmos DB (SQL)\nStandard\nMedium\nAzure Functions, Azure Synapse, Azure Monitor, Azure Private Link\nEnterprise Data\nhttps://collab.lilly.com/sites/CloudPaaSDatabases\nGraph-based querying and relationship modeling\nAzure Cosmos DB (Gremlin)\nStandard\nHigh\nGremlin API, Azure Monitor, Azure Synapse, Azure Functions\nEnterprise Data\nhttps://collab.lilly.com/sites/CloudPaaSDatabases\nGraph-based relationship querying\nAWS Neptune\nStandard\nHigh\nGremlin, SPARQL, Amazon SageMaker, AWS Glue, IAM\nEnterprise Data\nhttps://collab.lilly.com/sites/CloudPaaSDatabases\nReal-time caching for AWS apps\nAWS ElastiCache for Redis\nStandard\nMedium\nAmazon EC2, Lambda, RDS, CloudWatch, VPC\nEnterprise Data\nhttps://collab.lilly.com/sites/CloudPaaSDatabases\nReal-time caching for Azure apps\nAzure Cache for Redis\nStandard\nMedium\nAzure App Service, Azure Kubernetes Service, Azure Monitor, VNet\nEnterprise Data\nhttps://collab.lilly.com/sites/CloudPaaSDatabases\nAWS DynamoDB\n‚Äã\nAmazon DynamoDB is a fully managed NoSQL database service designed for key-value and document data models. It offers single-digit millisecond performance at any scale and is ideal for serverless applications, IoT, gaming, and mobile backends. It integrates tightly with AWS Lambda, API Gateway, and IAM for secure, scalable architectures.\nAmazon Redshift\n‚Äã\nAmazon Redshift is a fully managed, petabyte-scale data warehouse service. It supports complex analytical queries across structured and semi-structured data. Redshift integrates with AWS Glue, S3, and QuickSight for end-to-end analytics pipelines.\nAzure Cosmos DB (SQL and Gremlin)\n‚Äã\nAzure Cosmos DB is a globally distributed, multi-model database service. The SQL API supports document-based workloads with rich querying, while the Gremlin API enables graph-based querying for complex relationships. Cosmos DB offers multi-region writes, low-latency reads, and five consistency models.\nAWS Neptune\n‚Äã\nAmazon Neptune is a purpose-built graph database optimized for storing and querying highly connected data. It supports both property graph (Gremlin) and RDF (SPARQL) models. Neptune is ideal for knowledge graphs, fraud detection, and recommendation engines.\nAWS ElastiCache for Redis\n‚Äã\nAWS ElastiCache for Redis is a fully managed in-memory data store and cache service. It supports sub-millisecond latency and is commonly used for session storage, real-time analytics, and pub/sub messaging.\nAzure Cache for Redis\n‚Äã\nAzure Cache for Redis is a fully managed, in-memory cache that enables high-throughput and low-latency data access. It supports Redis data structures and is ideal for caching, session state, and real-time analytics in Azure-hosted applications.\nWas this helpful?\nTags:\ndata\nsolution-guide\ntechhq\nEdit this page\nPrevious\nüß≠ Integration Middleware\nNext\nüõ†Ô∏è Engineering Enablement\nAWS DynamoDB\nAmazon Redshift\nAzure Cosmos DB (SQL and Gremlin)\nAWS Neptune\nAWS ElastiCache for Redis\nAzure Cache for Redis\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 3,
    "depth": 3,
    "crawled_at": "2026-02-25T10:09:06.986762"
  },
  "https://techhq.dc.lilly.com/docs/solution/engineering/engineering-blt": {
    "url": "https://techhq.dc.lilly.com/docs/solution/engineering/engineering-blt",
    "title": "üóÑÔ∏è Engineering Enablement BLT | Tech HQ",
    "description": "The Engineering Enablement Big List of Technology (BLT) is maintained using data from the Enterprise Architecture Tech Positioning Database and LillyFlow, with AI-assisted updates. While we aim for accuracy, please verify critical information and contact the EBA team with questions or corrections.",
    "h1": [
      "üóÑÔ∏è Engineering Enablement BLT"
    ],
    "h2": [
      "Acunetix 360‚Äã",
      "ALM‚Äã",
      "AppliTools‚Äã",
      "Artifactory‚Äã",
      "Artifactory X-Ray‚Äã",
      "Atlassian Confluence‚Äã",
      "Atlassian Jira‚Äã",
      "AutonomIQ‚Äã",
      "CARD‚Äã",
      "Cypress‚Äã",
      "GHAS - Code Scanning‚Äã",
      "GHAS - Secret Scanning‚Äã",
      "GitHub‚Äã",
      "Github - Dependabot‚Äã",
      "GitHub Actions‚Äã",
      "GitHub Codespaces‚Äã",
      "GitHub Container Registry (GCR)‚Äã",
      "GitHub Packages‚Äã",
      "Loader.IO‚Äã",
      "Lucid‚Äã",
      "Microsoft Project‚Äã",
      "NewMan‚Äã",
      "PlayWright‚Äã",
      "PostMan‚Äã",
      "Prisma‚Äã",
      "Prochain‚Äã",
      "RoadMunk‚Äã",
      "Saucelabs‚Äã",
      "Slack‚Äã",
      "WIP/STaRT‚Äã",
      "Wrike‚Äã",
      "Declining ‚Üí Retired‚Äã"
    ],
    "h3": [
      "AppDynamics‚Äã",
      "Appium‚Äã",
      "Crucible-Fisheye‚Äã",
      "Managed Jenkins (EDAT)‚Äã",
      "Microfocus LoadRunner‚Äã",
      "Microfocus Mobile Center‚Äã",
      "Microfocus UFT‚Äã",
      "Mission Control (MICO)‚Äã",
      "Oasis‚Äã",
      "Selenium‚Äã",
      "Snyk‚Äã"
    ],
    "text_content": "üóÑÔ∏è Engineering Enablement BLT | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüóÑÔ∏è Engineering Enablement BLT\nüß≠ AI Code Assistants\nAPI Standards\nüß≠ ML Ops\nüß≠ Test Automation\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nüõ†Ô∏è Engineering Enablement\nüóÑÔ∏è Engineering Enablement BLT\nOn this page\nüóÑÔ∏è Engineering Enablement BLT\nThe Engineering Enablement Big List of Technology (BLT) is maintained using data from the Enterprise Architecture Tech Positioning Database and LillyFlow, with AI-assisted updates. While we aim for accuracy, please verify critical information and contact the EBA team with questions or corrections.\nCan't find a technology?\nUse TechHQ's search feature to look across all technology documentation. For questions or to suggest additions to this list, contact the\nEnterprise Business Architecture team\n.\nAcunetix 360\n‚Äã\nPosition\nSpecialized\nTags\ndev-tools\nInformation Sensitivity\nUnder Review\nArchitect\nJoshua Day\nContact(s)\nJoshua Day\nPreferred Dynamic Application Security Test (DAST) for External & Red CI Applications.\nUse Cases\n‚Äã\nAny external facing Red CI application needing a DAST Scan.\nNotes\n‚Äã\nRight now only Red CI applications which are externally facing can use this application\nALM\n‚Äã\nPosition\nSpecialized\nTags\ndev-tools\nInformation Sensitivity\nOrange\nArchitect\nJosh Bloxsome\nContact(s)\nPallavi Bora\nCMDB CI\nCI00000015858471\nJira is the preferred test management tool over ALM. Test management tool to manage various application lifecycle stages from requirements gathering and testing to defect management.\nUse Cases\n‚Äã\nProvide highly regulated teams (especially for GXP or GMP applications) a way to formally track requirements, test artifacts and defects.\nNotes\n‚Äã\nJira, along with testing capabilities provided through Xray, should be considered as an alternative when possible.\nSee Also\n‚Äã\nhttps://collab.lilly.com/sites/EISQualityCenterService/SitePages/Home.aspx\nAppliTools\n‚Äã\nPosition\nStandard\nTags\ndev-tools\nInformation Sensitivity\nOrange\nArchitect\nYu Zhang\nContact(s)\nMatt Wambsganss\nCMDB CI\nCI00000000443330\nArtifactory\n‚Äã\nPosition\nStandard\nTags\ndev-tools\nInformation Sensitivity\nOrange\nArchitect\nJosh Bloxsome\nContact(s)\nChris Tornatta\nPreferred Artefact Storage Tool.\nUse Cases\n‚Äã\nDocker, PyPi, NPM, Binary, Docker, CoCoPods hosting.\nSee Also\n‚Äã\nhttps://developer.lilly.com/tools/artifactory%23artifactory-access-management\nArtifactory X-Ray\n‚Äã\nPosition\nSpecialized\nTags\ndev-tools\nInformation Sensitivity\nOrange\nArchitect\nJosh Bloxsome\nContact(s)\nChris Tornatta\nSpecialized Container Image Scanning tool for containers stored within Artifactory.\nUse Cases\n‚Äã\nUsed for scanning packages in Artifactory.\nNotes\n‚Äã\nIs a really manual tool to set up, this will soon be replaced by Prisma\nAtlassian Confluence\n‚Äã\nPosition\nStandard\nTags\ndev-tools\nInformation Sensitivity\nOrange\nArchitect\nJosh Bloxsome\nContact(s)\nAshley Jenkins\nPreferred tool for Software Product Documentation and Collaboration\nUse Cases\n‚Äã\nAgile Projects needing a place to collaborate on documentation and post stand up notes, etc.\nSee Also\n‚Äã\nhttps://developer.lilly.com/tools/confluence%23confluence-access-management\nAtlassian Jira\n‚Äã\nPosition\nStandard\nTags\ndev-tools\nInformation Sensitivity\nOrange\nArchitect\nJosh Bloxsome\nContact(s)\nAshley Jenkins\nPreferred tool for Agile based development lifecycle management\nUse Cases\n‚Äã\nAgile Projects needing a place to store progress of works and track completion deadlines.\nNotes\n‚Äã\nProject happening right now to try and get GXP + GMP Projects using Jira instead of needing to use ALM (or using them both together in an auotmated way)\nSee Also\n‚Äã\nhttps://developer.lilly.com/tools/jira%23jira-service-catalog-management\nAutonomIQ\n‚Äã\nPosition\nSpecialized\nTags\ndev-tools\nInformation Sensitivity\nOrange\nArchitect\nYu Zhang\nContact(s)\nMatt Wambsganss\nCMDB CI\nCI00000041565559\nEnterprise owned and supported.\nUse Cases\n‚Äã\nLow-code UI testing tool owned by and integrated with Sauce Labs.\nWhere low-code tools make sense over traditional test automation.\nCARD\n‚Äã\nPosition\nStandard\nTags\ndev-tools\nInformation Sensitivity\nOrange\nArchitect\nMike McCoy\nContact(s)\nMike McCoy\nSpecialized for IT/IDS financial planning and analysis.\nNotes\n‚Äã\nSAP is source finanical system.  CARD uploads plan to SAP for IDS function globally and tracks actuals to projects and sub functions within IDS.\nCypress\n‚Äã\nPosition\nStandard\nTags\ndev-tools\nInformation Sensitivity\nOrange\nArchitect\nYu Zhang\nContact(s)\nMatt Wambsganss\nEnterprise owned and supported.\nAn enterprise preferred test automation framework.\nUse Cases\n‚Äã\nChromium Based Browser Testing for Modern Web UI Applications.\nAny modern application with a UI (SPA)\nGHAS - Code Scanning\n‚Äã\nPosition\nStandard\nTags\ndev-tools\nInformation Sensitivity\nRed\nArchitect\nJosh Bloxsome\nContact(s)\nAmruta Tanksale\nPreferred Static Application Security Testing (SAST) Tool for Modern Software Applications.\nUse Cases\n‚Äã\nApplications needing a SAST Scan. Specific Languages can be found\nNotes\n‚Äã\nRelease in Q1 2021, will be used by most of the enterprise by end of Q2 2021.\nSee Also\n‚Äã\nhttps://developer.lilly.com/tools/github%20advanced%20security\nGHAS - Secret Scanning\n‚Äã\nPosition\nStandard\nTags\ndev-tools\nInformation Sensitivity\nRed\nArchitect\nJosh Bloxsome\nContact(s)\nAmruta Tanksale\nPreferred Secret Scanning Tool. Positioned for helping teams find leaked secrets and vulnerabilities.\nUse Cases\n‚Äã\nAny team using GitHub Repos.\nSee Also\n‚Äã\nhttps://developer.lilly.com/tools/github%20advanced%20security\nGitHub\n‚Äã\nPosition\nStandard\nTags\ndev-tools\nInformation Sensitivity\nRed\nArchitect\nJosh Bloxsome\nContact(s)\nAmruta Tanksale\nPreferred Tool for storing Source Code.\nUse Cases\n‚Äã\nBuilding Software / Storing Code\nNotes\n‚Äã\nA lot of focus from an enterprise perspective is going on GitHub. The more your developers live in GitHub, the more it's aligning to enterprise direction.\nSee Also\n‚Äã\nhttps://dev.lilly.com/docs/platforms-and-tools/github/\nGithub - Dependabot\n‚Äã\nPosition\nStandard\nTags\ndev-tools\nInformation Sensitivity\nRed\nArchitect\nJosh Bloxsome\nContact(s)\nAmruta Tanksale\nOpen Source Analysis (OSA) Tool for Modern Software Applications. Only used when Snyk is not an option.\nUse Cases\n‚Äã\nLow Risk use cases not needing to run OSA as part of a CI/CD process.\nNotes\n‚Äã\nWill soon become the enterprise tool to replace Snyk once better support comes in.\nSee Also\n‚Äã\nhttps://developer.lilly.com/tools/github%20advanced%20security\nGitHub Actions\n‚Äã\nPosition\nStandard\nTags\ndev-tools\nInformation Sensitivity\nRed\nArchitect\nJosh Bloxsome\nContact(s)\nChris Johnson\nPreferred Tool for Orchestrating automated tests and checks on an application, as well as deployment if required.\nUse Cases\n‚Äã\nEvery GitHub Repo looking to drive any sort of automation\nSee Also\n‚Äã\nhttps://lillydev.com/tools/github%20actions#github-actions-introduction\nGitHub Codespaces\n‚Äã\nPosition\nStandard\nTags\ndev-tools\nInformation Sensitivity\nRed\nArchitect\nJosh Bloxsome\nContact(s)\nChris Johnson\nA SaaS IDE offered by GitHub that provides temporary compute resources that allow a developer to develop directly in the browser versus consuming a thick client locally on their computer. This solution also allows for .dev containers that provide developer parity between developers virtual IDEs. GitHub CodeSpaces increases Lilly security posture by removing development from Lilly endpoints.\nUse Cases\n‚Äã\nSaaS Developer IDE\nOnline IDE for Software Engineering/Development\nAllows for executing Container images\nSee Also\n‚Äã\nhttps://docs.github.com/en/codespaces\nGitHub Container Registry (GCR)\n‚Äã\nPosition\nSpecialized\nTags\ndev-tools\nInformation Sensitivity\nRed\nArchitect\nJosh Bloxsome\nContact(s)\nAmruta Tanksale\nEmerging Storage Mechanism for Containers\nUse Cases\n‚Äã\nStoring containers within GitHub\nNotes\n‚Äã\nI can see in 1/2 years GitHub Container Registry (Alongside GitHubPackages) will replace Artifactory.\nGitHub Packages\n‚Äã\nPosition\nSpecialized\nTags\ndev-tools\nInformation Sensitivity\nRed\nArchitect\nJosh Bloxsome\nContact(s)\nAmruta Tanksale\nEmerging Use Case for Application Packages (Npm, PyPi, etc.)\nUse Cases\n‚Äã\nPackage Hosting\nNotes\n‚Äã\nSpecalized use case for handling packages in Github\nLoader.IO\n‚Äã\nPosition\nStandard\nTags\ndev-tools\nInformation Sensitivity\nOrange\nArchitect\nYu Zhang\nContact(s)\nMatt Wambsganss\nLucid\n‚Äã\nPosition\nSpecialized\nTags\ndev-tools\nInformation Sensitivity\nYellow\nArchitect\nJosh Bloxsome\nContact(s)\nPallavi Bora\nCMDB CI\nCI00000000408911\nSpecialized for modern software teams needing to build architecture diagrams of their product.\nUse Cases\n‚Äã\nArchitects needing a place to visualize how an application is going to look.\nSee Also\n‚Äã\nhttps://developer.lilly.com/tools/lucidchart%23lucidchart-access-and-authentication\nMicrosoft Project\n‚Äã\nPosition\nStandard\nTags\ndev-tools\nInformation Sensitivity\nOrange\nPreferred for basic (waterfall) project management development lifecycle management\nUse Cases\n‚Äã\nTeams running projects without a professional project manager\nNotes\n‚Äã\nMust have a Microsoft license\nNewMan\n‚Äã\nPosition\nStandard\nTags\ndev-tools\nInformation Sensitivity\nOrange\nArchitect\nYu Zhang\nContact(s)\nMatt Wambsganss\nCMDB CI\nCI00000062946598\nPlayWright\n‚Äã\nPosition\nStandard\nTags\ndev-tools\nInformation Sensitivity\nOrange\nArchitect\nYu Zhang\nContact(s)\nMatt Wambsganss\nCMDB CI\nCI00000018681883\nPostMan\n‚Äã\nPosition\nStandard\nTags\ndev-tools\nInformation Sensitivity\nOrange\nArchitect\nYu Zhang\nContact(s)\nMatt Wambsganss\nCMDB CI\nCI00000062946598\nPrisma\n‚Äã\nPosition\nEmerging\nTags\ndev-tools\nArchitect\nJoe Varga\nContact(s)\nJoshua Day\nSpecialized Container Image Scanning tool for containers globally. Right now in Emerging in POC, but will expand in coming quarters.\nUse Cases\n‚Äã\nLimited Scope right now, only in POC, waiting for finding.\nNotes\n‚Äã\nCurrently not approved, but is looking for funding in 2021 for a rollout later this year or 2022\nProchain\n‚Äã\nPosition\nSpecialized\nTags\ndev-tools\nInformation Sensitivity\nRed\nArchitect\nAnthony M Soller\nContact(s)\nHal Levine\nCMDB CI\nCI00000000216001\nPreferred for critical chain project management development lifecycle management\nUse Cases\n‚Äã\nTeams running projects with the leadership of a critical chain trained project manager\nNotes\n‚Äã\nMust have a ProChain license and be trained by the Project Management Office\nRoadMunk\n‚Äã\nPosition\nSpecialized\nTags\ndev-tools\nInformation Sensitivity\nYellow\nArchitect\nJosh Bloxsome\nContact(s)\nPallavi Bora\nCMDB CI\nCI00000000403942\nRoadmap Diagram Tool\nUse Cases\n‚Äã\nProject Managers or leaders needing a place to show higher leadership overview roadmaps.\nNotes\n‚Äã\nLucidchart or Jira Align can be used in place of Roadmunk.\nSee Also\n‚Äã\nhttps://developer.lilly.com/tools/roadmunk%23roadmunk-access-and-authentication\nSaucelabs\n‚Äã\nPosition\nStandard\nTags\ndev-tools\nInformation Sensitivity\nOrange\nArchitect\nYu Zhang\nContact(s)\nMatt Wambsganss\nCMDB CI\nCI00000000388043\nEnterprise owned and supported.\nUse Cases\n‚Äã\nCross browser, device, and OS functional testing.\nNotes\n‚Äã\nRight now doesn't work with our AWS Enviroment within Lilly. This is a problem due to authentication\nSee Also\n‚Äã\nhttps://developer.lilly.com/tools/sauce%20labs\nSlack\n‚Äã\nPosition\nSpecialized\nTags\ncollaboration-tools\nInformation Sensitivity\nYellow\nArchitect\nJosh Bloxsome\nContact(s)\nPeter Burke\nCMDB CI\nCI00000000315410\nSlack is a collaboration tool that can be used in the context of DevOps application development projects only. For general collaboration use one of the Enterprise/Full Scale alternatives\nWIP/STaRT\n‚Äã\nPosition\nStandard\nTags\ndev-tools\nInformation Sensitivity\nOrange\nArchitect\nMichelle Schnabel\nContact(s)\nMichelle Schnabel\nPreferred for IDS Project Portfolio Management\nUse Cases\n‚Äã\nSource of truth for approved IDS project investments. Facilitates investment decisions, business case details, resource demand, and provides communication and reporting on the health of IDS programs and projects.  Primary source for IDS leadership project portfolio reporting.\nNotes\n‚Äã\nEvaluating replacement options for WIP/STaRT.  In-depth requirements assessment in process. Current architecture being sured up appropriately for remainder of life.\nWrike\n‚Äã\nPosition\nSpecialized\nTags\ndev-tools\nInformation Sensitivity\nUnder Review\nArchitect\nChad Stout\nContact(s)\nChad Stout\nContent & Project Management Tools within the Business Units. Specifically BUIDS.\nUse Cases\n‚Äã\nRight now only positioned to be used within BU. Greater comparisons between current tools would need to be done to expand.\nDeclining ‚Üí Retired\n‚Äã\nAppDynamics\n‚Äã\nPosition\nRetired 2021-04-01\nTags\ndev-tools\nAppium\n‚Äã\nPosition\nDeclining 2023-01-01\nTags\ndev-tools\nArchitect\nYu Zhang\nContact(s)\nMatt Wambsganss\nCMDB CI\nCI00000054241386\nEnterprise replaced with PlayWright\nUse Cases\n‚Äã\nApple or Android (Windows & Mac) Automated Testing\nNotes\n‚Äã\nNot the easiest tool to use - should only be used by teams that are experinced developers.\nSee Also\n‚Äã\nhttps://developer.lilly.com/tools/test%20automation\nCrucible-Fisheye\n‚Äã\nPosition\nRetired 2020-12-31\nTags\ndev-tools\nManaged Jenkins (EDAT)\n‚Äã\nPosition\nRetired 2022-03-31\nTags\ndev-tools\nInformation Sensitivity\nRed\nArchitect\nJosh Bloxsome\nContact(s)\nAmruta Tanksale\nNo new pipelines or Jenkins Masters should be created unless an exception is granted by EDAT\nUse Cases\n‚Äã\nRunning CI for applications on Heroku, Artifactory, OpenShift, etc.\nNotes\n‚Äã\nEnterprise Managed Jenkins is now retired.\nSee Also\n‚Äã\nhttps://developer.lilly.com/tools/cloudbees%20jenkins%20enterprise\nMicrofocus LoadRunner\n‚Äã\nPosition\nDeclining 2023-11-29\nTags\ndev-tools\nInformation Sensitivity\nOrange\nArchitect\nYu Zhang\nContact(s)\nMatt Wambsganss\nCMDB CI\nCI00000000004044\nEnterprise owned but declining.\nUse Cases\n‚Äã\nAPI or SPA or Monolithic Load Testing.\nSee Also\n‚Äã\nhttps://developer.lilly.com/tools/test%20automation%23test-automation-available-tools-loadrunner\nMicrofocus Mobile Center\n‚Äã\nPosition\nDeclining 2022-01-02\nTags\ndev-tools\nArchitect\nRob Blacketter\nContact(s)\nAndrew Marshall\nDeclining Tool for Carrying out Mobile Tests.\nUse Cases\n‚Äã\nApple or Android Automated Testing\nNotes\n‚Äã\nTeams should be using Appium or other services avaiable.\nSee Also\n‚Äã\nhttps://developer.lilly.com/tools/test%20automation\nMicrofocus UFT\n‚Äã\nPosition\nDeclining 2023-11-29\nTags\ndev-tools\nInformation Sensitivity\nOrange\nArchitect\nYu Zhang\nContact(s)\nMatt Wambsganss\nCMDB CI\nCI00000000361960\nEnterprise owned but declining.\nUse Cases\n‚Äã\ndesktop functional testing tool.\nSee Also\n‚Äã\nhttps://developer.lilly.com/tools/test%20automation\nMission Control (MICO)\n‚Äã\nPosition\nRetired 2022-03-31\nTags\ndev-tools\nInformation Sensitivity\nRed\nArchitect\nRob Blacketter\nContact(s)\nRob Blacketter\nDeclining Salesforce Change Control Tool.\nUse Cases\n‚Äã\nTeams building on Salesforce kneeing change control.\nNotes\n‚Äã\nTeams will be hearing more from EDAT in the coming months about this. I think this has a retiremtn date of June.\nSee Also\n‚Äã\nhttps://developer.lilly.com/tools/mission%20control%23mico-access-management\nOasis\n‚Äã\nPosition\nRetired 2023-04-01\nTags\ndev-tools\nArchitect\nZach Siebers\nRetirement positioning guidance to be updated mid-March\nNotes\n‚Äã\nPositioned for applications on-premise needing performance monitoring\nSee Also\n‚Äã\nhttps://lilly.service-now.com/kb_view.do?sysparm_article=KB0093077\nSelenium\n‚Äã\nPosition\nDeclining 2023-01-01\nTags\ndev-tools\nArchitect\nYu Zhang\nContact(s)\nMatt Wambsganss\nEnterprise replaced with Cypress and PlayWright\nUse Cases\n‚Äã\nSPA or Monolithic Web Application Automated Testing\nNotes\n‚Äã\nNot the easiest tool to use - should only be used by teams that are experinced developers.\nSee Also\n‚Äã\nhttps://developer.lilly.com/tools/test%20automation%20self%20service%23test-automation-self-service-available-tools\nSnyk\n‚Äã\nPosition\nRetiring\nTags\ndev-tools\nInformation Sensitivity\nOrange\nArchitect\nJosh Bloxsome\nContact(s)\nChristopher Tornatta\nPreferred Open Source Analysis (OSA) Tool for Modern Software Applications.\nUse Cases\n‚Äã\nSoftware pulling in third party dependencies.\nNotes\n‚Äã\nWill soon only be used for specilised use cases where Dependabot support isn't avaiable.\nSee Also\n‚Äã\nhttps://developer.lilly.com/backing%20services/snyk\nDocument generated from Stack Overflow Enterprise articles (prioritized) and ea_tech.csv (fallback)\nWas this helpful?\nEdit this page\nPrevious\nüõ†Ô∏è Engineering Enablement\nNext\nüß≠ AI Code Assistants\nAcunetix 360\nALM\nAppliTools\nArtifactory\nArtifactory X-Ray\nAtlassian Confluence\nAtlassian Jira\nAutonomIQ\nCARD\nCypress\nGHAS - Code Scanning\nGHAS - Secret Scanning\nGitHub\nGithub - Dependabot\nGitHub Actions\nGitHub Codespaces\nGitHub Container Registry (GCR)\nGitHub Packages\nLoader.IO\nLucid\nMicrosoft Project\nNewMan\nPlayWright\nPostMan\nPrisma\nProchain\nRoadMunk\nSaucelabs\nSlack\nWIP/STaRT\nWrike\nDeclining ‚Üí Retired\nAppDynamics\nAppium\nCrucible-Fisheye\nManaged Jenkins (EDAT)\nMicrofocus LoadRunner\nMicrofocus Mobile Center\nMicrofocus UFT\nMission Control (MICO)\nOasis\nSelenium\nSnyk\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 5,
    "depth": 3,
    "crawled_at": "2026-02-25T10:09:09.333252"
  },
  "https://techhq.dc.lilly.com/docs/solution/engineering/ai_code_assistants": {
    "url": "https://techhq.dc.lilly.com/docs/solution/engineering/ai_code_assistants",
    "title": "üß≠ AI Code Assistants | Tech HQ",
    "description": "Stack Overflow Article: üß≠ AI Code Assistants",
    "h1": [
      "üß≠ AI Code Assistants"
    ],
    "h2": [
      "Quick Reference‚Äã",
      "Detailed Overview‚Äã",
      "Enterprise Integration: Lilly Code‚Äã"
    ],
    "h3": [
      "GitHub Copilot‚Äã",
      "Cline‚Äã",
      "Claude Code‚Äã",
      "Amazon Q‚Äã",
      "Access Steps‚Äã",
      "Key Features‚Äã"
    ],
    "text_content": "üß≠ AI Code Assistants | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüóÑÔ∏è Engineering Enablement BLT\nüß≠ AI Code Assistants\nAPI Standards\nüß≠ ML Ops\nüß≠ Test Automation\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nüõ†Ô∏è Engineering Enablement\nüß≠ AI Code Assistants\nOn this page\nüß≠ AI Code Assistants\nSynced daily from Stack Overflow\nSolution Guide\nLifecycle: Draft\nLast Update: 2025-10-16\nCapability Owner: Bala Devaraj\nEBA Lead: Josh Bloxsome\nContributors & Reviewers:\nA comprehensive guide to AI code assistants including GitHub Copilot, Cline, Claude Code, and Amazon Q for enhanced developer productivity.\nQuick Reference\n‚Äã\nHere's a comparison of available AI assistants:\nAI Assistant\nPositioning\nComplexity\nKey Integrations\nUsage Requirements\nGitHub Copilot\nStandard\nLow\nVS Code, GitHub, GitHub Codespaces, IDEs\nCopilot subscription (monthly basis)\nCline\nEmerging\nMedium\nVS Code, Development Tools, Lilly Code\nUse via Lilly Code (SSO integration)\nClaude Code\nEmerging\nMedium\nIDEs, Development Workflow, Lilly Code\nUse via Lilly Code (SSO integration)\nAmazon Q\nSpecialized\nMedium\nAWS Sagemaker Studio, JetBrains, Amazon Quicksight\nAWS Subscription\nDetailed Overview\n‚Äã\nGitHub Copilot\n‚Äã\nGitHub Copilot provides AI-powered code suggestions and completions directly in your IDE, helping developers write code faster with context-aware recommendations based on natural language comments and existing code patterns.\nUpcoming Feature: Copilot Coding Agent\n‚Äã\nGitHub is introducing a new\nCopilot Coding Agent\nfeature that will extend beyond simple code completion to provide more autonomous coding capabilities. This agent will be able to understand complex development tasks, make multi-file changes, and provide more comprehensive coding assistance while maintaining the seamless integration with GitHub's ecosystem.\nCline\n‚Äã\nCline offers autonomous coding capabilities with the ability to create, edit, and run files while providing step-by-step explanations. It can handle complex multi-file editing tasks and interact with terminal commands for comprehensive development workflows.\nClaude Code\n‚Äã\nClaude Code is a Terminal-native AI assistant with VS Code integration support. Perfect for command-line workflows and development environments.\nAmazon Q\n‚Äã\nAmazon Q is an AI-powered assistant designed for specialized scenarios within the AWS ecosystem. It provides intelligent assistance in environments like AWS Sagemaker Studio with JetBrains and offers data analysis capabilities within Amazon Quicksight.\nEnterprise Integration: Lilly Code\n‚Äã\nLilly Code is an enterprise tool that provides seamless authentication integration between Lilly's SSO and AI Coding tools like Claude Code and Cline, enabling secure access to AI capabilities for the AI team.\nAccess Steps\n‚Äã\nLilly Code is in process of migrating from alpha to production. To access the alpha version for evaluation purposes please reach out to the LLM Gateway team:\nhttps://gateway.llm.lilly.com\n.\nAfter go-live, Lilly Code will be available through the Developer Platform Front Door at\nhttps://dev.lilly.com\nKey Features\n‚Äã\nSSO Integration\n: Use your existing Lilly credentials\nEasy Setup\n: One-command installation with automatic configuration\nTerminal Native\n: Works directly in your command line workflow\nUnopinionated\n: General purpose, low-level model access\nVersatile\n: Fits into existing workflows and tools, including Claude Code and Cline support\nFuture-Ready\n: Models keep getting better, your setup stays the same\nWas this helpful?\nTags:\ngithub-copilot\nsoftware-engineering\nsolution-guide\ntechhq\nEdit this page\nPrevious\nüóÑÔ∏è Engineering Enablement BLT\nNext\nAPI Standards\nQuick Reference\nDetailed Overview\nGitHub Copilot\nCline\nClaude Code\nAmazon Q\nEnterprise Integration: Lilly Code\nAccess Steps\nKey Features\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 8,
    "depth": 3,
    "crawled_at": "2026-02-25T10:09:12.610941"
  },
  "https://techhq.dc.lilly.com/docs/solution/engineering/api_standards": {
    "url": "https://techhq.dc.lilly.com/docs/solution/engineering/api_standards",
    "title": "API Standards | Tech HQ",
    "description": "Stack Overflow Article: API Standards",
    "h1": [
      "üß≠ API Standards"
    ],
    "h2": [
      "Backstage‚Äã",
      "Azure API Management‚Äã",
      "FastAPI Framework for Building REST APIs‚Äã",
      "Strawberry Framework for Building GraphQL APIs‚Äã",
      "Deploying APIs at Lilly‚Äã"
    ],
    "h3": [],
    "text_content": "API Standards | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüóÑÔ∏è Engineering Enablement BLT\nüß≠ AI Code Assistants\nAPI Standards\nüß≠ ML Ops\nüß≠ Test Automation\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nüõ†Ô∏è Engineering Enablement\nAPI Standards\nOn this page\nüß≠ API Standards\nSynced daily from Stack Overflow\nSolution Guide\nLifecycle: Published\nLast Update: 2025-10-02\nCapability Owner: Christopher P Blessing\nEBA Lead: Larry Zetzl\nContributors & Reviewers: Jacob Bishop, Matt Van Auwelaer, Vinoth Vijayakumar, Larry Zetzl\nA guide for standardizing API development, management, and lifecycle practices across the enterprise using modern platforms and frameworks.\nUse Case/Scenario\nPositioning\nComplexity\nTech Recommendation\nIntegrations & Interoperability\nOwning Org/Team\nNext Step\nLooking for re-usable APIs from other teams\nStandard\nNone\nBackstage\nWeb\nEnterprise Data\nView the\nBackstage Entity Catalog\nStarting a new API project\nStandard\nLow\nBackstage\nWeb\nEnterprise Data\nUse\nBackstage Accelerators\nDeploying APIs\nStandard\nMedium\nCATS\n,\nLilly Kubed\nWeb, GitHub, AWS\nSoftware Product Engineering (SPE)\nVisit the\nCATS\n,\nLilly Kubed\ndocumentation websites\nSecuring APIs\nStandard\nMedium\nAzure API Management (APIM)\nHTTPS\nEnterprise Data\nGetting Started\nBackstage\n‚Äã\nBackstage\nis a developer portal that centralizes software components, APIs, and documentation. It enhances discoverability and governance by providing a catalog of reusable Lilly APIs and services.\nCheck out Backstage documentation at the links below:\nüìö\nEntity Catalog\n: Search for reusable APIs, Components, and Resources enterprise-wide before trying to rebuild one from scratch.\nüöÄ\nAccelerators\n: Jumpstart development for a new API, Website, and more using templated code with Lilly best practices built-in.\nüìö\nTechDocs\n: Write some markdown and let Backstage host your docs website and integrate it with search on your behalf using the mkdocs framework.\nüîç\nSearch\n: All reusable assets added to Lilly Backstage and the matching items from LillyFlow can be found using the Search feature in the sidebar.\nüìÇ\nClient Code Generation\n: Found an API to reuse? Let Backstage generate client side code efficiently with built-in best practices to consume faster.\nüìä\nPlatform Insights\n: See our estimations for how much time Backstage has saved Lilly developers so far!\n‚ÄºÔ∏è Does your team own some re-usable software? You can add it to the Entity Catalog with our\nCatalog Integration Tutorial\nin just 3 easy steps!\nAzure API Management\n‚Äã\nEnterprise Data provides Azure API Management (APIM) as a global API Gateway service.  Delegate authentication, authorization, and threat protection to APIM so you can focus on your API's functionality and business value.\nCheck out APIM documentation at the links below:\nüìö\nPlatform Overview\n: Learn more about the platform and how it fits in solution architectures.\nüöÄ\nGetting Started\n: Get your APIs published in / secured by Azure APIM.\nFastAPI Framework for Building REST APIs\n‚Äã\nFastAPI is a modern Python framework for building high-performance APIs with automatic OpenAPI documentation. It is ideal for lightweight services and microservices.\nFeatures:\nAutomatic OpenAPI and JSON Schema generation\nfor documentation and validation\nAsynchronous support\nusing Python‚Äôs async/await for high-performance APIs\nType hinting and data validation\nwith Pydantic for cleaner, safer code\nFast development cycles\nwith minimal boilerplate and intuitive syntax\nBackstage Accelerators\nhas a FastAPI Template to help you get started with this framework.\nStrawberry Framework for Building GraphQL APIs\n‚Äã\nStrawberry is a Python library for building GraphQL APIs using modern Python features like type hints and dataclasses. It simplifies GraphQL schema creation and integrates well with Python-based services. Strawberry is ideal for teams looking to expose flexible, queryable APIs and works seamlessly with Hangar CI/CD and Azure API Management for deployment and monitoring.\nFeatures:\nNative support\nfor Python type hints and dataclasses\nAutomatic GraphQL schema generation\nfor documentation and validation\nEasy integration\nwith ASGI frameworks like FastAPI\nBackstage Accelerators\nhas a GraphQL Template to help you get started with this framework.\nDeploying APIs at Lilly\n‚Äã\nOur team highly recommmends using the Kubernetes clusters by the\nCATS\nand\nKubed\nteams at Lilly for containerized deployments. These platforms provide enterprise support and have several features making your deployment experience easier and more reliable.\nFor more information on containerized deployments, see\nContainer Services\n.\nCOMING SOON: CATS and Lilly Kubed are merging into one platform called\nHangar\n.\nWas this helpful?\nTags:\napi\nsoftware-engineering\nsolution-guide\ntechhq\nEdit this page\nPrevious\nüß≠ AI Code Assistants\nNext\nüß≠ ML Ops\nBackstage\nAzure API Management\nFastAPI Framework for Building REST APIs\nStrawberry Framework for Building GraphQL APIs\nDeploying APIs at Lilly\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 7,
    "depth": 3,
    "crawled_at": "2026-02-25T10:09:14.824567"
  },
  "https://techhq.dc.lilly.com/docs/solution/engineering/ml_ops": {
    "url": "https://techhq.dc.lilly.com/docs/solution/engineering/ml_ops",
    "title": "üß≠ ML Ops | Tech HQ",
    "description": "Stack Overflow Article: üß≠ ML Ops",
    "h1": [
      "üß≠ ML Ops"
    ],
    "h2": [
      "Key Concepts‚Äã",
      "Artifact Storage‚Äã",
      "Overview of Common Technologies‚Äã",
      "Overview of Common Patterns‚Äã",
      "How to Get Started at Lilly‚Äã",
      "Further Information on Setting Up MLOps Tools‚Äã"
    ],
    "h3": [
      "What is MLOPS?‚Äã"
    ],
    "text_content": "üß≠ ML Ops | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüóÑÔ∏è Engineering Enablement BLT\nüß≠ AI Code Assistants\nAPI Standards\nüß≠ ML Ops\nüß≠ Test Automation\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nüõ†Ô∏è Engineering Enablement\nüß≠ ML Ops\nOn this page\nüß≠ ML Ops\nSynced daily from Stack Overflow\nGuide Status\nLifecycle: Draft\nLast Update: 2025-10-03\nCapability Owner: Kristen Hlava\nEBA Lead: Karl Mayer\nContributors & Reviewers: Kristen Hlava, Will Skertic, David Gayda, Rhian Taylor\nWhat is MLOPS?\n‚Äã\nMLOPS stands for Machine Learning Operations. It is a set of practices that aims to automate, simplify, and standardize machine learning workflows, processes, or deployments. Its goal is to decrease time-to-value, maintain and improve model performance over time, and ensure that machine learning delivers sustained business impact.\nKey Concepts\n‚Äã\nDevOps:\nMLOps builds directly on the foundational principles of DevOps. DevOps is a set of practices that unifies software development and IT operations, emphasizing automation, collaboration, and continuous integration and delivery (CI/CD) to streamline software deployment and improve reliability.\nMLOps extends these principles to the unique challenges of machine learning, adding processes for managing data, tracking experiments, and monitoring models in production. MLOps incorporates both code and data, ensuring that machine learning models are reproducible, scalable, and maintainable throughout their lifecycle.\nInfrastructure as Code:\nA foundational practice in MLOps that enables teams to automate the provisioning and management of computing resources using code, rather than manual processes. By defining infrastructure in version-controlled scripts, we can ensure that machine learning environments are consistent, reproducible, and scalable across development, testing, and production. This approach accelerates model deployment, reduces configuration errors, and optimizes resource usage‚Äîmaking it easier to manage complex ML workflows and adapt quickly to changing requirements.\nArtifact Storage\n‚Äã\nArtifact storage in MLOps unifies the management of code, data, and model objects, creating a reliable foundation for scalable, reproducible, and auditable machine learning operations.\nCode:\nCode artifacts encompass everything from infrastructure scripts to model training routines and pipeline definitions. Storing code in version-controlled repositories (such as Git) allows teams to track changes, collaborate effectively, and reproduce experiments. This practice ensures that the exact code used for model development and deployment is always accessible, supporting robust CI/CD workflows and minimizing configuration drift.\nData:\nData artifacts include raw datasets, processed features, and metadata generated throughout the ML pipeline. Centralized storage solutions‚Äîsuch as feature stores or cloud object stores‚Äîenable teams to manage data lineage, version datasets, and maintain high data quality. Automated pipelines facilitate the extraction, transformation, and loading (ETL/ELT) of data, while lineage tracking and metadata management provide visibility into how data evolves and is used across experiments.\nModel Objects:\nModel objects refer to the serialized representations of trained machine learning models, such as\n.pkl\n,\n.joblib\n, or framework-specific files. These artifacts are stored in model registries or cloud storage, allowing for easy retrieval, deployment, and monitoring. By cataloging and versioning model objects, teams can roll back to previous versions, audit model changes, and ensure that only validated models are promoted to production.\nOverview of Common Technologies\n‚Äã\nMany ML tools and platforms have similar functionality. Part of the consideration for these tools will be dependent on the environment you deploy your models in (AWS vs Azure vs CATS vs AWB, etc). MagTrain/Run.AI is a great environment for model training specifically. Additional consideration will be simply in terms of technologies you may already be familiar with (Kubernetes, Docker, Nextflow, etc). When going through the AI registry process, MLOps can also be discussed as part of the architecture review.\nOverview of Common Patterns\n‚Äã\nCommon patterns include modular pipelines for data ingestion, feature engineering, model training, and evaluation; CI/CD workflows for automated deployment; and monitoring frameworks for drift detection and performance tracking. These patterns emphasize reproducibility, scalability, and collaboration between data science and engineering.\nHow to Get Started at Lilly\n‚Äã\nAs stated previously, the environment your project is in will help drive decisions on which tools to use. When filling out the AI registry for your project, be sure to ask about or include MLOps when having your architecture review discussion ‚Äì the AI architects can help guide in the best solution for your specific use case.\nFurther Information on Setting Up MLOps Tools\n‚Äã\nEnterprise Data Team:\nhttps://data.lilly.com/\nThe AWB platform already has installations of Sagemaker, MLFlow, and Fabric.\nCloud Team:\nhttps://cloud.lilly.com/lcs/\nFor Azure, AWS, or other cloud deployment.\nCATS:\nhttps://cats.lilly.com/\nFor deploying workflows via the CATS platform.\nMagTrain:\nhttps://github.com/EliLillyCo/MagTrain\nFor using the MagTrain cluster to run/train models.\nWas this helpful?\nTags:\nsoftware-engineering\nsolution-guide\ntechhq\nEdit this page\nPrevious\nAPI Standards\nNext\nüß≠ Test Automation\nWhat is MLOPS?\nKey Concepts\nArtifact Storage\nOverview of Common Technologies\nOverview of Common Patterns\nHow to Get Started at Lilly\nFurther Information on Setting Up MLOps Tools\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 5,
    "depth": 3,
    "crawled_at": "2026-02-25T10:09:17.364501"
  },
  "https://techhq.dc.lilly.com/docs/solution/engineering/test_automation": {
    "url": "https://techhq.dc.lilly.com/docs/solution/engineering/test_automation",
    "title": "üß≠ Test Automation | Tech HQ",
    "description": "Stack Overflow Article: üß≠ Test Automation",
    "h1": [
      "üß≠ Test Automation"
    ],
    "h2": [
      "Footnotes‚Äã"
    ],
    "h3": [],
    "text_content": "üß≠ Test Automation | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüóÑÔ∏è Engineering Enablement BLT\nüß≠ AI Code Assistants\nAPI Standards\nüß≠ ML Ops\nüß≠ Test Automation\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nüõ†Ô∏è Engineering Enablement\nüß≠ Test Automation\nüß≠ Test Automation\nSynced daily from Stack Overflow\nSolution Guide\nLifecycle: Living Document\nLast Update: 2025-03-24\nCapability Owner: Matt Wambsganss\nEBA Leads: Greg Graf\nContributors & Reviewers: Matt Wambsganss, Yu Zhang, Suparna Kumari\nA solution guide for common test automation use cases, using services that can handle up to Orange Lilly information.\nFor more information on Enterprise Test Automation capabilities., visit\nTesting.Lilly.com\n.\nStay up to date on new capabilities and announcements at\nTest Automation CoE Viva Engage\n.\nStart a new project or get a consultation at\nEnterprise Automation Intake Form\n.\nUse Case/Scenario\nComplexity\n1\nTech Recommendation\nIntegrations & Interop\n2\nOwning Org/Team\nNext Step\nNo-code web UI testing\n1\nMabl\ndemo\nWeb, Jira\nEnterprise Automation\nRequest \"Mabl Editor\" Access\nNo-code desktop UI testing\n1\nTestComplete\ndemo\nWindows, web, ALM\nEnterprise Automation\nRequest \"TestComplete User\" Access\nAI chatbot testing\n0\nPre-alpha stage UI app\nAny chatbot with an API endpoint\nEnterprise Automation\nWatch\nTest Automation CoE Viva Engage\nfor announcements\nAI automated test script generation\n0\nPre-alpha stage UI app\nAny web UI\nEnterprise Automation\nWatch\nTest Automation CoE Viva Engage\nfor announcements\nCoded web UI testing\n3\nPlayWright\n,\nCypress\nBDD Cucumber, Jira, AppliTools, SauceLabs\nEnterprise Automation\nMore information and links to GitHub repos\nAPI testing\nMabl-2, PlayWright / Cypress-3\nMabl\n,\nPlayWright\n,\nCypress\nAPIs, Jira\nEnterprise Automation\nEnterprise Automation Intake Form\nAccessibility testing\nMabl-1, Deque with Cypress or PlayWright-3\nMabl\n,\nDeque\nWeb, Jira\nEnterprise Automation\n, Digital Office\nEnterprise Automation Intake Form\nPerformance testing\nK6-3, Mabl-1\nK6\n,\nMabl\nWeb, APIs\nEnterprise Automation\nEnterprise Automation Intake Form\nETL testing\n3\nETL Testing Framework\nAWS\nEnterprise Automation\nEnterprise Automation Intake Form\nVisual regression testing\nAppliTools-3, Mabl-1\nAppliTools\n,\nMabl\nEnterprise Automation\nEnterprise Automation Intake Form\nCross-platform testing\n3\nSauceLabs\nPlayWright, Cypress\nEnterprise Automation\nEnterprise Automation Intake Form\nMobile app testing\n3\nPre-alpha stage Appium framework\nAndroid, IOS, Jira\nEnterprise Automation\nWatch\nTest Automation CoE Viva Engage\nfor announcements\nAudio/Visual testing\n3\nPre-alpha stage TestRTC framework\nWeb\nEnterprise Automation\nWatch\nTest Automation CoE Viva Engage\nfor announcements\nFootnotes\n‚Äã\n(0) Ready-to-use, (1) Configurable, (2) Customizable, (3) Requires integration, (4) High-complexity\n‚Ü©\nOnly the most significant integrations & interoperability are listed.\n‚Ü©\nWas this helpful?\nTags:\nsoftware-engineering\nsolution-guide\ntechhq\nEdit this page\nPrevious\nüß≠ ML Ops\nNext\nüõ∞Ô∏è Observability & Reliability\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 3,
    "depth": 3,
    "crawled_at": "2026-02-25T10:09:19.834720"
  },
  "https://techhq.dc.lilly.com/docs/solution/observability/observability-blt": {
    "url": "https://techhq.dc.lilly.com/docs/solution/observability/observability-blt",
    "title": "üóÑÔ∏è Observability & Reliability BLT | Tech HQ",
    "description": "The Observability & Reliability Big List of Technology (BLT) is maintained using data from the Enterprise Architecture Tech Positioning Database and LillyFlow, with AI-assisted updates. While we aim for accuracy, please verify critical information and contact the EBA team with questions or corrections.",
    "h1": [
      "üóÑÔ∏è Observability & Reliability BLT"
    ],
    "h2": [
      "Splunk Observability‚Äã"
    ],
    "h3": [],
    "text_content": "üóÑÔ∏è Observability & Reliability BLT | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüóÑÔ∏è Observability & Reliability BLT\nInfrastructure Performance Monitoring\nLog Management & Analytics\nNetwork Monitoring\nüß≠ Resiliency & Disaster Recovery\nüöÄ Team Productivity\nüé® User Experience & Design\nüõ∞Ô∏è Observability & Reliability\nüóÑÔ∏è Observability & Reliability BLT\nOn this page\nüóÑÔ∏è Observability & Reliability BLT\nThe Observability & Reliability Big List of Technology (BLT) is maintained using data from the Enterprise Architecture Tech Positioning Database and LillyFlow, with AI-assisted updates. While we aim for accuracy, please verify critical information and contact the EBA team with questions or corrections.\nCan't find a technology?\nUse TechHQ's search feature to look across all technology documentation. For questions or to suggest additions to this list, contact the\nEnterprise Business Architecture team\n.\nSplunk Observability\n‚Äã\nPosition\nStandard\nTags\nobservability\nInformation Sensitivity\nYellow\nArchitect\nJarret DiPace\nContact(s)\nKetan Pradeep Dessai\nStandard.  Replaced Oasis\nUse Cases\n‚Äã\nServer Incident Monitoring\nServer Performance Monitoring\nDocument generated from Stack Overflow Enterprise articles (prioritized) and ea_tech.csv (fallback)\nWas this helpful?\nEdit this page\nPrevious\nüõ∞Ô∏è Observability & Reliability\nNext\nInfrastructure Performance Monitoring\nSplunk Observability\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 5,
    "depth": 3,
    "crawled_at": "2026-02-25T10:09:22.494744"
  },
  "https://techhq.dc.lilly.com/docs/solution/observability/infrastructure_performance_monitoring": {
    "url": "https://techhq.dc.lilly.com/docs/solution/observability/infrastructure_performance_monitoring",
    "title": "Infrastructure Performance Monitoring | Tech HQ",
    "description": "Stack Overflow Article: Infrastructure Performance Monitoring",
    "h1": [
      "Infrastructure Performance Monitoring"
    ],
    "h2": [],
    "h3": [],
    "text_content": "Infrastructure Performance Monitoring | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüóÑÔ∏è Observability & Reliability BLT\nInfrastructure Performance Monitoring\nLog Management & Analytics\nNetwork Monitoring\nüß≠ Resiliency & Disaster Recovery\nüöÄ Team Productivity\nüé® User Experience & Design\nüõ∞Ô∏è Observability & Reliability\nInfrastructure Performance Monitoring\nInfrastructure Performance Monitoring\n# üß≠ Infrastructure Performance Monitoring (IPM): AI-Ready Observability Strategy\n<ExternalLink>[Synced daily from Stack Overflow](https://elilillyco.stackenterprise.co/articles/4204)</ExternalLink>\n:::info[Solution Guide]\n- Lifecycle: Draft\n- Last Update: 2025-10-10\n- Capability Owner: David Fleming\n- EBA Lead: [TBD]\n:::\nA guide for designing and implementing infrastructure performance monitoring solutions that support AIOps, automation, and predictive intelligence across enterprise environments.\n:::tip\nYou can use the <ExternalLink>[TechHQ Solution Guide Drafter](http://link-to-drafter.com)</ExternalLink>.\n:::\nUse Case/ScenarioTech RecommendationPositioningComplexityKey IntegrationsOwning Org/TeamNext StepFull-stack infrastructure observabilityAppDynamicsStrategic CoreMediumServiceNow, VMware, Cisco, OasisEnterprise MonitoringRequest access via IT ServiceNowContainer and cloud-native monitoringPrometheus + GrafanaSpecializedMediumKubernetes, Docker, Azure MonitorCloud Platform TeamDeploy via Helm charts| AI-driven anomaly detection   | Dynatrace               | Emerging          | High       | Azure, AWS, VMware, OpenTelemetry                     | Observability & AI Ops   | Initiate pilot with AI Ops team    |\n| Legacy server performance tracking | Nagios                  | Declining         | Medium     | SNMP, Windows/Linux agents                            | Infrastructure Ops       | Review migration strategy          |\n| Multi-cloud infrastructure monitoring | Datadog                 | Standard          | Medium     | AWS, Azure, GCP, Kubernetes, Docker                   | Cloud Engineering        | Submit onboarding request          |\n| Real-time network performance monitoring | SolarWinds NPM          | Specialized       | Medium     | Cisco, F5, SNMP                                       | Network Services         | Contact Network Ops for setup      |\n:::warning\nAvoid deploying multiple overlapping monitoring tools without a clear integration strategy. Redundant tooling increases cost, complexity, and data fragmentation.\n:::\n## Why This Matters: Shifting from Health Checks to Proactive Intelligence\nInfrastructure Performance Monitoring (IPM) is no longer a passive check of CPU and RAM. For technical leaders and architects, IPM is the foundational data layer that feeds all AIOps, capacity planning, and automation initiatives. A poor IPM strategy creates blind spots, leads to reactive firefighting, and prevents successful deployment of advanced automation.\n### The Goal\nMove from simply knowing that a server is down (reactivity) to understanding why a specific component is degrading a critical business service and predicting the failure before it happens (proactivity).\n---\n## üìä Architecture Diagram\n!<ExternalLink>[Infrastructure Performance Monitoring Stack Architecture](https://us-prod.asyncgw.teams.microsoft.com/v1/objects/0-cus-d5-7afe023df1b166e88371e957f94c9307/viewsModern Infrastructure Monitoring\nArchitects must ensure the IPM solution collects the right types of data, with sufficient context, to be considered \"AI-Ready.\"\n### 1. High-Cardinality Metrics (The \"What\")</ExternalLink>\n:::info[High-Cardinality Metrics]\n- Use unified agents (e.g., Splunk Observability Agents, Prometheus exporters)\n- Store raw metrics in a TSDB (e.g., Splunk Metrics, Thanos)\n- Retain data for 12‚Äì18 months for capacity planning and anomaly detection\n:::\n### 2. Centralized, Structured Logging (The \"Why\")\n:::info[Structured Logging]\n- Enforce JSON format at the source using Fluentd or Logstash\n- Ensure logs are tagged with host metadata for correlation\n- Link logs to topology maps in AIOps platforms (e.g., ITSI)\n:::\n### 3. Topology & Dependency Mapping (The \"Where\")\n:::info[Dependency Mapping]\n- Build service models using CMDB and NetBrain\n- Use automated discovery tools to avoid manual mapping\n- Feed topology into AIOps for impact analysis and prioritization\n:::\n---\n## Architectural Decision: The Unified Agent Strategy\nApproachPros for AIOpsCons for AIOpsUnified Agent (Recommended)Deep context, high-cardinality metrics, logs/traces, automation-readyRequires standardization and operational discipline| Legacy Agentless (SNMP/WMI) | No install required                                                   | Shallow metrics, slow polling, not AI-ready         |\n:::tip[Agent Deployment Insight]\n- Use Sidecar pattern for Kubernetes\n- Use Golden Image for VMs\n- Avoid manual agent installs\n:::\n---\n## Real-World Use Cases: Best-Fit Technology\nScenarioGoal & Business ValueTechnology FocusCapacity Planning & Cost OptimizationPrevent resource exhaustion, justify cloud rightsizing (Cost Savings)TSDB, Splunk/Grafana dashboards, long-term metrics| Proactive Bottleneck Detection | Predict degradation before impact (Improved MTTR)                  | AIOps Platform (ITSI), adaptive thresholds            |\n| Host-Level Self-Healing        | Auto-resolve low-risk issues (Reduced Toil)                        | ITSI + Automation Engine (Ansible)                    |\n| Rapid Root Cause Analysis (RCA) | Accelerate diagnosis (Reduced MTTR)                                | Unified Agent, single-pane correlation                |\n---\n## Technology Details\n### AppDynamics\nStrategic platform for full-stack observability. Deep integration with ServiceNow and Oasis. Supports automated RCA and incident response.\n### Prometheus + Grafana\nOpen-source stack for cloud-native environments. Ideal for Kubernetes and Docker. Supports high-cardinality metrics and custom dashboards.\n### Dynatrace\nAI-powered monitoring with automatic anomaly detection. Supports OpenTelemetry and dynamic environments. Emerging within enterprise AIOps.\n### Nagios\nLegacy tool for basic server and network monitoring. Limited cloud-native support. Consider migration to modern platforms.\n### Datadog\nStandard tool for multi-cloud observability. Unified dashboards and alerting. Supports OpenTelemetry and hybrid environments.\n### SolarWinds NPM\nSpecialized for network performance. SNMP-based telemetry. Ideal for real-time traffic and latency monitoring.\n---\n### Recommendations to Improve Guide Usability\n1. **Add Decision Trees** to help architects choose technologies based on environment type (cloud-native, legacy, hybrid).\n2. **Embed Deployment Playbooks** for agent rollout, TSDB setup, and logging standardization.\n3. **Include Governance Notes** to clarify approval and onboarding processes for each tool.\nWas this helpful?\nTags:\nobservability\nsolution-guide\ntechhq\nEdit this page\nPrevious\nüóÑÔ∏è Observability & Reliability BLT\nNext\nLog Management & Analytics\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 7,
    "depth": 3,
    "crawled_at": "2026-02-25T10:09:24.736294"
  },
  "https://techhq.dc.lilly.com/docs/solution/observability/log_management_analytics": {
    "url": "https://techhq.dc.lilly.com/docs/solution/observability/log_management_analytics",
    "title": "Log Management & Analytics | Tech HQ",
    "description": "Stack Overflow Article: Log Management & Analytics",
    "h1": [
      "Log Management & Analytics"
    ],
    "h2": [],
    "h3": [],
    "text_content": "Log Management & Analytics | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüóÑÔ∏è Observability & Reliability BLT\nInfrastructure Performance Monitoring\nLog Management & Analytics\nNetwork Monitoring\nüß≠ Resiliency & Disaster Recovery\nüöÄ Team Productivity\nüé® User Experience & Design\nüõ∞Ô∏è Observability & Reliability\nLog Management & Analytics\nLog Management & Analytics\n# üß≠ Log Management & Analytics\n<ExternalLink>[Synced daily from Stack Overflow](https://elilillyco.stackenterprise.co/articles/4205)</ExternalLink>\n:::info[Solution Guide]\n- Lifecycle: Draft\n- Last Update: 2025-10-10\n- Capability Owner: David Fleming\n- EBA Lead: David Fleming\n- Contributors & Reviewers:\n:::\nyour text`\nA guide for designing and implementing log management and analytics platforms that support real-time diagnostics, business intelligence, and AI-ready observability.\n:::tip\nYou can use the <ExternalLink>[TechHQ Solution Guide Drafter](http://link-to-drafter.com)</ExternalLink>.\n:::\nUse Case/ScenarioTech RecommendationPositioningComplexityKey IntegrationsOwning Org/TeamNext StepReal-time diagnostics & RCASplunk Enterprise + DSPStrategic CoreHighUnified Agent, CMDB, ITSI, ServiceNowObservability & AI OpsRequest onboarding via ITSI| Security & Compliance Auditing    | Splunk SIEM, Elastic Security     | Standard          | Medium     | Syslog, Auditd, Windows Event Logs                    | Cybersecurity Operations | Submit request to Security Ops     |\n| Business Activity Monitoring (BAM)| Fluentd + Databricks              | Specialized       | Medium     | Kafka, REST APIs, Cloud Storage                       | Data Engineering         | Deploy pipeline via Terraform      |\n| AIOps Pattern Detection           | Splunk ITSI + Unified Agent       | Strategic Core    | High       | Metrics, Logs, Traces, CMDB                           | Observability & AI Ops   | Enable correlation in ITSI         |\n| Cost-Optimized Log Retention      | Amazon S3 + Athena                | Specialized       | Low        | Fluent Bit, CloudTrail, Lambda                        | Cloud Platform Team      | Configure lifecycle policies       |\n| Developer Debugging & QA          | ELK Stack (Elasticsearch, Logstash, Kibana) | Standard          | Medium     | Jenkins, GitHub, App Logs                             | DevOps Engineering        | Provision ELK via Helm             |\n:::warning\nAvoid ingesting unfiltered, unstructured logs into your analytics platform. This leads to high costs, poor performance, and unusable data for AI models.\n:::\n---\n## Why This Matters: From Dumb Storage to Smart Context\nTraditional log management focused on storing raw text for compliance. Today, logs must be structured, correlated, and enriched to support real-time diagnostics, RCA, and predictive analytics. A modern log strategy transforms high-volume, unstructured data into low-latency, high-value insights.\n---\n## The Three Imperatives for AI-Ready Logs\n### 1. Structured Ingestion (The \"Parser\")\n:::info[Structured Logging]\n- Enforce JSON or key-value format at the source (e.g., Log4j, SLF4J)\n- Mandate NTP synchronization across all hosts\n- Use ISO 8601 timestamps with timezone offsets\n:::\n### 2. Intelligent Routing & Filtering (The \"Pipeline\")\n:::info[Smart Pipelines]\n- Filter debug/heartbeat logs at the edge using DSP or Fluent Bit\n- Route high-value logs (e.g., security events) to long-term indexes\n- Use short-term, low-cost storage for transient logs\n:::\n### 3. Cross-Domain Correlation (The \"Link\")\n:::info[Unified Correlation]\n- Tag all logs with host_id, service_name, and trace_id\n- Enrich logs with CMDB metadata (e.g., environment, owner, tier)\n- Enable one-click pivot between metrics and logs in observability platforms\n:::\n---\n## Architectural Decision: The Log Pipeline Strategy\nComponentFunctionBest-Fit TechnologiesCollector/ForwarderGathers logs from servers, containers, endpointsUnified Agents (Splunk, Dynatrace), Fluentd| Processor/Router    | Filters, structures, enriches, and routes data       | Splunk DSP, Logstash, Fluent Bit                 |\n| Analytical Store    | Indexes structured data for search and ML            | Splunk, Elasticsearch, Databricks, Athena        |\n:::tip[Process Near the Source]\nPlace filtering and structuring logic in the collector or forwarder to reduce bandwidth and indexing load.\n:::\n---\n## Technology Details\n### Splunk Enterprise + DSP\nStrategic platform for enterprise-wide log analytics. DSP enables edge filtering, enrichment, and routing. Deep integration with ITSI and CMDB supports RCA and AIOps.\n### Splunk SIEM\nStandard platform for security log analytics. Supports long-term retention, forensic investigation, and compliance reporting. Integrates with syslog, Windows Event Logs, and audit frameworks.\n### Fluentd + Databricks\nSpecialized solution for business activity monitoring. Fluentd collects structured logs; Databricks transforms them into metrics for dashboarding and analysis.\n### ELK Stack\nStandard open-source stack for developer debugging and QA. Elasticsearch indexes logs; Logstash parses and routes; Kibana visualizes. Ideal for CI/CD pipelines.\n### Amazon S3 + Athena\nCost-effective solution for long-term log retention. Fluent Bit routes logs to S3; Athena enables SQL-based querying. Best for low-frequency access and compliance storage.\n---\n## Real-World Use Cases\nScenarioGoal & Business ValueTechnology FocusSecurity & Compliance AuditingMeet SOC 2/PCI requirements, enable forensic investigationSplunk SIEM, Elastic Security| Business Activity Monitoring    | Track user sign-ups, order volume, feature usage                   | Fluentd, Databricks                                   |\n| Advanced AIOps Pattern Detection| Detect event sequences that precede failures                       | Splunk ITSI, Unified Agent                            |\n| Troubleshooting (RCA)          | Jump from metric anomaly to root cause log                         | Splunk ITSI, ELK Stack                                |\n---\n### Recommendations to Improve Guide Usability\n1. **Add Architecture Diagrams** showing the log pipeline from source to analytics platform.\n2. **Include Deployment Playbooks** for agent rollout, DSP configuration, and index setup.\n3. **Link to Governance Policies** for log retention, security, and compliance.\nWas this helpful?\nTags:\nobservability\nsolution-guide\ntechhq\nEdit this page\nPrevious\nInfrastructure Performance Monitoring\nNext\nNetwork Monitoring\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 6,
    "depth": 3,
    "crawled_at": "2026-02-25T10:09:26.965252"
  },
  "https://techhq.dc.lilly.com/docs/solution/observability/network_monitoring": {
    "url": "https://techhq.dc.lilly.com/docs/solution/observability/network_monitoring",
    "title": "Network Monitoring | Tech HQ",
    "description": "Stack Overflow Article: Network Monitoring",
    "h1": [
      "Network Monitoring"
    ],
    "h2": [],
    "h3": [],
    "text_content": "Network Monitoring | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüóÑÔ∏è Observability & Reliability BLT\nInfrastructure Performance Monitoring\nLog Management & Analytics\nNetwork Monitoring\nüß≠ Resiliency & Disaster Recovery\nüöÄ Team Productivity\nüé® User Experience & Design\nüõ∞Ô∏è Observability & Reliability\nNetwork Monitoring\nNetwork Monitoring\nSynced daily from Stack Overflow\nNetwork Monitoring Strategy: From Connectivity Checks to Observability\nWhy This Matters: The Network is the True Service Backbone\nFor architects, the network is the hidden dependency where root causes often reside, yet it is frequently the most opaque domain. Traditional network monitoring (SNMP, basic ping) only confirms connectivity; it does not reveal performance bottlenecks, security lateral movement, or application transaction latency caused by the network itself. An AI-ready strategy treats the network as a high-fidelity sensor for application behavior and security.\nThe Goal: Move beyond monitoring device health to understanding network behavior and traffic flow, ensuring low-latency delivery of critical business services and providing rich security context.\nThe Three Imperatives for AI-Ready Network Data\nTo unlock advanced use cases like network-based anomaly detection and security investigation, architects must prioritize the collection of high-context, rich network data.\nNetwork Flow Data (NetFlow, IPFIX, V-Flow)\nFlow data reveals who is talking to whom, how much, and for how long. This is crucial for capacity planning, cost allocation, and identifying unexpected traffic patterns.\nRequirement\nWhy it Matters\nActionable Guidance\nComprehensive Coverage\nFlow data must be collected across the entire enterprise (data center, cloud VPCs, and endpoints) to avoid blind spots in east-west traffic.\nMandate flow logging in all cloud environments (VPC Flow Logs) and ensure on-prem network devices (routers/switches) are configured to export NetFlow/IPFIX to a central collector.\nConsistent Aggregation\nOver-aggregating flow data loses granularity needed for forensic analysis and machine learning pattern detection.\nSet flow export timers appropriately (e.g., 60 seconds or less). Use flow analysis tools that retain detailed, unsummarized flow records for at least 30 days.\nHigh-Fidelity Packet Data (The \"Deep Dive\")\nPacket data (or deep packet inspection/DPI) provides the unfiltered, raw ground truth necessary for troubleshooting complex, intermittent latency issues or validating application protocol behavior.\nRequirement\nWhy it Matters\nActionable Guidance\nIntelligent Capture and Storage\nStoring all packets is prohibitively expensive. Only metadata and suspicious packets should be retained long-term.\nDeploy network agents/probes that perform Deep Packet Inspection (DPI) to extract application-level metadata (e.g., HTTP response codes, database queries) and only capture/store full packets when triggered by a flow anomaly or security alert.\nDecrypted Insight\nEncrypted traffic prevents analysis. The monitoring solution must be able to view traffic after decryption.\nImplement monitoring points behind load balancers/proxies where TLS decryption has already occurred, or integrate with decryption technologies (e.g., in a cloud environment or using security tools).\nNetwork Metadata & Health (The \"Context\")\nThis category includes SNMP data, device configuration, and synthetic health checks that provide context about the network's state.\nRequirement\nWhy it Matters\nActionable Guidance\nService Dependency Mapping\nNetwork components (firewalls, load balancers) are critical service dependencies and must be mapped to the business service model.\nIntegrate network discovery tools (e.g., NetBrain, network configuration management systems) with your AIOps Service Model to automatically update component relationships.\nAgentless vs. Agent-Based\nUsing the right tool for the job. SNMP is great for device health; agents are great for endpoint performance.\nUse SNMP for core router/switch health (CPU, interface stats). Use lightweight agents on servers for local network latency and throughput tests (synthetic checks).\nArchitectural Decision: The Network Data Collector Strategy\nNetwork data collection demands specialized collectors due to high volume (flows) and processing power requirements (packets). Architects must choose the right collector placement and type.\nComponent\nFunction\nStrategic Placement\nWhy it Matters\nDedicated Flow Collector\nIngests, normalizes, and indexes high-volume NetFlow/IPFIX records.\nCentralized Logging/Analytics Zone\nDecouples flow ingestion from performance of core routing infrastructure.\nDPI/Packet Probe\nPerforms Deep Packet Inspection and extracts application metadata.\nBehind Load Balancers or Critical Network Taps\nRequires high-performance hardware/VMs; placement must be strategic to capture relevant application traffic.\nCloud VPC Flow Logs\nCaptures all internal (east-west) traffic within the cloud boundary.\nIntegrated via Native Cloud Services (e.g., AWS S3, Azure Blob)\nEnsures visibility into internal microservice communication, which is often a blind spot.\nThe \"Single Pane of Network Truth\" Insight\nAvoid maintaining separate tools for flow, packet, and SNMP data. Architect for a single observability platform that can ingest all three data types and use a common correlation key (IP, port, hostname) to link them instantly.\nReal-World Use Cases: Best-Fit Technology\nIntegrating comprehensive network data enables advanced analysis far beyond simple network performance monitoring.\nScenario\nGoal & Business Value\nTechnology Focus\nTroubleshooting Intermittent Latency\nGoal: Prove or disprove that a performance issue is network-related (Faster RCA).\nFocus: Cross-correlation between APM trace data and high-fidelity Packet/DPI metadata to pinpoint the exact hop or network segment introducing delay.\nShadow IT & Security Audit\nGoal: Detect unauthorized internal communication, data exfiltration, or new unknown services (Reduced Security Risk).\nFocus: Machine learning on Flow Data to baseline normal traffic volumes and ports, alerting on sudden spikes or unusual protocol usage (Network Behavioral Analysis).\nBandwidth Hog Identification\nGoal: Proactively manage and allocate network capacity across different business units (Cost Optimization).\nFocus: Analyzing Flow Data aggregated by user, application, or geography to identify top bandwidth consumers and plan network upgrades or traffic shaping.\nVPN/Remote Access Monitoring\nGoal: Ensure a quality experience for remote workers and identify regional ISP or last-mile issues (Improved End-User Experience).\nFocus: Synthetic agents on remote user endpoints performing continuous Network Latency Checks and reporting back to the central monitoring platform.\nWas this helpful?\nTags:\nobservability\nsolution-guide\ntechhq\nEdit this page\nPrevious\nLog Management & Analytics\nNext\nüß≠ Resiliency & Disaster Recovery\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 5,
    "depth": 3,
    "crawled_at": "2026-02-25T10:09:29.298833"
  },
  "https://techhq.dc.lilly.com/docs/solution/observability/resiliency_disaster_recovery": {
    "url": "https://techhq.dc.lilly.com/docs/solution/observability/resiliency_disaster_recovery",
    "title": "üß≠ Resiliency & Disaster Recovery | Tech HQ",
    "description": "Stack Overflow Article: üß≠ Resiliency & Disaster Recovery",
    "h1": [
      "Resiliency & Disaster Recovery"
    ],
    "h2": [
      "Resiliency Design Steps‚Äã",
      "Critical Systems Governance‚Äã",
      "Resiliency Tier / Infrastructure Matrix‚Äã",
      "Tier 0-A‚Äã",
      "Tier 0-B‚Äã",
      "Critical High Cloud‚Äã",
      "Critical High On-Prem‚Äã",
      "Critical Medium Cloud‚Äã",
      "Critical Medium On-Prem‚Äã",
      "Critical Low Cloud‚Äã",
      "Critical Low On-Prem‚Äã",
      "Local HA Manufacturing‚Äã",
      "Critical Data‚Äã",
      "All Other Systems‚Äã"
    ],
    "h3": [],
    "text_content": "üß≠ Resiliency & Disaster Recovery | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüóÑÔ∏è Observability & Reliability BLT\nInfrastructure Performance Monitoring\nLog Management & Analytics\nNetwork Monitoring\nüß≠ Resiliency & Disaster Recovery\nüöÄ Team Productivity\nüé® User Experience & Design\nüõ∞Ô∏è Observability & Reliability\nüß≠ Resiliency & Disaster Recovery\nOn this page\nResiliency & Disaster Recovery\nSynced daily from Stack Overflow\nSolution Guide\nLifecycle: Draft\nLast Update: 2025-03-28\nCapability Owner: Smita Carneiro\nEBA Lead: Todd Walters\nContributors & Reviewers: Karl Mayer\nThis guide outlines our approach to system resiliency, the ability to withstand disruptions, recover from failures, and\nmaintain continuous operations. This encompasses disaster recovery (DR) capabilities, high availability (HA)\narchitecture, and business continuity planning. Proper resiliency ensures that critical business functions continue\noperating during hardware failures, software issues, network outages, or even catastrophic events. This document\nprovides guidance on determining your application's resiliency requirements based on business criticality and\nimplementing appropriate architectural patterns to achieve the required level of service availability.\nResiliency Design Steps\n‚Äã\nAssess Application Criticality\n: Begin by identifying the Business Application CI ID in ServiceNow. Detailed\ninstructions can be found in the\nDetermine Application Criticality document\n.\nThis step is crucial for understanding the criticality of your application within its business context.\nDefine Resiliency Requirements\n: Next, use the\n[Resiliency Tier / Infrastructure Matrix](#resiliency-tier--infrastructure-matrix) below to determine the system's\nresiliency requirements. This will help you identify the necessary measures to ensure your application's availability\nand reliability. You can also refer to the\nResiliency Requirements document\nfor additional detail for your application's tier.\nImplement Resilient Architecture\n: Based on your application's tier, follow the guidelines in the\nResiliency Patterns document\n.\nThis resource provides comprehensive information on building high resiliency for various infrastructure components\nlike VMs, databases, and containers. It also explains different disaster recovery (DR) models, helping you design a\nrobust and fault-tolerant system.\nCritical Systems Governance\n‚Äã\nTier 0, Critical Systems and Local HA Manufacturing must follow the governance processes for Critical Systems,\ndocumented here\n.\nResiliency Tier / Infrastructure Matrix\n‚Äã\nTier\nCloud\nOn-Prem\nTier 0-A\n[Tier 0-A](#tier-0-a)\nsame as cloud\nTier 0-B\n[Tier 0-B](#tier-0-b)\nsame as cloud\nCritical High\n[Critical High](#critical-high-cloud)\n[Critical High](#critical-high-on-prem)\nCritical Medium\n[Critical Medium](#critical-medium-cloud)\n[Critical Medium](#critical-medium-on-prem)\nCritical Low\n[Critical Low](#critical-low-cloud)\n[Critical Low](#critical-low-on-prem)\nLocal HA Manufacturing\nN/A\n[Local HA Manufacturing](#local-ha-manufacturing)\nAll Other Systems\n[All Other Systems](#all-other-systems)\nsame as cloud\nIf you have a system where you only want a subset of critical recovered after a disaster, rather than the entire app\nrefer to [this section](#critical-data) .\nTier 0-A\n‚Äã\nThis tier includes critical applications like authentication and networking. An Active-Active DR model is ideal.\nDR RTO\n: < 1 hour\nDR RPO\n: < 15 minutes\nHigh Availability RTO\n: < 1 hour\nHigh Availability RPO\n: < 15 minutes\nBackups\n: One copy locally, one in a secondary region\nMonitoring\n: Implement monitoring and alerting\nDR Testing\n: Annual tests to ensure recovery within RTO and RPO\nHA Testing\n: Test before production. For Active-Active, DR tests also serve as HA tests\nDR Assessments\n: Periodic assessments for compliance\nTier 0-B\n‚Äã\nThis tier includes applications essential for recovering other applications after a disaster, such as backup systems.\nThese have high resiliency requirements, though less stringent than Tier 0-A. If Active-Active is not feasible for DR,\nconsider Warm-Active or Pilot Light models.\nDR RTO\n: < 4 hours\nDR RPO\n: < 15 minutes\nHigh Availability RTO\n: < 2 hours\nHigh Availability RPO\n: < 15 minutes\nBackups\n: One copy locally, one in a secondary region\nMonitoring\n: Implement monitoring and alerting\nDR Testing\n: Annual tests to ensure recovery within RTO and RPO\nHA Testing\n: Test before production. For Active-Active, DR tests also serve as HA tests\nDR Assessments\n: Periodic assessments for compliance\nCritical High Cloud\n‚Äã\nThis tier includes applications hosted in native Azure or AWS (not AVS) that, if unavailable for more than 8 hours,\nwould negatively impact the enterprise. Pilot Light models are sufficient for these applications.\nDR RTO\n: < 8 hours\nDR RPO\n: < 1 hour\nHigh Availability RTO\n: < 4.5 hours\nHigh Availability RPO\n: < 15 minutes\nBackups\n: One copy locally, one in a secondary region\nMonitoring\n: Implement monitoring and alerting\nDR Testing\n: Annual tests to ensure recovery within RTO and RPO\nHA Testing\n: Test before production. For Active-Active, DR tests also serve as HA tests\nDR Assessments\n: Periodic assessments for compliance\nCritical High On-Prem\n‚Äã\nThis tier includes applications hosted on-prem or in AVS that, if unavailable for more than 8 hours, would negatively\nimpact the enterprise. Pilot Light models are sufficient. The DR team offers a managed solution for this. To onboard\nsystems, check out this\nlink\n.\nNote that there is a\ncharge\nfor this.\nDR RTO\n: < 8 hours\nDR RPO\n: < 1 hour\nBackups\n: One copy locally, one in a secondary region\nMonitoring\n: Implement monitoring and alerting\nDR Testing\n: Annual tests to ensure recovery within RTO and RPO\nDR Assessments\n: Periodic assessments for compliance\nCritical Medium Cloud\n‚Äã\nThis tier includes applications hosted in native Azure or AWS (not AVS) that, if unavailable for more than 24 hours,\nwould negatively impact the enterprise. Pilot Light models are sufficient for these applications.\nDR RTO\n: < 24 hours\nDR RPO\n: < 1 hour\nHigh Availability RTO\n: < 8 hours\nHigh Availability RPO\n: < 15 minutes\nBackups\n: One copy locally, one in a secondary region\nMonitoring\n: Implement monitoring and alerting\nDR Testing\n: Annual tests to ensure recovery within RTO and RPO\nHA Testing\n: Test before production. For Active-Active, DR tests also serve as HA tests\nDR Assessments\n: Periodic assessments for compliance\nCritical Medium On-Prem\n‚Äã\nThis tier includes applications hosted on-prem or in AVS that, if unavailable for more than 24 hours, would negatively\nimpact the enterprise. Pilot Light models are sufficient. The DR team offers a managed solution for this. To onboard\nsystems, check out this\nlink\n.\nNote that there is a\ncharge\nfor this.\nDR RTO\n: < 24 hours\nDR RPO\n: < 1 hour\nBackups\n: One copy locally, one in a secondary region\nMonitoring\n: Implement monitoring and alerting\nDR Testing\n: Annual tests to ensure recovery within RTO and RPO\nDR Assessments\n: Periodic assessments for compliance\nCritical Low Cloud\n‚Äã\nThis tier includes applications hosted in native Azure or AWS (not AVS) that, if unavailable for more than 72 hours,\nwould negatively impact the enterprise. Pilot Light models are sufficient for these applications.\nDR RTO\n: < 72 hours\nDR RPO\n: < 1 hour\nHigh Availability RTO\n: < 24 hours\nHigh Availability RPO\n: < 15 minutes\nBackups\n: One copy locally, one in a secondary region\nMonitoring\n: Implement monitoring and alerting\nDR Testing\n: Annual tests to ensure recovery within RTO and RPO\nHA Testing\n: Test before production. For Active-Active, DR tests also serve as HA tests\nDR Assessments\n: Periodic assessments for compliance\nCritical Low On-Prem\n‚Äã\nThis tier includes applications hosted on-prem or in AVS that, if unavailable for more than 72 hours, would negatively\nimpact the enterprise. Pilot Light models are sufficient. The DR team offers a managed solution for this. To onboard\nsystems, check out this\nlink\n.\nNote that there is a\ncharge\nfor this.\nDR RTO\n: < 72 hours\nDR RPO\n: < 1 hour\nBackups\n: One copy locally, one in a secondary region\nMonitoring\n: Implement monitoring and alerting\nDR Testing\n: Annual tests to ensure recovery within RTO and RPO\nDR Assessments\n: Periodic assessments for compliance\nLocal HA Manufacturing\n‚Äã\nThis tier includes applications hosted on-prem at a manufacturing site, excluding global manufacturing applications. If\na manufacturing site goes down, only the data at that site is recovered, not a full IT system recovery. Manufacturing\ncannot afford downtime due to component failure, so high availability is essential, typically achieved with dual\ncomputer rooms.\nHigh Availability RTO\n: < 2 hours\nHigh Availability RPO\n: < 0 minutes\nBackups\n: One copy locally, one in a secondary region\nMonitoring\n: Implement monitoring and alerting\nDR Assessments\n: Periodic assessments for compliance\nCritical Data\n‚Äã\nIf you have a system that does not need to be brought up immediately after a disaster but contains data needed\nimmediately, refer to the section titled ‚Äò\nSaving data separately from applications\n‚Äô in\nResiliency Patterns\nIf you have a question, you can discuss it with your\nDR Steward\nor send an email to\nresiliency@lists.lilly.com\n.\nAll Other Systems\n‚Äã\nAll production systems not covered by the above categories fall into this one. In case of a disaster, these systems will\nbe recovered via backup. This is a best effort process and may take 3-4 months.\nDR RPO\n: < 24 hours\nBackups\n: One copy locally, one in a secondary region\nWas this helpful?\nTags:\nobservability\nsolution-guide\ntechhq\nEdit this page\nPrevious\nNetwork Monitoring\nNext\nüöÄ Team Productivity\nResiliency Design Steps\nCritical Systems Governance\nResiliency Tier / Infrastructure Matrix\nTier 0-A\nTier 0-B\nCritical High Cloud\nCritical High On-Prem\nCritical Medium Cloud\nCritical Medium On-Prem\nCritical Low Cloud\nCritical Low On-Prem\nLocal HA Manufacturing\nCritical Data\nAll Other Systems\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 3,
    "depth": 3,
    "crawled_at": "2026-02-25T10:09:31.481002"
  },
  "https://techhq.dc.lilly.com/docs/solution/productivity/productivity-blt": {
    "url": "https://techhq.dc.lilly.com/docs/solution/productivity/productivity-blt",
    "title": "üóÑÔ∏è Team Productivity BLT | Tech HQ",
    "description": "The Team Productivity Big List of Technology (BLT) is maintained using data from the Enterprise Architecture Tech Positioning Database and LillyFlow, with AI-assisted updates. While we aim for accuracy, please verify critical information and contact the EBA team with questions or corrections.",
    "h1": [
      "üóÑÔ∏è Team Productivity BLT"
    ],
    "h2": [
      "Browsers‚Äã",
      "Centralized Apps‚Äã",
      "Everbridge‚Äã",
      "GoTo Meeting‚Äã",
      "GoToTraining‚Äã",
      "GoToWebinar‚Äã",
      "Hosted Shared Desktops (XenApp)‚Äã",
      "iObeya‚Äã",
      "Microsoft Forms‚Äã",
      "Microsoft Power Apps‚Äã",
      "Microsoft Power Automate‚Äã",
      "Microsoft Power BI‚Äã",
      "Microsoft Teams‚Äã",
      "Microsoft Viva Engage (Yammer)‚Äã",
      "Microsoft Whiteboard‚Äã",
      "Native Installation‚Äã",
      "On24‚Äã",
      "Outlook / Exchange‚Äã",
      "PDF‚Äã",
      "ServiceNow Surveys‚Äã",
      "SharePoint Communication Sites‚Äã",
      "SharePoint Team Sites‚Äã",
      "Teams Live Events‚Äã",
      "Virtual Desktop (XenDesktop)‚Äã",
      "WalkMe‚Äã",
      "Yammer Live Events‚Äã",
      "Declining ‚Üí Retired‚Äã"
    ],
    "h3": [
      "Box (QuickCollab)‚Äã",
      "Documentum‚Äã",
      "Form Assembly‚Äã",
      "Forms Builder (Force)‚Äã",
      "Home Drives on LAN‚Äã",
      "Infopath‚Äã",
      "LAN Drives - Collaboration & Content Management‚Äã",
      "Loop (Group / Space)‚Äã",
      "Microsoft SharePoint Team Sites (on-prem)‚Äã",
      "MySite (SharePoint)‚Äã",
      "Salesforce BP&A‚Äã",
      "SharePoint Portal Site (on-prem)‚Äã",
      "Sharepoint Workflows‚Äã",
      "Skype for Business‚Äã",
      "VMWare Virtual Desktops‚Äã",
      "Windows Home Drives (H: Drive)‚Äã"
    ],
    "text_content": "üóÑÔ∏è Team Productivity BLT | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüóÑÔ∏è Team Productivity BLT\nüß≠ Diagramming\nüé® User Experience & Design\nüöÄ Team Productivity\nüóÑÔ∏è Team Productivity BLT\nOn this page\nüóÑÔ∏è Team Productivity BLT\nThe Team Productivity Big List of Technology (BLT) is maintained using data from the Enterprise Architecture Tech Positioning Database and LillyFlow, with AI-assisted updates. While we aim for accuracy, please verify critical information and contact the EBA team with questions or corrections.\nCan't find a technology?\nUse TechHQ's search feature to look across all technology documentation. For questions or to suggest additions to this list, contact the\nEnterprise Business Architecture team\n.\nBrowsers\n‚Äã\nPosition\nStandard\nTags\napp-delivery-platforms\nInformation Sensitivity\nRed\nArchitect\nAngela Lacy\nContact(s)\nAngela Lacy\nPreferred for modern web applications\nUse Cases\n‚Äã\nweb applications, progressive web apps\nNotes\n‚Äã\nThe use of Java or other client frameworks and plugins should be avoided for new applications\nCentralized Apps\n‚Äã\nPosition\nStandard\nTags\napp-delivery-platforms\nInformation Sensitivity\nRed\nArchitect\nAngela Lacy\nContact(s)\nDamien Mills\nPreferred for virtualization\nUse Cases\n‚Äã\nIntermittent access to a single or a group of application when the users does not have access to a Lilly desktop\nNotes\n‚Äã\nModern applications can be deployed without leveraging virtualization either via the end point browser (web application) or a native installation. XenApp Virtual application or Hosted Shared Desktops are the preferred virtualization solutions\nSee Also\n‚Äã\nhttps://virtual.lilly.com\nEverbridge\n‚Äã\nPosition\nSpecialized\nTags\nevent-broadcasting\nInformation Sensitivity\nOrange\nContact(s)\nHayley Edmondson\nCMDB CI\nCI00000000002758\nEverbridge is Lilly‚Äôs Emergency Mass Notification System (Primary Use Case) ‚Äì Also used as a notification system for ServiceNow across many teams including EDAT.\nUse Cases\n‚Äã\nHR\nIncident Notifications\nGoTo Meeting\n‚Äã\nPosition\nSpecialized\nTags\nevent-broadcasting\nInformation Sensitivity\nRed\nArchitect\nMary Codrington\nContact(s)\nRay Gonzales\nHighly collaborative online meetings. Maximum of 250 attendees. Participants can be internal and external.\nUse Cases\n‚Äã\nNearly all meeting situations should be met by Teams Meetings today. GoToMeeting should only be necessary if a very specific requirement can&%2339;t be met by a Teams meeting.\nA meeting or event where you want all participants to be able to participate as presenters / speakers.\nNotes\n‚Äã\nSee the Modern Meetings Matrix for more information:\nhttps://collab.lilly.com/:p:/r/sites/mm/Shared%20Documents/Meeting-Services-Matrix_33590.pptx?d=w4645bb9e47c24b2fbb6bedca4bcccb68&csf=1&web=1&e=DqnYqX\nSee Also\n‚Äã\n/:w:/r/sites/mm/_layouts/15/Doc.aspx?sourcedoc=%7BFF92910C-DFEA-4AFD-832B-1CB35BF59A7F%7D&file=GoToMeeting%20Best%20Practices%20from%20Home.docx&action=default&mobileredirect=true&cid=f8a59235-9e7f-44a3-9fed-b3eddff85466\nGoToTraining\n‚Äã\nPosition\nSpecialized\nTags\nevent-broadcasting\nInformation Sensitivity\nRed\nArchitect\nMary Codrington\nContact(s)\nRay Gonzales\nUser-led online meetings, designed to create a virtual classroom environment. Participants can be internal and external.\nUse Cases\n‚Äã\nTraining / classroom style meeting.\nNotes\n‚Äã\nSee the Modern Meetings Matrix for more information:\nhttps://collab.lilly.com/:p:/r/sites/mm/Shared%20Documents/Meeting-Services-Matrix_33590.pptx?d=w4645bb9e47c24b2fbb6bedca4bcccb68&csf=1&web=1&e=DqnYqX\nSee Also\n‚Äã\n/:w:/r/sites/mm/_layouts/15/Doc.aspx?sourcedoc=%7BFF92910C-DFEA-4AFD-832B-1CB35BF59A7F%7D&file=GoToMeeting%20Best%20Practices%20from%20Home.docx&action=default&mobileredirect=true&cid=f8a59235-9e7f-44a3-9fed-b3eddff85466\nGoToWebinar\n‚Äã\nPosition\nSpecialized\nTags\nevent-broadcasting\nInformation Sensitivity\nRed\nArchitect\nMary Codrington\nContact(s)\nRay Gonzales\nUser-led broadcasts in a one-to-many environment. Supports large audiences (licensing available for up to 3,000) for internal and external participants.\nUse Cases\n‚Äã\nSelf-produced large audience events engaging a targeted audience where an external participant is attending or presenting. If internal only participants and attendees, either a Teams or Yammer Live Event should meet this need instead.\nNotes\n‚Äã\nSee the Modern Meetings Matrix for more information:\nhttps://collab.lilly.com/:p:/r/sites/mm/Shared%20Documents/Meeting-Services-Matrix_33590.pptx?d=w4645bb9e47c24b2fbb6bedca4bcccb68&csf=1&web=1&e=DqnYqX\nSee Also\n‚Äã\n/:w:/r/sites/mm/_layouts/15/Doc.aspx?sourcedoc=%7BFF92910C-DFEA-4AFD-832B-1CB35BF59A7F%7D&file=GoToMeeting%20Best%20Practices%20from%20Home.docx&action=default&mobileredirect=true&cid=f8a59235-9e7f-44a3-9fed-b3eddff85466\nHosted Shared Desktops (XenApp)\n‚Äã\nPosition\nStandard\nTags\napp-delivery-platforms\nInformation Sensitivity\nRed\nArchitect\nAngela Lacy\nContact(s)\nDamien Mills\nPreferred Desktop Virtualization solution\nUse Cases\n‚Äã\nLarge number of users consuming a standard list of applications (e.g. Third party support organizations such as TCS, Infosys, etc)\nNotes\n‚Äã\nTthe use of Java or other client frameworks and plugins should be avoided for new applications. Existing application can be provisioned without virtualization as long as they can sustain frequent client OS and plugin updatesModern applications can be deployed without leveraging virtualization either via the end point browser (web application) or a native installation\nSee Also\n‚Äã\nhttps://virtual.lilly.com\niObeya\n‚Äã\nPosition\nSpecialized\nTags\ncollaboration-tools\nInformation Sensitivity\nOrange\nArchitect\nRajesh Singh\nContact(s)\nNick Miller\nUse in complex persistent whiteboarding collaborations\nUse Cases\n‚Äã\nMulti-room whiteboarding, Integration with Jira, Rich functionality, Support for Lean concepts\nNotes\n‚Äã\nIndividual licencing by business area. Consult for iObeya external collaboration. This positioning statements refers to the private cloud implementation of iObeya which is used primarily by MQ and the IBU. Medicines Development uses the public cloud option for iObeya.  The custodian is Chris DePoy\nSee Also\n‚Äã\nhttps://lillymqit.iobeya.com\nMicrosoft Forms\n‚Äã\nPosition\nStandard\nTags\nlow-code-platforms\nInformation Sensitivity\nRed\nArchitect\nSteve Scott\nContact(s)\nSteve Scott\nUse for simple forms/surveys/polls directed at internal & external (use caution) users\nUse Cases\n‚Äã\nDevelop a basic form/surveys, Develop a basic survey/poll for internal audiences, Develop a form that can be used on mobile devices\nSee Also\n‚Äã\nhttps://collab.lilly.com/sites/msformscommunity\nMicrosoft Power Apps\n‚Äã\nPosition\nStandard\nTags\nlow-code-platforms\nInformation Sensitivity\nRed\nArchitect\nSteve Scott\nContact(s)\nSteve Scott\nCMDB CI\nCI00000000396107\nUse to rapidly build apps using low-code, drag-and-drop tools such as pre-built data connectors and templates.\nUse Cases\n‚Äã\nPower Apps can be used to build apps ranging from a basic, single-screen data entry application to more complex solutions that require the use of professional developer skills and tooling.\nNotes\n‚Äã\nSupport for any applications built using Power Apps are the responsibility of the app maker. Use cases that require access to data outside of Office 365 will require additional premium (paid) licensing. Please consult with functional Tech@Lilly and\\or Quality resources for questions about whether or not Power Apps is suitable for your use case.\nSee Also\n‚Äã\nhttps://collab.lilly.com/sites/powerappsandflowcommunity\nMicrosoft Power Automate\n‚Äã\nPosition\nStandard\nTags\nenterprise-automation\nInformation Sensitivity\nRed\nArchitect\nSteve Scott\nContact(s)\nSteve Scott\nCMDB CI\nCI00000000396107\nUse to rapidly build automated processes using low-code, drag-and-drop tools such as pre-built data connectors and templates.\nUse Cases\n‚Äã\nDevelop a basic workflow to augment the capabilities of a PowerApp developmentCommon use cases would include automation of repetitive processes such as document routing\\approval, management of files\\folders in cloud storage such as SharePoint, OneDrive or Teams, or building integrations between disparate systems.\nNotes\n‚Äã\nSupport for any cloud flows built using Power Automate are the responsibility of the app maker. Use cases that require access to data outside of Office 365 will require additional premium (paid) licensing. Please consult with functional Tech@Lilly and\\or Quality resources for questions about whether or not Power Automate is suitable for your use case.\nSee Also\n‚Äã\nhttps://collab.lilly.com/sites/powerappsandflowcommunity\nMicrosoft Power BI\n‚Äã\nPosition\nStandard\nTags\nbusiness-intelligence\nInformation Sensitivity\nRed\nArchitect\nMalika Mahoui\nContact(s)\nSrihari S.\nRecommended tool  to create Dashboards, Visualization artifacts supported with Augmented Analytics and tight O365 integration. Suitable when you require various ways to manipulate data\nUse Cases\n‚Äã\nCreate visualization dashboards using different chart options. Compelling story-telling using data from different data stores. Create dashboard and collaborate with others using Microsoft teams and SharePoint online. Use this tool for data analytics and modeling. Use this tool if you want to perform AUTO ML tasks without data science skills\nNotes\n‚Äã\nCost effective enterprise tool to create visualizations. Desktop is free. Create reports, dashboards and visualizations by connecting to data from various data sources. Use this tool if you have a requirements to share the dashboards through SharePoint Online, Teams and integration with Microsoft PowerApps\nSee Also\n‚Äã\nhttps://collab.lilly.com/sites/EIP/SitePages/Enterprise-Analytics.aspx\nMicrosoft Teams\n‚Äã\nPosition\nStandard\nTags\ncollaboration-tools\nInformation Sensitivity\nRed\nArchitect\nBryce Williams\nContact(s)\nKala Cherney\nPreferred for small and private unstructured collaboration, including document sharing. Use for real-time and persistent chat between individuals or small groups, link sharing to content, and conducting online 1:1/Small group meetings\n‚Äã\nHighly collaborative online meetings. Maximum of 1,000 attendees. Participants can be internal and external.\nUse Cases\n‚Äã\nCollaborate with a project/team\nCollaborate on documents\nClosed group discussion\nText based chat\nMeeting/call\nPersistent async chat\nSend communication to individuals/small groups\nWork out loud\nCreate/access shared knowledge\nShare rich media\nNotes\n‚Äã\nSee the Modern Meetings Matrix for more information:\nhttps://collab.lilly.com/:p:/r/sites/mm/Shared%20Documents/Meeting-Services-Matrix_33590.pptx?d=w4645bb9e47c24b2fbb6bedca4bcccb68&csf=1&web=1&e=DqnYqX\nSee Also\n‚Äã\nhttps://collab.lilly.com/sites/microsoftteams\nMicrosoft Viva Engage (Yammer)\n‚Äã\nPosition\nStandard\nTags\ncollaboration-tools\nInformation Sensitivity\nYellow\nArchitect\nBryce Williams\nContact(s)\nBryce Williams\nPreferred online community engagement solution\nUse Cases\n‚Äã\nWork out loud\nCreate/access shared knowledge\nCreate broad awareness on a topic\nCreate a virtual community around a specific topic/subject\nDevelop a place for OCM and communcation on a specific subject\nCreate a communcation space for events (RSVP)\nCreate a local function/business unit communcation site / portal\nProvide online customer support\nShare rich media\nAbility to augment the interactive capabilities of O365/SP Communcation Sites\nNotes\n‚Äã\nAbility to augment the interactive capabilities of O365/SP Communcation Sites\nSee Also\n‚Äã\nhttps://collab.lilly.com/sites/yammerpocsupportcommunity\nMicrosoft Whiteboard\n‚Äã\nPosition\nStandard\nTags\ncollaboration-tools\nInformation Sensitivity\nRed\nArchitect\nMary Codrington\nContact(s)\nMike Cooper\nPreferred whiteboarding for meetings/working sessions\nUse Cases\n‚Äã\nIntegration with Surface Hub, capturing notes and drawings from on-line meetings, Integration with Microsoft Team / O365\nUse with external meeting attendees works during a Teams Meeting only, on a Whiteboard initiated during that meeting. It does NOT currently work outside of a Teams meeting with third parties, nor on Whiteboard files created prior to a meeting starting. That is on a future roadmap for a more complete external user sharing & co-collaboration use pattern.\nNotes\n‚Äã\nLicenced as part of the Microsoft EA. Jira integration in the roadmap for Q4-2021\nSee Also\n‚Äã\nhttps://collab.lilly.com/sites/DiscoverIT/SitePages/Microsoft-Whiteboard.aspx\nNative Installation\n‚Äã\nPosition\nStandard\nTags\napp-delivery-platforms\nInformation Sensitivity\nRed\nArchitect\nAngela Lacy\nContact(s)\nAngela Lacy\nPreferred solution for modern native applications\nUse Cases\n‚Äã\nUser have access to a Lilly device\nNotes\n‚Äã\nModern applications can be deployed without leveraging virtualization via a native installation\nOn24\n‚Äã\nPosition\nSpecialized\nTags\nevent-broadcasting\nInformation Sensitivity\nRed\nArchitect\nJim Spilman\nContact(s)\nJim Spilman\nProfessionally produced broadcast services that support large audiences with internal and external participants.\nUse Cases\n‚Äã\nProfessional quality all employee broadcasts or large organization town halls.\nNotes\n‚Äã\nPlease contact Jim B Spilman and Brian Albertsen for more information\nSee Also\n‚Äã\n/:p:/r/sites/mm/Shared%20Documents/Meeting-Services-Matrix_33590.pptx?d=w4645bb9e47c24b2fbb6bedca4bcccb68&csf=1&web=1&e=DqnYqX\nOutlook / Exchange\n‚Äã\nPosition\nStandard\nTags\ncollaboration-tools\nInformation Sensitivity\nYellow\nArchitect\nEric Toetz\nContact(s)\nEric Toetz\nUse for simple written async communications between parties, broad push of communications, notification of important information, link sharing to content\nUse Cases\n‚Äã\nSend communication to individuals/small groupsPush a communication to a broad audience\nSee Also\n‚Äã\nhttps://collab.lilly.com/sites/globalmessaging\nPDF\n‚Äã\nPosition\nSpecialized\nTags\nlow-code-platforms\nInformation Sensitivity\nRed\nArchitect\nKevin Shingleton\nUse only for quick simple forms that require online and offline use.\nUse Cases\n‚Äã\nDevelop a basic form, Develop a form that can be used Offline, Export form (questions and answers) to a PDF document for archiving\nServiceNow Surveys\n‚Äã\nPosition\nSpecialized\nTags\nlow-code-platforms\nInformation Sensitivity\nOrange\nArchitect\nKarl Mayer\nContact(s)\nCamille Taylor\nUse for Service Management related surveys in the context of a ServiceNow process/workflow.\nUse Cases\n‚Äã\nSchedule/Trigger/Create/Send a customer satisfaction survey from the Service Desk or a Specialized Support Team for a service management process or request fulfillment, Enable better reporting, analytics and improvements to service management processes and request fulfillment through better availability of survey data within the Service Desk platform\nSee Also\n‚Äã\nhttps://docs.servicenow.com/bundle/london-servicenow-platform/page/administer/survey-administration/reference/r_SurveyManagementLandingPage.html\nSharePoint Communication Sites\n‚Äã\nPosition\nStandard\nTags\ncollaboration-tools\nInformation Sensitivity\nRed\nArchitect\nSteve Scott\nContact(s)\nSteve Scott\nUse for structured, custom and organized portal spaces targeting internal audiences\nUse Cases\n‚Äã\nCreate broad awareness on a topic\nCreate a virtual community around a specific topic/subject\nDevelop a place for OCM and communcation on a specific subject\nCreate a communcation space for events (RSVP)\nCreate a local function/business unit communcation site (portal)\nNotes\n‚Äã\nProvides a broad set of capabilities when used in integration with other Office 365 suite components\nPrimary content search engine\nSee Also\n‚Äã\nhttps://collab.lilly.com/sites/sharepointcommunity\nSharePoint Team Sites\n‚Äã\nPosition\nStandard\nTags\ncollaboration-tools\nInformation Sensitivity\nRed\nArchitect\nSteve Scott\nContact(s)\nSteve Scott\nAll-round collaboration with formal structure, content presentation and advanced access control. Use also for controlled non-regulated transitory and non-transitory content. Preferred option for external collaboration with third-parties along with Teams.\nUse Cases\n‚Äã\nCollaborate with a project/team\nCollaborate on documents\nCollaborate on or share content with external partners\nManage controlled\nnon-regulated\ntransitory and non-transitory content\nManage content that supports a local\nnon-regulated\nbusiness process\nNotes\n‚Äã\nAll-round collaboration with formal structure, content presentation and advanced access control. Use also for controlled\nnon-regulated\ntransitory and non-transitory content in the context of a team/project collaboration. Note that all business records must be manager per global records-keeping and records-retention processes (see\nhttps://now.lilly.com/landingoverview/global-global-records-and-information-management/global-grrs\n).\nFor regulated content review the Content Management positioning document\nSee Also\n‚Äã\nhttps://collab.lilly.com/sites/sharepointcommunity\nTeams Live Events\n‚Äã\nPosition\nStandard\nTags\nevent-broadcasting\nInformation Sensitivity\nRed\nArchitect\nBryce Williams\nContact(s)\nRay Gonzales\nUser-led broadcasts in a one-to-many style  (up to 10k attendees). Supports large audiences (targeted audience distribution) for internal participants.\nUse Cases\n‚Äã\nSelf-produced large audience events engaging a targeted audience\nExternal use is limited to supporting External Presenters. External attendees is only possible in very limited, specialized scenarios.\nNotes\n‚Äã\nSee the Modern Meetings Matrix for more information\nhttps://collab.lilly.com/:p:/r/sites/mm/Shared%20Documents/Meeting-Services-Matrix_33590.pptx?d=w4645bb9e47c24b2fbb6bedca4bcccb68&csf=1&web=1&e=DqnYqX\nSee Also\n‚Äã\nhttps://collab.lilly.com/sites/ITPortal/SitePages/Live-Events-in-Teams-and-Yammer.aspx\nVirtual Desktop (XenDesktop)\n‚Äã\nPosition\nSpecialized\nTags\napp-delivery-platforms\nInformation Sensitivity\nRed\nArchitect\nAngela Lacy\nContact(s)\nDamien Mills\nPersistent Desktops - by exception only\nUse Cases\n‚Äã\nAny use cases where Hosted Shared Desktops do not meet the business needs (e.g. a custom list of applications is required)\nNotes\n‚Äã\nXenDesktop Virtual Desktop always require an exception. Non-persistent desktops are default. Persistent desktops only by additional exception\nSee Also\n‚Äã\nhttps://virtual.lilly.com\nWalkMe\n‚Äã\nPosition\nSpecialized\nTags\nenterprise-automation\nInformation Sensitivity\nUnder Review\nArchitect\nGreg Graf\nContact(s)\nJon Thomas\nUse to guide users thru the steps of a process / workflow\nUse Cases\n‚Äã\nprovide assistance/guidance and  navigate users thru a process/workflow build on a web, html, css/javascript application\nNotes\n‚Äã\nConsult for external use cases. Support will transition from GS IT to GIS/Automation Enablement\nYammer Live Events\n‚Äã\nPosition\nStandard\nTags\nevent-broadcasting\nInformation Sensitivity\nYellow\nArchitect\nBryce Williams\nContact(s)\nRay Gonzales\nUser-led broadcasts in a one-to-many style (up to 10k attendees). Supports large audiences (open Yammer Community) and active engagement for internal Lilly √•ttendees. Recordings are published into Stream for SharePoint for playback and accessibility benefits.\nUse Cases\n‚Äã\nSelf-produced large audience events engaging an open community\nNotes\n‚Äã\nSee the Modern Meetings Matrix for more information:\nhttps://collab.lilly.com/:p:/r/sites/mm/Shared%20Documents/Meeting-Services-Matrix_33590.pptx?d=w4645bb9e47c24b2fbb6bedca4bcccb68&csf=1&web=1&e=DqnYqX\nSee Also\n‚Äã\nhttps://collab.lilly.com/sites/ITPortal/SitePages/Live-Events-in-Teams-and-Yammer.aspx\nDeclining ‚Üí Retired\n‚Äã\nBox (QuickCollab)\n‚Äã\nPosition\nRetired 2020-12-31\nTags\ncollaboration-tools\nInformation Sensitivity\nRed\nArchitect\nBryce Williams\nContact(s)\nBryce Williams\nUse when external document sharing is the predominant requirement. Exit by 31/DEC/2020\nNotes\n‚Äã\nO365 alternatives available from July 2020. Existing QuickCollabs will either retire or migrate. Exit by 31/DEC/2020\nDocumentum\n‚Äã\nPosition\nRetired 2023-06-30\nTags\ncontent-management\nInformation Sensitivity\nRed\nArchitect\nJannell Gunn\nContact(s)\nJannell Gunn\nDocumentum is declining with a predicted exit date of end of 2023 (TBC). No new projects/repositories. Alternatives: Veeva quality docs, SharePoint. Some content may be archived long term during migration. The Documentum team will contact business areas to discuss the individual migration plans. Previous positioning: Preferred solution for long-term in-use content management with advanced document management needs. Each business area in Documentum is completing an assessment of the correct location for the migration of their content.\nUse Cases\n‚Äã\nLong-term in-use and complex document management. Controlled and/or regulated documents. Drug development & marketing content, including regulatory content. Training content and material. Collaborate on team/project content. Manage content that supports a local business processShare content within a community. Manage enterprise content.\nForm Assembly\n‚Äã\nPosition\nRetired 2021-12-31\nTags\nlow-code-platforms\nInformation Sensitivity\nOrange\nArchitect\nJon Thomas\nContact(s)\nJon Thomas\nForm Assembly was specialized for forms that require Force.com integration and was retired Q4 2021.\nUse Cases\n‚Äã\nDevelop a basic form or advanced form when Force.com integration is required\nNotes\n‚Äã\nBeginning 01-JAN-2020 No new forms on Form Assembly without an EA exception (known exception: Force.com integration)\nForms Builder (Force)\n‚Äã\nPosition\nRetired 2019-03-31\nTags\nlow-code-platforms\nInformation Sensitivity\nRed\nArchitect\nJon Thomas\nContact(s)\nJon Thomas\nInformation Sensitivity\nRed\nArchitect\nJon Thomas\nContact(s)\nJon Thomas\nHome Drives on LAN\n‚Äã\nPosition\nRetiring\nTags\ncollaboration-tools\nInformation Sensitivity\nRed\nArchitect\nJarret DiPace\nContact(s)\nMehri Fox\nLegacy ICS Solution. Use OneDrive as the preferred solution for ICS\nNotes\n‚Äã\nFor retirement plans and alternatives contact Mehri Fox\nInfopath\n‚Äã\nPosition\nRetired 2022-12-31\nTags\nlow-code-platforms\nInformation Sensitivity\nRed\nArchitect\nSteve Scott\nContact(s)\nSteve Scott\nLAN Drives - Collaboration & Content Management\n‚Äã\nPosition\nDeclining 2021-04-01\nTags\ncollaboration-tools, content-management\nInformation Sensitivity\nOrange\nArchitect\nScott Kuckuck\nContact(s)\nScott Kuckuck\nLAN Shares must not be used for data or applications without an EA Exception.\n\"Sites/Affiliates having low network bandwidth\" is Specialized\n\"Long-term storage of inactive files or business records\" is Exiting\n\"All other collaboration & content management\" is Declining\nNotes\n‚Äã\nIf there is a need to use LAN drives to start content subject to long term retention and regulation an exception should raised with Enterprise Architecture. For retirement plans and alternatives, contact Scott Kuckuck\nLoop (Group / Space)\n‚Äã\nPosition\nRetired 2019-07-31\nTags\ncollaboration-tools\nInformation Sensitivity\nOrange\nArchitect\nBryce Williams\nContact(s)\nBryce Williams\nMicrosoft SharePoint Team Sites (on-prem)\n‚Äã\nPosition\nRetired 2020-03-31\nTags\ncollaboration-tools\nInformation Sensitivity\nRed\nArchitect\nSteve Scott\nContact(s)\nSteve Scott\nRetire by Q1-2020\nNotes\n‚Äã\nRemove sites or transition to O365 SharePoint or Teams. Complex SharePoint Application must move out of SharePoint\nMySite (SharePoint)\n‚Äã\nPosition\nRetired 2019-12-31\nTags\ncollaboration-tools\nInformation Sensitivity\nRed\nArchitect\nSteve Scott\nContact(s)\nSteve Scott\nSalesforce BP&A\n‚Äã\nPosition\nRetired 2021-05-24\nTags\nenterprise-automation\nInformation Sensitivity\nRed\nArchitect\nRob Blackketter\nContact(s)\nRob Blackketter\nSharePoint Portal Site (on-prem)\n‚Äã\nPosition\nRetired 2020-12-31\nTags\ncollaboration-tools\nInformation Sensitivity\nRed\nArchitect\nSteve Scott\nContact(s)\nSteve Scott\nSharepoint Workflows\n‚Äã\nPosition\nRetired 2020-03-31\nTags\nenterprise-automation\nInformation Sensitivity\nRed\nSkype for Business\n‚Äã\nPosition\nRetired 2022-03-31\nTags\ncollaboration-tools\nInformation Sensitivity\nYellow\nArchitect\nSteve Scott\nContact(s)\nSteve Scott\nUse for 1:1/Small Group chat and online meetings. Declining. Exit by 31/DEC/2021\nUse Cases\n‚Äã\nText based chatMeetings/calls\nVMWare Virtual Desktops\n‚Äã\nPosition\nRetired 2020-12-31\nTags\napp-delivery-platforms\nDocument generated from Stack Overflow Enterprise articles (prioritized) and ea_tech.csv (fallback)\nWindows Home Drives (H: Drive)\n‚Äã\nPosition\nRetiring\nTags\ncollaboration-tools\nInformation Sensitivity\nOrange\nArchitect\nJarret DiPace\nContact(s)\nJarret DiPace\nRetire Windows Home Drives. The target is to migrate data to Microsoft O365 (OneDrive). 414TB of Home drives across 48,977 named users.\nUse Cases\n‚Äã\nIndividual Content Storage of User Files\nNotes\n‚Äã\nEnable usage of O365 security, privacy, retention/records mgmt, and assessment tools.\nMove to single location for Windows Home Drives.\nAvoid Capital lifecycle purchases in late 2023.\nMigration: Migrate or retire 48,977(414TB) Home Drives totaling 740TB of on premises storage.\nSee Also\n‚Äã\nhttps://wip.am.lilly.com/(S(1qyafr5ron2ygq5f1bjm3ni1))/WIP1_Work_Request_Registration.aspx?WIP=53199\nDocument generated from Stack Overflow Enterprise articles (prioritized) and ea_tech.csv (fallback)\nWas this helpful?\nEdit this page\nPrevious\nüöÄ Team Productivity\nNext\nüß≠ Diagramming\nBrowsers\nCentralized Apps\nEverbridge\nGoTo Meeting\nGoToTraining\nGoToWebinar\nHosted Shared Desktops (XenApp)\niObeya\nMicrosoft Forms\nMicrosoft Power Apps\nMicrosoft Power Automate\nMicrosoft Power BI\nMicrosoft Teams\nMicrosoft Viva Engage (Yammer)\nMicrosoft Whiteboard\nNative Installation\nOn24\nOutlook / Exchange\nPDF\nServiceNow Surveys\nSharePoint Communication Sites\nSharePoint Team Sites\nTeams Live Events\nVirtual Desktop (XenDesktop)\nWalkMe\nYammer Live Events\nDeclining ‚Üí Retired\nBox (QuickCollab)\nDocumentum\nForm Assembly\nForms Builder (Force)\nHome Drives on LAN\nInfopath\nLAN Drives - Collaboration & Content Management\nLoop (Group / Space)\nMicrosoft SharePoint Team Sites (on-prem)\nMySite (SharePoint)\nSalesforce BP&A\nSharePoint Portal Site (on-prem)\nSharepoint Workflows\nSkype for Business\nVMWare Virtual Desktops\nWindows Home Drives (H: Drive)\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 2,
    "depth": 3,
    "crawled_at": "2026-02-25T10:09:33.706080"
  },
  "https://techhq.dc.lilly.com/docs/solution/productivity/diagramming": {
    "url": "https://techhq.dc.lilly.com/docs/solution/productivity/diagramming",
    "title": "üß≠ Diagramming | Tech HQ",
    "description": "Stack Overflow Article: üß≠ Diagramming",
    "h1": [
      "Diagramming"
    ],
    "h2": [
      "Use Cases‚Äã",
      "Tech Recommendations‚Äã"
    ],
    "h3": [
      "Quick visualizations and non-technical presentations‚Äã",
      "Detailed technical diagrams and network diagrams‚Äã",
      "Collaborative projects and cloud architecture diagrams‚Äã",
      "Visualizing and communicating software architecture to technical teams‚Äã"
    ],
    "text_content": "üß≠ Diagramming | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüóÑÔ∏è Team Productivity BLT\nüß≠ Diagramming\nüé® User Experience & Design\nüöÄ Team Productivity\nüß≠ Diagramming\nOn this page\nDiagramming\nSynced daily from Stack Overflow\nSolution Guide\nLifecycle: Draft\nLast Update: 2025-03-28\nCapability Owner: Sanish Haridas\nEBA Lead: Kapil Kamat\nContributors & Reviewers:\nThis guide provides recommendations for diagramming tools and frameworks.\nUse Case/Scenario\nTech Recommendation\nPositioning\nComplexity\nNotable Integration/Interop\nOwning Org/Team\nNext Step\nCollaborative product and architecture diagrams\nLucidchart\nStrategic Core\nLow\nWeb-based\nSPE EDAT\nAutomated Lucidsuite license\nQuick visualizations and non-technical presentations\nPowerPoint\nStrategic Core\nLow\nDigital Core Collaboration\nUse\nApp & web wireframing, collaborative whiteboarding\nFigma\nStrategic Core\nLow\nWeb-based\nDigital Office UX\nLogin using Lilly email\nCollaborative Whiteboarding\nMicrosoft Whiteboard\nStandard\nLow\nMicrosoft Teams, web, or iPad\nDigital Core Collaboration\nUse\nDetailed technical diagrams and network diagrams\nMicrosoft Visio\nDeclining\nLow\nWindows only\nDigital Core Collaboration\nOrder Visio or use Visio Viewer\nMarkdown-based diagramming and charting\nC4 Model\nStandard\nLow\nDigital Core EBA\nUse\nProgrammatic flowchart and diagram creation\nMermaid\nEmerging\nMedium\nDigital Core EBA\nUse\nUse Cases\n‚Äã\nQuick visualizations and non-technical presentations\n‚Äã\nFocuses on visual communication of complex software architecture to stakeholders who may not have a technical\nbackground. These visualizations help to simplify intricate concepts, making them more accessible and easier to\nunderstand.\nDetailed technical diagrams and network diagrams\n‚Äã\nTechnical and network diagrams are vital for technical teams, offering a detailed blueprint of software architecture.\nThey illustrate system components, data flows, and interactions, helping developers and IT professionals understand\ncomplex interdependencies. Network diagrams show the physical and logical layout of network infrastructure, essential\nfor troubleshooting, performance optimization, and ensuring scalability and security.\nCollaborative projects and cloud architecture diagrams\n‚Äã\nCollaborative projects and cloud architecture diagrams are key in modern software development. They enable teams to work\ntogether smoothly, regardless of location. These diagrams map out cloud infrastructure, showing how services integrate\nand interact. They are vital for planning, deploying, and managing scalable, resilient, and secure cloud applications.\nCollaboration allows for continuous feedback and improvement, resulting in more robust cloud solutions.\nVisualizing and communicating software architecture to technical teams\n‚Äã\nVisualizing and communicating software architecture to technical teams involves creating representations that capture\nthe system's complexity while ensuring clarity and comprehensibility. This process includes developing detailed\nflowcharts, UML diagrams, and architectural blueprints that delineate modules, dependencies, and data flow. These visual\naids not only facilitate better understanding among team members but also serve as a reference during development,\ntroubleshooting, and scaling phases.\nTech Recommendations\n‚Äã\nPowerPoint\n: Use for quick visualizations, presentations to non-technical audiences, and adding diagrams to slides.\nMicrosoft Visio\n: Use for detailed technical diagrams, network diagrams, and complex process flows.\nLucidchart\n: Use for collaborative projects, cloud architecture diagrams, and integrating with web-based tools.\nC4 Model\n: Use for visualizing and communicating software architecture to technical teams.\nMermaid\n: Use for creating diagrams and visualizations using text-based descriptions, ideal for technical\ndocumentation, integrating with Markdown, and embedding in web applications.\nWas this helpful?\nTags:\nsolution-guide\nproductivity\ntechhq\nEdit this page\nPrevious\nüóÑÔ∏è Team Productivity BLT\nNext\nüé® User Experience & Design\nUse Cases\nQuick visualizations and non-technical presentations\nDetailed technical diagrams and network diagrams\nCollaborative projects and cloud architecture diagrams\nVisualizing and communicating software architecture to technical teams\nTech Recommendations\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 3,
    "depth": 3,
    "crawled_at": "2026-02-25T10:09:35.899839"
  },
  "https://techhq.dc.lilly.com/docs/solution/ux/ux-blt": {
    "url": "https://techhq.dc.lilly.com/docs/solution/ux/ux-blt",
    "title": "üóÑÔ∏è User Experience & Design BLT | Tech HQ",
    "description": "The User Experience & Design Big List of Technology (BLT) is maintained using data from the Enterprise Architecture Tech Positioning Database and LillyFlow, with AI-assisted updates. While we aim for accuracy, please verify critical information and contact the EBA team with questions or corrections.",
    "h1": [
      "üóÑÔ∏è User Experience & Design BLT"
    ],
    "h2": [
      "Adobe XD‚Äã",
      "Deque axe Tools‚Äã",
      "Figma‚Äã",
      "Google Analytics 4‚Äã",
      "Illustrator‚Äã",
      "InDesign‚Äã",
      "JAWS‚Äã",
      "Photoshop‚Äã",
      "The Lilly Design System (LDS)‚Äã",
      "UserZoom‚Äã",
      "Declining ‚Üí Retired‚Äã"
    ],
    "h3": [
      "Axure‚Äã",
      "Miro‚Äã",
      "Optimal Workshop‚Äã",
      "Sketch‚Äã",
      "Tobii‚Äã"
    ],
    "text_content": "üóÑÔ∏è User Experience & Design BLT | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nüß© Solution Overview\nü§ñ AI & Intelligent Agents\nüè¢ Business Enablement\n‚òÅÔ∏è Cloud & Infrastructure\nüîê Cybersecurity\nüìä Data & Analytics\nüõ†Ô∏è Engineering Enablement\nüõ∞Ô∏è Observability & Reliability\nüöÄ Team Productivity\nüé® User Experience & Design\nüóÑÔ∏è User Experience & Design BLT\nüß≠ Accessibility Enablement\nüé® User Experience & Design\nüóÑÔ∏è User Experience & Design BLT\nOn this page\nüóÑÔ∏è User Experience & Design BLT\nThe User Experience & Design Big List of Technology (BLT) is maintained using data from the Enterprise Architecture Tech Positioning Database and LillyFlow, with AI-assisted updates. While we aim for accuracy, please verify critical information and contact the EBA team with questions or corrections.\nCan't find a technology?\nUse TechHQ's search feature to look across all technology documentation. For questions or to suggest additions to this list, contact the\nEnterprise Business Architecture team\n.\nAdobe XD\n‚Äã\nPosition\nStandard\nTags\nux\nArchitect\nLibby Edwards\nContact(s)\nLibby Edwards/Sarah Hemaida\nAdvanced interface and prototyping app that lets you design websites, mobile apps, and more. Get feedback, collaborate, and iterate on your experiences for any screen.\nUse Cases\n‚Äã\nUI design\nSee Also\n‚Äã\nhttps://www.adobe.com/products/xd.html\nDeque axe Tools\n‚Äã\nPosition\nSpecialized\nTags\nux\nArchitect\nLibby Edwards\nContact(s)\nLibby Edwards\nDigital accessibility toolkit. Robust, efficient and accurate accessibility testing for all.\nUse Cases\n‚Äã\nAccessibility Evaluations\nSee Also\n‚Äã\nhttps://www.deque.com/\nFigma\n‚Äã\nPosition\nStandard\nTags\nux\nArchitect\nLibby Edwards\nContact(s)\nLibby Edwards/Logan Weilenman\nA collaborative web application for interface design\nUse Cases\n‚Äã\nUI design, UX design, design evaluations, inclusive design\nSee Also\n‚Äã\nhttps://www.figma.com/\nGoogle Analytics 4\n‚Äã\nPosition\nStandard\nTags\nux\nInformation Sensitivity\nYellow\nArchitect\nKarl Mayer\nContact(s)\nMichele Johnson\nCMDB CI\nCI00000000260611\nIndustry-standard tool for website/webapp traffic tracking and reporting.\nGoogle Analytics 4\nhas replaced\nGoogle Universal Analytics\n, which you may also see referred to as\nGoogle Analytics 360\n. Google Universal Analytics has been retired.\nUse Cases\n‚Äã\nClickstream Analysis, A/B Testing, Web Usability Benchmarking\nSee Also\n‚Äã\nServiceNow Request:\nManage Google Analytics 4 360 Property\nIllustrator\n‚Äã\nPosition\nStandard\nTags\nux\nInformation Sensitivity\nYellow\nArchitect\nLibby Edwards\nContact(s)\nLibby Edwards/Sarah Hemaida\nAdvanced graphic design app that lets you create illustrations, logos, and more. Use precise tools to make vector artwork that can scale to any size.\nUse Cases\n‚Äã\nVisual design\nSee Also\n‚Äã\nhttps://www.adobe.com/products/illustrator.html\nInDesign\n‚Äã\nPosition\nStandard\nTags\nux\nInformation Sensitivity\nYellow\nArchitect\nLibby Edwards\nContact(s)\nLibby Edwards/Sarah Hemaida\nAdvanced page design and layout app that lets you create and publish print and digital documents. Make posters, books, digital magazines, e-books, interactive PDFs, and more.\nUse Cases\n‚Äã\nVisual design\nSee Also\n‚Äã\nhttps://www.adobe.com/products/indesign.html\nJAWS\n‚Äã\nPosition\nStandard\nTags\nux\nInformation Sensitivity\nGreen\nArchitect\nLibby Edwards\nContact(s)\nLibby Edwards\nA screen reader that will assist users who are blind or low-vision to use a Windows computer. JAWS has a variety of features, including Braille support, multi-lingual speech synthesis, and multi-screen support.\nUse Cases\n‚Äã\nAccessibility evaluations\nNotes\n‚Äã\nFor Microsoft Windows browsers only\nSee Also\n‚Äã\nhttps://www.freedomscientific.com/products/software/jaws/\nPhotoshop\n‚Äã\nPosition\nStandard\nTags\nux\nInformation Sensitivity\nYellow\nArchitect\nLibby Edwards\nContact(s)\nLibby Edwards/Sarah Hemaida\nAdvanced image editing app that lets you transform photos, graphics, and illustrations. Edit and create with layers, brushes, AI tools, and more.\nUse Cases\n‚Äã\nVisual design, design evaluations, accessibility evaluations, inclusive design\nSee Also\n‚Äã\nhttps://www.adobe.com/products/photoshop.html\nThe Lilly Design System (LDS)\n‚Äã\nPosition\nStandard\nTags\nux, lilly-design-system\nInformation Sensitivity\nYellow\nA library of robust, accessible, and reusable UI components with easily consumable documentation\nUse Cases\n‚Äã\nResponsive Design System for any web-based application or website.\nPartnerships\n‚Äã\nLilly Brand Office\nAccessibility Team\nRegulatory\nUserZoom\n‚Äã\nPosition\nSpecialized\nTags\nux\nArchitect\nLibby Edwards\nContact(s)\nLibby Edwards/Sarah Hemaida\nA solution designed to help product developers and digital teams gain actionable insights into customer behavior to enhance digital experiences with brands.\nUse Cases\n‚Äã\nAttitudinal, behavioral, quantitative, and qualitative research\nSee Also\n‚Äã\nhttps://www.userzoom.com/\nDeclining ‚Üí Retired\n‚Äã\nAxure\n‚Äã\nPosition\nRetired 2020-12-31\nTags\nux\nInformation Sensitivity\nUnder Review\nArchitect\nLibby Edwards\nContact(s)\nLibby Edwards\nHigh-fidelity prototyping application primarily for complex interactions, UIs and pseudo-code\nUse Cases\n‚Äã\nRemote Visual Design Flow, Complex Prototype Building, Remote Prototype Sharing\nNotes\n‚Äã\nUCD Phase 2: Define, UCD Phase 3: Ideate, UCD Phase 4: Prototype, UCD Phase 5: Test\nMiro\n‚Äã\nPosition\nRetired 2020-12-31\nTags\nux\nInformation Sensitivity\nUnder Review\nArchitect\nLibby Edwards\nContact(s)\nLibby Edwards\nOnline collaborative whiteboard platform to bring remote teams together from anywhere.\nUse Cases\n‚Äã\nRemote Synthesis / Affinity / Brainstorming, Remote Visual Design Flow, Remote Journey and Empathy Map Builder\nNotes\n‚Äã\nUCD Phase 2: Define, UCD Phase 3: Ideate\nOptimal Workshop\n‚Äã\nPosition\nRetired 2021-11-17\nTags\nux\nInformation Sensitivity\nUnder Review\nArchitect\nLibby Edwards\nContact(s)\nLibby Edwards\nOnline collaborative research tool suite enabling remote user research\nUse Cases\n‚Äã\nRemote Card Sorting, Remote Moderated and Unmoderated Usability Testing, Remote Note Taking, Remote User Surveys, Desirability Studies, Research Participant Panel Management, Remote Tree Testing, Click Testing (Remote 5s and 1 click)\nNotes\n‚Äã\nUCD Phase 1: Empathy Research, UCD Phase 5: Test\nSketch\n‚Äã\nPosition\nDeclining 2023-06-30\nTags\nux\nInformation Sensitivity\nYellow\nArchitect\nLibby Edwards\nContact(s)\nLibby Edwards/Sarah Hemaida\nAll-in-one platform for digital design with collaborative design tools, prototyping and developer handoff\nUse Cases\n‚Äã\nUI design, UX design, design evaluations, inclusive design\nNotes\n‚Äã\nOnly available for macOS\nSee Also\n‚Äã\nhttps://www.sketch.com/\nTobii\n‚Äã\nPosition\nRetired 2021-07-01\nTags\nux\nInformation Sensitivity\nUnder Review\nArchitect\nLibby Edwards\nContact(s)\nLibby Edwards\nInformation Sensitivity\nUnder Review\nArchitect\nLibby Edwards\nContact(s)\nLibby Edwards\nDocument generated from Stack Overflow Enterprise articles (prioritized) and ea_tech.csv (fallback)\nWas this helpful?\nEdit this page\nPrevious\nüé® User Experience & Design\nNext\nüß≠ Accessibility Enablement\nAdobe XD\nDeque axe Tools\nFigma\nGoogle Analytics 4\nIllustrator\nInDesign\nJAWS\nPhotoshop\nThe Lilly Design System (LDS)\nUserZoom\nDeclining ‚Üí Retired\nAxure\nMiro\nOptimal Workshop\nSketch\nTobii\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright ¬© 2026 Eli Lilly and Company",
    "links_found": 2,
    "depth": 3,
    "crawled_at": "2026-02-25T10:09:38.689670"
  }
}