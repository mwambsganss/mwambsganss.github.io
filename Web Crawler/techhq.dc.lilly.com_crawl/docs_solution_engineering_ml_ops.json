{
  "url": "https://techhq.dc.lilly.com/docs/solution/engineering/ml_ops",
  "title": "ğŸ§­ ML Ops | Tech HQ",
  "description": "Stack Overflow Article: ğŸ§­ ML Ops",
  "h1": [
    "ğŸ§­ ML Ops"
  ],
  "h2": [
    "Key Conceptsâ€‹",
    "Artifact Storageâ€‹",
    "Overview of Common Technologiesâ€‹",
    "Overview of Common Patternsâ€‹",
    "How to Get Started at Lillyâ€‹",
    "Further Information on Setting Up MLOps Toolsâ€‹"
  ],
  "h3": [
    "What is MLOPS?â€‹"
  ],
  "text_content": "ğŸ§­ ML Ops | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nğŸ§© Solution Overview\nğŸ¤– AI & Intelligent Agents\nğŸ¢ Business Enablement\nâ˜ï¸ Cloud & Infrastructure\nğŸ” Cybersecurity\nğŸ“Š Data & Analytics\nğŸ› ï¸ Engineering Enablement\nğŸ—„ï¸ Engineering Enablement BLT\nğŸ§­ AI Code Assistants\nAPI Standards\nğŸ§­ ML Ops\nğŸ§­ Test Automation\nğŸ›°ï¸ Observability & Reliability\nğŸš€ Team Productivity\nğŸ¨ User Experience & Design\nğŸ› ï¸ Engineering Enablement\nğŸ§­ ML Ops\nOn this page\nğŸ§­ ML Ops\nSynced daily from Stack Overflow\nGuide Status\nLifecycle: Draft\nLast Update: 2025-10-03\nCapability Owner: Kristen Hlava\nEBA Lead: Karl Mayer\nContributors & Reviewers: Kristen Hlava, Will Skertic, David Gayda, Rhian Taylor\nWhat is MLOPS?\nâ€‹\nMLOPS stands for Machine Learning Operations. It is a set of practices that aims to automate, simplify, and standardize machine learning workflows, processes, or deployments. Its goal is to decrease time-to-value, maintain and improve model performance over time, and ensure that machine learning delivers sustained business impact.\nKey Concepts\nâ€‹\nDevOps:\nMLOps builds directly on the foundational principles of DevOps. DevOps is a set of practices that unifies software development and IT operations, emphasizing automation, collaboration, and continuous integration and delivery (CI/CD) to streamline software deployment and improve reliability.\nMLOps extends these principles to the unique challenges of machine learning, adding processes for managing data, tracking experiments, and monitoring models in production. MLOps incorporates both code and data, ensuring that machine learning models are reproducible, scalable, and maintainable throughout their lifecycle.\nInfrastructure as Code:\nA foundational practice in MLOps that enables teams to automate the provisioning and management of computing resources using code, rather than manual processes. By defining infrastructure in version-controlled scripts, we can ensure that machine learning environments are consistent, reproducible, and scalable across development, testing, and production. This approach accelerates model deployment, reduces configuration errors, and optimizes resource usageâ€”making it easier to manage complex ML workflows and adapt quickly to changing requirements.\nArtifact Storage\nâ€‹\nArtifact storage in MLOps unifies the management of code, data, and model objects, creating a reliable foundation for scalable, reproducible, and auditable machine learning operations.\nCode:\nCode artifacts encompass everything from infrastructure scripts to model training routines and pipeline definitions. Storing code in version-controlled repositories (such as Git) allows teams to track changes, collaborate effectively, and reproduce experiments. This practice ensures that the exact code used for model development and deployment is always accessible, supporting robust CI/CD workflows and minimizing configuration drift.\nData:\nData artifacts include raw datasets, processed features, and metadata generated throughout the ML pipeline. Centralized storage solutionsâ€”such as feature stores or cloud object storesâ€”enable teams to manage data lineage, version datasets, and maintain high data quality. Automated pipelines facilitate the extraction, transformation, and loading (ETL/ELT) of data, while lineage tracking and metadata management provide visibility into how data evolves and is used across experiments.\nModel Objects:\nModel objects refer to the serialized representations of trained machine learning models, such as\n.pkl\n,\n.joblib\n, or framework-specific files. These artifacts are stored in model registries or cloud storage, allowing for easy retrieval, deployment, and monitoring. By cataloging and versioning model objects, teams can roll back to previous versions, audit model changes, and ensure that only validated models are promoted to production.\nOverview of Common Technologies\nâ€‹\nMany ML tools and platforms have similar functionality. Part of the consideration for these tools will be dependent on the environment you deploy your models in (AWS vs Azure vs CATS vs AWB, etc). MagTrain/Run.AI is a great environment for model training specifically. Additional consideration will be simply in terms of technologies you may already be familiar with (Kubernetes, Docker, Nextflow, etc). When going through the AI registry process, MLOps can also be discussed as part of the architecture review.\nOverview of Common Patterns\nâ€‹\nCommon patterns include modular pipelines for data ingestion, feature engineering, model training, and evaluation; CI/CD workflows for automated deployment; and monitoring frameworks for drift detection and performance tracking. These patterns emphasize reproducibility, scalability, and collaboration between data science and engineering.\nHow to Get Started at Lilly\nâ€‹\nAs stated previously, the environment your project is in will help drive decisions on which tools to use. When filling out the AI registry for your project, be sure to ask about or include MLOps when having your architecture review discussion â€“ the AI architects can help guide in the best solution for your specific use case.\nFurther Information on Setting Up MLOps Tools\nâ€‹\nEnterprise Data Team:\nhttps://data.lilly.com/\nThe AWB platform already has installations of Sagemaker, MLFlow, and Fabric.\nCloud Team:\nhttps://cloud.lilly.com/lcs/\nFor Azure, AWS, or other cloud deployment.\nCATS:\nhttps://cats.lilly.com/\nFor deploying workflows via the CATS platform.\nMagTrain:\nhttps://github.com/EliLillyCo/MagTrain\nFor using the MagTrain cluster to run/train models.\nWas this helpful?\nTags:\nsoftware-engineering\nsolution-guide\ntechhq\nEdit this page\nPrevious\nAPI Standards\nNext\nğŸ§­ Test Automation\nWhat is MLOPS?\nKey Concepts\nArtifact Storage\nOverview of Common Technologies\nOverview of Common Patterns\nHow to Get Started at Lilly\nFurther Information on Setting Up MLOps Tools\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright Â© 2026 Eli Lilly and Company",
  "links_found": 5,
  "depth": 3,
  "crawled_at": "2026-02-25T10:09:17.364501"
}