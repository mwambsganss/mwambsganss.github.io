{
  "url": "https://techhq.dc.lilly.com/docs/innovate/emerging_tech/ai_video_generation",
  "title": "AI Video Generation | Tech HQ",
  "description": "- Last Update: 2025-06-06",
  "h1": [
    "AI Video Generation"
  ],
  "h2": [
    "Executive Summaryâ€‹",
    "Introductionâ€‹",
    "AI Video Generation Technology Overviewâ€‹",
    "Major Players and Platforms in AI Video Generationâ€‹",
    "Industry Impact and Use Casesâ€‹",
    "Strategies for Adoptionâ€‹",
    "Challenges and Risksâ€‹",
    "Future Outlookâ€‹",
    "Sourcesâ€‹"
  ],
  "h3": [
    "OpenAI â€“â€¯Soraâ€‹",
    "Runway â€“â€¯Gen-2â€¯(and beyond)â€‹",
    "Google â€“â€¯Veo 3â€¯andâ€¯Flowâ€‹",
    "Flowâ€‹",
    "Kuaishou â€“â€¯Klingâ€‹",
    "Pika Labs â€“â€¯Pika Videoâ€‹",
    "MiniMax â€“â€¯Hailuo Video-01â€‹",
    "Luma Labs â€“â€¯Dream Machine (Ray 2)â€‹",
    "Stability AI â€“â€¯Stable Video Diffusionâ€‹",
    "Adobe â€“â€¯Firefly Generative Videoâ€‹",
    "Marketing and Advertisingâ€‹",
    "Learning & Development / Trainingâ€‹",
    "Commercial and Promotionalâ€‹",
    "Quality and Consistencyâ€‹",
    "Misrepresentation and Deepfakesâ€‹",
    "Regulatory Complianceâ€‹",
    "Intellectual Property and Training Dataâ€‹",
    "Ethical Considerationsâ€‹",
    "Technical Dependency and Evolutionâ€‹",
    "Security and Privacyâ€‹",
    "Public Perceptionâ€‹",
    "Longer & Real-Time Generationâ€‹",
    "Improved Fidelityâ€‹",
    "Interactive Video & Multimodalityâ€‹",
    "Industry-Specific Marketing Models/Toolsâ€‹",
    "Regulatory Frameworksâ€‹",
    "Talent and Workflow Shiftsâ€‹",
    "Competitive Landscapeâ€‹",
    "New Content Formatsâ€‹",
    "Conclusionâ€‹"
  ],
  "text_content": "AI Video Generation | Tech HQ\nSkip to main content\nTech HQ\nInnovate\nPlan\nSolution\nAccess\nLearn\nContribute\nUpdates\nLillyFlow\nğŸ’¡ Tech@Lilly Innovation Pipeline\nğŸ› Our Innovation Stages\nğŸï¸ Fast Start\nğŸ“¡ Emerging Tech\nAI Avatars / Digital People\nAI Speech\nAI Video Generation\nğŸ“¡ Emerging Tech\nAI Video Generation\nOn this page\nAI Video Generation\nEmerging Tech Report\nLast Update: 2025-06-06\nTech Innovation Pipeline\n, Enterprise Business Architecture, Tech@Lilly Enterprise\nAuthor: Doug Gorr\nExecutive Summary\nâ€‹\nArtificial Intelligenceâ€¯video generationâ€¯is emerging as a transformative technology that enables the automatic creation\nof video content from text prompts, images, or other inputs. Recent breakthroughs inâ€¯generative AI modelsâ€¯â€“ especially\ndiffusion and transformer-based models â€“ have dramatically improved the realism and coherence of AI-generated video\nclips. Todayâ€™s leading AI video tools can produce short high-definition videos (typically a few seconds up to ~20\nseconds) withâ€¯cinematic visuals, diverse styles, and even synchronized audio. Major AI providers\nlikeâ€¯OpenAI,â€¯Google,â€¯Adobe, and a wave of startups are rapidly advancing this field. For tech and business leaders, AI\nvideo generation promisesâ€¯significant opportunities: dramatically lowering the cost and time required to produce\nmarketing videos, training content, or promotional media; enabling mass personalization of video messages; and unlocking\ncreative concepts that would be impractical with traditional filming. Early adopters in marketing (including pharma\ncommercial teams) are already experimenting with AI-generated b-roll footage, product visualizations, and even\nAI-generated spokespeople. At the same time, this technology raisesâ€¯new challenges and risksâ€¯â€“ from output quality\nlimitations (e.g. surreal physics or inconsistent characters) to ethical concerns like deepfakes and brand safety. The\ncurrent state-of-the-art models still struggle with longer narrative coherence and some realism aspects, but progress is\nrapid. Competitors in the US and China are pushing the boundaries: for example, OpenAIâ€™sâ€¯Soraâ€¯model can generate up to\n20-second 1080p videos with rich detail, while Googleâ€™s newly unveiledâ€¯Veo 3â€¯model goes further byâ€¯integrating\naudioâ€¯(music, sound effects, even speech) directly into generated videos. Well-funded startups like Pika Labs and\nindustry players like Adobe (with Firefly) are adding features such asâ€¯storyboarding tools,â€¯keyframe control, and direct\nintegration into creative workflows. Lilly shouldâ€¯pay close attentionâ€¯to this fast-evolving landscape â€“ the companies,\nmodels, and capabilities discussed in this report â€“ to craft strategies for adoption. In the near term, AI video\ngenerators can augment creative teams, accelerate content production, and enable novel marketing strategies, if\norganizations also institute proper oversight (to manage legal, ethical, and quality concerns). This report provides a\ndeep dive into the technology, key players (and their relative strengths/weaknesses), feature comparisons, industry use\ncases (with a focus on marketing, training, and pharma applications), and recommendations forâ€¯strategic adoptionâ€¯of AI\nvideo generation.\nIntroduction\nâ€‹\nThe ability for AI toâ€¯generate video contentâ€¯on demand â€“ once a futuristic concept â€“ is now becoming reality. Over the\npast few years, generative AI has progressed from creating still images to producing short video clips. In 2022-2023,\nearly text-to-video research prototypes (e.g. Metaâ€™s Make-A-Video) showed the potential but were limited to a few\nseconds of low-resolution, often jittery footage. Fast-forward to 2024-2025, and we see the emergence ofâ€¯commercial AI\nvideo generation toolsâ€¯that anyone can try. These systems take a prompt (text description and optionally reference\nimages or video clips) and synthesize a new video clip matching the description. Under the hood, most utilizeâ€¯diffusion\nmodelsâ€¯or advanced transformers that have been trained on vast amounts of video data to learn how to generate\nconsecutive frames coherently. Essentially, they extend the techniques that made AI image generators (like DALLÂ·E and\nStable Diffusion) so successful, adding a temporal dimension to handle motion and physics.\nThis technology arrives at an apt time: organizations are hungry for more video content than ever (for social media,\ne-learning, personalized marketing), but traditional video production is time-consuming and costly. AI video generation\npromises toâ€¯democratize video creation, making it almost as easy as writing a script. A marketer could simply â€œtellâ€ an\nAI,â€¯â€œShow a medicine droplet traveling through a bloodstream and destroying viruses, in a cinematic style,â€â€¯and get a\ncustom animated clip within minutes â€“ something that would have taken a VFX studio weeks to produce. Likewise, a\ntraining team could generate scenario videos with different backgrounds or actors at the click of a button. The\nimplications span many industries: media and entertainment (AI-assisted filmmaking and pre-visualization), advertising\n(rapid content iteration and localization), education (illustrative videos), and more. In pharma marketing specifically,\nwe envision uses likeâ€¯mechanism-of-action animations, doctor-patient roleplay scenarios for training, or personalized\npatient education videos generated for different demographics and languages.\nHowever, itâ€™s important to approach this innovation with open eyes.â€¯Current limitationsâ€¯mean AI-generated videos are not\nyet completely indistinguishable from real footage in most cases. Videos are typically short (a few seconds long) and\nmay exhibit artifacts or surreal glitches, especially for complex motions or human anatomy. Ensuring a consistent\ncharacter or narrative across a longer video remains challenging (some models allow chaining multiple short clips, but\ntrue long-form consistency is still an R&D topic). There are alsoâ€¯ethical considerations: generative video opens the\ndoor to hyper-realistic deepfakes or misleading content, which is a particular concern in regulated industries like\npharmaceuticals. The source of content leveraged, the machine learning processes used to create these capabilities,\nremains an open legal concern. As with any new technology, Lilly must weigh the potential benefits and risks together\nwhen determining a strategy for use and adoption. As we delve into the technology, differences between tools, and\nstrategies for adoption, we will highlight both theâ€¯opportunitiesâ€¯andâ€¯risks.\nAI Video Generation Technology Overview\nâ€‹\nAt its core, AI video generation extends image generation with the dimension of time. Early approaches\ninvolvedâ€¯Generative Adversarial Networks (GANs)â€¯andâ€¯recurrent models, but todayâ€™s state-of-the-art largely\nusesâ€¯diffusion models with transformersâ€¯trained on video data. A diffusion model gradually â€œdenoisesâ€ random noise into\na coherent image; for video, this process is applied to a sequence of video frames (often in a latent compressed space\nto reduce computation). For example, OpenAIâ€™sâ€¯Soraâ€¯model uses aâ€¯latent video compressorâ€¯and represents video as a series\nof â€œspacetime patchesâ€ that a transformer can process. The model is conditioned on a text prompt (and optionally\nstarting frames or images) and learns to predict the next frames of video that match the prompt. With enough training\n(using millions of videos and images), such a model can generate remarkably diverse and complex scenes.\nKey architectural innovations have improved video generation quality recently. One is the use ofâ€¯unified training on\nimages and videosâ€¯â€“ since images are essentially 1-frame videos, including large image datasets helps the model learn\nhigher resolution details. Sora, for instance, is a â€œgeneralistâ€ trained on both images and videos of variable lengths,\nwhich enables it to output higher fidelity and longer durations (up to ~1 minute internally). Another advance is the\nincorporation ofâ€¯spatial-temporal attentionâ€¯mechanisms (as seen in Googleâ€™s and othersâ€™ models) that help preserve\nconsistency over time. Google DeepMindâ€™s latestâ€¯Veo 3â€¯model reportedly usesâ€¯3D spatiotemporal attentionâ€¯and a\nspecializedâ€¯3D VAEâ€¯(Variational Autoencoder) to achieve cinema-quality results. In practice, many systems break the task\ninto two stages: first generate a lower-resolution or lower-frame-rate video with AI, thenâ€¯upscale or interpolateâ€¯it to\nHD and smooth frame rates (some platforms include built-in AI upscalers and frame interpolation to reach 1080p30 or\nbeyond).\nDespite these advances, current models have constraints. Mostâ€¯clip lengthsâ€¯are short â€“ e.g. 4â€“5 seconds for many public\ntools, sometimes extendable to ~20 seconds with higher tiers or chaining clips. Maintainingâ€¯object permanence and\ncharacter consistencyâ€¯across even those seconds is non-trivial (e.g. a personâ€™s face might subtly change between frames\nif not controlled). Each frame must not only look plausible on its own but also flow logically from the previous,\nrespecting physics and causality. This is an area where models still stumble: early-gen systems producedâ€¯warping limbs\nand melting objectsâ€¯under motion. Newer models are improving â€“ for example, Minimaxâ€™s video model was noted to handle\nmotion physics (inertia, shadows, momentum) far more naturally than earlier models. Still, complex sequences (e.g. a\nhuman performing a sports move) can appear unnatural or â€œroboticâ€ if the AIâ€™s understanding of physics isnâ€™t perfect.\nResolutionâ€¯andâ€¯frame rateâ€¯are other technical factors. Producing full 1080p or 4K frames is computationally heavy, so\nmany models trade off length or fidelity. Runwayâ€™s Gen-2 model, for instance, initially generated only ~480p-equivalent,\nlow-framerate videos (appearing â€œslideshow-likeâ€) to conserve compute. Today there are models that directly output HD â€“\nOpenAIâ€™s Sora Turbo can do 1080pâ€¯and Adobeâ€™s Firefly Video generates 1080p by defaultâ€¯â€“ but usually for very short clips\n(~5â€“10 seconds). Some platforms provideâ€¯flexible aspect ratios and orientationsâ€¯(portrait, square, etc.), recognizing\nneeds for social media content.\nImportantly, AI video generators are increasingly handlingâ€¯multimodal inputs: not just plain text prompts, but\nalsoâ€¯image inputs (for image-to-video)â€¯andâ€¯initial video clips (video-to-video). Image-to-video means you can give a\nsingle frame or picture and have the model animate it or use it as the scene start. Many tools (Runway, Pika, Sora,\nLuma, Adobe, etc.) support this because it helps with consistency and user control â€“ e.g. you can provide a character\nimage to ensure the generated video revolves around that characterâ€™s appearance. Video-to-video allows taking an\nexisting video and transforming it (changing style or continuing it beyond original length). For example, Runway Gen-1\nwould take an input video and apply an AI-generatedâ€¯â€œstyle transferâ€â€¯to turn real footage into an animated look. Several\nplatforms now let youâ€¯extend videosâ€¯by generating new frames after the last frame (both Sora and Googleâ€™s Flow have a\nfeature toâ€¯â€œextendâ€â€¯orâ€¯â€œcontinueâ€â€¯a clip seamlessly). These workflow features open the door toâ€¯storyboard-driven\ngeneration: creators can stitch multiple AI clips into a longer sequence, maintaining some continuity by carrying over\nlast frames as the next clipâ€™s start.\nAnother frontier isâ€¯audio integration. Until recently, AI video tools have only produced silent clips â€“ adding sound was\na separate task. Googleâ€™s Veo 3 made headlines by generating synchronized audio along with the video. This includes\nsound effects and even rudimentary voices or music that match the scene, greatly enhancing realism and emotional impact.\nWhile not yet common across all platforms, this indicates a trend toward full multimedia generation (Google achieved\nthis by drawing on its text-to-audio research and perhaps its new Gemini multimodal AI to understand the scene context).\nAdobeâ€™s approach, meanwhile, integrates anâ€¯AI voice model for dubbing/translationâ€¯rather than generating sound effects\nfor imaginary scenesâ€“ an example of focusing on practical editing needs like translating a videoâ€™s narration into\nmultiple languages with matching lip sync. We anticipate more convergence ofâ€¯AI video + audio, enabling one-shot\ncreation of a complete video with soundtrack.\nFinally,â€¯content moderation and safetyâ€¯are built into the technology stack of responsible providers. Providers like\nOpenAI and Google constrain their models to avoid disallowed content (e.g. extreme violence, pornographic or hate\ncontent), and they addâ€¯watermarks or metadataâ€¯to identify AI-generated videos. OpenAIâ€™s Sora, for example, attaches C2PA\nmetadata tags to all generated videos for transparency. Adobe trains Firefly on licensed, uncontroversial content to\nmake it â€œcommercially safeâ€ and avoid IP violations. These safeguards are critical given how easily video can mislead;\nthey are also important for adoption. For example, Lilly would have to ensure any AI-generated visuals do not\ninadvertently create off-label claims or inappropriate imagery. Additional layers of content moderation/control/filters\nwill remain a Lilly-specific need.\nIn summary, theâ€¯key capabilitiesâ€¯of AI video generation to be aware of include multiple dimensions to be evaluated as we\napproach new models and tools, including but not limited to:\nquality of visual output (resolution, frame rate, fidelity)\nability to portrayâ€¯realistic physics and lighting\noptions forâ€¯camera movement/angles\ndegree ofâ€¯controlâ€¯via prompts and references (storyboards, keyframes, etc.)\nsupport forâ€¯audioâ€¯output and moderationâ€¯features\nMajor Players and Platforms in AI Video Generation\nâ€‹\nThe AI video generation ecosystem in 2025 spans Big Tech companies, well-funded startups, and open-source projects.\nHaving surveyed these major players â€“ OpenAI, Runway, Google, Kuaishou (Kling), Pika, MiniMax, Luma, Stability, Adobe â€“\nitâ€™s evident that the field is crowded with innovation.\nEach brings something slightly different: OpenAI and Google push the frontier of capability, Adobe and Luma focus on\ncreator workflow integration, startups like Pika and MiniMax carve out specialized strengths or faster iteration, and\nopen projects like Stability ensure thereâ€™s room for custom solutions.\nBelow we break down the notable models/tools and their current state-of-the-art capabilities, as well as how they\ndiffer:\nOpenAI â€“â€¯Sora\nâ€‹\nOpenAIâ€™sâ€¯Soraâ€¯is a cutting-edgeâ€¯text-to-videoâ€¯model introduced in early 2024 and moved out of research preview by the\nend of last year. It is designed as a general-purpose video generator that can take text (and optionally image/video\ninputs) and produce new videos. Sora operates as a consumer product via a web interface (ChaGPT.com and Sora.com) and\nthrough API integration with OpenAI and the Microsoft Azure platform. The latter of which, seems to be the most likely\navenue for Lillyâ€™s adoption.\nSora was initially capable of up toâ€¯1 minuteâ€¯of video in research settings, but for end-users the service currently\nallows generating clips up toâ€¯20 secondsâ€¯long at up toâ€¯1080pâ€¯resolution. Users can choose aspect ratios (widescreen,\nsquare, vertical) and evenâ€¯bring their own assetsâ€¯â€“ e.g. provide an initial or final frame, or a short video to\nbeâ€¯extended or blendedâ€¯into new content. OpenAI has emphasizedâ€¯Soraâ€™sâ€¯prompt adherence and visual quality, aiming to\nclosely match the input description while keeping outputs photorealistic or stylistically high-quality. The model\nleverages aâ€¯diffusion transformerâ€¯architecture and was trained on an extremely large dataset of image and video content,\nmaking it a â€œgeneralistâ€ that can handle everything from cartoon-like clips to natural scenery.\nOne of Soraâ€™s distinguishing features is a built-inâ€¯storyboard toolâ€¯in its interface. This allows users to script out a\nsequence of scenes or specify key frames, giving more control over each moment in a longer narrative. For example, a\nuser could specify that at second 0 a character is in Location A, and by second 5 the character moves to Location B, and\nSora will try to fill in the transition. Such tools help mitigate the â€œconsistencyâ€ problem by explicitly guiding the\nmodel. Sora also supportsâ€¯image-to-videoâ€¯(you can upload an image to influence the first frame or style)\nandâ€¯video-to-videoâ€¯(upload a short clip to have Sora modify or continue it) generation, making it flexible in creative\nworkflows.\nStrengths:\nSora is backed by OpenAIâ€™s leading research, so it benefits from state-of-the-art training and\nfine-tuning. It produces impressivelyâ€¯detailed and vivid visualsâ€¯across a range of subjects â€“ reviewers have showcased\nexample prompts from cinematic city scenes to fantasy creatures, rendered with convincing lighting and depth of field.\nIt can maintain coherence for longer durations than many competitors (20s or more), and prompt alignment is generally\nstrong. The integration with ChatGPT means a large user base can experiment easily, and OpenAI has put in place\nrobustâ€¯content moderationâ€¯andâ€¯origin taggingâ€¯(every Sora video is cryptographically tagged as AI-generated). OpenAIâ€™s\nbrand has also lead the GenAI field and continues to innovation across a variety of AI models and advanced capabilities.\nAdditionally, OpenAIâ€™s partnership with Microsoft makes it the most viable option for large enterprises, like Lilly.\nWeaknesses:\nDespite its prowess, Sora (like others) still has limitations. OpenAI itself notes that the model can\nstruggle withâ€¯realistic physics and long complex actionsâ€¯â€“ e.g. if a prompt involves a very intricate movement sequence,\nSora often introduces artifacts or odd motion. Early testers observed that human figures from Sora could have a stiff or\nunnatural motion compared to real video. Also, because of the computational cost, generating 20 seconds of HD video\nisâ€¯resource intensive.â€¯Theâ€¯cost structureâ€¯for heavy use is thus non-trivial; Lilly may want to negotiate custom pricing.\nAnother consideration is that Sora is aâ€¯closed modelâ€¯â€“ unlike some open-source alternatives, you cannot self-host or\nretrain it on proprietary data, so there is reliance on OpenAIâ€™s innovation focus and reseller partner, Microsoft.\nOverall, Sora represents theâ€¯high-quality, generalist approachâ€¯to AI video: it aims to handle as many scenarios as\npossible with good quality and provide a safe, managed platform for users. Itâ€™s a bellwether for where mainstream AI\nvideo tech is, demonstrating realistic scene generation and moderate length, with expectation of rapid improvement as\nmodels scale further.\nRunway â€“â€¯Gen-2â€¯(and beyond)\nâ€‹\nRunway ML is a startup well-known among creatives for providing accessible AI tools, and they were one of the first to\nofferâ€¯text-to-videoâ€¯generation to the public. Runwayâ€™sâ€¯Gen-2â€¯model (launched mid-2023) garnered attention as a\ncommercially available text-to-video system available via a simple web interface. It followed their earlierâ€¯Gen-1, which\nwas limited to video stylization (applying AI-generated styles to existing videos). Gen-2 marked the ability to generate\ncompletely new video frames from a text prompt or an image prompt. Users could sign up on Runwayâ€™s website and generate\nshort clips (initially ~4 seconds long) for free or with a subscription. This early availability made Runway Gen-2 a\npopular choice for designers and artists to experiment with AI video in 2023.\nGen-2 outputs a few seconds of video atâ€¯low resolutionâ€¯(around 720Ã—1280 or less) and without sound. It supportsâ€¯text\npromptsâ€¯as well as anâ€¯image + text comboâ€¯â€“ for example, you can upload a reference photo and ask it to animate that\nscene with some described action. The generation process is cloud-based and quick (often under a minute to get results).\nRunwayâ€™s interface emphasizes ease of use: you enter a prompt, hit generate, and get a preview you can download as an\nMP4. Gen-2 can produce someâ€¯artistic and creative results, but it has notable limitations. Reviewers found that Gen-2â€™s\nvideos were oftenâ€¯choppy in frame rateâ€¯(almost like a fast slideshow) andâ€¯grainyâ€¯in appearance. This appears to have\nbeen a trade-off to make the service widely accessible (lowering GPU requirements by limiting quality). Gen-2 also\nstruggled withâ€¯complex promptsâ€¯â€“ it might latch onto one word and ignore nuance, and it failed at precise instructions\nlike â€œa slow zoom-inâ€ (the output didnâ€™t execute the camera move). Consistency issues were present: objects could warp,\nand human figures often had surreal artifacts (blended limbs, etc.).\nRunway has been continuously improving its models. By late 2023, they were experimenting with features to increase\ncoherence (some community members referred to a Gen-3 or new features like â€œlast frame continuityâ€ â€“ allowing one\ngenerationâ€™s last frame to carry into the next, somewhat like what others call extend or storyboard). Itâ€™s reported\nthatâ€¯Runway Gen-4â€¯is on the horizon, aiming for higher quality and longer duration, but as of early 2025 theâ€¯Gen-2â€¯model\n(with incremental upgrades) is whatâ€™s publicly available. Runwayâ€™s comparative advantage is beingâ€¯creator-focused: their\nplatform is an all-in-one creative suite, with tools for video editing, composting, and AI effects. For instance, Runway\noffersâ€¯green screen background removalâ€¯andâ€¯image generationâ€¯in the same app, so a user can mix AI video with other\nediting easily. Theâ€¯priceâ€¯of Runway is subscription-based, with a free tier that gives some generation credits and paid\ntiers for more usage.\nRunway shows theâ€¯accessibilityâ€¯side of AI video. It may not have the absolute best fidelity, but it pioneered getting\nthe tech into usersâ€™ hands. This has allowed a community of artists to develop around it (for example, the first AI\nmusic videos were often made with Runway Gen-2, accepting the rough edges as an aesthetic). Runwayâ€™s trajectory suggests\nthey will continue to close the quality gap. Notably, Runway collaborated in the development of Stable Diffusion (image\nmodel) previously, indicating strong AI research chops. We anticipate Runwayâ€™s future models will improve frame\nsmoothness and possibly length (their roadmap mentions working towards 15+ second clips). Businesses might use Runway\ncurrently forâ€¯quick prototypes or social media contentâ€¯where a slightly stylized or surreal look is acceptable. However,\nfor polished, photoreal marketing videos, other solutions (or further model maturity) would be needed.\nGoogle â€“â€¯Veo 3â€¯andâ€¯Flow\nâ€‹\nGoogle entered the AI video arena through its DeepMind team, and in May 2025 it announcedâ€¯Veo 3, a next-generation video\nmodel, along with a tool calledâ€¯Flowâ€¯for creators. Veo 3 represents one of the most advanced text-to-video models to\ndate, notable for deliveringâ€¯high realism and integrated audio. In tests, Veo 3 generated videos ofâ€¯realistic human\nactors with synchronized sound and music, a leap in overall cinematic quality. For example, an Ars Technica report\ndescribes Veo 3 producing a clip of a person that was startlingly close to real footage, complete with background music\nthat fit the mood. This puts Veo 3 at the cutting edge, potentially ahead of OpenAIâ€™s and othersâ€™ models in certain\naspects as of mid-2025.\nVeo 3 can generate HD video content with accompanying audio in response to a prompt. It excels atâ€¯text rendering within\nvideoâ€¯(e.g. generating legible signage or captions in the scene â€“ something many image AIs fail at); testers found Veo 3\nmore consistently renders written text correctly in frames compared to others. The audio component means if your prompt\nimplies a sound (â€œa dog barking in a parkâ€), the output video might have a barking sound. This dramatically increases\nthe emotional impact and usefulness of the raw generations. Veo 3â€™s visual fidelity is extremely high:â€¯natural motion,\nbetter handling of complex scenes (e.g. multiple characters or dynamic camera movements) and fewer obvious glitches have\nbeen reported. It uses heavy-duty AI (likely an evolution of Googleâ€™sâ€¯Imagen Videoâ€¯research combined with their Gemini\nmultimodal model) and thus is computationally demanding â€“ one test noted ~5 minutes or more to generate a clip with\nGoogleâ€™s servers.\nFlow\nâ€‹\nTo make Veo 3 accessible, Google launchedâ€¯Flow, an AI filmmaking web app. Flow is essentially the user interface and\nworkflow around Veo (and other models like Googleâ€™s Imagen image model). Itâ€™s available to Googleâ€™s AI Cloud subscribers\n(Pro and Ultra tiers) as a beta tool. Flow is built â€œwith and for creativesâ€ and introduces robust storytelling\nfeatures: you can generate individualâ€¯shotsâ€¯usingâ€¯Text to Video,â€¯Frames to Video, orâ€¯Ingredients to Video.â€¯Frames to\nVideoâ€¯lets you specify aâ€¯start and end frameâ€¯(essentially keyframes) and the AI will animate the transition.â€¯Ingredients\nto Videoâ€¯means you can feed the AI specific elements â€“ e.g. an image of a character or an art style â€“ and it will ensure\nthe output video includes those elements. This modular approach helps keep characters consistent and styles unified\nacross scenes. Flow also has aâ€¯Scene Builder: once you have a clip you like, you can add it to Scene Builder and then\neither â€œExtendâ€ the scene (Flow will use the last second of frames to continue the action into a new clip) or â€œJump toâ€\na new scene (preserving context like the main character, but allowing a change of setting or action). This approach\nleverages Googleâ€™s powerful Gemini AI behind the scenes to maintain coherence when transitioning shots. In effect, Flow\nallows the creation of a multi-shot short film completely within the AI tool, something quite revolutionary. When\nfinished, users can upscale their clips to 1080p and download them or even get GIF previews.\nStrengths:\nGoogleâ€™s solution is arguably the mostâ€¯comprehensiveâ€¯and advanced: highest quality visuals withâ€¯audio,\nand a pro-oriented workflow that tackles many prior limitations (short length, lack of consistency, etc.). The\nintegrated audio is a standout â€“ in comparisons, testers found scenes from Veo 3 much more engaging due to background\nsounds and music, even when Kling (a competitor) had slightly better prompt accuracy in some cases. Veo 3 also shows\nstrength inâ€¯cinematography: it tends to choose cinematic camera angles and color grading that enhance the promptâ€™s mood.\nThis may be due to training on film footage or specific tuning. Googleâ€™s cloud infrastructure means it can roll this out\nat scale when ready (currently itâ€™s limited to certain users but expected to expand). Google will likely integrate these\ncapabilities into Google Cloud offerings (GCP), making it easier to use via API or within Googleâ€™s productivity tools in\nthe future.\nWeaknesses:\nThe main drawbacks areâ€¯accessibility and cost. As of now, Flow (with Veo 3) is not open to everyone â€“\nitâ€™s an exclusive service for paid subscribers, likely with limited invites. The compute intensity also means itâ€™s slow\n(minutes per generation) and presumably expensive; Googleâ€™s tiered credits system in Flow indicates heavy usage could\nrack up costs (Flow has â€œFastâ€ and â€œHighest Qualityâ€ modes, with Highest using Veo 3 and consuming more credits). In\ntesting, system congestion led to errors for Veo 3 generation, implying the service can be bottlenecked. Another\nconsideration is that while Veo 3 pushes realism, itâ€™s not infallible â€“ some prompts found itsâ€¯prompt adherenceâ€¯weaker\nthan a rival in specific aspects. For example, in a test prompt of â€œa woman runs away from a giant spider,â€ Klingâ€™s\nmodel actually showed the woman running away from the spider (as instructed) while Veo 3â€™s output had the elements but\nnot the exact action. So creative control might still need multiple attempts or careful prompt tuning. Also, if using\nFlowâ€™s image-to-video via â€œIngredientsâ€ mode, note that those videos apparently comeâ€¯without audioâ€¯(so the audio\ngeneration only occurs in pure text-to-video mode, at least currently).\nGoogleâ€™s entry with Veo 3 is a sign thatâ€¯AI video is reaching new heights. Lilly should watch this space because what is\na limited release today could be a widely available tool in a year. If Google integrates Flow into its ecosystem it\ncould accelerate adoption dramatically. In terms of strategy, Googleâ€™s heavy emphasis onâ€¯storytelling workflowâ€¯(e.g. the\nScene Builder and keyframes) suggests that even as AI gets more powerful,â€¯human creative direction remains crucialâ€¯â€“ the\nwinners will be those who combine AI capabilities with clear narrative guidance.\nKuaishou â€“â€¯Kling\nâ€‹\nKling is an AI video generator developed byâ€¯Kuaishou, a major Chinese social media and short-video company (often seen\nas a rival to ByteDance). Kuaishou launched Kling in 2024 and has rapidly iterated on it; by mid-2025 they\nreleasedâ€¯Kling 2.1, explicitly targeting the leadership in AI video generation. Kling has gained a reputation for\nproducingâ€¯cinema-grade short videosâ€¯and is widely used in China for creating meme videos, animated satires, and more. In\nfact, Kling became popular among meme-makers to animate political satire videos of public figures (e.g. face-swapping\nfamous people into movie scenes). This hints at Klingâ€™s strengths in generating reasonablyâ€¯realistic human\ncharactersâ€¯and its permissiveness (though presumably with some safeguards).\nKling stands out for allowing relativelyâ€¯long outputs â€“ up to 2 minutesâ€¯of video in its latest version, far surpassing\nmost Western counterparts currently. It also supportsâ€¯high resolution (1080p)â€¯generation and smooth frame rates (25+\nFPS). To handle different use cases, Kling 2.1 is offered in multiple quality tiers. According to a review, there\nareâ€¯Standard,â€¯Professional, andâ€¯Masterâ€¯modes: Standard produces 720p ~5-second clips, Pro yields 1080p with good\nquality, and Master (the most advanced) also at 1080p but with maximum fidelity and consistency. The Master tier uses\nthe latest model with those 3D spatial-temporal techniques we mentioned, and it is slower and costlier (100 credits for\n5s, vs 35 credits for Pro). Notably,â€¯prompt adherenceâ€¯in Kling 2.1 is reported to be excellent â€“ it captures complex\ninstructions well and keeps on-script better than many models. For instance, if asked to show specific text on a robotâ€™s\nbody and a certain action, Kling usually gets the details right (especially in Master mode). It does have some\ndifficulty if the text or element is not the main focus (it might drop minor details when focusing on bigger things),\nbut overall itsâ€¯accuracy to the prompt is a selling point.\nKling offers a rich set of features akin to others:â€¯text-to-video, image-to-video, video extension, camera control, even\nface and lip-sync capabilities. The platform lists tools likeâ€¯Motion Brushâ€¯(perhaps to guide motion in a specific\nregion),â€¯Start/End Frameâ€¯input (like keyframes),â€¯Virtual Try-Onâ€¯(indicating a specialization in changing outfits on\npeople in video), andâ€¯Lip Syncâ€¯which suggests you can input audio dialogue and have a generated character speak it. This\naligns with Kuaishouâ€™s needs for user-generated content (imagine an app where users can type a script and have an AI\navatar video speak it). Kling also provides anâ€¯API, meaning developers can integrate it into apps or pipelines. The\nvolume of content generated is huge â€“ by one report, Kling had already output over 10 million videos within a short time\nof launch, reflecting its popularity on a large social platform.\nStrengths:\nKlingâ€™sâ€¯natural motion and authenticityâ€¯have been highlighted. A Decrypt comparison noted Kling 2.1\nproduces footage that looks genuinely cinematic, with characters moving naturally and complex action sequences playing\nout without obvious AI artifacts. Emotions on faces feel more authentic than prior models in some tests. Also,\nKlingâ€™sâ€¯fast pace of improvementâ€¯is notable: within one year, Kuaishou jumped from version 1.0 to 2.1, each time\ntightening the gap with the cutting edge. The multi-tier offering is smart for business use â€“ one can use cheaper modes\nfor drafts and then upscale final output with Master mode. For an enterprise, Kling could be cost-effective especially\nfor image-to-video tasks; in fact, tests found Kling 2.1â€¯excels at image-to-video conversionâ€¯(turning a static image\ninto a moving scene) relative to peers. Itsâ€¯camera movements and VFXâ€¯are also advanced; for example, it can handle\ndynamic shots (one test had it do a time-lapse city transformation with camera movement â€“ challenging, but it partially\nsucceeded). The presence of specialized effects (like presumably theâ€¯Elementsâ€¯orâ€¯VFX presetsâ€¯on their site) means it\nmight offer fun creative filters natively.\nWeaknesses:\nOne limitation isâ€¯accessibility for non-Chinese users. Klingâ€™s official interface is primarily in\nChinese (though Pollo AI, a third-party site, provides an English gateway to try Kling models). Payment and support\nmight be geared towards Chinese market, so international enterprise adoption could be tricky at the moment. Another\nfactor: while Kling leads in some metrics, Googleâ€™s Veo 3 has the edge in integrated audio and possibly in overall\nâ€œatmosphereâ€ (color grading, etc.). In a direct face-off, each had scenarios where it did better: Kling was better at\nstrictly following the action described, whereas Veoâ€™s clip with audio delivered a more cinematic feel even if it\ndeviated slightly. Also, Klingâ€™s Master mode, though high-quality, isâ€¯compute-heavyâ€¯â€“ some users might find the wait\ntimes (and costs) for the highest quality prohibitive for very iterative work. Lastly, as an AI that can do\ndeepfake-like outputs (given meme usage and lip-sync), one must ensure ethical use. Itâ€™s likely Kuaishou has moderation\non their platform, but if one is using the API, compliance with any deepfake laws (like requiring disclaimers for\nAI-generated realistic people) is necessary.\nIn summary, Kling is a powerhouse in AI video and a direct competitor to Sora and Veo. For our global company, Kling\ndemonstrates thatâ€¯the Chinese market has an opportunity to leverage models not accessible to all geographies. Minimally,\nitâ€™s a model to benchmark against. Lilly should keep an eye on whether Kuaishou or partners make Kling available\nglobally, and on the kinds of content it enables (e.g. itâ€™s very adept at producingâ€¯short-form, eye-catching clipsâ€¯which\nis exactly the content driving social media engagement).\nPika Labs â€“â€¯Pika Video\nâ€‹\nPika Labsâ€¯is a startup that has quickly risen in the AI video field. Co-founded by two PhD students in 2023, Pika Labs\nmade waves by mid-2024 when it secured aâ€¯US$135 million funding round, valuing the company at $470M.â€¯This substantial\nbacking (notably involving investors from both the US and Asia) signals that Pika is viewed as a serious contender.\nPikaâ€™s AI video generator â€“ often just calledâ€¯Pika AIâ€¯â€“ is known for beingâ€¯user-friendlyâ€¯and creatively versatile. Itâ€™s\noffered via a web interface and API, and has even been integrated into third-party creative apps (e.g. theâ€¯Captionsâ€¯app\nintegrated Pika to allow users to generate b-roll clips from text inside a video editing workflow).\nPika supports bothâ€¯text-to-video and image-to-video, similar to others. It can generate approximatelyâ€¯5-second clips by\ndefault, and the team has been extending that (the platform notes up to ~2 minutes by chaining or iterating, likely in\ntheir newer version 1.5+). The resolution it works with is aroundâ€¯720p (1296Ã—720)â€¯in current versions for generation.\nPikaâ€™s style flexibility is a strong suit â€“ it can doâ€¯â€œvarious styles, such as cinematic, animated, or cartoonishâ€â€¯just\nby adjusting the prompt or selecting presets. They have been updating the model rapidly (the Pollo AI interface shows\nPika v2.2 as latest, implying several version jumps). One unique feature Pika introduced isâ€¯â€œPika Effectsâ€\n(dubbedâ€¯Pikaffects). These are essentially AI-drivenâ€¯video effects that manipulate objectsâ€¯in the generated scene. For\nexample, users can apply effects likeâ€¯Inflate, Melt, Explode, Squash, Crush,â€¯or evenâ€¯â€œCake-ifyâ€â€¯to objects. This\nsuggests if you generate a video of, say, a statue, you could then have it melt or explode in a physically plausible way\nvia the AI â€“ an exciting tool for dynamic visuals and likely very popular for fun content creation. This focus\nonâ€¯object-level controlâ€¯is somewhat unique to Pika and caters to short-form video creativity (imagine TikTok-style\nvisual gags, etc.).\nPikaâ€™s interface is noted to beâ€¯intuitive for newcomers. They likely have templates or guided workflows (enter your\nprompt, choose a style, etc.). Also, Pika Labs has emphasizedâ€¯continuityâ€¯features: much like others, you can use the\nlast frame of one video as the first of the next to string together longer scenes with a persistent character. A\ncommunity of users have tested Pikaâ€™s capability on things like B-roll (it does well in generating generic footage like\ntech product pans), cartoon animation (smooth results), and abstract visuals.\nStrengths:\nPika appears to combineâ€¯research-grade tech with a product mindset.â€¯Raising $135M so early means they\nhave resources to push the modelâ€™s quality and scale quickly. Already by June 2024, Pikaâ€™s results were impressing many\nâ€“ it producesâ€¯smooth camera movements and lightingâ€¯effects, and in some comparisons was among the top for certain tests.\nFor instance, a Medium reviewer noted Minimax and Pika as two standout platforms that â€œactually workâ€ for creators\nneeding reliable output. Pikaâ€™s multi-style capability means marketers or creators can use it for a range of content: a\nrealistic stock-footage-like clip for one project, a whimsical cartoon for another. Theâ€¯Pikaffectsâ€¯give an extra\ndimension of creativity, allowing users to generate not just static scenes but scenes with anâ€¯effect or\ntransformationâ€¯(great for grabbing viewer attention). Pika also supportsâ€¯API integration, which means businesses can\nbuild it into their own tools or pipelines (for example, an e-learning platform could call Pikaâ€™s API to generate custom\nvideo illustrations on the fly for course content). Finally, Pikaâ€™s background â€“ co-founders with strong academic\ncredentials (one profile mentions a co-founder, Wenjing Guo, is a Harvard CS grad lauded as a â€œgenius girlâ€ in China) â€“\nsuggests a talent advantage and possibly cross-collaboration between US and Chinese AI communities.\nWeaknesses:\nWhile Pika is promising, itâ€™s still anâ€¯upstartâ€¯in a field with emerging giants. Its model may not yet\nmatch the absolute fidelity of OpenAI or Googleâ€™s latest â€“ for example, resolution being 720p vs others pushing 1080p,\nand itâ€™s unclear if Pika has audio generation (likely not at the moment; most mention is about visuals). Some features\nin Pika are marked as â€œv1â€ or â€œv1.5â€ in comparison tables, indicating they are in active development (e.g. perhaps\nmotion brush or mid-frame control is not fully there yet). Also, as a smaller company, Pika might not have as extensive\ncontent filtering or enterprise support as OpenAI, Adobe, and Google, so Lilly would need to vet its outputs carefully\nfor IP or moderation issues. The competitive landscape is another challenge â€“ multiple competitors means Pika needs to\ndifferentiate; it seems to do so with effects and ease-of-use, but others can emulate those too. Pikaâ€™s strength in\nâ€œvarious stylesâ€ could also mean it might not yet dominate any one style in quality (jack of all trades risk).\nFor Lilly, Pika Labs is a startup to watch or possibly partner/invest/acquire. Its agility means new features (like\nthose quirky video effects) come out quickly, which could be leveraged for marketing novelty. For instance, our creative\nteams might use Pika to generate an attention-grabbing visual clip (imagine a pill that â€œexplodesâ€ into confetti to\nsymbolize treatment success â€“ an effect Pika might handle well). With its sizable funding, Pika could also become a\ntakeover target or major player globally. The key will be how they scale their service and continue improving model\nperformance.\nMiniMax â€“â€¯Hailuo Video-01\nâ€‹\nMiniMaxâ€¯is a Chinese startup (behind the model namedâ€¯Hailuo Video-01) that has also made a name for itself among AI\nvideo creators. Often just referred to as â€œMiniMax video generator,â€ it gained recognition in late 2023 for delivering\nimpressivelyâ€¯realistic motion continuityâ€¯at 720p resolution. MiniMaxâ€™s approach appears to prioritize the fundamental\nchallenges of video generation: physics, continuity, and camera work.\nHailuo Video-01, as offered by MiniMax, generatesâ€¯high-definition (720p) videos at 25 fpsâ€¯and supports\nbothâ€¯text-to-video and image-to-videoâ€¯modes. A typical output is ~5 seconds long, but users have successfully stitched\nclips to create longer sequences (one user made a 30-second video by concatenating 5s segments, with seamless flow).\nMiniMax particularly shines withâ€¯cinematic camera movementsâ€¯â€“ reviewers were impressed by how well it handled complex\npans and drone-like aerial shots while keeping the scene coherent. For example, flying over a beach then tilting toward\na waterfall in one continuous motion was executed smoothly. The model also excelled atâ€¯B-roll style footage: generating\na professional-looking slow-motion laptop product shot with correct focus and lighting transitions. This indicates\nstrong understanding of focus depth, blur, and other cinematographic details.\nIn terms of content, MiniMax did very well withâ€¯cartoon animations and abstract visuals. It produces smooth,\nhigh-quality cartoon motion, making it great for creators of animated shorts. And for abstract or surreal scenes, it\nmaintained immersive and stunning visuals. When it comes to continuity, MiniMax has a feature where you can take the\nlast frame of a clip and feed it as the first frame of the next generation â€“ this allowed that 30-second continuous\nvideo with a character, where the character stayed consistent throughout. That consistency is a major plus; the user\nessentially stiched longer stories by manual continuity, and the tool worked nicely.\nStrengths:\nMiniMaxâ€™s biggest strength isâ€¯realistic motion physics. Movements in its videos look real and believable\nâ€“ the AI seems to have a better grasp of inertia and momentum, avoiding the jerky or floaty feel others sometimes have.\nIn side-by-side tests, motions that looked robotic in Sora were more lifelike in MiniMax, suggesting their model\narchitecture may explicitly model physics or was trained on plenty of real video. The continuity (the ability to chain\nclips) is another strength, enabling content creators to tell longer stories if needed. Also, MiniMaxâ€™s output quality\nfor its resolution is high â€“ 720p might sound lower than 1080p, but given many AI videos are still in that ballpark,\nMiniMaxâ€™s effective quality is top-tier within that. It is also reportedlyâ€¯free or low-cost to tryâ€¯(it was available on\nplatforms like Anakin.ai for users to experiment), which has helped it gain traction among the community.\nWeaknesses:\nMiniMax isnâ€™t as globally known as some others, and documentation in English is sparse.\nItsâ€¯image-to-video with humansâ€¯was noted as a weak spot â€“ when given a real human photo to animate, the results were not\nvery convincing (unnatural transitions, lack of detail). It performed better withâ€¯cartoonish imagesâ€¯than real human\nphotos. So for tasks like animating a real personâ€™s photo, it may not be the best (other specialized avatar tools or\nSynthesia might do better). Also, MiniMax had difficulty withâ€¯very complex movement scenariosâ€¯â€“ e.g. a football player\nkicking a ball, where precise interaction is needed, looked clunky. This suggests limits in handling intricate\nmulti-object physics (ball interacting with foot etc.). Another limitation is that, at least as of late 2024, MiniMax\noutputs were capped to short durations and required manual effort to chain longer videos â€“ it doesnâ€™t inherently\ngenerate a 30s story in one go; you have to curate it scene by scene.\nFor Lilly, MiniMaxâ€™s existence is proof that there are multiple players beyond the most hyped. If we want to focus on\nexperimenting with as many options as possible, MiniMax would be a good model to assess for scenarios needingâ€¯believable\nmovementâ€¯(say you want an AI video of a pill bottle gently rotating and tilting â€“ physics heavy â€“ MiniMax might nail\nthat). However, since itâ€™s less of a full product and more of a model accessible via certain interfaces or APIs\n(Segmind, Replicate, etc.), leveraging it requires engineering or technical integrations if not working with a provider\nthat offers the model as part of a service. Itâ€™s one of the â€œbehind the scenesâ€ engines that some AI video platforms\ncould incorporate for their motion strengths.\nLuma Labs â€“â€¯Dream Machine (Ray 2)\nâ€‹\nLuma AIâ€¯is known for its 3D scanning and NeRF (neural radiance field) technology, which allows creating 3D models from\nphone captures. In 2023, Luma expanded into generative AI with itsâ€¯Dream Machineâ€¯app, aiming to be a â€œvisual thought\npartnerâ€ for both images and videos. Lumaâ€™s Dream Machine is unique in that it blends 2D and 3D AI capabilities: it\nincludes an image generation model (Photon) and a video model (Ray 2) working in tandem. The focus is on giving\ncreatorsâ€¯fluid controlâ€¯andâ€¯fast iterationâ€¯in making spectacular visuals without needing prompt engineering skills.\nDream Machine allows you to generate bothâ€¯images and videosâ€¯and move seamlessly between them. Notably, you can\nuseâ€¯natural languageâ€¯to not only create but alsoâ€¯editâ€¯andâ€¯refineâ€¯outputs (e.g. â€œmake it a 90s vibeâ€ or â€œadd a fisheye\nlens effectâ€ to modify an image/video). For videos, Lumaâ€™s Ray 2 model is designed forâ€¯fast, coherent motion and\nultra-realistic details. Dream Machine includes robust features forâ€¯controlling outputs: you canâ€¯â€œDirect the perfect\nshot with start/end framesâ€â€¯â€“ meaning keyframe control similar to Adobeâ€™s and Googleâ€™s approach. You can alsoâ€¯â€œloopâ€â€¯a\nvideo seamlessly if desired. It emphasizes ability to createâ€¯unique, consistent characters from a single imageâ€¯â€“ so you\nprovide an image of a character and the AI can generate new scenes with that character consistently present. Thereâ€™s\nalso support forâ€¯visual style references: you can upload or select reference images for style or specific elements,\nguiding the generation.\nDream Machineâ€™s interface touts features likeâ€¯Brainstormâ€¯(the AI suggests ideas or variations if youâ€™re not sure where\nto go next), andâ€¯Share & Remixâ€¯which encourages a community aspect (users can share their creations and even the â€œbehind\nthe scenesâ€ prompt+refs so others can iterate on them). Under the hood,â€¯Photonâ€¯(image model) helps ensure any single\nframe or detail is high-quality, whileâ€¯Ray 2â€¯(video model) handles the temporal aspect â€“ this dual approach likely\ncontributes to theâ€¯sharp, detailed frames and accurate text renderingâ€¯that Luma claims (they specifically listâ€¯â€œsharp\nand accurate text renderingâ€â€¯as a feature, implying their model can place legible text in video). They also\nhighlightâ€¯â€œaccurate lighting and physicsâ€â€¯andâ€¯â€œclean and consistent animation styleâ€, indicating the model was trained\nto respect physical realism and maintain a coherent style throughout a clip.\nStrengths:\nLumaâ€™s Dream Machine is praised for itsâ€¯polished user experience. Tomâ€™s Guide in late 2024 called it â€œone\nof the best interfacesâ€ among AI video platforms. This ease and fluidity could reduce the learning curve for non-AI\nexperts. Theâ€¯range of controlâ€¯is also a key strength: Dream Machine basically offers every control feature weâ€™ve\ndiscussed â€“ keyframes, reference images, style control, camera movement, looping, region modifications â€“ within one app.\nThis is very powerful for artists or marketing teams who want to fine-tune outputs to match brand guidelines (e.g.\nensuring a brand mascot stays on-model, or a scene has the exact color palette desired). Moreover,â€¯Ray 2â€¯being a\nâ€œlarge-scale video modelâ€ indicates it has been trained extensively, likely yielding veryâ€¯high success ratesâ€¯of usable\ngenerations (Luma claims Ray2 outputs are â€œsubstantially more production-readyâ€ than prior gen video AI). Dream\nMachineâ€™s ability to generate both images and videos means users can prototype a concept with still images (faster) and\nthen seamlessly switch to video mode to animate it â€“ this interoperability can save time and ensure consistency between\ncampaign imagery and videos.\nWeaknesses:\nLuma Dream Machine might be slightly under the radar in enterprise circles compared to OpenAI,\nMicrosoft, Google or Adobe. Itâ€™s a newer entrant and was initially iOS-only (though now on web too). Being an app that\ntargets creatives, it might not yet have the integrations into enterprise tech stacks or the compliance features\nbusinesses need (no mention of watermarking or governance, for instance). Also, itsâ€¯output lengthâ€¯is not explicitly\nstated â€“ likely similar short durations, possibly extendable with looping or chaining. If someone wants a 30-second\npolished piece, they may have to puzzle-piece multiple generations. Additionally, while having lots of control is great\nfor power users, it could overwhelm others; Luma tries to mitigate this with the Brainstorm auto-suggestions, but using\nall features effectively might require training/experience.\nFor Lilly, Luma could be a fantastic prototyping tool â€“ e.g. you could create a complex scene of a molecular world or\npatient journey, iterating quickly with text prompts and tweaks until itâ€™s right, then generate final video frames. Its\nstrength inâ€¯â€œexceptional adherence to detailed instructionâ€â€¯means if you have a very specific storyboard or shot list,\nDream Machine might follow it closely. Itâ€™s also worth noting Lumaâ€™s heritage in 3D: they might eventually integrate\ntrue 3D scene generation or AR content, which could be useful for pharma (think interactive 3D MOA visuals). At present\nthough, Dream Machine is a strong sign thatâ€¯user experience and granular control are becoming differentiatorsâ€¯in AI\nvideo platforms.\nStability AI â€“â€¯Stable Video Diffusion\nâ€‹\nStability AI, known for open-source image generator Stable Diffusion, has also ventured into video. In late 2023, they\nreleasedâ€¯Stable Video Diffusion (SVD)â€¯in research preview. This model is based on Stable Diffusionâ€™s image generation\nbut extended to produce short videos. True to Stabilityâ€™s mission, the model was open-sourced (code on GitHub and\nweights on HuggingFace) for researchers and developers. This makes Stable Video Diffusion an important project for those\nwho wantâ€¯custom or self-hosted AI video solutions.\nThe initial release of Stable Video Diffusion can generate very short clips â€“ specificallyâ€¯14 or 25 framesâ€¯of video. At\ncommon frame rates, thatâ€™s roughly 0.5 to 1 second of footage, so extremely brief, although one can run it sequentially\nto make a few seconds. It allows setting aâ€¯frame rate between 3 and 30 fpsâ€¯for those frames. The focus was on proving\nthe concept and providing a foundation to build on, rather than competing on length/quality with commercial models.\nStability mentioned the model could handle different frame rates and aspect ratios in principle, and that with\nfinetuning it could do tasks likeâ€¯multi-view (novel view) synthesisâ€¯â€“ e.g. generating different angles of a scene from\none input image. They positioned it as a base model on top of which many specialized video models might be built,\nsimilar to how Stable Diffusion spawned many fine-tuned image models for various styles.\nStability also launched aâ€¯beta web interfaceâ€¯(as a waitlist) for text-to-video using this model, and an API on their\ndeveloper platform. By 2024â€™s end, they announcedâ€¯Stable Video 4D 2.0, which was geared towards â€œ4D generation and novel\nview synthesis from a single video inputâ€. This suggests a branch of their work focusing on turning a single video into\na 3D scene or generating new angles (useful for AR/VR or VFX where you need more camera coverage).\nStrengths:\nThe big advantage of Stable Video Diffusion isâ€¯openness and customizability. Researchers have full access\nto the model to fine-tune on their own data, which could be useful for a company wanting a model trained specifically\non, say, pharmaceutical TV commercials or lab experiment videos (ensuring output is in-domain). Also, because itâ€™s based\non Stable Diffusion, it benefits from a huge open-source community. Already enthusiasts have built demos like\nimage-to-video â€œanimationâ€ notebooks and integrated SVD into tools (one example: a free web tool offered\nstable-video-diffusion image-to-video with trial runs). Stability claimed that at release, their modelâ€¯â€œsurpass[ed] the\nleading closed models in user preference studiesâ€â€¯â€“ if true, thatâ€™s a strong endorsement of quality, though it might\nrefer to comparisons with earlier or simpler models. Theâ€¯speedâ€¯is decent; Stability said it can create videos inâ€¯â€œ2\nminutes or lessâ€â€¯for those short clips, and being lightweight means it could run on accessible hardware eventually (they\noften optimize models for consumer GPUs). For a company with skilled ML engineers and low risk tolerance, Stable Video\nDiffusion provides a platform to customize without needing to send data to third-party APIs.\nWeaknesses:\nIn terms of raw capability, SVD lags behind the likes of Sora, Veo, or others in this list. The\nextremely short length (1 second) and likely lower resolution (their paper suggests training at smaller resolutions)\nmean itâ€™s not immediately useful for direct content creation yet â€“ more of a tech demo. It doesnâ€™t produce audio. Also,\nusing it requires ML expertise; itâ€™s not an off-the-shelf product for end-users. Stabilityâ€™s emphasis on research means\npolished features like storyboards or content moderation are left to the implementer. Indeed, one has to be careful: an\nopen model might produce disallowed content if fine-tuned incorrectly, and the onus is on the user to enforce moderation\nor filtering.\nStable Video Diffusion is relevant if you have a strategy aroundâ€¯open-source AIâ€¯and want to possiblyâ€¯build in-house\ncapability. A pharma company might, for example, fund a project to fine-tune SVD on their library of MOA animation\nframes to create a custom model that generates new scientific animations consistent with their style â€“ something you\nwouldnâ€™t want to put into a public model for IP reasons. That could be a competitive differentiator long-term. Stability\nAI also tends to improve their models iteratively, and with community contributions, so by late 2025 we might see SVD\n2.0 that can do several seconds at higher quality. Itâ€™s theâ€¯â€œkeep an eye on this if you want a differentiated AI video\nsolutionâ€â€¯contender.\nAdobe â€“â€¯Firefly Generative Video\nâ€‹\nAdobe is integrating generative AI across its Creative Cloud, and for video content Adobeâ€™s solution is theâ€¯Firefly\nVideo Modelâ€¯(often just calledâ€¯Generative Video in Firefly). Announced in 2024 and rolled out in beta this April (2025),\nAdobeâ€™s tool is a bit different in focus from others: itâ€™s designed toâ€¯enhance video editing workflowsâ€¯(think of it as\nan assistant to Premiere Pro users) rather than to replace the video camera entirely. Adobe is leveraging its vast\nlibrary of licensed assets (Adobe Stock) to train this model, ensuring the outputs areâ€¯commercially safe for useâ€¯(no\nunlicensed content).\nAs of launch, Fireflyâ€™s generative video supports two main generation modes:â€¯Text-to-Video and Image-to-Video. Users can\nenter a descriptive prompt to get aâ€¯5-second, 1080pâ€¯video clip. Crucially, Adobeâ€™s tool allows uploading aâ€¯start frame\nand end frameâ€¯to guide the motion. This is essentially keyframe control: for example, you could upload a still image of\na scene as the start and another image for the end, and the AI will animate a transition between them. Firefly also\nprovidesâ€¯drop-down menus for camera shot size and angle, as well as presets for camera motionâ€¯(e.g. pan, zoom, tilt) to\nfurther direct the result. This GUI approach aligns with Adobeâ€™s pro users who may be more comfortable picking settings\nthan writing long textual prompts for everything.\nIn addition to generation, Adobe introducedâ€¯Translate Video and Translate Audioâ€¯features with the Firefly launch. These\nuse an AI voice model toâ€¯dub existing videosâ€¯into different languages (with some lip-sync capability for enterprise\nusers). While not generation from scratch, itâ€™s a complementary AI feature that can be big for training or marketing â€“\ne.g. easily turn an English video into French, Spanish, etc., with the voice and lips aligned. It shows Adobeâ€™s holistic\napproach: not just make new content, but modify and repurpose content efficiently.\nAdobeâ€™s Firefly video is integrated in the cloud (firefly.adobe.com) as a beta, with plans to bring it into Premiere Pro\nlater. It emphasizes use cases likeâ€¯filling gaps in footage, generatingâ€¯b-roll of scenery, addingâ€¯atmospheric\nelementsâ€¯to existing shots (like generate an overlay of smoke or dust on a green screen). Indeed, the FAQ suggests top\nuse cases as adding elements, animating static images (e.g. making a waterfall flow), or creating stylized graphics and\ntext animations.\nStrengths:\nAdobeâ€™s key advantage isâ€¯seamless integration for creatives. Editors can incorporate generative clips\nright inside their familiar tools (eventually in Premiere/AfterEffects), which lowers adoption friction. Also,\ntheâ€¯ethics and legal safetyâ€¯â€“ every Firefly output is trained on licensed content and can be used commercially with\nconfidence. This is a huge factor for enterprises worried about copyright (some companies avoid using other AI image\ngenerators due to unclear training data provenance; Adobe solves that). The resolution (1080p) and aspect ratio options\n(16:9 and 9:16 supported out of the gate) meet professional standards. Fireflyâ€™sâ€¯camera and keyframe controlsâ€¯mean users\ncan get precisely the shot they envision â€“ rather than hoping a pure text prompt yields the right camera angle, you can\nexplicitly say you want a â€œwide shot, overhead angle, slow dolly movement,â€ etc. This dramatically improves reliability\nfor production use, because it reduces ambiguity for the AI. Additionally, Adobeâ€™s inclusion ofâ€¯translation and voice\nAIâ€¯in the suite is a bonus for people who want an all-in-one solution for localizing and creating content.\nWeaknesses:\nFireflyâ€™s generative video is currently limited toâ€¯5 secondsâ€¯max for generation, which is shorter than\nsome competitors like Sora (20s) or Kling (minutes). It is clearly meant for quick clips and fillers, not full scenes.\nIf a marketing team wants to create a 30-second ad purely from AI, theyâ€™d need to string together many Firefly\ngenerations and likely do some manual stitching. Another limitation is that as of now itâ€™s a separate web beta, not\nfully in Premiere â€“ meaning an extra step to use (though this will likely change). The content domain might also be\nsomewhat constrained: given Adobeâ€™s professional focus, the model might excel at certain types of shots (e.g. landscape\nb-roll, simple subject animations) but perhaps not be as wildly imaginative or diverse in style as some others. Early\nbeta users will certainly find edges to what it can do. Also, because Adobe prioritizes safe training data, the training\nset might exclude a lot of web videos â€“ itâ€™s likely biased towards stock footage and professionally created content.\nThis means it might not have learned some of the â€œinternetâ€™s imaginationâ€ that open models did; whether thatâ€™s a\ndownside (less breadth) or upside (more reliability) depends on use case.\nFor Lilly, Adobeâ€™s generative video could be immediately useful inâ€¯post-production enhancement. Imagine you have a live\naction commercial but need to add an effect â€“ say a glowing aura around a patient to symbolize improvement â€“ Firefly\ncould potentially generate that element on a transparent background to overlay. Or if youâ€™re storyboarding a concept,\nyou can quickly generate video mockups of scenes to show stakeholders, all within the Adobe ecosystem. The translation\nfeature is directly relevant for global pharma marketing: you could take a video with an English voiceover and get a\nmultilingual version in minutes, which is often needed for global training or promotion (with the caveat that lip-sync\nfor different languages is still a work in progress). Adobeâ€™s move also signals thatâ€¯AI video will be a standard tool in\ncreative departments, not a niche experiment.Shape\nIndustry Impact and Use Cases\nâ€‹\nThe advent of AI-generated video impacts multiple sectors. Here we focus onâ€¯marketing, learning & development\n(L&D)/training, and specifically the pharmaceutical industryâ€¯context, while noting broader implications:\nMarketing and Advertising\nâ€‹\nContent Volume & Personalization\nâ€‹\nMarketing teams are under pressure to produce more content than ever, tailored to different audiences and channels. AI\nvideo generation can produceâ€¯quick video variantsâ€¯at scale. For example, a global brand campaign normally requires\nshooting different versions of an ad for each region â€“ with generative AI, one could create localized versions by\nchanging the background, actorsâ€™ apparent ethnicity, language of on-screen text, etc., without a reshoot. AI video could\ngenerate the same scene in different hospital settings or swap the text on a prescription bottle label to a different\nlanguage, saving huge production costs. Personalized video ads (addressable TV or online ads) could also be\nauto-generated: an AI might create thousands of slight variations of a medical device ad, each tuned to a particular\ndemographic or even individual (with appropriate compliance checks).\nCreative Brainstorming & Prototyping\nâ€‹\nAI video is a boon to the creative process. Agencies should be saving time and passing cost savings to their customers\nwith tools like Sora or Lumaâ€™s Dream Machine toâ€¯storyboard concepts in motion, not just static frames. This makes it\neasier for non-technical stakeholders to grasp a concept. Pharma marketing often involves explaining abstract concepts\n(like how a drug works in the body). Instead of expensive 3D animations made after weeks of work, a team could prototype\nan MOA animation with AI in a day to visualize the idea, then refine it. It accelerates the iteration cycle on creative\nideas. Several companies already use DALL-E or Midjourney for concept art â€“ extending that to video is the next logical\nstep. Business leaders may appreciate seeing aâ€¯mock commercialâ€¯orâ€¯animated conceptâ€¯early on, which guides\ndecision-making on whether to invest in a full production or adjust messaging.\nCost and Time Savings\nâ€‹\nParticularly forâ€¯B-roll and filler content, AI can reduce or eliminate the need for stock footage or shoots. Need a\n5-second establishing shot of a laboratory at sunrise? Instead of searching stock libraries or setting up a shoot, an AI\ncould generate one that fits the precise vision. This is what Adobe is targeting â€“ filling timeline gaps. Marketing\nvideos often have many such moments. Over time, as reliability improves, AI might handle even primary footage for\ncertain types of ads (especially animated or stylized ones). Lower production cost means marketers can do moreâ€¯A/B\ntestingâ€¯of video ads by trying multiple versions. Lilly could use AI to make rapid tweaks post-approval (e.g. if a claim\nneeds a different visual emphasis, an AI could alter the scene accordingly without reshooting, if it doesnâ€™t change the\napproved message).\nChallenges for Marketing\nâ€‹\nCreatives and brand teams must ensureâ€¯brand consistencyâ€¯â€“ ironically a potential issue with AIâ€™s variability. We likely\nneed to lock down style guides and possibly train custom models on brand assets to keep outputs on-brand. Thereâ€™s also\nreputational risk: poorly made AI videos could reflect badly if they slip through (imagine awkward visuals or errors â€“\nit could appear tone-deaf or unprofessional). So likely, in near term, AI video will be used for internal drafts, social\nmedia (where lower fidelity is more acceptable), or supplemental content, while critical campaigns still use high-end\nproduction with AI augmentation. Over time, as quality stabilizes, this will shift.\nLearning & Development / Training\nâ€‹\nScenario Simulations:â€¯In corporate or medical training, video role-plays and scenarios are common (e.g. demonstrating\ndoctor-patient interactions, or proper use of a device). AI video can produceâ€¯custom training scenarios on demand. For\nexample, a pharma sales training could generate a video of a physician consultation scenario tailored to a specific\nspecialty or objection handling â€“ without hiring actors. Tools like Synthesia already generate talking avatar videos\nfrom text (not quite these full scene generators, but adjacent tech). With text-to-video, one could specify the scenario\nand have the AI create an original scene. Character consistency becomes key here; a trainer might want the same\nAI-generated actor to appear across multiple modules â€“ technologies like DeepMotion or others could even give that AI\nactor movement. Googleâ€™s Flow enablingâ€¯Jump to new shot while preserving characterâ€¯is highly relevant. That means you\ncould maintain the same virtual â€œpersonâ€ in different situations.\nLocalized and Updated Content:â€¯L&D often needs to deliver content in multiple languages and keep it updated regularly.\nUsing generative video plus integrated voice translation (as Adobe Firefly offers), a training department can take an\nEnglish training video andâ€¯automatically generateâ€¯the French, Spanish, Chinese versions with dubbed voices and adjusted\nvisuals if needed. The AI can even lip-sync the translated speech to the video (Adobe is working on that for\nenterprise). Additionally, if a procedure changes or guidelines update, new training videos can be spun up quickly by\nediting the prompt or script, rather than reshooting footage. This is particularly useful in pharma where information\nchanges (new study data, new regulations requiring a tweak in message, etc.).\nMicro-learning and Personalized Learning:â€¯With AI, itâ€™s plausible to generateâ€¯personalized training videosâ€¯for employees\nor customers. Imagine an onboarding video where the scenarios depicted are customized to that employeeâ€™s role or common\nknowledge gaps. Or patient education videos that adjust explanations to the patientâ€™s literacy level or cultural context\nâ€“ AI could alter the scenery or analogies in the video accordingly. Such on-demand generation was impossible at scale\nwith traditional methods. Pharma companies doing patient support programs could benefit by tailoring educational content\n(for example, showing a patient of the same demographic in the video for relatability, which AI can swap out easily).\nChallenges in Training:â€¯Ensuringâ€¯accuracyâ€¯is paramount â€“ any AI hallucination or incorrect depiction in a training video\ncould misinform, which in pharma could have serious consequences. Thus, for factual or procedural content, AI outputs\nmust be carefully reviewed, or possibly a hybrid approach used (AI generates visuals but humans script it tightly and\nverify every frame). Thereâ€™s also an emotional quality aspect: human-made training videos use real actors to connect\nemotionally. AI avatars are improving, but thereâ€™s still an â€œuncanny valleyâ€ risk if not done well. Over time, as\ncharacter generation improves, this will lessen. Another challenge is platform adoption: training departments will need\nnew workflows to incorporate AI generation tools, and staff will require some upskilling (e.g. learning prompt-writing\nand basic editing of AI outputs).\nCommercial and Promotional\nâ€‹\nThe pharma industry, being highly regulated, will approach AI video with both excitement and caution. Hereâ€™s how it\nspecifically stands to gain or face challenges:\nMechanism of Action (MOA) and Scientific Visualization\nâ€‹\nPharma marketing relies on complex animations to show how a drug works in the body. These are expensive 3D animations\ntoday. In the future, a text prompt like â€œShow T-cells attacking a cancer cell, zoomed in at cellular level, cinematic\nlightingâ€ could produce a decent MOA animation clip. Already, Soraâ€™s examples (like wooly mammoths in a field, or waves\ncrashing on cliffs) indicate the ability to create nature and complex textures â€“ extending to microscopic imagery is a\nquestion of training data. If we can fine-tune models on biomedical imagery, we could get AI that generates\nscientifically accurate animations quickly. This would let marketing and medical teams experiment with different visual\nmetaphors for MOA and pick the most effective ones. It could also be used inâ€¯medical educationâ€¯â€“ helping doctors\nvisualize mechanisms or trial results via generated visuals.\nHCP Marketing and Peer Influence\nâ€‹\nPharma often involves key opinion leaders (KOLs) giving talks or explaining data. In the near future, one can imagine\ngeneratingâ€¯avatar videos of KOLsâ€¯presenting data, in multiple languages. While using a real personâ€™s likeness has\nethical/IP issues (would need permission and careful use), generative tech could create a convincingâ€¯virtual\nspokesperson. Alternatively, completely fictional yet authoritative-looking avatars might be used to relay information.\nThis could help scale expert content delivery (though likely for internal training or markets where the actual speaker\ncannot be present â€“ regulations on promotional use of an AI â€œdoctorâ€ would be tricky).\nPatient Engagement\nâ€‹\nUsing AI, we could create more engaging patient resources â€“ e.g. an AI-generatedâ€¯explainer cartoon for kidsâ€¯about how to\nuse an inhaler, or a reassuring scenario video for patients starting a new therapy showing what to expect. Emotional\nexpressiveness is something some models like Sora and Veo are working on. If an AI can portray empathy or excitement\nthrough a characterâ€™s face and voice, patient communication could benefit. However, Lilly should remember to be\ntransparent if an avatar or voice bot is being leveraged to maintain trust.\nCompliance and Moderation\nâ€‹\nEvery promo or medical video in pharma must go through approval for claims, safety info, etc. AI introduces a new\nvariable â€“ the visuals might inadvertently introduce something non-compliant (e.g. showing a use of a drug thatâ€™s\noff-label). Content moderation tools will need to extend to video. We may need to adopt a practice ofâ€¯frame-by-frame\nreviewâ€¯of AI outputs just like they review every word of a brochure. This slows things, but some AI tools might allow\nlocking certain parameters to ensure compliance (for instance, an AI could be instructed not to show the pill being\ntaken with any other medication or not to depict certain patient populations if not approved). As OpenAI noted with\nSora, their deployed model is limited in what it can show realistically (some complex or sensitive scenarios are\nfiltered). Lilly will need even tighter guardrails when using AI video generation.\nSummary\nâ€‹\nIf harnessed well, Lilly could significantlyâ€¯increase engagementâ€¯with both physicians and patients. Video content that\nwas once too costly to produce for smaller audiences (like a rare disease community) could be made cost-effectively with\nAI, making communications more visual and relatable. It also opens up possibility forâ€¯interactive contentâ€¯â€“ e.g. a\nvoicebot experience where a patientâ€™s questions trigger dynamically generated video answers (a far future concept, but\nnot impossible as tech converges). Another area isâ€¯internal communications: using AI to generate internal announcement\nvideos or CEO messages (with consent of course) to add a personal touch without pulling executives into studio every\ntime.\nThe biggest risks are legal (IP considerations), regulatory, and inaction. Pharma is already under scrutiny for how it\nmarkets â€“ if it came out that an AI generated video inadvertently created a misleading impression (even subtly, via\nimagery), or leveraged an a tool associated with copyright infringement, it could result in legal penalties or public\ntrust issues.\nStrategies for Adoption\nâ€‹\nFor anyone planning to leverage AI video generation, here are strategic considerations and steps:\nWatch and Experiment (Now):â€¯Itâ€™s important toâ€¯stay informedâ€¯on this fast-moving field (as this report has shown,\nupdates in 2024-2025 were frequent). Assign a team or innovation leader to experiment with various tools â€“ e.g. a\ncreative or technology team who can consistently assess models and output across a wide range of features and\ncapabilities â€“ leveraging the same prompts or end goal in mind. Encourage sharing of results and set up brainstorming\nsessions on â€œWhat could we do with this in our business?â€\nDetermine High-Impact Use Cases (Now):â€¯Not every video needs to be AI-generated. Look forâ€¯pain points or\nopportunitiesâ€¯in your current content pipeline: Is it hard to get budget for certain types of videos (maybe training\nscenarios or international market content)? Those are prime targets to try AI. For Lilly, maybe start with non-public\ncontent (internal training, mechanism explainer for reps) to avoid regulatory risk while the tech is still new.\nAnother ideal use case is social media: social teams always need fresh short videos â€“ AI can help create those\nquickly and the lower stakes environment (a playful Twitter video) can tolerate slightly less polish.\nDraft Policy and Guidelines (Now):â€¯Just as there are guidelines for using stock images or social media conduct,\ncreateâ€¯guidelines for AI-generated content. This includes: what types of content are allowed (e.g. perhaps no AI\ngeneration of real person likenesses without permission to avoid deepfake issues, or avoid certain sensitive health\nscenarios to be safe); requirement that every AI video is reviewed by a subject matter expert for accuracy; and\nguidelines for disclosure (if needed). For external content, considerâ€¯transparencyâ€¯â€“ for example, some companies\nmight add a line in the video description that it contains AI-generated imagery, especially if regulations or brand\ntrust demands it. Given OpenAIâ€™s Sora tags metadata, if an image verification tool sees itâ€™s AI, we donâ€™t want the\ncompany to look like it was hiding that fact.\nGive creatives opportunities to Upskill:â€¯AI tools require new skills â€“ prompt writing, iterative refinement, and\nbasic video editing to polish AI outputs. Invest in developing and onboarding tools so teams can begin learning how\nto effectively harness this technology.\nAlpha Projects:â€¯Choose a few of low-risk Alpha products to pursue. For example,â€¯create an internal training video\nseries using AI avatars, orâ€¯generate supplemental social media content for a product campaign. Monitor how long it\ntakes, the quality feedback from viewers, and iterate. Use these pilots to build a case study: did it save money? Did\nit engage better (or worse)? What were the unforeseen hiccups? This builds institutional knowledge and also helps\njustify further investment to higher management.\nEthics and Authenticity:â€¯Review/revise our policy and approach toâ€¯ethical AI use (if needed). For pharma especially,\npatient trust is key â€“ avoid anything that could be seen as deceptive. If you use an AI avatar of a â€œdoctorâ€ in a\npatient video, maybe have a disclaimer like â€œVisuals are computer generated for illustrationâ€ if required. Also avoid\nsensitive contexts â€“ e.g. generating a video of a real patient story would be unethical unless clearly fictionalized.\nKeep humans in the loop for empathetic or sensitive communications (AI can generate the draft, but let a human review\nthe tone).\nMonitor Regulatory and Legal Developments:â€¯Regulations around AI-generated content (so-called deepfake laws or\nrequired disclosures) are evolving globally. The EU, China, some US states have begun requiring disclosures for\ncertain AI-generated media. Ensure your compliance team keeps abreast of these so your use of AI video doesnâ€™t run\nafoul of any new rule. For instance, if a law says â€œAI-generated realistic videos of humans must be labeledâ€, youâ€™d\nincorporate that into your practice. Also, look to ongoing legal conflicts related to AI image generation to\nanticipate future issues with video generation.\nConsider a Long-term Strategy:â€¯Think how AI video fits into our digital transformation. This will not replace\neverything but could augment many processes. Consider organizing aâ€¯center of excellenceâ€¯for generative media, pooling\nexpertise from IT, creative, legal. Long-term, also consider investing in custom model development for commercial\nneeds â€“ e.g. partner with a vendor to fine-tune a model on Lilly video content (especially relevant for pharma with\nlots of scientific visuals). Owning or heavily customizing a model could give competitive edge, if it could generate\ncontent that is unique to Lilly IP and brand style, which others using generic models cannot.\nChallenges and Risks\nâ€‹\nWeâ€™ve touched on several challenges in context, but letâ€™s consolidate the majorâ€¯risks and challengesâ€¯of AI video\ngeneration adoption:\nQuality and Consistency\nâ€‹\nWhile rapidly improving, AI-generated video can sometimes produceâ€¯glitchesâ€¯â€“ a personâ€™s face warping for a frame, odd\nbackground artifacts, inconsistent lighting continuity, etc. In a professional setting, these can be jarring. Ensuring\nevery frame is clean might require frame-by-frame editing or re-generation cycles. Thereâ€™s also the continuity over time\nissue â€“ making sure the AI doesnâ€™t â€œforgetâ€ what it was showing. Until models are truly robust, teams must budget time\nfor QC and maybe manually fixing AI output (via editing or inpainting tools).\nMisrepresentation and Deepfakes\nâ€‹\nAI can createâ€¯very realistic fake people or events. In pharma, imagine an AI video of a â€œpatientâ€ giving a testimonial â€“\nif itâ€™s not clearly fictional, thatâ€™s ethically problematic (real patient testimonials require consent and actual\npatient stories). Deepfakes of public figures endorsing a drug (even if done jokingly) would be bad. Lilly should set\nstrict lines: e.g. never impersonate real individuals or fabricate quotes without prior consent and compensation. The\nthreat of malicious deepfakes (outside actors using AI to create fake news about a company or fake exec statements) is\nalso real â€“ another reason transparency and detection are important. On the flip side, we do not want Lilly to\naccidentally beâ€¯accusedâ€¯of using a deepfake without disclosure.\nRegulatory Compliance\nâ€‹\nFor pharma, any promotional content must comply with FDA (or EMA, etc.) regulations. AI doesnâ€™t change the requirement,\nbut it adds complexity to verification. If an AI video shows a patient using a drug, the visual could inadvertently\nbecome aâ€¯â€œclaimâ€. For example, if it shows the patient doing something that implies a benefit not on label, thatâ€™s a\ncompliance issue. Or if the AI inadvertently generates a medically inaccurate portrayal (like pills of wrong color or a\ndevice used incorrectly), thatâ€™s problematic. Thus, a deep integration of regulatory review is still needed â€“ possibly\neven training the AI on whatâ€¯notâ€¯to show. Regulators themselves may start scrutinizing AI-generated content differently;\ncompanies might need to provide more substantiation that the content is accurate and not misleading. There might even be\nfuture guidance specific to AI in pharma advertising.\nIntellectual Property and Training Data\nâ€‹\nA lingering question: if an AI video model was trained on, say, thousands of movie clips or YouTube videos without\npermission, is its output legally safe to use? The model developers claim yes, itâ€™s transformative. But cases are in\ncourts (for images, Getty vs. Stability for e.g.). Using output commercially could pose IP risks if the output\nunintentionally replicates a copyrighted scene or character. While Adobe avoids this by training only on licensed data,\nothers might not. So one risk is the potential for a third-party claim: e.g. a background in an AI video looks too\nsimilar to a scene from a known film. The best mitigation is using providers who have clear training sources or\nindemnification. Or limiting use of AI for things where this risk is minimal (unique scenarios rather than known\ncharacters).\nEthical Considerations\nâ€‹\nBeyond legal compliance, ethical use concerns include: not reinforcing biases (if models were trained on biased content,\nthey might output stereotypes â€“ careful prompting or model choice is needed to avoid, say, always depicting doctors as\none gender or certain roles in subservient ways, etc., which could slip into outputs unconsciously). Also, thereâ€™s\naâ€¯human elementâ€¯â€“ if marketing and comms become too AI-heavy, does it lose authenticity? We will have to find the right\nbalance between automation and human touch. And internally, treating employees openly about use of AI, offering\nre-skilling, etc., is important to maintain morale.\nTechnical Dependency and Evolution\nâ€‹\nIf a business builds processes around a third-party AI tool, they have to manage dependency risk. What if that service\nchanges pricing significantly, or a policy shift (e.g. an AI model decides to disallow certain medical content\ngeneration)? Executing a strategy that involves multiple options or maintaining the ability to switch models quickly is\nhighly advised. Also, file formats and integration: ensuring these AI outputs can integrate into existing video editing\nworkflows smoothly (most give MP4 outputs which is fine, but if you want something like masking info for overlays, not\nall provide that yet â€“ Adobe likely will for theirs).\nSecurity and Privacy\nâ€‹\nPrompts and any input data sent to AI cloud services could contain sensitive info (especially if youâ€™re generating\nsomething based on confidential product data or an unreleased device design). Thereâ€™s the usual cloud risk â€“ ensure the\nvendor has enterprise agreements if needed. Alternatively, do sensitive work on-prem with open models and sticking to\ngreen data for experimentation.\nPublic Perception\nâ€‹\nAs AI-generated content becomes more common, there may be public skepticism (â€œIs this video real or AI?â€). If misused by\nbad actors, there could be a consumer backlash or calls for regulation. If we begin using it in consumer-facing ways\nshould be prepared to address questions about authenticity. Being on the honest, transparent side will guard brand\nreputation. For instance, if an AI is used to simulate a patient story, one might label it as a â€œdramatizationâ€ which is\na term already used in ads for reenactments. Itâ€™s about not breaking trust.\nIn summary, the opportunities and challenges are significant. They manageable with a proactive approach. The key is\ntoâ€¯not treat AI video as magic; itâ€™s a powerful new tool that augments human creativity but still needs human oversight\nand strategic implementation.\nFuture Outlook\nâ€‹\nThe trajectory of AI video generation suggests that what is cutting-edge today could become commonplace and vastly more\ncapable in the next 2â€“3 years. Lilly should anticipate and prepare for the following likely developments:\nLonger & Real-Time Generation\nâ€‹\nModels will continue scaling (OpenAI hinted that bigger models = longer videos). We can expect by mid-2026 some AI\nsystems will routinely generate a few minutes of video with coherent storylines. There is even the possibility\nofâ€¯real-timeâ€¯or streaming generation â€“ imagine AI creating video on the fly as you watch (some research is headed there\nfor live avatar conversations, etc.). This could revolutionize live customer service (AI video agent responding in real\ntime) or live training sessions with dynamic visuals. While true real-time high-res generation might be a bit farther,\nreal-time at lower res may come sooner.\nImproved Fidelity\nâ€‹\nAlready Veo 3â€™s people are highly realistic; with more specialized training (and perhaps hybrid approaches like grafting\nAI-generated elements into real video backgrounds), AI videos will reach the point where an average viewer canâ€™t tell AI\nvs real footage, at least for short sequences. This will blur the line of what content is â€œshotâ€ vs â€œgeneratedâ€. For\ncreators, that means tremendous creative freedom (any idea can be visualized without practical constraints) but also\ngreater need forâ€¯ethical guidelinesâ€¯to maintain trust.\nInteractive Video & Multimodality\nâ€‹\nCombining video generation with interactivity (powered by LLMs like GPT-4/5) could yield interactive media. For example,\na user could talk to an AI character who responds with generated video and audio. This could be a new form of engaging\ncontent or education tool. In pharma, a patient could ask an AI â€œWhat will the surgery be like?â€ and get a custom visual\nexplanation video. Weâ€™re moving toward AI that canâ€¯understand scene contextâ€¯deeply (Gemini is multimodal â€“ it might\nalign narrative with visuals tightly). The merging of text, images, audio, and video AI means future tools will handle\nwhole multimedia generation. Googleâ€™s Flow already hints at that synergy (Gemini assisting with consistency across\nshots).\nIndustry-Specific Marketing Models/Tools\nâ€‹\nWe will likely see models fine-tuned for specific domains: e.g. aâ€¯Medical VideoGenâ€¯that knows how to accurately depict\nanatomy, or anâ€¯Architectural VideoGenâ€¯for real estate walkthroughs. These specialized models could be offered by\nvertical SaaS companies or open communities. For pharma, a model trained on, say, all publicly available MOA animations\nand medical footage could become the go-to for generating regulatory-compliant medical visuals (with knowledge of what\nnot to show because of how the body actually works).\nRegulatory Frameworks\nâ€‹\nGovernments and industry bodies will establish more formal guidelines. We might see something like an â€œAI Content\nCertificationâ€ where companies can submit AI-generated ads for a special review or to get a certification mark that itâ€™s\nbeen vetted for accuracy. Thereâ€™s also likely to be technology solutions for provenance: cryptographic signing of AI\ncontent so that it can be traced. OpenAI and Adobe already tag content; this could become standardized so that any\nscreen can potentially alert viewers â€œThis video is AI-generated.â€ Lilly should anticipate that transparency may become\nnot just best practice but mandated.\nTalent and Workflow Shifts\nâ€‹\nRoles will shift â€“ we might seeâ€¯â€œAI video prompt scriptwritersâ€,â€¯â€œAI ethics reviewersâ€, andâ€¯â€œcontent curatorsâ€â€¯as common\njobs. Teams might reorganize such that an AI tool is part of every content meeting (e.g. brainstorm sessions always\ninvolve someone quickly prototyping ideas with AI to show the room). The speed of execution will increase, meaning\ncompetition in marketing might revolve around who can ideate and deploy creative concepts fastest (with AI doing the\nheavy lifting). The flip side is potential content glut â€“ so focusing on strategy and creativity (the human part)\nremains vital to stand out.\nCompetitive Landscape\nâ€‹\nItâ€™s possible that only a few foundation models end up dominating (like we see in LLMs, a handful of leaders). But many\ncompanies might build on those via fine-tuning. We may see consolidation where big players acquire some startups\n(perhaps OpenAI buys a Pika, or Adobe buys an Runway, purely speculative). Or big tech might all have their offerings\nand split the market (like cloud providers). Keeping flexibility is wise â€“letâ€™s not get too locked into since another\nwill likely surpass it. It could be analogous to the early days of web browsers or mobile OS â€“ eventually a stable set\nof widely used platforms emerged. Likely a mix of open (Stability) and closed (OpenAI, Google) will persist, each with\npros and cons.\nNew Content Formats\nâ€‹\nAs AI lowers production costs, we might see entirely new forms of media. For instance, personalized short films as a\nservice (you input some info and get a short film about you). Or dynamic video content on websites that adjusts to\nviewer profile. In pharma, maybe dynamic visual aids for reps that change based on the doctor theyâ€™re speaking to (if\nthe doctor is more visual, the AI generates more mechanism animation; if they care about data, it shows charts, etc.,\nall in real time). The boundaries between video, animation, and software could blur.\nConclusion\nâ€‹\nIn conclusion, AI video generation is poised to become a standard tool in the content creation toolbox â€“ much like\ndesktop publishing in the 80s or digital video editing in the 2000s. It will not outright replace humans, but those\nwhoâ€¯harness it effectivelyâ€¯will outpace those who donâ€™t, by producing more tailored and creative content with less\neffort. For the pharmaceutical industry and similar fields, the key will be to integrate these capabilities in a way\nthatâ€¯enhances communication and understandingâ€¯(of complex science, of patient stories) whileâ€¯maintaining the rigorous\nstandardsâ€¯of accuracy and ethics that the industry demands.\nLilly should take a proactive but careful approach: embrace the innovation, experiment with it, and begin with low-risk\nuse cases with some oversight eyeing the entire spectrum of use cases. By doing so, we can significantly boost its\ncontent innovation capacity and be ready for the future where AI-driven media is ubiquitous. The companies that succeed\nwill likely be those that combineâ€¯the creative imagination of their teamsâ€¯withâ€¯the generative power of AI, in a governed\nand purposeful manner â€“ turning what used to be costly visual dreams into vivid (and responsible) reality.\nSources\nâ€‹\nVideo generation models as world simulators | OpenAI\nhttps://openai.com/index/video-generation-models-as-world-simulators/\nï¿¼\nSora is here | OpenAI]\nhttps://openai.com/index/sora-is-here/\nï¿¼\nRunway's Gen-2 shows the limitations of today's text-to-video tech | TechCrunch\nhttps://techcrunch.com/2023/06/09/runways-gen-2-shows-the-limitations-of-todays-text-to-video-tech/\nBest Short-Form AI Video Generator? Kling 2.1 vs Google Veo 3 - Decrypt\nhttps://decrypt.co/323056/best-short-form-ai-video-generator-kling-google-veo\nGenerative Video. Now in Adobe Firefly. : r/premiere\nhttps://www.reddit.com/r/premiere/comments/1inrroo/generative_video_now_in_adobe_firefly/\nFlow\nhttps://labs.google/fx/tools/flow/faq\nVideo generation models as world simulators | OpenAI\nhttps://openai.com/index/video-generation-models-as-world-simulators/\nRunway's Gen-2 Text-to-Video Tool Now Available to Everyone for Free | Tom's Hardware\nhttps://www.tomshardware.com/news/runway-gen-2-text-to-video-available-to-all\nRunway's Gen-2 shows the limitations of today's text-to-video tech | TechCrunch\nhttps://techcrunch.com/2023/06/09/runways-gen-2-shows-the-limitations-of-todays-text-to-video-tech/\nI Tried Minimax AI Video Generator to Generate These Insane Videos: Here is My Honest Review | by Amdad H | Towards AGI\n| Medium\nhttps://medium.com/towards-agi/i-tried-minimax-ai-video-generator-to-generate-these-insane-videos-here-is-my-honest-review-832f5a6e8b7c\nRunway's Gen-2 shows the limitations of today's text-to-video tech | TechCrunch\nhttps://techcrunch.com/2023/06/09/runways-gen-2-shows-the-limitations-of-todays-text-to-video-tech/\nLuma Dream Machine: New Freedoms of Imagination | Luma AI\nhttps://lumalabs.ai/dream-machine\nGenerative Video. Now in Adobe Firefly. : r/premiere\nhttps://www.reddit.com/r/premiere/comments/1inrroo/generative_video_now_in_adobe_firefly/\nBringing generative AI to video with Adobe Firefly Video Model | Adobe Blog\nhttps://blog.adobe.com/en/publish/2024/09/11/bringing-gen-ai-to-video-adobe-firefly-video-model-coming-soon\nVideo generation models as world simulators | OpenAI\nhttps://openai.com/index/video-generation-models-as-world-simulators/\nRunway's Gen-2 shows the limitations of today's text-to-video tech | TechCrunch\nhttps://techcrunch.com/2023/06/09/runways-gen-2-shows-the-limitations-of-todays-text-to-video-tech/\nRunway's Gen-2 shows the limitations of today's text-to-video tech | TechCrunch\nhttps://techcrunch.com/2023/06/09/runways-gen-2-shows-the-limitations-of-todays-text-to-video-tech/\nGoogle launches Veo 3, an AI video generator that incorporates audio\nhttps://www.cnbc.com/2025/05/20/google-ai-video-generator-audio-veo-3.html\nAI video just took a startling leap in realism. Are we doomed?\nhttps://arstechnica.com/ai/2025/05/ai-video-just-took-a-startling-leap-in-realism-are-we-doomed/\nVideo generation API - MiniMax\nhttps://www.minimaxi.com/en/news/video-generation-api\nGenerate videos with Minimax's Hailuo video-01 model - Replicate\nhttps://replicate.com/minimax/video-01\nKling AI Free: Try This AI Video Generator Now! | Pollo AI\nhttps://pollo.ai/m/kling-ai\nPika AI (Pika Art) free: Try Pika Labs AI Video Generator Now | Pollo AI\nhttps://pollo.ai/m/pika-ai\nProfile | China â€˜genius girlâ€™ Guo Wenjing, Harvard graduate, co-founder of tech firm backed by US$135 million funding |\nSouth China Morning Post\nhttps://www.scmp.com/news/people-culture/china-personalities/article/3268811/china-genius-girl-guo-wenjing-harvard-graduate-co-founder-tech-firm-backed-us135-million-funding\nPika AI Video Generator: Create B-Roll From Text - Captions\nhttps://www.captions.ai/features/pika-ai-video-generator\nBest AI Video Generator - Comparison â€¢ Quality & Price Comparison\nhttps://aianimation.com/best-ai-video-generation-platforms/\nPika AI (Pika Art) free: Try Pika Labs AI Video Generator Now | Pollo AI\nhttps://pollo.ai/m/pika-ai\nThe Ultimate Guide to MiniMax Hailuo AI Video Models - Getimg.ai\nhttps://getimg.ai/blog/the-ultimate-guide-to-minimax-hailuo-ai-video-models\nPika AI (Pika Art) free: Try Pika Labs AI Video Generator Now | Pollo AI\nhttps://pollo.ai/m/pika-ai\nBest AI Video Generator - Comparison â€¢ Quality & Price Comparison\nhttps://aianimation.com/best-ai-video-generation-platforms/\nStable Video Diffusion is awesome! : r/StableDiffusion - Reddit\nhttps://www.reddit.com/r/StableDiffusion/comments/183ync6/stable_video_diffusion_is_awesome/\nStable diffusion video tutorial â€” generate AI video for free - Medium\nhttps://medium.com/@codeandbird/stable-diffusion-video-tutorial-generate-ai-video-for-free-29538ca45ef5\nBringing generative AI to video with Adobe Firefly Video Model | Adobe Blog\\\nhttps://blog.adobe.com/en/publish/2024/09/11/bringing-gen-ai-to-video-adobe-firefly-video-model-coming-soon\nGenerative Video. Now in Adobe Firefly. : r/premiere\nhttps://www.reddit.com/r/premiere/comments/1inrroo/generative_video_now_in_adobe_firefly/\nRunway's Gen-2 shows the limitations of today's text-to-video tech | TechCrunch\nhttps://techcrunch.com/2023/06/09/runways-gen-2-shows-the-limitations-of-todays-text-to-video-tech/\nBringing generative AI to video with Adobe Firefly Video Model | Adobe Blog\nhttps://blog.adobe.com/en/publish/2024/09/11/bringing-gen-ai-to-video-adobe-firefly-video-model-coming-soon\nWas this helpful?\nTags:\ninnovation\nEdit this page\nPrevious\nAI Speech\nExecutive Summary\nIntroduction\nAI Video Generation Technology Overview\nMajor Players and Platforms in AI Video Generation\nOpenAI â€“â€¯Sora\nRunway â€“â€¯Gen-2â€¯(and beyond)\nGoogle â€“â€¯Veo 3â€¯andâ€¯Flow\nFlow\nKuaishou â€“â€¯Kling\nPika Labs â€“â€¯Pika Video\nMiniMax â€“â€¯Hailuo Video-01\nLuma Labs â€“â€¯Dream Machine (Ray 2)\nStability AI â€“â€¯Stable Video Diffusion\nAdobe â€“â€¯Firefly Generative Video\nIndustry Impact and Use Cases\nMarketing and Advertising\nLearning & Development / Training\nCommercial and Promotional\nStrategies for Adoption\nChallenges and Risks\nQuality and Consistency\nMisrepresentation and Deepfakes\nRegulatory Compliance\nIntellectual Property and Training Data\nEthical Considerations\nTechnical Dependency and Evolution\nSecurity and Privacy\nPublic Perception\nFuture Outlook\nLonger & Real-Time Generation\nImproved Fidelity\nInteractive Video & Multimodality\nIndustry-Specific Marketing Models/Tools\nRegulatory Frameworks\nTalent and Workflow Shifts\nCompetitive Landscape\nNew Content Formats\nConclusion\nSources\nCommunity\nEBA Viva Engage\nEBA SharePoint\nQuestions?\nReach us on Viva Engage!\nCopyright Â© 2026 Eli Lilly and Company",
  "links_found": 1,
  "depth": 2,
  "crawled_at": "2026-02-25T10:06:24.740441"
}